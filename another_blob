a ^ log(b) = b * e ^ a 

a^b = e^{b log(a)}

g^n = h^m

m = n log_{h}(g)

predetermine a sequence of generators, and pregenerate powers of 2.
then make a circuit that intakes a bitstring and the number it should represent.
apply the bitstring to the generator powers, a linear combination, and also apply the bistring to the raw powers of two and compare the result to the claimed number.

p=2
1
2,4,3,1
3,4,2,1
4,1

1,2,3,0
2,0
3,2,1,0
0

p=3
1
2,4,1
3,2,6,4,5,1
4,2,1
5,4,6,2,3,1
6,1

6 * (1 - 1/2)(1 - 1/3) = 6 * 1/2 * 2/3 = 2

2p+1 is safe prime
multiplicative group has 2p elements
since ordere of any element must divide 2p, it must divide either 2 or p, or be 2p, and since both are primes, the order must be either 2 or p, or 2p.
the order of any subgroup must divide 2p, so again, subgroups must have order either 2 or p. but there is a unique subgroup for each such order, so this group is composed of one subgroup of order 2 and one of order p, and also the 2p group (in the case p=2, we just have a single subgroup) (in the case of p=3, we have subgroups order 3 and order 2).

2p * 2p = 4 * p^2 mod 2p+1

consider the subgroup of order p. this is our group of interest. our generators will be in this group. and our exponents can be in the range [1,p], which has the same size but not the same elements as our group of interest. 

we need a bijective efficiently computable mapping between them. suppose we have one. then we'd have our field as Zp, but then our field would not support many elements of the desired group, which is twice as large.

suppose we use the whole group (Z_{2p})^* with exponents in range [1, 2p] or [0,2p-1], which in both cases include all the elements in the field Z_{2p+1} which has 2p+1 elements, except 1 which is 0 or 2p respectively. So either 0 or 2p must be forbidden as an exponent, maybe we allow them both but justify the overlap not hurting security.

i'm realizing we may be better with elliptic curves, because i only know how to do exponentiation in the multiplicative group by dissassembling into bits, but now im realizing for a single element there are thousands of bits, rather than hundreds as for an elliptic key, and each of those will need their own keys. for an elliptic curve we don't need to exponentiate, only multiply and confirm inverses. but the problem with elliptic curves is efficiently hashing from a curve point to the field, as necessary in sumcheck. now i'm wondering if it would be better to use a method like QSP that does not involve sumcheck.

let me see if I can remember. we have some witness poly that encodes all the data and we have another dividor poly that is trivially constructed and is the trivial poly with a certain set of roots, those roots for which the data poly should have. then the prover computes the quotient poly h, commits to that and the data poly and the verifier queries both at a single random point, and also manually evaluates the the dividor poly (what i'll call the zero poly) at the same point, then multiplies to see that data(r) = h(r)*zero(r). Now to remember how the data poly looks like. I think its mulivariate. suppose we want to represent an arithmetic circuit. suppose we have a vector of the trace. then our constraints are of the form that inputs and outputs match as they should. there is a constraint for each gate. suppose we have a trace poly. using gates i suppose the constraint poly could be 3-variate, and we'd require it to have a root for every gate. or since there's two gate types there could be only linear gates so 4-variate. then evalutation at random point would mean reducing evaluation from 4 points to one. now we'd like our trace polys to be multilinear. so it may make more sense to use 4*n variate polys where log2(n) is length of trace vector so n is degree of the trace poly. 
but i think the problem with multivariate is its hard to divide.
and the problem with univaraite is it doesn't reduce.
before trying to solve either of these problems, first see if we can use multivarate for trace while univariate for rest.


one thing we know is multiplying out multilinear polys creates exponential terms which is infeasible to handle. 


r^{a*2^2 + b*2^1 + c*2^0} = r^{a*4} * r^{b*2} * r^{c*1} = (r^4)^a * (r^2)^b * (r^1)^c = (r^a)^4 * (r^b)^2 * (r^c)^1

suppose we have univerate poly of degree d, that is d+1 terms which suppose is a power of 2, ie 2^v. suppose we wish to evaluate at r. suppose we replace term j from 0 to d with
\prod_{i=0 to v-1} (r^{ith bit for j})^{2^i}


simply set the ith variable from 0 to v-1 to be r^(2^i). so in order our variables are x0=r^1, x1=r^2, x2=r^4, x3=r^8, ...

a 1
b 2
c 4
ab 1 + 2 = 3
ac 1 + 4 = 5
bc 2 + 4 = 6
abc 1 + 2 + 4 = 7

so now when asked to evaluate a poly at r, we can instead evaluate a multilinear poly. 

suppose we're reducing evalutation from two points r,s to one.
xi = r^{2^i}
yi = s^{2^i}
zi = xi*t + yi*(1 - t) = r^{2^i}*t + s^{2^i}*(1 - t)


so my hope is that verification consists of evaluate a basic format of a constraint poly, which involes evaluating the trace poly together with an in-out poly, and the zero poly and the divisor poly. the trace and divisor are reduced to single points, then they can be reduced to one poly. 

but we still have the trouble of a one way function from the elements of the field to the field. if we put it through an elliptic curve, we need a uniform mapping from the curve to the field. 
https://eprint.iacr.org/2014/595.pdf uses subset sum over the additive group, which will require that our input is in bitstring format or only log(field size) length. 

H(a) + H(b) = H(a + b)

H(a + H(b + c)) + H(a' + H(b' + c')) = H(a + H(b + c) + a' + H(b' + c')) = H((a + a') + H((b + b') + (c + c')))

H(H(a) + H(b))


m log(2d+1) > m log(2d) > log(p)

m > log(p/2d)

lets have (2d + 1)^m/2 = p so m/2 log(2d + 1) = log(p) so m = 2*log(p)/log(2d+1)

17^6

so the bijective map could be take the value in range p, and find its representation in base 2d+1, and to represent the number one should need log(p)/log(2d+1) digits. compute each such digit then offset it around 0. this is a bijective mapping.
(2d+1)^{log(p)/log(2d+1)} = e^{log(p)/log(2d+1) * log(2d+1)} = p^1 = p
to verify, one is given the final reps, un-offsets them, multiplies them by radixes, and sums. 


H(a,b) + H(a',b') = \sum ai*zi + \sum bj*zj + \sum a'i*zi + \sum b'j*zj = \sum (ai + a'i)zi + \sum (bj + b'j)zj = H(a + a', b + b')

r1 = H(x1 . ^x2)
r2 = H(r1 . x2^)
s1 = H(y1 . ^y2)
s2 = H(s1 . y2^)

r2 + s2 = H(r1 . x2^) + H(s1 . y2^) = 


we have contraint poly, in and out polys, trace poly, zero poly, and divisor poly. 

(z - a)(z - a^2)

f(r) = z1
g(r) = z2

f(t) = 

p = p1, p2
p1*r + p2

use powers of a generator to make other generators for commitment. then use FFT to 

g^a h^b = g^a' h^b'

g^{a-a'} h^{b-b'} = 1

g^{a} (g^2)^{b} = g^{a'} (g^2)^{b'}
g^{a-a'} (g^2)^{b - b'} = 1
g^{a - a'} g^{2b - 2b'} = 1
g^{a - a' + 2b - 2b'} = 1
g^{}
g^a g^{2b} = g^{a + 2b}

so can't do that. generators must be chosen privately by verifier. this acutally means a setup that I wasn't aware of. 

ri*(Gi*(1 - t) + Hi*t)
r1*G1 + r2*G2
r1*H1 + r2*H2

r1*(G1*(1 - t) + H1*t) + r2*(G2*(1 - t) + H2*t)
=
t*((r1*H1 + r2*H2) - (r1*G1 + r2*G2)) + (r1*G1 + r2*G2)

cant do this either. 

have a problem. i think discrete log must be implemented in a prime size group. 

use multiplicative group with generator for whole group, but make sure blinder is trustworthy published and not a residue. 

could also do discrete log over binary field


we'd actually like our native field as small as possible for efficiency. the security without composition only relies on the poly lemma. 80 bits or so is fine. but discrete log on elliptic curve, at least the one by NIST, has field size no less than 192 bits. if 192 is not too large for the native field, we can natively support curve arithmetic. but if its signficantly more efficient to do like 80 bit native field, then we do discrete log using long arithmetic. would it really be possible to natively support curve arithmetic without doing any spreading (ie bit separation and reconstruction)? 
the size of the field for a curve should be twice the security parameter. 

but if our construction requires owfs and we use the subset sum, then our input must be deconstructed. 
do we need owfs at all? if we use sumcheck yes. if we reduce the same poly at two points to one then yes. otherwise no. oh well i guess applications will crucially rely on them with merkle trees.

but consider reducing a poly to one point without owf. the special case is we are evaluating at rg^i for i=0,1,2,etc. consider just 0 and 1 for now. we could do dot product of powers of g with the poly vector then we have transformed the problem to evaluating two polys at the same point r. notice r is particular to each proof, but g is not so the vector of g powers can be made before, maybe even speeding up proof of dot product which I don't know yet. suppose we have the vector of powers of g. then to get the vector for higher powers of g, we just add the vector to itself, or exponentiate it.
0,1,2,3,4
0,2,4,6,8
0,3,6,9,12
g^2^0,g^2^1,g^2^2,g^2^3,g^2^4
g^3^0,g^3^1,g^3^2,g^3^3,g^3^4
maybe we could reduce even further by only requiring proofs to keep track of the poly at the point r and leaving the poly at rg^i for i>0 as a derivative. but then the proof can't evalute the formula so some reduction of two formulas to one would be needed. the formulas are already viewed as polys having an unknown and this means reducing satisfiability for two unknown to one, which is the same as reducing eval at two points to one. but we may be able to take advantage of formula structure to avoid the standard procedure which requires owf. 
if we reduce two points to one it takes this form: r*t + r*g*(1-t) = r(t + g(1-t))



since dlp is secure in the whole multiplicative group we could revert to using that as the exponential and the owf. but the field size is still costly. 



0, 				 a1*x^2, b2*x^2, 0, 				<a1,b2>*x^2
a1, 			 a2, 		 b1, 		 b2, 				<a,b>
a2*x^{-2}, 0, 		 0,  		 b1*x^{-2}, <a2,b1>*x^{-2}
a1 + a2*x^{-2}, a1*x^2 + a2, b2*x^2 + b1, b2 + b1*x^{-2}, <a1,b2>*x^2 + <a,b> + <a2,b1>*x^{-2}

a' = a1*x + a2*x^{-1}
b' = b1*x^{-1} + b2*x

a'*x^{-1}, a'^x, b'*x, b'*x^{-1}, <a',b'> 
	= <a1*x + a2*x^{-1}, b1*x^{-1} + b2*x> 
	= <a1*x,b1*x^{-1}> + <a1*x,b2*x> + <a2*x^{-1},b1*x^{-1}> + <a2*x^{-1},b2*x>
	= <a1,b1> + <a1,b2>*x^2 + <a2,b1>*x^{-2} + <a2,b2>
	= <a1,b2>*x^2 + <a,b> + <a2,b1>*x^{-2}


multilinear interpolation on binary intputs is easy


now attention should go towards optimizing circuit size and prover time. we could define the minimal problem of the identity function where one receives a bit and returns it, or some other simple problem. then we optimize for the smallest circuit that can verify this computation together with verifying other proofs. so we don't need to distinguish between prover and verifier times, but just circuit size. we should probably set the minimal problem as a variable with its inputs, outputs, wiring, and trace. 

now to optimize we'll need to decide what computations verifying circuits are dominated by. this means choosing our model of proof and if requiring owfs, which ones. actually we assume applications are packed with owfs. so we optimize heavily for that anyway. we need to pick one. we also need to optimize for the discrete log and figure out how that will be performed. 

whether we do long arithmetic or native arithmetic for the discrete log depends on how large our field has to be anyway. that in turn is determined by the owf we choose. so first choose an owf. the only other restriction on field size is the poly lemma, which doesn't require a large field size. only discrete log and hash have externel security requirement, so depending on what we choose we determine minimal field size. 
if we do long arithmetic, characteristic 2 may be helpful. 



m = 2nlog(q)


nlog(q) < 2nlog(q) < q/{2n^4}

4nlog(q) = q/n^4


for subset sum i realize the matrix may need to be 


the subset sum i'd like is having 2n random elements each of size 2^n mapping 2n bits to an element of size 2^n. 
oh wait, subset SUM in characteristic 2 is then xor which is feasible. what about subset PRODUCT? 
another problem with characteristic 2 is the discrete log group will not have prime order. 


when we homomorphically add vectors we need the arithmetic to stay in the field. this means there should be f possible exponents where f is the size of our field. oh, this isn't a problem because most safe groups for discrete log, eg one of order (p - 1)/2 is prime when p is a safe prime. so we need to match the discrete group size itself (not its host group) to the size of our native field. does this elliminate possibilities?
well apart from this i realize discrete log requires integers as exponents, actually natural numbers, so characteristic 2 isn't an option unless we somehow translate it. a field modulo a prime then seems necessary for the native field. well we need the homomorphic property to be xor of the bits. if we translated the bits to integers the arithmetic wouldn't work, and if we kept them as bits and committed each one with a discrete log then we would need the discrete log group to have order 2 which of course is not a secure group. so DLP woulnd't work for characteristic 2, though if we could find an xor homomorphic commitment it would work but i don't know of one. 
we can't use the multiplicative group modulo p because that has order p-1 instead of p. so if we use a multiplicative group it must be a prime subgroup of some larger multiplicative group.
now for elliptic curves, the group of the curve, whether the whole curve or a subgroup must have order of the field. seems unlikely to find one where the subgroup size is the same as the field size, though in NIST they are multiplicatively close but we need exact matching. and one where the whole curve is the group and is the same as the field size is vunerable to smart's attack. so we'd work over some curve in some non-native field.
so it seems we will work in a non-native field, though with an elliptic curve its possible that field could be even smaller. unless we can get some structure between native field and that of log, we'll need to do long arithmetic, so we want to minimize discrete log computations. we simply want to receive commitments to polys as non-native elements, and multiply (or add for elliptic curves) them in the non-native field iteratively for calculating powers and non-iteratively for just combining two commitments. that's it, no more tricks with DLP. 

but regarding characteristic 2 we have Yao's XOR lemma. 
F(x1,...,xn) = XOR_i f(xi)
F(x1,...,xn) xor F(y1,...,yn) = XOR_i f(xi) xor XOR_i f(yi) = XOR_i f(xi) xor f(yi) ?=? XOR_i f(xi xor yi) = F(x1 xor y1,...,xn xor yn)
so i think if we find a weak owf f that is homomorphic we have a strong owf F that is homomorphic.
oh actually the regular lemma is for a f being a predicate, meaning the output is a single bit. actually we'd like the output of f to be the same size as input so it could be a permutation. 


need field size to match dlp group size. thus dlp group size must be power of a prime. 
we need to optimize for the collision resistent hash functions for merkle trees. these must have output compatible as input, and ideally map 2 to 1, ie compression factor 1/2. if we use elliptic curves we do discrete log with two or more exponentiations. so input size is log(group_size), output size is 2*log(field_size). and the two must be equal so group_size = field_size^2, and this seems unlikely. I suppose the problem could be mitigated by having the input size be 2*log(field_size) and its committed by a 2 combination of generators. then we just need group_size > field_size, which I think is possible. and actually if done over a prime field, we could avoid all bit spreading, as the output would be immediately interpretable as exponents for input. need to analyze group vs field size to prevent collisions. 

for discrete log we can pre-compute powers of the bases for efficient hashing. but with the vector commitment the base to multiply will depend on the vector. sumcheck will require hashing, as will the application in general, but sumcheck requires uniform output distribution. if we use discrete log for hashing, probably most efficient method is to use elliptic curve over the native field. remember the field is then a curve over another field. the only cost maybe unnecessary is the large native field necessary for the elliptic curve. 

consider the commitment. its curve size is the size of the native field. there is a bijection easily computable from the field to the curve by exponentiating a base point. but the prover must submit the curve point because the prover can't compute the bijection the other way. 

it may be costly to use the fiat transform. we need an efficient collision resistant with uniform output. what if we add the two points of output of a curve point. probably still not uniform. 
if its too hard we have to go without sumcheck and instead poly div.  


prover and verifier nodes could engage in own interactive proof for prover to convince verifier that evaluated at random point gives certain result. this reduces bandwidth and avoids sending whole witness, even if in zk. 
the prover should not have to keep a giant witness around waiting for the time to prove. so when receiving a proof a prover verifies it, and waits until using that proof to generate his own proof and interact with another party at which point the witness is generated. so after verifying one just keeps the commitment, having been convinced its valid, and will use it in its own proof. 


suppose we find a curve with group order same as field size, requiring then that cofactor > 1 to avoid smarts attack. this means we can map from the field easily to the group via a generator. maybe this would satisfy all compatibility issues. the native field would be that of the curve. all discrete log operations would be performed on the discrete log group, having same size as the field. this includes the commitment, and hash functions. the fiat problem still unsolved without a uniform mapping from the curve to the field. 
due to Hasse's bound, this is not possible. prove this in the paper and later show how its the (missing) proof for the 1-cycle in the paper on elliptic curve cycles. 


suppose we have a cycle of elliptic curves. call the two fields Fp and Fq. So #E(Fp) = q, #E(Fq) = p. Cofactors are not possible. Suppose we have a proof over Fp, and we commit the coefficients in E(Fq) because this admits p distinct powers. Now its the verifiers turn, who is implemented in Fq. Thus the verifier can accept the commitment. The verifier can have a bit representation of p, and it can choose a random bit string of this length and multiply the commitment as such, then add the commitment to another. 
now what about hash functions for application. suppose we're in Fp. we have a hash output from the q-order curve consisting of two elements of Fp. Then these become exponents to a curve of order p. So every layer of the merkle tree shifts circuits. 
so a proof in Fq starts when verfying proofs made in Fp and when its job is to compute discrete log hashes when the base is in q and the exponents as bit vectors of log(p) size which is the input. We're gonna want exponents in bit form anyway and have our bases hard coded.
now p and q should both be large enough for security, but they don't need to be primes, only powers of primes. 

still have problem of fiat transform. if we use the discrete log we need a mapping from two elements of Fq which form a point on a curve of order p to Fp. Note that for every x there are exactly two y (sometimes maybe one). now twice the number of x points + 1 is then the whole curve size which is size p. (actually not sure about that cuz then every curve would have order 2n+1). 
note that dlp already does compression. so maybe to commit p values, we should do them in a group of half the size of p. then the output of the curve would be two elements of size p/2, summing lengths to p. but is this secure? consider committing a single value. so suppose g has order p/2, an integer. suppose we commit n, an integer between 0 and p-1, g^n. suppose we find m in 0 and p-1 such that g^n=g^m. then g^{n-m} = 1 so one just needs to find n and m that subtract to p/2. 
we have the ability to give a lexicographic order to the curve points by comparing two curve points. but we need a way to immediately calculate the index. 
our best option may be the subset sum problem with input the log rep of the x coordinate together with a bit for the y coordinate, thus basically no compression. this works because even though we ignore most of the data of y, we still have a surjective mapping from the points to bitstrings of log the curve order. so DLP does the compression, and subset sum does the randomization. a verifier in Q will have to verify that a bitstring corresponds to a set of Fq elements. but remember, we want to avoid any bispreading 
even if we just add or multiply x and y (which is arguably appropriate, as y reshuffles x) we still end up with an element of q when we need one of p. as far as proving, say, addition of x and y, we need to show there are few other curve elements that will add the same, and that they have a random distribution, such that even if one knows what they are, one must solve the discrete log to access them. 
y^2 = x^3 + ax + b
y'^2 = x'^3 + ax' + b
f(x,y) = f(x',y')
we want to choose f such that f(x,y) = f(x',y') implies the two form the same point but we want f to have a simple form.
well consider f(x,y) = x + y
suppose we have point X,Y. Then consider Z = X + Y. Then consider the line through the modular table of the field that passes through all points x,y such that x + y = Z. how many of these points will be on the curve? note this line will always be diagonal 45 degrees from top left to bottom right but it will overflow as needed. 
x + y = x' + y' = c
(c - x)^2 = c^2 - 2cx + x^2 = x^3 + ax + b => x^3 - x^2 + (a + 2c)x + (b - c^2) = 0
(c - x')^2 = c^2 - 2cx' + x'^2 = x'^3 + ax' + b => x'^3 - x'^2 + (a + 2c)x' + (b - c^2) = 0
implies x and x' are roots of cubic poly p(x) = x^3 - x^2 + (a + 2c)x + (b - c^2)
which can have at most 3 roots. so for any c we have a poly p, and at most 2 additional pairs of x and y can satisfy it. That is few enough. we now need to show they have no structured relationship between them so knowing access to one doesn't admit access to others. we'd like a reduction from finding a collision to the discrete log for another arbitrary value.
first, as far as uniformity, we say there are at most 3 collissions even possible. so there are p points on the field Fq and for each Fq there's at most 3 points, so the image is at least p/3 elements of Fq. 
another possibility is just selecting the x value because then there are at most 2 collisions. then we need to make sure the inverse is easy to access. But this seems easy. If g^n = c then g^{-n} = c^{-1} so just take the inverse of the exponent. 

regarding sumcheck, prover of circuit in p sends a commitments to coefficients. verifier in q accepts those commitments, generate random p values from them, and then must add them and first multiply some by powers of that random p value. 

we can have as much verificaiton off thread as possible as long as the implication is collapsable, like tail associative. we'll need different parts of verification, but maybe they can all take the form of a poly commitment evaluated at a random point having a certain value (whether multi or univariate). these can always be collapsed. two different points for the same poly can be reduced to one point on that poly. two polys on the same point can be reduced to one poly on that point. thuse any set of polys at any points can be reduces to one poly at one point. but what is the merging process? for two polys one point, suppose we have the polys over p. given the commitments in q, we need to generate a random point in p then multiply one commitment by that, then add the commitments. this would probably be done in q arithmetic with p represented in binary when is then used to multiply a (un-preset) commitment (such multiplication requires a bit representation of the multiplier anyway I think). the output is a new commitment to a p poly expressed in q. for the two points one poly we would treat the polys as multilinear over field Fp. commitments given in Fq. points to evaluate at given as binary in p. small poly t given 


what if we make the native language of the circle the additive group of the elliptic curve? so its only a group, no distinction of addition and multiplication. suppose the circuit is over the curve group order p (which is over Fq). values are curve points in q, but for any point generated one should keep track of the base and the index, a scalar in Fp. 

suppose we consider an elliptic curve as a vector space over a finite field. then we can have linear operators. for a prime order curve, the dimension of the space is 1 because scaling any point spans all other points. then any linear map is just a single scalar. 
could think of elliptic curve as function from p to q assuming we add outputs. we could then compose dual or cyclic functions. then we could have a function space. 

if we had a curve for a native language we'd need a way to represent the points as a poly. if we keep track of each point's index, we could just do polys over the field. so if curve has order p, its over field Fq so we commit indices in Fq (and thus curve points) via the p order curve. so the circuit can accept commitments to its own type of values and alternating proofs may not be necessary. 
Now to what degree could just an additive circuit merge conditions? For two polys one point, it accepts two inputs but it appears it must have knolwedge of the field to do multiplication.

seems such additive circuits would have much restriction anyway and performing application ops other than hashes via curve computations could be costly.


s_j(0) + s_j(1) = s_{j+1}(r_{j+1})
s_{j-1}(0) + s_{j-1}(1) = s_j(r_j)

a*s_j(0) + a*s_j(1) = a*s_{j+1}(r_{j+1})
b*s_{j-1}(0) + b*s_{j-1}(1) = b*s_j(r_j)

(a*s_j + b*s_{j-1})(0) + (a*s_j + b*s_{j-1})(1) = a*s_{j+1}(r_{j+1}) + b*s_j(r_j)

so take all polys s_i and multiply each by random scalar then add them. then evaluate result at 0 and 1, sum and compare to right side which doesn't have such a convenient merging. 


for poly, use subgroups for small domains


if poly constraint method, verifier receives poly commitments. the verifier sends back random values for each. prover commits to random linear combination of them over the zero polys. verifier then evalutes both sides of equation at random point derived from commitment of h.

reducing two poitns to one. poly in p, commitments in q. calculate evaluation points in p, receive claims in p. receive t poly in p. evaluate t and 0 and compare with one claim, and t at 1 and compare with the other claim. hash t and get random p and evaluate t at that p and get new claim for poly at that p. 
suppose we do this in the context of p. receive commitment as bits. evaluation points and claims in p, as well as t. poly composed with t is given without commitment as p values. its evaluated at 0 and 1 and compared with claims all in p. then p values that make up t must be hashed to a new random p value (idk how). t evaluated at this random p value and new claim is generated about the original commitment. notice unlike for the case of two polys one point, the commitment doesn't even need to be seen. 


if we perform them we need to perform operations like reducing two to one in the native field.
suppose we have an exponentiation map as like a one way hash function with compression possible, from each field to the other. this assumes we have found a way to safely compress the two output components to one field element. this exponentiation map must be performed in the field of the output. 
another one way function, but without compression, that could transfer between the fields is the subset sum. this would also best be performed in the output field. 
so our outputs will be generated in their native field, while inputs will be expressed as bitstrings in the other field. 
consider reducing two points on one poly. suppose the field is p. almost everything should be done in field p but upon receiving the intermediate poly its coefficients must be hashed into a field element for p, which means a round trip through q. 
i'd like to consider the possibility of avoiding round trips and only single trips by having a proof in one field produce a random value for another proof in the other field. for each poly, the bit forms would be sent to one poly for hashing, while the elements would be sent to the other field for evaluation. or maybe they'd be partially commited first. the trouble is whatever way its done there must be redundancy of data between the polys, so there must be a consistency check between witnesses, which are in different fields. in the case of sumcheck, maybe the top statement doesnt just refer to data in its own witness, but can refer to values in the witness for the other and if using constants not variables for this referencing it should not matter that the other witness poly is in another field. i think the interdependence wouldn't need to affect soundness because we'd require that both proofs are valid, so manipulting one will hurt the other, and manipulating both for cancellation of harm maybe we can show is infeasble. but how to resolve the interdependence at the end if both witnesses rely on each other? 


one worry is we won't be able to efficiently do curve multiplication. scalar is input as bit vector. suppose all powers of 2 of generator is preset as constant. then for each 1 in the bitstring we want to add those generators. we'll multiply each power by corresponding bit, using some reprentation for the identity. my worry, which I suppose isn't too bad is we then perform curve addition for all those elements (when really we could ignore all the identities). 
note that doubly a point is actually a different calculation that adding two different points.


maybe we can take a commitment of p as elements of q and in the field q use the two output elements of q to produce a random element of p expressed as a bitstring. then no sharing is necessary between fields, and this bitstring could immediately be used to exponentiate the commitment. but this would actually only partly some only some cases. for two polys to one it would allow for efficient exponentiation. but then recalculating claims would need to be done with long arithmetic. I still think the interdependence is better. 

to commit, be in base field. to manipulate committed values, be in base field. 

what if have a hash function, however complex, with the property that the statement
H(a) = b and H(a') = b'
can be efficiently reduced to one like H(a + a') = b + b'. this means the implication goes the opposite way as a usual homomorphism. then we may not need to perform any of the hashes inside the circuit, just like we don't perform poly evaluation in the circuit. instead we only include the claims and we merge them as we go. does DLP have the property desired?
suppose the claims are made aG = A, bG = B and at least one is false
then a random r is chosen, and we infer the claim (a*r + b)G = r(aG) + bG = rA + B
if only one claim is wrong, eg aG = A' != A then (a*r + b)G = r(aG) + bG = rA' + B !== rA + B
suppose both claims are wrong, aG = A' !== A and bG = B' !== B
then (a*r + b)G = rA + B
=> r(aG) + bG = rA + B
=> rA' + B' = rA + B
=> r(A' - A) + (B' - B) = ID
since r is determined after A,A',B,B' this only holds with small probability.
however, using this still requires a single computing rA + B which includes a multiplication and can be considered a crypto operation (while a*r + b is not considered crypto) so we can't fully avoid crypto operations. 
this is similar to the poly merging reduction but we are not given the commitments. but now we have the new cost of generating a random r. we want to avoid this. can be merge this too? suppose the base is Q and the exponents are in Fp. Given the claims, in p, we need a random p value. this brings us back to the original problem.

a1*Q1 + ... + an*Qn = A, b1*Q1 + ... + bn*Qn = B. then choose r, and have (a1*r + b1)*Q1 + ... + (an*r + bn)*Qn = r*A + B
but we choose r to be based upon both A and B, which belong to a different field than r. Disregarding how the interdependence works, suppose we switch to the other field and we end up with something like r*Q_{n+1} = R. then we leave with ((avec*r + bvec) concat r)*Qvec = r*A + B + R. 

The basic idea is to merge n hashes we need a n-1 hashes to generate n-1 random numbers to multiply by the n-1 of the n original claims to induce enough randomness for a new valid claim. The costs cancel out, because n merging to one, with the addition of another n-1 hashes reduces n to (n-1)+1 = n. Suppose we start with 2 hashes but they are multivalued of length n, so they already have compression. We take the 2 commits and generate a random number via an additional hash, then reduce the whole thing to one multivalued hash of length n+1. The hope is that the the additional claim frees us from having to do any hashing ourselves. Suppose we work in the same field (rather than complementary fields). 

start with
a1*Q1 + ... + an*Qn = A, b1*Q1 + ... + bn*Qn = B
c1*P1 + ... + cn*Pn = C, d1*P1 + ... + dn*Pn = D

then generate random values as
Rq = C[1]*Q_{n+1} + C[2]*Q_{n+2} + D[1]*Q_{n+3} + D[2]*Q_{n+4}
Rp = A[1]*P_{n+1} + A[2]*P_{n+2} + B[1]*P_{n+3} + B[2]*P_{n+4}

then merge them as
(Rp[1]*a1 + Rp[2]*b1)*Q1 + ... + (Rp[1]*an + Rp[2]*bn)*Qn
= Rp[1]*A + Rp[2]*B
(Rq[1]*c1 + Rq[2]*d1)*P1 + ... + (Rq[1]*cn + Rq[2]*dn)*Pn
= Rq[1]*C + Rq[2]*D

finally also merge with new claim about random values
Nq =
(Rp[1]*a1 + Rp[2]*b1)*Q1 + ... + (Rp[1]*an + Rp[2]*bn)*Qn
+ C[1]*Q_{n+1} + C[2]*Q_{n+2} + D[1]*Q_{n+3} + D[2]*Q_{n+4}
= Rp[1]*C + Rp[2]*D + Rq
Np =
(Rq[1]*c1 + Rq[2]*d1)*P1 + ... + (Rq[1]*cn + Rq[2]*dn)*Pn
+ A[1]*P_{n+1} + A[2]*P_{n+2} + B[1]*P_{n+3} + B[2]*P_{n+4}
= Rq[1]*A + Rq[2]*B + Rp

so verifier is given the vectors
[a1,...,an], [b1,...,bn], [c1,...,cn], [d1,...,dn] along with A,B,C,D asserting the initial claim.
then given Rq, Rp along with their claimed form with respect to C,D and A,B.
verifier can then calculate new claims left side as
	[...(Rp[1]*ai + Rp[2]*bi)..., C[1], C[2], D[1], D[2]] and [...(Rq[1]*ci + Rq[2]*di)..., A[1], A[2], B[1], B[2]]
verifier also calculates the new claims right side as
	Rp[1]*C + Rp[2]*D + Rq and Rq[1]*A + Rq[2]*B + Rp
so now we have two new claims each of size 4 more than the previous. we reduced 4 claims to 2. 
now think about which computations are performed in which field
consider circuit in Fp. it receives [a1,...,an], [b1,...,bn], C, D, Rp. With these values it can calculate the new claims left side. But now I realize calculating the right side (of the other claim) requires two exponentiations, thus not reducing the cost from just calculating the random values directly.

suppose we calculate each hash function, regardless the compression, at unit cost. Is there any way to merge them and reduce cost in this case?
H(a1,...,an) = a
H(b1,...,bn) = b
H(r*a1 + b1,...,r*an + bn) = r*a + b
H(a,b) = r
on first thought it seems this is not possible. 

notice that in vector commitment opening, the prover must reproduce the outputs for each input, before multiplying them all together to get the commitment value. however, given claimed outputs one cannot bind them to the commitment. 

continuing the above which seems impossible, what we need to do is extend further making use of compression in another way
reduce H(ai) = a, H(bi) = b, H(ci) = c, H(di) = d, etc
to H(a,b,c,d) = r, H(r^3*ai + r^2*bi + r*ci + di) = r^3*a + r^2*b + r*c + d
if we have two polys, univariate, and we evaluate both at the same random point, they will most likely evaluate differently. 
so breaking this without break the DLP requires that r^3*ai' + r^2*bi' + r*ci' + di' = r^3*ai + r^2*bi + r*ci + di which means two polys at a random point coincide for all i.
so I am exploring this in the case that the verifier knows all these values, but is just not willing to compute H. Note this works even if the initial sequences are not of the same length as the shorter ones can be padded with 0s. the reduction reduces from any number of arbitrarily long hashes to two hashes, where the first is of length the number of hashes and the second is of length the longest hash. 
could it also be used when the verifier does not know the initial sequences and only has the commitments? Verifier could calculate right side of second output, but would need to hold on to all of first output. so it doesn't fully translate where output state is analogous to input state. but it could be used to reduce evaluation of multiple polys at the same point to one poly at the same point. what about reducing evaluation of one poly at two points to the poly at just one point? I think this also works.

now i remember, the above was an idealized version, where output and input were both same field of scalars. but now we have to adjust to our case, where input is scalar in one field, while output is elliptic curve point in another field. actually this prevents us from doing what we planned, because the elliptic points may not be able to function as coefficients of a poly. the only possibility is that we use complementary fields and justify reducing elliptic curve outputs to one field element. otherwise we would need to find an H that is additively homomorphic, allows for compression, and operates over the same field, which need to be prime. H(a + b) = H(a) + H(b). 
but from below we see reducing output components to one (eg using addition) won't work homomorphically
H(ai) = (a1 + a2)
H(bi) = (b1 + b2)
H(ai + bi) = (a .+ b)1 + (a .+ b)2 != (a1 + b1) + (a2 + b2)
because (a .+ b)1 != a1 + b1, that is, addition is not just component wise.

so it seems the verifier needs to perform the exponentiations, so reduction may no longer be advantageous. so we return to how to efficiently compute them with interaction between the fields. just computing r values (with respect to a standard basis) seems more convenient than reducing (then performing exponentiation with respect to a non-standard basis). 

one possibility is to have a homomorphic hash function from Fp to Fp^2 with output operation point addition, by composing the two elliptic curves in complementary fields, with the cost that each DLP requires hashing twice. but evern this wouldn't prevent the verifier from needing to compute logs. also its probably not homomorphic cuz it goes through two different fields, but I'm not positive. 

if we implement with our current best option which is communicating between the circuits and performing hashing in them, we will be doing a lot of hashing, and for every exponentiation we'll need to do log(F) point additions (assuming hardcoded powers) which is like 200 of them, but at least each one only require elements and not bit spreading. we'd assume each addition is of two different points, and indeed this should be the case for exponentiation, but when multivalue hashing its an assumption and if two are the same (with negligable probability) we can have an escape hatch for the prover to generate a different proof by some free variable. (but if it happens with probably less than someone cheating we could maybe just ignore it, and this would mean negligible completness). 

review subset sum. it is a suitable owf from one field to the other, performed in the field of the output. but it is not homomorphic, and doesn't compress. it could be a final conversion from an elliptic curve output in one field to an element in the opposite field, closing the loop. it could take the binary version of the x value and a single bit corresponding to the lexicographic look of y. the other option for conversion to a single element from a curve point is adding the components, but this does not close the loop. 
if we could get the subset sum to compress enough we might be able to avoid the dlp for generating random elements and just (maybe recursively) use the subset sum without need to transfer between fields. 

we may be able to remove the need for fiat shamir using a setup where a random point of evaluation is encoded in the exponents and hidden from all. for sumcheck, suppose for a multilinear poly a random point is chosen and all possible terms are encoded in exponents from a single base. prover sends coefs to each poly. verifier can evaluate at 0 and 1 and sum results, but evaluating at the random point I realize requires like a bilinear map. however, maybe the evaluation with bilinear maps could happen offthread. 

what if use QAPs but without pairings. proof is commitment to the divisor poly and the transcript. verifying the proof means evaluating at a random evaluation of the encoder polys. given two proofs evaluation happens at different points. can we reduce to one point efficiently? suppose we treat all encode polys as multilinear. remember we can evaluate a univariate at any point by evaluating multilinear at particular corresponding point. so all encoder polys are to be evaluated at same multilinear point for each proof. verifier creates line between this points. then prover sends linear (or higher if reducing more than 2 proofs at the same time) polys for all each encode poly, for each proof. these are too many for verifier to handle but for now suppose its ok. verifier evaluates at 0 and 1 checking each series corresponds to set vector of random points that upon evaluation of transcript satisfies proof (again too much for V to do). then v chooses random point on line, generates new random point and how is left with condition that both proofs are valid at that same point (whereas previously they were on different points). 

maybe we could have a single constraint for each value in the transcript, in particular the computation that yields its value. this would leave with a STARK type construction. 

what if encrypt evaluation point, prover computes encrypted evaluation at that point. then given two polys and two claims verifier can just add the polys and add the claims to get a new claim (probably no need for random exponentiation because evaluation point is unknown). addition of curve points is thus still required but we have relieved ourselves of the need to generate a random element. i think offthread proofs are still doable in zk using the log dot product argument. this doesn't allow for multiplications without pairings. 

really all we need for fiat shamir for sumcheck is a function that take coefficients of a low degree poly and maps to a single element such that it is infeasible to find a pre-image that maps to one of its roots. at least I think we can reduce security to this. 


in coefficient form, there are (p-1)p^d polys of degree d. the p-1 is for non-zero leading coefficient. the p^d is for the ordered d coefs for other d monomials. in root form, there are (p-1)((p + d - 1) choose d) polys of degree d. 
(p + d - 1) choose d = (p - 1 + d)! / ( d! (p - 1)!) = (\prod_{i, 0, d-1} (p - 1 + d - i)) / d! = (\prod_{i, 1, d} (p + d - i)) / d!
no finite field is algebraically closed, so there are always more polys in coef form than in root form, and using the calculations above we can calculate the ratio. prover will always send polys in coefs form because some (the irreducable ones) are not representable in root form. we still care about root form, however, because we'd like to calculate tightly how likely it is to map to a root. for now though, don't worry about tightness and just consider the each degree d poly has at most d roots. 
we want a function H from d+1 elements of Fp to one element of Fp. 
suppose for any element of Fp there are I polys of degree <= d that have the element as a root. 
clearly H does compression. suppose H is invertible, that is given any output one can compute all inputs that lead to that output. this means H is not collision or 2nd-preimage resistant. consider final round of sumcheck. tv is fixed, while prover needs to choose a tv' such that H(tv') = r and tv'(r) = tv(r) or r is a root of tv'-tv. To do this, prover leaves tv' as a variable, calculates the degree d poly t = tv'-tv which has coefficients containing variables. prover replaces the unknown in the poly t with H(tv') in variable form and expands t to obtain a multivariate poly in the variables of tv'. then the prover find a root for this poly and set tv' to this root. no step in this process could be infeasible for the prover except expanding t with H(tv') and finding a root. what must be the nature of H if this is to be infeasible?


https://crypto.stackexchange.com/questions/65975/snark-friendly-one-way-compression-functions?rq=1
https://eprint.iacr.org/2018/1162.pdf


Edwards curve
Twisted Edwards curve


for poly commitments we will need to use DLP on a curve.
for hash trees for application I think DLP on a curve will work.
for fiat-shamir I think we just need owf to a field element. this is the hardest to decide. DLP on a curve would amortize our already-heavy use of it, but we need it to map to a single field element. hmm, well since collision resistance is not necessary for fiat-shamir we could just use the x component or maybe add the components. 


i imagine there is no way to reduce oracle queries. if there were could do oracles offline. do we need a homomorphic hash to do reduction? suppose like Mimc the hash is a univariate poly of super large degree. if it was multilinear of low degree we could reduce evaluation from two points to one point. but i don't know of such a construction and also don't know of one security enough with a useful homorphic property (some exist but its not a helpful homomorphism). 


what about doing constraints without multiplication? we would make heavy use of constants, like for comparisons. so all our constraints would be in the form of linear combination equations (so we allow multiplication by constants).
I'm thinking of a modification of QAPs. we'd still have encoder polys. note that if we have more than 1 term they can reduce to one, so assume just one term. it would the dot product of the transcript with the encoder polys. we'd have something like 
f1 . K(x) = m1*z(x)
f2 . K(x) = m2*z(x)
where K(x) = [k1(x),...,kn(x)] is the vector of encoder polys. n is the size of the transcript, and the degree of these polys is the same as the zero poly and equal to the number of constraints (that is why the divisor poly is just a constant).
now we can add these two proofs to get
(f1 + f2) . K(x) = (m1 + m2)*z(x)
of course we have completness, that if the first two are valid then their sum is valid. what about soundness? if only one is incorrect soundness holds, but if both are wrong they can cancel each other. after given f1,f2,m1,m2, one must choose random r and instead yield
(r*f1 + f2) . K(x) = (r*m1 + m2)*z(x)
now f * K(r) for some r can be interactively computed using the log-comm dot product argument, but the verifier must compute K(r) as well as the right size m*z(r) by itself, but this is all fine because it is done offthread.
now while the proving system seems ideal here, with prover work linear, this model of computation may be limited in several ways.
first, we don't just want a linear combination of elements. also we want to include constants. for this we includ an extra encoder poly c(x) that encodes constants not multiplied by any transcript element. degrees remain as they are.
f1 . K(x) + c(x) = m1*z(x)
f2 . K(x) + c(x) = m2*z(x)
(r*f1 + f2) . K(x) + 2c(x) = (r*m1 + m2)z(x)
=> 2^{-1}(r*f1 + f2) . K(x) + c(x) = 2^{-1}(r*m1 + m2)z(x)
actually I think we can reduce an aribtrary number of them at a time still with a single fiat-shamir computation to get r (using powers of r). 
now of course our application requires multiplication, so if this can't be done with linear constraints this method won't work. at best it would require long arithmetic. since this method requires fiat-shamir anyway (though only one), and only really benefits the prover, it may not be worth much. we still need to do quite a bit of multiplication for elliptic curves which we still need because of poly commitment long arithmetic would be costly.

but what if we define the transcript the be vectors to be elements of some abstract group, and linear combinations to be multiples. notice the field size doesn't matter compared to the group size, but coefs for inverses depend on group size. now we still use the root method, were constraints are of the form a linear combination of the transcript plus a constant must equate to the identity. the left side is really a linear map (with a shift). the transcript is a list of vector elements, and the encoder polys are a matrix serving as a linear map. I don't think there's a way to do the right side. anyway, i realize merging proof would require operations in the field not just the group. 


so at the moment our state is that we use sumcheck type proof and we are only lacking an appropriate fiat-shamir transformation, or justifying that dlp is good enough and in what such way.
if we choose the x for hash output then we have a collision but that is I think submitting the negation of the exponents which means the negated poly, but the negated poly has the same roots as the original poly so this collision is harmless. 


to reduce the cost of exponentiations maybe use something like "Addition Chain Heuristics". I think the details and optimization can be deferred to prover strategy. we take a small 'window size', eg 6 bits so 64 elements and pre-compute them all. then instead of adding n pre-computed powers of 2 for a bitstring input of length n, we add 

43210
2^4*6

maybe we could do all hashing both for hash tree and for fiat-shamir with a fixed base always of the same length, probably 2 or 3 or 4. this would be the radix of the tree, and for fiat-shamir transforms with more elements we could iterately apply it because we don't need homomorphism. we only need homomorphism on the transcript commitment, and that also requires many more bases but all that happens off-thread. with this all hashing would be done with the exact same pre-computed base, so hopefully we could introduce some optimizations. suppose our tree is binary. then from what I was think we need to hash both the components, so at every branch we have 4 hashes to compute. i think 4 is also convenient for GKR style (if we use a single sumcheck on subject poly consisting of at most 3 multiplying multilinear polys with the same variable, so each message is at most a cubic unvariate poly consisting of 4 elements to be hashed). but we will still need some interation for certain cases like reducing 2 points of evaluation to one, where poly degree will depend on length of transcript and be much more than 4. but i think its safe to assume a fixed base of length, say, 4. what optimizations can we apply to hashing? we can assume many such hashes are taking place. can we amortize them in any way?
oh but there one important instance of DLP without a fixed basis, and that is when reducing two polys to one. remeber we can reduce eg 4 of them by computing r^3w + r^2x + ry + z = r(r^2w + rx + y) + z = r(r(rw + x) + y) + z which means exponentiating by r 3 times. this is cheaper than binary merging because only one r must be generated and maybe the repeating of the exponent r can be useful. for a fixed exponent, one finds the shortest addition chain and uses it multiple times for exponentiation (it is provers motivation to make addition chain short, verifier only needs to add it up to verify it comuputes correct r). 

we need to design a proof system with complementary fields such that the randomness from a reduction is influenced by those things it reduces. so eg in sumcheck taking two polys and generating randomness with them applied to the other poly is unfortunately not secure because one of them can be committed after its randomenss is generated. so if we use dlp for randomness then we take commitment of p values giving us q value, then we need convert this back to a p value. we can do that by passing it again through the dlp or maybe intepreting q as p. but whatever circuit does the verification must have access to the initial p values and to the r value and be confident that r is the correct output. if the circuit is given the q value and it then computes the second iteration of dlp, how does it know the q value is correct? It must verify whatever proof did the q calculation. 
suppose we have a p proof and we have a q circuit and a p circuit both verifiying. which circuit plays which roles? 
q circuit: takes p elements and commits them into q values; takes the transcript commitment together with its hash (no have) and reduces them to a new commitment (point of eval and claimed result no have).
p circuit: takes p values and checks them with respect the output of their commitments (no have); take the transcript commitment and hashes it. ... nasty

the fields may have to communicate with each other purely through bitstrings. we cannot really take advantage of asymmetry between the fields and their circuits and what those circuits are expected to perform because both circuits have the (large) minimum requirement of helping verify circuits in the other field. but what are some small asymmetries we can take advantage of?
the larger field can accept elements of the smaller field, but inverses will be interpreted differently. they can only be interpretted correctly if viewed as natural numbers, which seems fine for exponents. but if we use dlp, which are are, the exponents must be submitted in 'spread' (not necessarily bit-spread) form anyway, so submitting as whole elements doesn't seem to optimize much. 

what if we take the simplest but maybe least efficient way of simply alternating, no parallelism, and all values are submitted from one proof to the next in bit form, and no values 'skip' a proof (which would enable them to be submitted in native form). so we start with a proof in p, transcript commitment in q, all other parts of the proof submitted to the next q circuit in bit form. the q circuit hashes the bit submitted values, generating random q values. q circuit also receives in bit form the claimed values for transcript. q circuit hashes these and combines this with hash of transcript must just adding the points. now q circuit can represent new commitment as old commitments multiplied with hash of the random q value generated. it can also just pass the claims in bit form the way it received them, but it can't multiply them without knowing the hash of the random q value (and it can't perform arithmetic in that field anyway). similarly, it can hash the other p value it receives but it can't complete the verification so it passes these values on in bitform but adds their q value hash. no when the q circuit passes to the next p circuit, it bit outputs must become element inputs, and its element outputs must become bit inputs. now the way this transfer happens depends on how one is to read the claim of a proof. ideally one can read the elements of the claim in any format. suppose this is the case. then we just pass the values as said, and the p circuit converts the bit submitted values to p values (hashing again or something) and then completes the verification. but how does the p circuit know the bit values are correct? it doesn't, but since it will begin the verification process of the q circuit itself, it can reduce validity to that claim anyway. actually we're not done. the r for merging two polys to one must be returned to yet a new q circuit in bit form in order to reduce compute the new commitment. then we're done. 

the above is too long, takes 3 verifier levels. we need to use parallelism, and before we get into the details we need to comfirm how circuits read each other and we can force agreement for a value when it is submitted to parallel circuits (on different fields) in different formats (one native and one bit). suppose we only use one layer of parallel circuits and they do everything needed and all we need to do is make sure about agreement of their values. maybe we could delegate this to a new proof that computes on both of their transcripts. or maybe special transcripts are made that hold the values to be compared for agreement. 

i just noticed we may require non-zk with the prover sending full trascript to the verifier. this is because if prover is to argue zk he has knowledge of openening he needs access to the transcript. if this is indeed the case maybe the whole scheme is better framed as a single scalable proof. well maybe it can still be zk if the final transcript is the same and maybe it can be a 'padding' proof at the end. or maybe just generate a random mask poly then reduce that and the real poly to one, proofs still trust it, and its blocked so can be sent to next prover zk.

fixed base
https://documents.uow.edu.au/~thomaspl/pdf/NegPlaRob19.pdf
fixed exponent
http://delivery.acm.org/10.1145/2840000/2835046/p89-dwivedi.pdf?ip=198.11.30.230&id=2835046&acc=ACTIVE%20SERVICE&key=B63ACEF81C6334F5%2E1FC7ACB276C876CF%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&__acm__=1566362109_2c9e246d3a2f07e121723d659008f284

how does one proof take other proofs as inputs? its just presented with those proofs as data that gets encoded in the transcript. but i'm wondering if the transcript of a proof includes its claim. theoretically yes, its part of the satisfying assignment. but we also want easy access to it, whereas we don't have easy access to transcript without a full evaluation. even encoding claim as an accessible part of the transcript, the viewer must evaluate it to verify if. but i suppose the reading circuit need not perform that part of the verification. so we can consider the statement of each proof to consist partly of bit values and partly of elements in its native language. i think its safe to say all native elements go to the native verifier proof and all the bit values go to the other field. by 'statement' I think we just mean output, not input. input gets encoded in the transcript in any way. so to separate the output from the rest of the transcript we'll need an extra bit. i think we'd need this bit anyway to direct constraints on or away from the output versus the statement. we should actually probably refer to them separately, as the statement and the transcript/trace. the terms of the statement and the transcript can be standardized so the proof accepting it doesn't need to evaluate it but can use it for reduction, just as done with the trace. 
the final verifying circuit, whatever field, will need to take native values from one circuit and bit values from another and do a final match between them to make sure they are the same. 

we need to decide exactly how random values will be generated. i think we agree, as will already be the case for two to one polys, the first stage will be DLP the natural direction, but we are left to find out how to translate it back to the native field. the reason we should probably do dlp for raw element hashing is simply the need for compression. 

i'm thinking about the compression method of adding or multiplying together outputs. because then finding a collision is almost as hard as solving a single instance I think. this might be applicable for hash trees. and if collision resistance I imagine it would be one-way. in general xor'ing outputs of owfs is not one way. can we prove its owf for addition and multiplication for certain owfs though? we know it is for DLP. what about, say, subset sum? suppose we add the outputs. finding collisions here seems easy. given, say, 2 bitstring inputs we consider any bit position where the inputs differ and we flip them, so the corresponding entry still remains in the final sum once. but if used for fiat-shamir maybe collisions are ok and we just require owf. inverting requires finding 2 bitstring inputs. hmm, i guess solving in this case is equivalent to solving in a single instance but with input not restricted to 0,1 but rather to 0,1,2. in general m instances on [0,1] and adding up results in equivalent to 1 instance on [0,1,2,...,m]. since our hashing is of a small number of values, like 5, we may be able to have m=5 and still make this owf, and even collision resistant, using the works seen. suppose this worked. we could take p values and generate a random p value from them all in the same circuit, but note that we still would need bit-spreading, however maybe we could reduce the bitspreading by a constant factor by not submitting bits but by submitting the small norm integers for the problem. our advantage is that we can take advantage of the small compression available in this method, whereas for a tree hash we need arbitrary compression. if this works we can leave open the question of application hashing to using an arbitrary collision resistant owf. 


at the minimum we use DLP for reducing two polys to one. this means taking their commitments and generating a random number from them and then exponentiating. this forces us to exponentiate with a non-fixed base and at best we can exponentiate multiple with the same (but not pregenerated) random r. suppose poly is in p, so commitment is in q and exponentiation will take place in q circuit. given q values one must generate random p values. I'm thinking we could do this in a p-circuit using subset sum with density exactly one (due to nearly identical field sizes). which is probably faster than DLP, which would take place in circuit p as well. since there is no compression we'd probably generate a random r for each poly being reduced and I think this is still safe. But now i'm thinking this won't be much more costly then using DLP as both require a single round in a p circuit with the same abount of bitspreading. it will only be useful if we can also use subset sum for fiat-shamir but that requires compression. 
i think for our parameters no-algo can work in poly time, ie for regular modular subset sum when density is a constant > 1. so if not poly time we can expect exp time and I think this measurement is with respect to the number of elements we sum, so an exp gets us time around our field size which is already huge. but note that inputs will not be random. i suspect this will work for our random generations, but again remember that bit-spreading is still necessary. we may still end up just doing regular subset sum which requires more random constant elements than if we repeat it over and over. if we compress by allowing small norm inputs we need to verify they are indeed of small norm, which may require lot of constraints, whereas if we use bits the constraints are simple but we have many more gates. 
maybe we could even establish security for hash trees.
if we can show hardness is exp in log of our field size (ie our field size) then we don't need to worry about concrete params because our field size is already large enough. 

papers
http://users.monash.edu.au/~rste/subsum.pdf
https://cseweb.ucsd.edu/~daniele/papers/Cyclic.pdf
https://web.eecs.umich.edu/~cpeikert/pubs/cyclic-crh.pdf
others
https://arxiv.org/pdf/1807.04825.pdf


https://eprint.iacr.org/2018/383.pdf
https://csidh.isogeny.org
what do we need for homomorphic commitment?
gX hY
g'X h'H



what if we don't even require homomorphism in the commitments, but push evaluating the poly on the coomitments to another proof that will open the commitments and evaluate. then out task is reduced to verifying that proof, but we can make a commitment to the poly of that proof. thus in total we have reduced evaluating one commitment to evaluating a single commitment. this would only require any form of compressive polynomial commitment. so the statement of the proof would be that a list of commits and evaluation points result in a list of outputs. oh, but I suppose the problem is that the transcript for the proof must include the transcript for all input transcripts so the total transcript will group linearly, rather than remain constant. so this idea isn't worth much.


in our protocol for two polys to one the point r and the claimed values v1,v2 are generated before the random linear scalar is. 
m1(r) = v1
m2(r) = v2
make univariate poly with unknown as scalar. 

i was thinking about making it quantum proof using a non-homomorphic commitment like tree to all evaluation of several polys. then reducing manual evaluation of them (which covers low degree testing) to manual evaluation of a random linear combination of them (with random coefficients actually powers of one random value). a benefit is smaller field size, and all arithmetic done in that field. one problem is commitment via trees seems to require a univariate poly, whereas for GKR we're looking at multilinear. 
note that univariate poly f can be represented as, wherein g and h only need to be committed to on fields of half the size. theoretically this means rather than committing one poly f on a large domain we commit two polys g and h on a field of half the size which constitutes the same number of commitment points (or a generalization like g1(x^3) + x*g2(x^3) + x^2*g3(x^3)). but we can exploit the latter because we will evaluate at a single point so we can place both evaluation of g and h in the same leaf. 
below we see recursively breaking it up is equivalent to just generalizing the top layer like we did above.
f(x) = g(x^2) + x*h(x^2)
g(y) = g1(y^2) + y*g(y^2)
h(y) = h1(y^2) + y*h(y^2)
f(x) = (g1(x^4) + x^2*g(x^4)) + x*(h1(x^4) + x^2*h(x^4))
	= g1(x^4) + x*h1(x^4) + x^2*g(x^4) + x^3*h(x^4)
so deciding how much to break up depends on layers prover performance and verifier layers to hash tradeoffs. so in univariate case a poly commitment can be made with breaking up into k pieces by making a hash tree and evaluating each of the k sub-polys over domain of original size divided by k and storing all evaluations together in the same leaf.
suppose we make the field actually very small, in particular suppose transcript size is 2^N, then we choose domain size 2^D such that 2^N/2^D = 2^{-s} such that v*s = 64 so we perform v verifications. eg s = 4, v = 16, so 2^D = 2^N * 2^s = 2^{N + 4} = 2^N * 16. so we perform v=16 queries to achieve soundness 2^{-v*s}. But now the domain is super small and so are field elements, almost as small as the transcript size. subset sum will not work in this case, but now we can use the vector version of subset sum that generates multiple values and we will use these values for multiple queries. we might even go to the extreme where v = 64 and s = 1. 

but wait, what exactly would I do supposing we could easily commit and open polys on a random point? using hash trees we would not know the encoded poly is indeed a poly, ie of low degree. using multilinear polys, we'd get to the end using usuall process and as usual reduce to evaluating a lot of polys at a single identical random point. then we generate random random coefficient(s) and ask for commitment to the linear combination. now via our assumption of reduction (like induction) we can assume the linear combination commitment indeed encodes precisely (not just close) some poly of appropriate degree. now our task is to compare the linear combination to the polys in the previous commitments. remember we can treat these commitments as oracles to real polys, not just hash trees of arbitrary values, under the condition that these polys are of arbitrarily high degree. now we want to polynomial identity test the linear combination commitment with what it should be via the previous polys. we can test at random points via the oracles. we use the zippel lemma, but we must assume the left side (previous polys) to be of maximum degree which corresponds to their size, so we must make this sufficiently smaller than the domain size. if not sufficiently small we just query multiple times. 

could require grinding with nonce for fiat-shamir. i think we can justify eg for 32 bits of grinding we can reduce number of queries above from v=16 to v=8

even if we adopted used univariates in our proofs applying the previous wouldn't suffice for a complete proof system because univariates cannot reduce two points to one unless we first convert to multilinear and recovering from that is the problem we aim to solve below anyway.

consider what happens when we swap two variables in a mutlilinear poly. which terms are affected? any terms that contain both or neither of those variables are not affected. terms that contain one xor the other are affected. suppose we group the terms into these four categories, those containing both x and y, those only containing x, those only containing y, then those containing neither. then we factor out the variables x and y so we end up with a poly of the form a*xy + b*x + c*y + d where a,b,c,d are multilinear polys in the other variables.
this b*x + c*y will turn into b*y + c*x, and adding these together we get
(b + c)(x + y)
subtracting bottom from top we get
b*x + c*y - b*y - c*x = b*(x - y) + c*(y - x) = (b - c)(x - y) which is also the result for subtracting the whole polys.
subtracting top from bottom we get
b*y + c*x - b*x - c*y = b*(y - x) + c*(x - y) = (b - c)(y - x) which is also the result for subtracting the whole polys.

suppose we consider the permutations under which the polynomial is invariant. doese these form a group? if its closed then yes, it forms a group. and indeed I think its closed. 

i realized we can use sumcheck to evaluate a multilinear with the coefficients committed via a univariate, but the subject in the sumcheck would be of high degree, so the prover would need to commit to all the intermediate univariates. while unfortunately the commitment of one poly is needed to generate the randomness necessary for the next poly so they must all be committed separately and we are unable to merge the commitments. 

what if we actually commit to a mutltivariate poly, even just a bivariate, but over a small domain, so that size raised to the variacy matches our original intention. this may have possibility but I stop here due to the problem beow.

wait, i don't know if I understood the soundness of what I step up before. if the commitment to a univariate is over domain size 2^D, prover could commit to a poly of degree 2^D so the zippel lemma will be identity comparing a degree d^N poly to one of degree 2^D over field of size 2^D. this yields an upper bound of error of max(2^D,2^N)/2^D = 1, so there is no soundness at all. the higher degree could interpolate the lower degree almost every except a few places, so it could be high degree but still pass the test. this means we would need some more sophisticated analysis like 'closeness' to low degree which I don't want to get into. I think this ends this possibility of a fully post quantum scheme in one field and we are left to continue with a homomorphic commitment of which discrete log is the only one I know. 


now before pursuing the previous by inspiration of univariates, I was considering isogeny crypto. but for several reasons I don't think that will work. Before that I was analyzing using sumcheck. before that I was considering the minimum requirements for proof data flow to comply with captibility between the fields. we need to do exponentiation with non-fixed base and exponent, but maybe everything else can be done with subset-sum hashing. I will complete my investigation of the security of subset sum if it seems desirable. even in the best case of using it we still need communication between circuits. given a p-circuit, its proof consists of a lot of p values and one pari of q values corresponding to the commitment. all the verification can be done in another p-circuit, including the generation of the random exponent, so this verification circuit will yield a conditional acceptance in the form of q values in bitstring format together with a random exponent, and the claim that at a certain point the polynomial (once exponentiated) will return a certain value. So in total the condition consists of q values in bit format, and 3 p values. a q-circuit will have to perform the exponentiation. this q-circuit will do this for multiple proofs, then add the points together to compress them to a commitment for randomlized linear combination. regarding p-proofs, I think a q-circuit will only perform this role and no more. it accepts q points and p elements in bitformat and exponentiates them and adds them together, and returns the answer, not knowing anything else about the p-proofs. 
but to use the information from any proof, the using proof must condition on the proof being used as correct. thus there must be a succinct way to state such a condition while also knowing the claim that proof makes and being able to relate that to the representation of the condition. 
the difference between proofs with the same logic is just the statement and the transcript. So for a proof in p, it could be represented in un-verified form as the transcript commitment which is a pair of q values, together with the p values that make up the statement (these p values are coefficients). I suppose it should also consist of an identifier for the code. if we use GKR we could have the statement be the output of the circuit rather than an extra input, but either way we still treat the statment as plain p values. since its a complete claim given these values it doesn't matter how they are presented. a circuit that receives them can pass them on to the next asserting its validity is dependent on the validity of that one. realize this representation can be reduced even further by hashing these values to a single one. however, in order to make use of the claim one must unwrap the hash enough to read the relevant part of the statement. 


NIST elliptic curves: https://nvlpubs.nist.gov/nistpubs/FIPS/NIST.FIPS.186-4.pdf

could we do QAPS but prevent the circuit from doing full verification (ie pairings computation) and instead do that part offthread. Suppose we are able to reduce verification of two proofs to one onthread. then yes, this will work. but reducing verification is troubled by the multiplication of polys, ie a sum of two transcripts is not another valid transcript.
the multiplication looks like this
(f . K1)*(f . K2) = (\sum_i f_i * K1_i)*(\sum_i f_i * K2_i) = \sum_{i,j} f_i*f_j*K1_i*K2_j
so we could pregenerate not just K1 and K2 but their outer product. so each encode power, now of form K1_i*K2_j would now be twice the degree as before. more importantly, the sequence would now be square in length. this would probably be too costly. I thought the linear combination of two outer products would be another valid outer product but that is not the case and there seems to be no way to combine transcripts to yeild another valid transcript, ie reducing verification of two proofs to one.


lattice based zk: https://eprint.iacr.org/2018/560.pdf
this paper claims we can achieve almost arbitrary compression


trying to think about how use the highly compressionable lattice based, limited homorphic commitment schemes for homomorphic poly commitment. the coefficients of the poly must be reduced to small norm. 
the sumcheck can be used to verify that one knows an opening for a commitment (vector of not-necessarily small enough norm). at the end the verifer would need to evaluate the constants vector at a random place and the input vector (the same) random place. so for this the input vector would be expressed as (bit access-oriented) coefficients in a multilinear poly. but we'd also like a protocol to verify the input is appropriate (or small enough norm). to do this we could construct the degree d poly consisting of the d roots 0,...,d-1 that appropriate values for the vector elements (infinite norm assumed). Then we use sumcheck to iterate over the poly with supposedly small norm and for each step we plug in the value to the small univariate poly and the result should be zero, and to make sure all add up to zero (instead of two cancelling each other) we multiply term by a random linear combination similar to how we plan to do for the general circuit. 

suppose we have some highly compressive collision free hash function that accepts elements of small norm, eg bits, and outputs something in some format. we require that it is computable by deterministically iterating over the bits in a uniform way a fixed number of times and combining them with constants. if adding is not the primary way of concatenation we can use intermediate polys to store values as long as those values remain of small norm, because we will need a commitment for each of them. in fact, using the sumcheck like this, each time computing new intermediate values stored in new polys, in a general construction we could use, somewhat similar to GKR. of course its assumed all polys at the end will be reduced to one for evaluation, and that all polys are multilinear.
now given a hash output, which is really a commitment to a vector of d-values (by d-values I mean values of small norm at most d). we can interpret it equivalently as a commitment to a multilinear poly with these values as coefs. so this poly commitment is dependent on the norm of the coefs, so this doesn't work for an arbitrary poly. 
we may now use such polys in non-interactive versions of sumcheck protocols where the commitment to the poly is given in the beginning, and at the end the verifier opens the commitment to evaluate. in fact, we could interpret the commitment as one to an arbitrary poly of coefs determined by the value hashed.
really what we care about is reducing evaluation of two polys to one poly. the way I've been thinking to do this requires the ability to compute the commitment to the addition of two polys from their respective commitments, or rather to verify it. In the commitment case above I'm interpreting the d-values and radix scalars, so adding two actual polys is not equivalent to adding the d-values, but adding them as a number base system with carrying. I think this should be possible using the sumcheck where we use commitments to the two input d-polys (polys encoding the d-values) and the output d-poly. oh, but the problem is at the end we must evaluate all three polys, including the input two, at a random place and that is exactly the problem we started with. this is not the case for DLP homomorphism because we can compute the commitment for the output poly directly from the input commitments, whereas here we rely not on the input commitments themselves but on the polys they represent. 
however, given a purported valid commitment, we can use sumcheck over that d-poly to verify that it is indeed a valid commitment, at the end of which we must only evaluate that d-poly. so if two commitments are claimed to add to another valid one, the verifier can add the commitments then check that it is indeed valid. 
if we have two valid commits, we divide them by 2, then we add them we are guaranteed to get another valid commit. Interpreted as polys, dividing the commit by 2 is equivalent to dividing the coefficients by 2, generating a new commit. so we have effectively generated from two commits to polys, a commit to another poly, in particular for f and g we generate poly (f + g)/2. now we'd like to reduce evaluating f and g at the same random point to evaluating some variant of (f + g)/2 at a random point. this is tricky because before we would scale by a random value r but now that would not yield a valid commitment. also we need to clarify what we mean by /2. we need integer vectors so we can't just divide by 2 cuz we might get rationals. hmm, trouble.
suppose we don't use the larger field of constants as our native field, but instead use the d-value range as our native field. call the smaller native field p and the larger one q. so p < q. we might be able to do what we want, but we would need to do something like long arithmetic when working with the q value commitments. suppose we're given two commitments (q values in bit form). suppose we multiply a commitment (a q value) by a p value, then reduce modulo p. 
(q1*p1 + q2*p2)*r mod p = q1*(p1*r) + q2*(p2*r) mod p = q1*(p1*r) mod p + q2*(p2*r) mod p = (q1 mod p)(p1*r mod p) + (q2 mod p)(p2*r mod p)
we end up with a p value, not a q value of course.
what if we work over q as before, but the p values are all even. this way we can divide two commits by 2, get two valid commits with odd values, then add those two commits together to get another valid commit with even values again. Now we need to randomize. (Maybe extend to not just 2 but arbitrary multiples). But we stil dont know what it means to divide by 2. intuitively we know what it means by natural numbers. but how do we perform this. suppose k is the inverse of 2.
(n*2)*k = n*(2*k) = n*1 = n
so really we just multiply by that inverse. we can generalize to multipels of m (even is multiples of 2). 
(n*m)*m^{-1} = n, n=0,1,...,d. maybe we choose m such that m*d covers most of the field. 
we can change m as we go. if we multiply by an arbitrary r, assuming no overflow, then our new m' is our old m multiplied by r. actually if overflow occurs I think we are still ok. suppose our d value is n. we start with scale m, then we scale by r, then we adjust our new scale to m*r and our new inverter to (m*r)^{-1}. we still get ((n*m)*r)*(m*r)^{-1} = n. so it appears the 'scale', (m, r, etc) can be anything in the field that is invertible, ie not zero. so whenever we have a poly we associate with it a scale relative to d, and we know that multiplying by the inverse of that scale should yield coefficients all in the range [0,d]. 

what if we multiply by p or p-1 or p-2? does it shuffle them? multiplying the numbers [1,...,d] by p gives all 0s. Multiplying by p-1 returns [p,p-1,...,p-(d-1)]. 

hmm, i'm wondering if I can establish that the hardness is not due to small numbers, but only having a few choices for the numbers. If this is the case, we can multiply d-values by any constant and the result is still a valid commitment so we don't have to worry about getting the nubmers back in a base form. Oh, but even if this works, when we add polys they must be have the same scale or else the numbers are not limited to ensure hardness. and if they are on the same scale they won't randomize. I don't think we can randomize by scaling. What about randomization by shifting? Again, when we add polys they must be in the same 'state'. 

and actually the above method of combining I don't think will work because it only works like once, and I've explored variants as well. eg the original was to take 2 evens, divide them by 2, then add them, but their sum might not be even. If we restrict the numbers such that the result will be even, then the result after that may not be even, etc.

maybe we can take two valid commits, add them, and assert that some standardized derivative of that sum as a poly should act a certain way. but the sum could have any value in [0,2d] if valid has values in [0,d]. oh, actually we can't just assert the derivative, we need to compute the commit to it. 

our intended construction was to unquestionably correctly compute the commitment to the sum from the commitments. but i'm wondering if that is necessary. we would compute some result poly that can be evaluated instead of the original polys. the verifier would be holding the commitmens to all 3 polys then the verification/proof proceedure using sumcheck starts and at the end the verifier must evaluate all 3 polys at a random place. we reduce eval the sub polys to the same point. then we need to randomize. but the problem is we will need to compute the randomized poly, but computing that will require using the sub polys and so at the end we'll need to evaluate the sub polys at a new point. if we didn't need randomization, we could compute the formal sum, then reduce evaluating the sub polys to evaluating their formal sum. the idea is if the computation was wrong, then the evaluation of the formal sum is wrong. but the problem is the prover knows we're just evaluating the formal sum, so at the end when we ask for the eval of the sub poly, the prover can give answers that are wrong for each but their sum is right for the formal sum. the only advantage the verifier has is that all three poly commitments, as constants, are given from the beginning. what if we just do polynomial identity testing. given those commits the verifier generates random points for evaluation but the problem is verifier either need to computer answers on sub polys manually, or needs to ask prover for answers, but then randomize the new poly rather than just the formal sum.

suppose we do sumcheck to compute formal sum commitment, which prover provides from the beginning. we go through the process with prover sending back messages as it likes that are probably not correct but pass the verifiers tests. then at the end the verifier is to evaluate the two subpolys. now keep in mind that while the prover send messages of his choice the verifier hashed them into messages the prover has no control over. so at the end the verifier is to evaluate the two subpolys at two random places uncontrolled by the prover. the verifier asks the prover for the values at those points. both the prover and the verifier know the line that connects these two points. the prover sends the evaluation of both on this one, implicitly specifying the answers to the random points. the prove strategically chooses these t-polys. verifier sums the two t-polys to form what the new commitment should be when evaluated on that line. for identity test verifier chooses random point on line and manually evaluates t-polys and reduces test to evaluating commitment poly at random point. if the t-poly sums is incorrect, the latter identity test will probably fail. So the prover needs to choose the t-polys such that their sum is correct. it also needs to choose the t-polys so that one evaluated at 0 gives one value and the other evaluated at 1 gives another value such that these two values satisfy an equation. oops i thought this method might give something but looks like prover can pass no problem.

our advantages are:
can compute anything on data encoded in polys using sumcheck at the cost that at the end we evaluate the data polys at random points.
can reduce evaluation of a poly at 2 points to one point without touching the poly.
can reduce evaluation of two polys at a point to evaluation of one poly at that point, as long as we can multiply one of the polys by a constant which requires touching it maybe homomorphically through its commitment.

suppose we make commitments to d values and interpret them as commitments to polys in d space, maybe d is a prime so they are polys in a field. when we add two commitments they may overflow d, but we ask the prover and have the verifier interpret a commitment with overflown coefs as modulo d such that they did not overflow. now we can multiply a commitment by any d value, and despite overflow, when interpreted modulo d the result is like a poly in the field d multiplied by an element in the field d. on the outside, the prover starts with commitments to poly1 and poly2 in field d. upon adding them the vectors together modulo d the prover obtains a commitment to (poly1 + poly2) in field 2 because indeed, adding two polys in field d just means adding their coefficients modulo d. so we have abandoned the idea of interpreting the d value polys as radix based representations for ambient space polys. instead our polys are in the semi-subfield d. maybe we don't need to have d a prime. maybe we can just view these as polys in the ambient space but with limited coefficients. i'm worried the addition modulo d will get in the way. using the polys in sumcheck to access data seems not to be a problem. at the end we must evaluate at a random point in the ambient space. the soundness is then with respect to the whole field size, not just d. so our primary worry now is that there's no clear relationship between polys f, g, and (f + g)_d = (f + g) \mod d. 
note that (f + g)_d(r) \mod d = (f + g)(r) \mod d. 

or what if our native space is the d-space (eg d could even just be 2), and we doing long arithmetic adding commitments in the ambient space.

actually I may have glossed over incompatibilities, eg the added commitments doesn't give a new commitment, at least not directly. Let me start again. when we add two valid commitmens F and G we get
F + G = (F + G)_d + [0,1]*d
where (F + G)_d is the modulo poly, and [0,1]*d is the overflow vector, because when adding two numbers in the range [0,d-1] the max possible is 2*d - 2 < 2*d so at most 1 overflow can occur. The above i'm only showing the vectors. now suppose we plug them into the function by multiplying by matrix S.
S(F + G) = S(F + G)_d + S[0,1]*d
the left side is what the verifier will compute given SF and SG. Now the prover could supply the two commitments on the right side and the verifier could check they add to the left side, but this does not ensure the provided commitments are valid. 
Suppose the prover provides 'S[0,1] (supposed S[0,1]) and verifier computes 'S(F + G)_d = S(F + G) - d*'S[0,1]. Suppose 'S[0,1] is wrong. Then 'S(F + G)_d is wrong and can either contain coefs < 0 (some large inverse number) or >= d. Suppose, audaciously, that we can prove that if 'S(F + G)_d (or rather 'S[0,1]) is wrong then the prover doesn't know an opening for 'S(F + G)_d. in particular the prover doesn't know a valid opening, so he will be caught on offthread, not onthread. if this works, this puts us back where we were before I had doubts. It puts in the position that given two valid commitments to d-polys the verifier (now with help from the prover) can compute the valid sum of those polys modulo d.


One question is, given valid openings F and G for F' and G', is it feasible for one to find a valid opening H for H' = F' + G'? Let us suppose that F + G is an invalid opening, that is F + G = (F + G)_d + [0,1]*d for [0,1] != [0,...,0]
F' = SF
G' = SG
H' = (F' + G') = S(F + G) = S((F + G)_d + [0,1]*d) = S(F + G)_d + d*S[0,1]
suppose indeed one computes valid H such that H' = SH
SH = S(F + G)_d + d*S[0,1]
only probabilistic, but split d*[0,1] into two parts P1 + P2 = d*[0,1] and maybe the split can be done such that 
H - P1 and (F + G)_d + P2 are valid openings. Then one has
SH - SP1 = S(F + G)_d + SP2 => S(H - P1) = S((F + G)_d + P2) => collision => infeasible
maybe this argument could be improved if we choose a d below the security level, so that in fact values of d or a little above still break the security assumption.

now remember that when we're talking about the hash function our setting can shift from modular arithmetic to integer, where inverses are natural integer negatives. 


i'm still considering subset sum for hashing because I think it can do a little compression and thats what we need while at the same time we need to output a single field element, whereas the output of the above function, despite the large compression, is about the security parameter field elements. before I return to figuring out the parameters and whether compression like 4 to 1 is ok, I'll first settle on whether to use elliptic curves or lattices. 



note that we can shift a commit by adding to it the sum of all constants, raising each element in the commit by 1. 

is there a way that given the addition of two commits, ie with values [0,2(d-1)], the verfier can compute modulo d by itself? 
suppose the ambient space was not a field so the constants were q for some, say, even value. then suppose d = 2. we could scale by q/2, so any 2 value gets mapped to q = 0 \mod q. any 1 element gets mapped to q/2. the result could then be multiplied by (q/2)^{-1} to renormalize. unfortunatively this inverse doesn't exist. even so it would still be helpful to renormalize to two values 0,q/2 rather than 3 values 0,1,2, but this is not possible since q will need to be prime.



how hard would it be to take the long-arithmetic approach where our native field is Fd for prime d. for the constants we could represent them in base d as a vector of the scalars. now a commit would be a vector (really a vector of vectors). would this work? I don't think so, because we need the vectors to add up modulo q, not d. only way seems to be preventing overflow d by making the number system large enough that they can all add up with no overflow. (btw, note that if we do this d will need to be small, otherwise q will be too huge). 


maybe we derive a different but similar problem than SIS from SVP. 

I think we can get concrete parameters by fastest SVP algo which approximates to a factor of (1.012)^n or something. So must choose n such that approximating this amount is not feasible. also need an estimate on the time it takes to do this.

Lattices are infinite, so we need finite representation. 
Lambda(upsilon, q) is the lattice of all integer sequences h_1,...,h_m such that \sum_i h_i*u_i = 0 \mod q for upsilon = <u_1,...,u_m>, u_i \in \Z^n. That is, a lattice point is a vector h \in \Z^m such that h is orthogonal to the set upsilon (not orthogonal to every element of the set). q and m will only be functions of n. 

maybe condense the commitment from n elements to 1 element by interpreting the n elements as coefficients of a poly (uni or multi). for a fixed random point, evaluate the poly and the result is the commitment. the same homomorphic property applies. finding a collision means finding not finding the same poly but finding a poly that evaluates the same on that random point. by zippel if commit1 != commit2 then commit1(r) = commit2(r) with probability degree/field_size, but actually this is not entirely so because r determined before the polys, whereas zippel assumes the polys are set before r is selected. ideally the analysis can be that the prover can only sample polys at random (by the hash function) and that for a random poly, its evaluation at random point r being any particular (not random) value has negliglbe probability. since r is drawn at random then multiple polys are drawn at random, we could instead forget the order and just ask, I think, what is the probability that for random polys p_1,...,p_n and random r that any of them evaluate to 0. I think this can be via zippel lemma 1 - (1 - degree/field_size)^n.

another possibility is to intepret the output as a point of evaluation of a degree one linear poly f with some random coefficients. this is also homomorphic as f(commit1 + commit2) = f(commit1) + f(commit2). but here we must rely on analysis different from zippel because the difference is the evaluation point not the poly. but analysis may still be good.






Our generalized SIS problem can be given a finite abelian group G, a set of m random elements from G and a target t, find an integer vector x of length m below some bound B relative to some norm N, such that the linear combination of v and the random elements equals the identity element. another parameter is the order of G. parameters must satisfy certain conditions for this to be owf. This can be turned into collision resistant hash function.

Oh, the lattices considered are subgroups of \R^n but this could be a subgroup of \Q^n or \Z^n.

I'm thinking we would need to use \R or \Q if we are going to invert it, because while an integer matrix has an inverse it may not be integer valued, but rather rational valued. I know rational inverses are rational because only standard field ops are performed, no ops like roots that could result in irrational values. 

Any finite abelian group is the direct product of a unique series of prime-power cyclic groups. the primes are those that divide the order of the group. 

I think a module is a vector space except over a ring rather than a field.
While lattice problems can be extended to have scalars (using modules) other than integers I think we'll stick with integers because it corresonds naturally to the underlying group. Also, I'm hoping to keep the proof field simple as having prime order, rather than an extension field.


Suppose we have a lattice (integer lattice) over \Q. We consider the approximate SVP problem. Given basis A our problem is to find non-zero integer vector x such that Ax = s is bounded in some norm N by some bound B. Note that s will be in \Q.
s = Ax
now A is a basis for the lattice and thus for the \Q space, so its invertible
A^{-1}s = x
An equivalent problem is finding \Q vector s of appropriate norm such that x is an integer vector. 

Suppose one can find an s then computes the greatest common demominator d and factors it out then brings it to the side with x getting
A^{-1}s' = d*x
where s' is integer valued and may no longer satisfy the bound. 

Suppose we try to make this compressive where A^{-1} has many more columns than rows. In this case A^{-1} is not actually an inverse so lets call it A'. To support the reduction we need to multiply the second equation and have it yield the first equation so we need AA' = I. So if A' is to be 'horizontal' then A is to be 'vertical'. So suppose we concat multiple lattice instances of A on top of each other (also increasing hardness). Can we find a right inverse A' for A?





lets figure out, which we should anyway, what we can do with subset sum.
now remember we're only considering subset sum for fiat-shamir, so we don't care if collision are possible, just that its owf and random looking and compressing. 

its possible we could use a general version of SIS hash where the group is our additive field, but it would have to be of size q^n = {n^{c*n}} which is way too big if n is our security parameter. 

subset sum with density about 1 are the hardest. I am wondering if they can have a constant factor difference, but assume for now density is precisely 1.
several sources cite density as n/log_2(q)
what is the security parameter? I think its m = log_2(q)

one source says
if m = c*n then it can be reduced to SVP. 
if m = c*log(n) then it can be reduced to poly time dynamic programming. 
I think m = log(q) is our security parameter
so we must have c1*log(n) < m < c2*n, or rather w(polylog(n)) ~ m ~ o(n)
this suggests something like m = n^2 is hard, but notice density n/m = n is far from 1.

another source says
m = O(n^2) can be attacked by SVP
m = O(log(n)) even O(log(n)^2) can be attacked by dynamic programming (referring to previous source)
this work attacks when m = n^e for some e < 1. or rather n = m^d for d > 1.
it solves this in time 2^O(n^e/log(n))
so having n a power of d is gone is not safe

what I'd like is just n = c*m for a small constant c > 1. 
then the algo above works with
m = (mc)^e => log(m) = e log(mc)
e = log(m)/log(cm) = 1/(log(c)/log(m) + 1)
and exp time is (cm)^{log(m)/log(cm)} / log_k(cm)
taking natural log we get exp time (log(m)/log(cm))*log(cm) - log(log_k(cm) = log(m) - log(log_k(cm))
exponentiating again we get exp time m/log_k(cm)
so our security param is no longer m but now has a log factor on bottom depending on c
even with the benefits of c=1 and natural log, for security 80 we need like m = 500, which is worse than elliptic curve numbers (but maybes still worth post quantum security). keep in mind how such a large field blows up the gate number since we need to do bit-spreading for subset sum. 

but note that just as we are taking our constants into account, we must take the running time constants of their algo into account.
also regarding constants, we shouldn't account for quantum algorithm until they are developed and set our constants only to account for current algorithms. 
if a large field is really too costly in terms of gates, we could use SIS hash and generate multiple hash values and I think pass them to a random multivariate, degree 1 poly, ie take a fixed random linear combination of them (remember collisions are ok) or use some other way of merging them while preserving randomenss. 



note we can use UOWHF for application where instead of regular hash tree, the hash function is selected at random from a family based on what is being hashed, ie we're just extending the hash, but this requires PSF to go from hash content to hash params. 


we assume that exp time algos will always be the best available for our problem distribution. but as algos get better we may have to change our security parameter. 



Given a bit commitment, we can perform any bit operation on it
for 0 -> a and 1 -> b on x perform (1 - x)a + xb
the only non-trivial such operation is negation

Now we'd like to consider what we can do on two different commits.
suppose on the first we send 0 -> a1 and 1 -> b1, and on the second we send 0 -> a2 and 1 -> b2, then we add them.
in general, the most we can do on two n commits is multiply each by a scalar then add another constant commit, eg affine.
so given two commits, suppose we adjust each one arbitrarily as we like, then we add them.

for a given boolean operation we need to find a matrix M of the form
[
	s1 s2 0  0
	s3 0  0  s4
	0  s5 s6 0
	0  s7 0  s8
]
such that for input vector [a1,a2,b1,b2] we get a desired boolean output.
I don't think any nontrivial operation is possible. for example, AND below has no solution
AND 
a1 + a2 = 0
a1 + b2 = 0
b1 + a2 = 0
b1 + b2 = 1
--
actually the only seemingly non-trivial operations are basically selecting one of the 4 bits, eg to select a1
b1 = b2 = a2 = q/2
a1 = q/2 + 1
--
a1 + a2 = 1
a1 + b2 = 1
b1 + a2 = 0
b1 + b2 = 0
--
but in fact this is trivial as its just projecting onto the first vector, cancelling the second.


suppose we switch modulus between proofs. so a commit ends in some state such that when switched to a particular new modulus reduces to a partiuclar 0,1 in it. this means a 1 should be an intended in position 1, while an intended 0 should be in a position congruent to the new modulus q'.


in the case of d-values, we need to modulo-out the d*{0,1} side-effect commitment. suppose our modulus from the beginning has d as a divisor. d*k = q. then we multiply by k. the side-effect commitment goes away, but now the new commitment is multiplied by k (and k has no inverse). getting rid of it means switching modulus to q' such that gcd(k,q') = 1.
suppose d = 2. need gcd(q/2,q') = 1.
	try q' = q - 2 (to remain even). gcd(q/2, q - 2) = gcd(q/2, (q - 2) - q/2) = gcd(q/2, q/2 - 2) = gcd(q/2 - (q/2 - 2), q/2 - 2) = gcd(2, q/2 - 2). now suppose q/2 is odd. then q/2 - 2*n is odd so we continue with gcd(2, q/2 - 2*n) until q/2 - 2*n = 1, then gcd(2,1) = 1. so this works iff q/2 is odd. but unfortunately it terminates next round. q'/2 = (q - 2)/2 = q/2 - 1 = odd - odd = even.
try for arbitrary d. need gcd(q/d,q') = 1. if we are to repeat this for same d, then we will need d to divide q', and gcd(q'/d,q'') = 1. 
	...d*gcd(q/d,q') = gcd(q,d*q')
	...d*gcd(q'/d,q'') = gcd(q',d*q'')
I now realize we could do d=2 with q' = q +- 4. gcd(q/2, q+4) = gcd(q/2, q+4 - q/2) = gcd(q/2, q+4 - q) = gcd(q/2, 4) = gcd(q/2-4, 4) = gcd(q/2-4*n, 4) = ... gcd(odd, 4). 4 won't divide odd, and 2 won't either, so only 1 will. then q'/2 = (q + 4)/2 = q/2 + 2 = odd + even = odd. So we can repeat. this allows us to do XOR. 


i imagine modulus switching could give a solution to doing operations on the commitment without help from prover (though we haven't ruled out help from prover). but we assume every circuit will have a fixed modulus, so switching modulus at all means switching modulus between proofs. this brings us back to the concept of switching between fields/rings aka environments (envs). we need to carefully analyze what comptabilities must exist between the prover and verifier environments.

suppose we're using constraint-based sumcheck. in this case we have a transcript poly and code polys all consisting of elements from some env, variables, and the constants 0 and 1. we want to prove the composition poly evaluates to 0 over a set of all combinations of 0 and 1. To do this we treat the evaluations as coefs of a multilinear poly evaluated at a random point using sumcheck. the random values chosen for this must be in the verifier env.

for unique poly testing, verification is best done in a field p, and indeed I think this can be done while still allowing computations with variable modulus. We would like to allow a custom modulus q for each constraint. Our transcript is in env q. So for a given constraint suppose we want to confirm that v1*v2 = v3 \mod q. This holds iff v1*v2 = m*q + v3 with 0 <= m <= q-2. If we were in env q then we'd reduce modulo 0 and get mq + v3 - v3 = mq = 0. But we are in env prime p. So if we subtract we get mq. We want to make sure this quantity is of the form mq for known q and unknown 0 <= m <= q-2. So we consider the range 0 <= mq <= q(q - 2), and only (q - 1) values within this range are valid out of (q(q - 2) + 1). This is roughly 1/q of the values. But if we reduce this sparse range modulo p it results in a dense range. One possibility that I don't think allows cheating is having an additional side-transcript with the integers in [0,q-2] to specify the various m values. so we'd do
(v1*v2 - v3)q^{-1} - m

suppose we figure out modulus switching. it would have to operate on whole commitments at a time and do so component wise. 

we will not be able to treat commited values as field coefs and perform regular poly techniques simply because the coefs will be from too small a field so for soundness we'd have to apply the techniques too many times. 


think about base change (base as in the constants used for commitment). upon choosing a random point at which to evaluate the poly, one can infer the new base. supposing we have two polys to evaluate at the same point, the problem of evaluating them is reduced to making sure they have a commitment opening for the new base. note that computing the new base only involes the old base (a standard), and the random point (little data), so the verifier can compute a commitment to the new base. 
now we'd like to reduce verifying two openings to verifying one opening. suppose we can do modular addition on the commitments. we'd like to prove that one knows an opening to the sum iff one knows openings to both commits summed.
i realize this won't work if we use large random evaluations since the commit to the new basis will involve those with exceed the bound.
but i'm thinking about how to efficiently increase soundness if we reduce the random values to d-values. 
if we do multilinear poly then a single point of evaluation should have sufficient soundness (the larger the commit the greater the soundness). but the new basis depends on the point of evaluation. 


our main trouble now is how to randomize the addition of two polys so they don't cancel. the simplest way, though costly with DLP and seemingly impossible with SIS, is to multiply one by a large random value first. is there any other way. if we leave the random value as a variable we could build up a multilinear poly but it would not be constant in size. if we multiply by a random d-value in our d-value field the problem is the soundness is only 1/d.
what if we ask for multiple random points what both polys evaluate to. Then we generate a random scalar and scale as normal. Then I think if we test each of the n asked points we get soundness 1/d^n despite only the single small random scalar. At this point we have many claims on the new poly. Then we could go through and reduce all these evaluations to one. oh but reducing two to one also has the same weak soundness. So the latter step doesn't help. However, I think it can still be used to reduce evaluating two polys at two independent sets of points to a single set of points. Well actually maybe not, cuz every application would involve testing the line-join at multiple points, and each line would be different so the number of points would not be constant. eg consider two polys with tw points to test each. we'd make two lines, and for each we must test 2 points, so the number of points doubles to 4. from this I think I realize for the first time how important it is that the field size is large so that we don't need to perform multiple tests. 
well actually suppose for each line we only did one evaluation. by itself thats not enough soundness but keep in mind we're doing multiple lines and they all must pass. so if we have two polys each with n points, suppose we connect each with another to make n lines (maybe some convention about how to match them), then for each line we evaluate once, ending up with n common points to evaluate these two polys. thus we have successfully reduced two independent sets to one set with soundness I think 1/d^n. 
But one problem I'm now worrying about is that we can only have multilinear polys of size at most 2^d (if univariate then only size d). One possible way around this is extending to general multilinear polys, were allowing individual degree at most m allows size (1 + m)^d. This increases the total degree of the poly. The limit would be something like m = d. we need to make sure the zippel lemma holds for this when... lol zippel says soundness is degree/d so if we want this < 1 we need d' = degree < d. so how many points can we encode with degree d'? for allowing all monomials we could do multilinear with 2^{d'}, but that would be too small. only something like d'=20 would be reasonable but in that case we have d like 30 or 40. for v variables of degree at most d' it can encode (d'+1)^v points if we allow all monomials. but obviously v is bounded by d because otherwise at least 2 variables will always take the same value. Does this mean we can validly encode (d' + 1)^d points using d variables of degree at most d' and the zippel lemma still holds with d'/d? If so, this would be sufficient. 

try to encode large cyclic group via small cyclic groups. we saw the direct product doesn't work. what about a subgroup of linear maps where the group operation is composition? Oh, but we would need the group to be abelian so most linear groups wouldn't work. Memember we want a finite abelian group, which is isomorphic to product of prime powered groups. What about an elliptic curves? remeber they have about the same order as the field. If we use direct products we need to make sure they are coprime. To reduce our field size we could still encode the q as a direct product of all prime cyclic groups up until q is large enough. I think largest cyclic group will be no larger than 100. If we can implement the appropriate arithmetic then our field only needs to have size < 100. Maybe there are other representations of finite abelian groups via small cyclic groups. Eg, the multiplicative group of an extension field. it has size p^m - 1 for some prime p and power m. and apparently this group is always cyclic! only trouble is implementing the poly modulation, but actually this may be easier than what we tried before. of course p would be d. then we just need a way to compute an iso between modulo q and this representation of it. now modulus switching in this context could be tough. it means changing the order of the group, which might be done by changing the irreducible poly, but this may be feasible.


I should aim for a more theoretical result. Constant size proof for NP. doubly efficient. hopefully can say enough by subset sum that don't need random oracle model. based on falsifiable assumptions. but this result would be impossible. i think the catch is the soundness deteriorates with length so if we want certain soundness it can only prove certain size statements, so size is actually not constant. but in practice the soundness shouldn't deteriorate. 



lets make sure we can implement modulus switcing on the general group problem in order to do addition modulo d. note if we can do addition modulo d we can do multiplication modulo d by repeated addition.
suppose n1 and n2 are two scalars in [0,d-1].
n1*g + n2*g = (n1 + n2)*g = (n1 + n2)_d*g + d*{0,1}*g.
we want to shave off the right term. this means we want the d'th power of g to be congruent to 0. But that is not the case already because the power of g can be anything up to the order of G which is much larger than d. So we need to operate on the value.
suppose we scale by s such that s*d = #G, such that the right term disappears. but then the left term becomes s*(n1 + n2)_d*g. we will need to switch to a modulus such that s is coprime with the new modulus. then we multiply by s^{-1} to get s^{-1}*s*(n1 + n2)_d*g.
By modulus we actually mean group order, so what must the new group order be such that s*d = #G and gcd(s, #G') = 1. Before we had d=2, and we chose #G' = #G +- 4. To understand this pattern suppose we try d=3. Then s = #G/3. gcd(#G/3, #G') = 

we need to make sure modulus switching is ok in terms of the constants. just because s^{-1} doesn't exist doesn't I think mean it can't be performed via an affine function. after multiplying by s we have put our d-values into a subgroup of size d. i don't think there is any way to map them out except by changing the group (ie modulus switching). So what's the problem with changing the modulus? 



first lets try the simpler technique of relying on the prover to provide the overflow commit.
well clearly the prover could provide anything such that after subtraction the result is a valid commit. thus just accepting commit from the prover is not safe. what if we could force the prover only submit a 0,1 commit. Suppose what should be a 0 is a 1. overflow should not have occured but the prover said it did. this results subtracting d from a value < d which results in underflow. likewise an opposite error results in overflow. now the verifier holds a commit calculated from an invalid opening, and we'd like to show the prover doesn't know a valid opening. suppose we set up the equation, transfer the underflow to the other side so now both side have at most overflow, no underflow. further, the overflow is at most to 2d-1 on both sides. now since overflow and underflow can't occur at the same place, both sides of the equation are different. if we choose 2d-1 to be in our bound then we have found a collision. 
but how would we verify the commit to be 0 and 1? we could process it with a sumcheck at the cost of having to evaluate the poly at the end. note that evaluating the poly doesn't assume it contains only 0 and 1 coefs so our argument isn't circular. but this by itself is not helpful because it just generates another poly to evaluate. every reduction of evaluating two polys to one requires (more than) one addition, so if each addition introduces a new poly we achieve nothing. is there some way to batch? could the prover encode multiple overflow polys as one? I think this works for doing multiplication in a single round where the commit it can actually be any valid commit, but each addition requires its own overflow commit. so batching would need to happen at the verification level. given a set of commit one wants to verify that for each one there exists a set of 0s and 1s such that the dot products with the constants result in these commits. well if there exists a valid commit then it can be split up into at most d different subcommits each consisting of 0s and 1s. but the problem is we don't just need to verify the sum of the commits but each commit for itself, and knowing a sum opening doesn't imply knowing the individual openings. 
with modulus switching we could multiply by q/2 to set everything to q/2 or 1, but then we'd need to change modulus.


note that for poly commitments finding collisions is not enough to cheat. a prover must find a collision that evaluates at a random point to a particular value. we can think of this as finding satisfying a dot product with the random value monomials. note that these random monomials are given after the first poly is determined, so it not collision resistance but 2nd pre-image resistance. we could add an additional bit of security for this if the monomials were uniformly distributed but unfortunately they are only pairwise independent. but remember we will be evaluating multiple points, so these would add multiple security bits. this is relevant to universal hash functions but the the hash and what defines a collision isn't fully defined until the random points are chosen which occurs after the initial poly commit. 


what if prover just gives normalized commit, and verifier multiplies by s and compares with unnormalized commit? i suppose this won't work because the prover can give a value that is not normalized but it will still scale to be the same. the s values 0,d,2d,3d,...,(s-1)d all map to 0 when multiplied by s. so the prover can give the correct answer, or any d-offset of it and all will pass. Now whatever the prover submits, it will be interpreted as the final commit for which the prover is expected to known an opening. So suppose the prover knows of an opening x such that
answer = S(F + G)_d + n*d = Sx
n*d = S(x - (F + G)_d)
now n*d for all n is a set of s numbers that are fixed from the beginning, even before S is chosen. now it may not even be that there exists such an opening for these numbers. well actually there are s such values and the domain size is s*d so with probability as high as 1/d something will hash to this, so the prover only needs to try like d values before it works.
i don't think modulus switching will work because the entries affect the translation of constants between the moduli. 


Consider decomposing the addition H' into the two parts that correspond to indices that overflow (R,r) and those that don't (L,l).
F' = SF
G' = SG
H' = (F' + G') = S(F + G) = SlL + Sr(R + d*[1]) = SlL + SrR + Sd[0,1]
what we want to compute i s SlL + SrR
suppose prover provides S_r[1], that is just the sum of all constants at indices that overflow. if this is provided correctly, the verifier can multiply it by d, subtract it from the sum, then get the right answer. but this is a 01 commit which we'd have to verify as such, and we already saw (on iphone notes) that that comes out at net 0. 

if the prover submits the claimed correct answer and the verifier can confirm via manually opening it that an answer is correct, then we can replace the manual opening with sumcheck. 


we need a batch way to verify openings. 



what about making for a single proof composed of a lot of small proofs an amortized evaluation or opening at the end. 

W = AY rxn
Z = SC + Y 
AZ = ASC + AY = TC + W

Z = SC 
AZ = ASC = TC


v is length of commitment. l is how many commits to prove. 
n is about security parameter, which is about size of regular commit (except is 01).
challenge with be of size l*n
response will be of size v*n
so this only amortizes when the number to prove (l) begins to exceed the security parameter, eg > 80.

in our general model, the only foreign data send from offthread to onthread are commitments, and only data sent from onthread to offthread are challenges. we distringuish between foreign data and derived data. foreign data is assume to have high entropy. 
this is our model as we can prove its safe given certain conditions satisfied, and we can describe the two implementations using DLP and lattices. model can also be independent of computation model mostly, but we might depend a large amount on being able to reduce evaluating a poly at two points to evaluating at one point, but just like reducing two polys to one we discovered to be a special case, so may this be. so multilinears and related computation models may be special cases, eg sumcheck. 
can describe as oriented towards one long indefinitely long computation, as opposed to a circuit or set of constraints. even in distributed environment it can be considered one large ongoing computation. 
proof size between parties will be the 'base size'. 
call them sagaproofs. but how are these different from streaming, where space compleity is important. 
present independnetly the theme that both models share, eg polynomial based
abstract model can have zk as last step between parties
note how sagas can be forked and also joined. they form a DAG. 
use the word 'recursive' justified by how we use prepare to use an answer before we know what it is. 


note that we could reduce polys to a standard size. then we have more polys but of smaller size, so amortization savings goes up but commitment savings in compression goes down. 

GKR model can be thought of as splitting the witness into layers so only the top layer has the original data and only the top layer needs to be evaluated. would there be any faster way to give a source map of derived values back to the original data other than layering through the intermediate values? 
in general note the relationship of witness size to depth. depending on the uniformity and depth of the computation I think there is a linear relationship to show. many models, like the univariate ones, are at one end of the spectrum with everything in the witness, because they are made to take advantage of particular ways of processing large witnesses, eg pairings and low degree testing. 


I thought at the most basic level we need a way to reduce eval of 2 polys to eval of 1, but I think there is a more basic level. we need a way to amortize commits so we don't have to manually open them. like we operate on them homomorphically or we batch their openings. we don't need the definition to be reducing to a new commit because as long as we can reduce to manually processing less data, we can then further reduce to processing this new data via a new commit. 
we need a way to reduce manually processing a lot of data to only touching some data. 


still to decide how best to represent data with SIS. but assume we use the amortization technique. assuming we use poly based proof, we need to reduce evaluation of polys to openings. one way is changing the basis so an opening for a commit under the new basis corresponds to an evaluation of the polys at a particular random point. another possibility is using the committed values to build up larger coefficients and always identifying the commit with the corresponding poly. but how to reduce this to openings? idk
another possibility is using modulo arithmetic, but here the eval points would be in a small field so we'd do multiple points. to do the arithmetic we would need to verify openings to valid side-effect commits (we'd combine the 01 commits for polys into regular commits). the extractor needing a double bound I think is ok because we already need that for the amortization proof. 


for crypto require all algos including attack time poly in security param (?), and advantage negligible in security param. 

a function family has one input, a key, that defines the family, and turns it into a regular function. 



SIS concerns a large finite additive group. given uniformly random elements find a 'short', non-trivial, integer combination (positive and negative and identity powers) that sum to the identity. 
usually parameterized by n,q,\beta,m. 
can be thought of as SVP problem on q-arty m-dimentional lattice.


i think application computation would benefit greatly from a small field, especially to encode a lot of 0s and 1s. this probably means using the modular technique is more efficient. for fiat-shamir, we split the few coefs into bits and get something out of density 1 (make field large enough that there are enough bits for security). then we pass it in again (cuz of density 1), get something out and get about as much out as the amount of coefs that went in (density 1). these coefs are our multiple evaluation points. 
for sumcheck I think we just evaluate on random polys with as many coefs as we want iterations. we don't even need to evaluate the polys themselves, it just replacing a random point with a random poly. I can prove this works I think. Though it gives prover more work, poly should be multilinear to reduce degree. in this case the number of random points to evaluate will be a power of 2. suppose we have base soundness 2. then say we want to go to 2^128 so we evaluate at a poly with 128 coefs. that is, a log(128) = 7 variable multilinear poly.
suppose we want field size 32. then for soundness 1/2 we want poly degree max 16, which is enough for transcript size. now for total soundness 1/2^128 we need to evaluate at a poly with 128 coefs, that is a log(128) = 7 degree multilinear poly. 
we can try to make the model map-reduce such that we always have degree 2 in the sumcheck. 
now since over a small field we would need to replace the random combination of constraints with polys. this increases the degree of sumcheck polys. we can avoid the need to do this if we adopt a fully map-reduce style, were a sumcheck doesn't check a series of constraints but rather computes the value of a derived poly at a random point. at the end of a proof the verifier first wants the raw data about the claim, and wants to convert this claim into the evaluation of one or more polys at random points. if the claim was a single value (maybe possible, eg the hash of the real claim) then it could be the output of a sumcheck. but suppose we have a claim consisting of multiple values. well we could suppose that singletons can come from reductions, or groups from mappings. for a singleton use the sumcehck. for a group, we are given the resulting poly. we'd like a sumcheck that results in a poly rather than a single value. we could do this by doing a sumcheck over polys rather than values. or maybe best is that groups of elements come from transcripts, so for every transcript we reduce all the points of evaluation of that transcript to a single point for it. remember we can reduce multiple points at once with a higher degree connecting poly. 

maybe we use the constraint method with the basis change method over large field, and the map-reduce method for the modular arithmetic over small field. 

evaluate on poly
lattice params are hard to select, depend on hermite factor, and larger params seem necessary in practice than theory


reduce c, see 8

carbonine

you have an 2 sets of inputs into this hash that give this output iff you have a set of inputs that give this related output in this related hash


ab+c
000 -> 0
001 -> 1
010 -> 0
011 -> 1
100 -> 0
101 -> 1
110 -> 1
111 -> 1


we can compute anything with data, so we can compute any variant of a commit as long as we are yet to evaluate the data in the commit. 



use the placement of elements in a large hash in order to encode the information. have an encoding such that two valid encodings can be added
ask for a new valid commit. use the data from that to backwards compute the verifier computed invalid commit and see if they match. 

what about sumcheck with a single data poly at a single spot, such that verifier can infer the necessary eval result. suppose final poly from prover is incorrect. its supposed to equal the subject poly with a single variable. if its wrong, it will be spotted when identity testing. what's different here is, depending on the random point the verifier chooses for identity testing, the expected output of the data poly will be inferred without the help of the prover. I am wondering if this may be a substitute to randomizing a claim before adding, and simply adding. oh I think cheating can be done since after the exepcted evaluation points are determined the prover knows the sum poly that will be evaluated, and can split that sum into any two polys that respectively satisfy the expected evaluations. then the rest will go fine for the prover.
we are hitting the information theoretical ceiling and I don't think we can get much further without non-trivial crypto assumptions. 


since we have to use the commit values, and they represent sums, it seems the only appropriate manipulation of them still pertaining to commits is linear combinations of them. 


since we can do manipulations of a commit, suppose we do a random suffling of one. given the new commit of the supposed suffle, the verifier can make sure it's correct, and reduce evaluating the original vector to evaluating the shuffled vector. again, we're pusing the info theoretic bound. 

i'm wondering why its easy to calculate new commit with DLP but not with SIS. maybe the amount of buildup before amortization makes sense is a measure of complexity. maybe this is cuz SIS is NP-complete, like subset sum. if two could be reduced probabilistically to one then verifying two NP statements could be reduced to verifying one of the same size. what our amortization says is that we can only reduce a collection NP statements to a smaller collection when the original collection exceeds some threshold. 
notice how our use of subset sum (SIS) means we are reducing any NP satement probabilistically to subset sum. but subset sum is in NP so it can in turn be reduced to other NP complete problems. note how DLP is not NP-complete like subset sum and that's probably why two instances are easily reduced to another. 



maybe provers only need to keep track of two polys to merged. in small field one they will be randomized many times then added, and it is all these commits and all the modular-side effect commits the verfier will hold. naturall we'll have about as many randomizations as the security parameter so after this they will be amortized and so the threashold will be for two polys. so between parties we only need to pass two polys (instead of as many as the security parameter). actually its only the side-effect polys that will be amortized until the fixed constants. but these side-effect polys can be obtained from the two original polys by the prover so they need not be transported. 



an important concept is saga proofs is the entropy of foreign data. we know we can do transformations on the data at no cost. but 'merging' or 'getting rid of' data while preserving soundness is what's hard. we are looking to see if different data satisfy a property with respect to the commits. we want to reduce this to testing the property of less than the sum of both. so we combine the commits but the data could be made so that neither have the property, while the combination of them does. thus the combination of them must be randomized in order to make the data independent from each other. well now I'm wondering if we could achieve independence in another way, where we take many commits and the randomization is which we combine with which. 
in the case of polys, we're considering the property of evaluation at a particular point. or rather whether an opening exists. i want to say that if instances are independent then with high probability, if either is invalid then their sum is invalid, but that is not true for 01 entries because 1+1=2 (valid) is about as likely as 0+2=2 (invalid). but if the sum of the two are valid, I think we can say the two inputs are not too far off. we ask for a renormalized commit, and then use it to check that it represents in 1-1 the merged commits. now this new commit contains the same about of information as the merged commit, but less info than the original sum of both data sizes. 

for maximum information in a single vector using euclidean norm, we could allow any euclidean norm including negative values. and note that since beta < q and the square of additive inverses is a it should be, we can add them add them up after squaring, and keeping track of overflow all along we verify it satisfies the bound. but adding won't be ok because at the extreme two vectors could be nearly inverses of each other and both way too big, but their sum will be small. 

another way is we encode d values but 2(d-1) is still safe. then we check if the result has values less than or equal to 2(d-1). for 01 vectors, the only way to cheat is to encode 2 values and hope the line up with 0 values. maybe we could have 2 values be benign for the verifier. if a 2 value is encoded, the prover may fail, but if not, it will simply be translated into the wrong value for computation.

if we just use 01 coordinates we are vector is always pointing at a side of the hypercube. 

remember verifier can negate (or generally shift) a vector without prover help.


suppose we use the method from the paper and the parameters work out. how do we do zk? suppose the last proof has finished. one way would be to put an additional proof at the end that transforms (no merging) each commit to evaluate into a related poly to evaluate with same soundness but zk, probably via masking poly, or maybe via random generator. this would be a general method for any system. the other possibility of doing the masking offthread seems to depend more on the commitment scheme. 
for blinding the polys I think can just generate pseudo random polys, figure out the eval of that poly at the random point, add that to the expected eval answer to get the new answer. somehow must compute the commit to the added polys. 

maybe have number base system where a number can be represented in bits or up to 2, and adding bit representations of two numbers results in the 2 representation of the sum. if the 2 rep is easily computable from the bit rep, then one can be given two bit reps, add the commits, ask for the bit rep of the sum, then compute on it to verify it represents the same number as the 2 rep of the committed sum. 


we will suppose the zi are positive, so that upon subtraction we get positive and negative but still with bound B instead of 2B. this is ok because this is only the theoretical reduction that uses negative values. assuming positive values shouldn't have cost on the reduction because if we assume prover can do it we can use that ability in the reduction just like the other. 


SWIFFT paper says subset sum from 1024 to 512 bits is intractiable.
note I think to compress output of subset sum we could add up bits


make amortization l equal to the field size so when multiplying by challenge, responses are already in correct poly form, and thus commitments to them need not be sent to verifier, beause verifier calculates them by challenge and old commits. then the verifier doesn't need to calculate using this data, only to check that is a valid opening. but we started with needing a valid opening to bit vectors but now we'll need valid opening to field vectors. but I think we can just reduce field vectors to bit vectors with the amortization still making sense.




I'd like prove my own amortization method. without zk. suppose the commits are bit vectors. 
the idea is we need randomness to operate on the commits, but only operations we can do on commits that result in almost-valid commits are addition of commits. therefore to have enough randomness we need enough commits and we will randomly decide which commits to add together. we will add them together in groups, maybe a commit appearing in multiple groups, such that the number of total commits is as few as possible for sufficient soundness.
suppose we have l commits and we make k groups completely at random. we can do this with a random challenge matrix C of dimensions l*k. 
ASC = TC
suppose A, S, and T are fixed. we'd like to say that if prover P has noticible probability e of passing the test with probability over C and its own randomness, then there is a ppt extractor that can use P to extract semi-preimges for all T. 
consider two random combinations and suppose we subtract one from the other. any commits that appear in both will disappear.
suppose we have two combinations that differ only in a single bit. and suppose there are valid z',z'' that open them. then subtraction gives the right side just t_i for differing bit i, and thus z'-z'' will give a semi-preimge to t_i. So a goal could be to obtain solutions z_i' and z_i'' for any two challenge vectors that differ only in bit i. the way the other paper does it is extractor E samples challenges until it finds one, as its exected in do in time 1/e. then it samples only the i'th challenge again until it aborts or finds one for which it succeeds that is different than the first i'th challenge. 

just consider a single challenge c. to pass all l challenges with noticible probability e, it must pass an individual challenge with probability e^{1/l} > e. what is probability we can sample two successful challenges that differ in a single bit, in particular bit i? well it takes expected 1/e^{1/l} time to generate a successful challenge. 
consider the space of all possibilities for success which consists of all possible challenges and all possible randomness. at least e^{1/l} of this instances are successful instances. we can put this space in matrix form with two columns, one for each possible bit i. an entry is filled if the instance is successful. we'd like to know how many rows have 1 in both columns. then we want to know the probability the extractor can sample one of these rows. 
the matrix contains e^{1/l} filled entries. 
suppose there is k random bits. there are 2^{k + n-1} rows and 2 columns. we say a row is heavy if at least a e^{1/l}/2 fraction of the entries are filled, ie it contains >= 2 * e^{1/l}/2 filled entries. now an e^{1/l} fraction of all entries in the matrix are filled. we have the lemma that at least half the filled entries in the entire matrix appear in heavy rows. thus e^{1/l}/2 of the filled entries are in rows containing 2 * e^{1/l}/2 filled entries. suppose we want heavy rows to contain at least two filled entries. 


try again
supposedly out of all possibilities of challenges and prover randomness, an e fraction of possibilities are success.
e > 2^{-s}
if we are going to use the lemma, we need the number of 1s in a heavy row to be at least 2. if there are j columns then
j e/2 = 2 => e = 4/j => 2^{-s} = 4/j => -s = log(4)-log(j) => log(j) = 2 + s => j = 2^{2 + s}
the strategy they use is to find any 1 in expected time 1/e. then to search for another 1 in the same row. if we set the abort time appropriately, then if the row is not heavy (impossible to find another 1 in the row), we will abort, while if the row is heavy (has at least one other 1) then we will probably find it before we abort. 
there are e 1s in the whole matrix and at least e/2 half them appear in heavy rows. this means that when we find a solution in a row, with probability 1/2 we are in a heavy row. 
how efficiently can one find a second 1 if the row is heavy? there are j samples in the row, >= 2 of them are 1s. but one of those 1s is the solution already find. so only >= 1 of the samples will be a valid solution. so there are j = 2^{2 + s} samples, and only >= 1 of them is valid. if the prover were to sample the columns of the rows directly, the probability of sampling the valid one is (1/4)2^{-s} < (1/4)e so this wouldn't help. 
but if there are only 2 valid samples as we assumed above then j e/2 = 2 => e = 4 / 2^{2 + s}  = 2^{-s} which is worst case. instead we need to keep in mind that e can be much more than 2^{-s}. so there are not just at least 2, but at least j e/2 > 2^{2 + s} 2^{-s}/2 = 2 valid samples, but we don't substitue for e, but instead keep 2^{2 + s} e/2 = e2^{s+1}. thus after subtracting the already-used solution, there are e2^{s+1}-1 valid solutions remaining. from 2^{2+s} total samples, we have probability (e2^{s+1}-1)/2^{2+s} = e/2 - 1/2^{2+s} probability of landing the second success if we sample uniformly (manually using the prover randomness). so if we iterate v times we fail with probability (1 - e/2 + 1/2^{2+s})^v. choose v and simplify such that this becomes negligible. this is the only constraint needed on v if v will be fixed (maybe a function of s), because if the row is not heavy (which only happens with probability 1/2) it only matters that don't execute too many times, and a fixed number is ok, since we're not expecting a solution anyway.
so we land on a heavy row, we'll succeed with negligible error. if we land on a not-heavy row, we'll abort in reasonable time. both of these scenarios happen with equal probability 1/2. thus if we execute s times we'll fail to hit a heavy row with probability 2^{-s}. 
now we've said all this without saying how this matrix is formed, what the rows and columns are. a bit of a complication is that the matrix represents all possibilities, which accounts for all possible randomizations of P. but upon an implementation P will have fixed randomness. oh, but I suppose we use P only to find the first solution, so its used as a subroutine over and over (I assume we have unlimited randomness to invoke it so many times, giving fresh randomness each time). what we have so far is a general method of using an ability to cheat to find two 1s in the same row to yield a solution. maybe we could extend to finding more than two 1s in a row. 
we could reduce the number of columns at the cost of requiring a larger gap between e and 2^{-s} but i don't think that's considered safe.
does it matter what ratio the dimensions of the matrix have, as long as the columns satisfies the lower bound? no matter how many rows, more than half the 1s will appear in heavy rows, so P will succeed with probability 1/2. No matter how many columns, at least e/2 fraction will be 1s, but will the amount beyond e/2 depend on the number of columns? the absolute amount matters but I don't think the fraction matters.
note that we could say the same about heavy-columns, and landing in an entry located in both a heavy row and a heavy column then happens with probability at least 1/4. 

if we're going to solve SIS without granting slack, we need to find a set of challenges that satisfy an additive equation, eg c1 + c2 = c3. but the problem is nothing prevents us from finding solutions that add to 0. but there are enough solutions that don't add to zero that we could maybe probabilistically sample until we find some.
suppose we start with 
c1 = ab
then we find c2 = Ab and c3 = aB
aB + Ab = (a + A)(B + b) = AB + ab
so it appears we'd need to find AB too and thus a square of solutions, not just a triangle. 

But note that if we used the triangle we would find a solution for AB = aB + Ab - ab which is closer to random than the initial commit, so this would be almost as legetimate as ISIS. how random is AB? suppose a separate party could distinguish it from uniform. then ...

if we want j*e/2 at least h then j e/2 = h => j = 2h/e = 2h2^s = 2^{s + 1 + log(h)} so the number of columns we need only grows logarithmically with the number of 1s in each row. the fraction of e/2 remains the same but now there is a higher lower bound in each row. so there are >= h in each row and we have a probability of e/2 - u*2^j of hitting one having already hit u of them. for any constant u we could do this, even if u is a function of s. 
so suppose we can hit heavy rows and columns and extract a constant number of hits from each. what could we do with this ability? this doesn't help us find a square because we can't claim anything about the distribution of the 1s within the matrix. keep in mind the density of 1s doesn't grow. but also note that we are free to permute the matrix columns and rows. for example, suppose we permute such that all heavy rows and columns appear at the top left corner of the matrix. then this submatrix contains half the 1s of the entire matrix. now assuming nothing about the distribution of 1s in the submatrix, maybe we could make a pigeon hole argument about how many squares there are. But remember finding a square is not enough as it may yield a 0 answer.
suppose we could lower bound the number of SIS solutions for our parameter choice. 


just thoughts about multilinear maps for multiplying commits.
f(a1*x1 + a2*x2, b1*y1 + b2*y2) = a1*f(x1, b1*y1 + b2*y2) + a2*f(x2, b1*y1 + b2*y2)
= a1*(b1*f(x1, y1) + b2*f(x1, y2)) + a2*(b1*f(x2, y1) + b2*f(x2, y2))
= a1*b1*f(x1, y1) + a1*b2*f(x1, y2) + a2*b1*f(x2, y1) + a2*b2*f(x2, y2)
f(a*x, b*y) = a*f(x, b*y) = a*b*f(x, y)
what if we make f such that it vanishes on the n^2-n mixed monomials. or even take the special case that we plug in the same value to both inputs of f, so it vanishes on (n^2-n)/2 = n(n-1)/2 values f(xi, xj) i!=j. we'd want it to output 1 when xi=xj. oh but of course we can't do this cuz f will only have 4 interpolation points if its of 2 variables and multilinear.
the number of interpolation points we can do with f variables is 2^v. in general if we do degree d we can interpolate (d+1)^v points. 
we need to take advantage of how multivariate polys can have many more roots than their degree, and we need to figure out what pattern these roots can have. we want roots of the form xi^d1*xj^d2 for i!=j. if we consider bivariate we think of it as a uni poly in x with coefficients as uni polys in y of same degree. actually we should think of it as a uni poly in x but in root form \prod_i (x - coef_i). the coefficients are uni polys in y. so every y poly is a map from the y value to an x value such that f will vanish on that x and y.
the total max number of roots is d^v. the total number of monomials is n^{d*v} I think. 
for bivariate, total roots is d^2. for monomials, total is n^{2d}. so the min number of terms we can leave non-zero is n^{2d} - d^2.
so the basic problem is the number of monomials is exponential in the degree but the number of roots is polynomial in the degree, so the roots can't keep up with the monomials. 
of course composing polys just results in another so that won't help.
actually the about count of roots is the number of custom roots. but i'm looking for patterns. for every value of y there is a set of x roots so that f vanishes, namely the x roots are the outputs of the y polys on that value of y. 
now the problem is that our monomials are composed of constants which have random relationships, so there is no pattern to them. 


notice prover who has knowledge of the small coefs can quickly compute the modulo of two commits by adding the commits, adding the coef vectors, seeing which overflow, then subtracting the relevant constants. Actually prover will just add together the relvant constants and give to the verifier as a commit to the side-effect poly. this, however is still linear, but will probably only take about half the work. for the challenge the prover can compute z quickly, but will probably need to re-represent it as a commit to a 01 vector, unless we want to use it as a data poly which requires more multiplications. 


consider the univariate sumcheck from aurora. prover sends commits to f and h, maybe g. then checking f sums to zero over coset can simplify to making sure a coef of h is zero, which can be done by adding q times the releant constant to the commit. then verifier must evaluate each for identity testing. maybe we could reduce identity testing for multiple groups by making a random linear combination. the addition cost is computing polys squares to explicitly compute g, h. but the benefit is simpler fiat-shamir. maybe just take the commit itself (consisting of multiple q values) and use those q values as random coefs, and construct as many identity tests as these q values (make it the security parameter). reducing identity tests requires addition and multilication so we will support those in the same way. in this case I think we'll require prover resonses to challenge to be encoded as 01 so they stay in the amortization box. 
actually checking the low degree seems like it requires multiplication by q, not addition. hmm, could we still make this work? maybe the verifier expects the coef to be zero, then adds d-1 to it to make it the maximum allowed coef. actually this wouldn't work cuz not matter how much verifier adds, prover an make it to after addition its a valid coef. but verifier will need to check they have valid openings, which is not required in the other model (which only requires 01 openings). another problem I'm realizing now is that we want a small field if we are to do modular arithemtic on commits, but univariates will require large field. so we need to rethink if this simple univariate 'sumcheck' can be helpful. I suppose it might be appropriate for the elliptic curve model. again the cost is quasilinearity, but the benefit is less fiat-shamir which in turn might minimize the number of times a proof verification gets tossed between the fields.
prover submits poly commits. as before, we must take this commits and generate a random value in the other field (can be done with subset sum). then we exponentiate and add. it seems this could indeed simplify fiat-shamir, where we no longer need to consider the intermediate sumcheck values, nor reducing two points to one, both of which require compressions, whereas reducing two polys to one does not. as for verifying g is of low degree, I realize we could just enforce the highest constant used, which will be preserved because we'll add linear combinations of g polys. field size is huge anyway, so it suits this. a proof in field q has commits in field p, and first goes as bits into field q where the subset sum will be used to compute a random q value. then the commit in regular form and q value in bit form will go to field p where exponentiation and addition will happen. that seems enough so thats only 2 tosses, whereas if we were in one field we'd do 1 toss. 
remember this didn't work for QAPs because it would need to be linear. so here too, we will need to actually evaluate at a random point, that is get a claim for a point. now unfortunately to reduce two polys to one we will need to reduce two points to one after all. this still saves the sumcheck sum fiat-shamir. so we will need compressive subset sum. but I think with such a large field size we may indeed be able to compress enough, we will need to compress 2log(transcript_size) elements to like 3 or so. oh also remember QAP didn't work because the polys aren't transcript polys but a different kind. the stark model, however, would work. we still need to figure out how to best represent constraints via vanishing points of a uni poly on a coset. of course the uni poly will be composed of others including the transcript. oh, and our ability to map-reduce won't exist and we can only execute one sumcheck, because the transcript poly will have to be evaluated at multiple pionts, and if we reduce those points to one point we shift into multivariate polys which can't be shifted back to univariates. one round makes sense too because we can't use the univariate sumcheck to evaluate the transcript because the output must be zero, so its suited for constraint checking anyway. as a result our transcript will not just contain original data but the whole trace.
now as for how to encode the circuit in the uni sumcheck. note we can use multiple virtual polys and as long as we evaluate them at the same place it can reduce to evaluating a large one at one one place. what aurora does is basically QAP structure, and uses sumcheck to check the linear relations. 
oh one important observation is the univariate sumcheck relies on summing over cosets, but we are in a large prime field for the ECC version so the additive group has no subgroups and thus no cosets. so I don't think we can do QAP/aurora structure unless we find a way to test the linear relations in a prime field. the other possibility for univariates is stark structure, but I see no benefit to that, and the more non-uniform the computation the more random elements we must select and the more poly reductions we must make. so I think this means we return to GKR style for ECC. but maybe for simpicity due to the large field, ie to avoid doing many sumchecks, we do the constraints based one instead of map-reduce. this means not following the idea of only commiting to initial data. but this can be a benefit to show how other ways, though not optimal, also work. the decision depends on optimising and how difficult it is to do map reduce verification in a circuit. 

and remember in order to have field compatibility we're planning on something like the multiplicative extension field group for the constants, and doing this together with the sublinear knowledge protocol requires that the matrices remain associative. this is indeed the case. 

note that reduce two polys at two points to one point can be done in one step by the prover sending the line connecting the points and the verifier using this to generate both the random point on the line and the random scalar. this is because previously we were going to use the output of the response on the random point to select the scalar, but the output is determined by the random input to the response which is determined by the response, so the scalar might as well also be determined by the response. oh, but note there will be two responses, one for each poly. both must together determine the common random input, but both can also determine the (single) random scalar. 

maybe we can justify properties of subset sum by SIS with n=1 and large q. 

regarding turning multivariates back into univariates, the whole idea of replacing two old variables with a connecting expression is flawed because the expression will be evaluated in one place and since its a function it will only yield one value, whereas we need it to yield two values.


Libra
zk is basically the insight that we only need as much blockage as evaluations the verifier sees. 


maybe do like lambda calculus, where our gates don't take two inputs but only one input and combine it with something already there. so for eg multiplication, we first compute the vector containing all left-inputs to the table. this is just a copy operation. we could even have it multiplying a constant vector. whats important is that they multiply component wise. then to get the final values we multiply this resulting component-wise by the initial vector on its right-inputs. in this form our wiring predicates will be multiplying the initial vector in both cases, and in both cases will only be with 2*s inputs, not 3*s. in fact the constants vector could be outside the sum. we could also have an added constants vector.

can we do recursive programming on a vector? we could indeed make new values from existing values, but at the end we'd have to evaluate at a random point which is itself, so circular. but maybe we could do something like have a single very long poly and evaluate at a random point partially. the goal would be depth-invariant verifier time but I think the only way to do that is by copying. 
another goal worth changing for is convenient code representation and organization. library function support would be great. of course variable length data, maybe using commits as pointers, would be great too.
but of course recursion is still desireable and I don't think regular circuits can do it, and we will rely on our code organization to make it possible.

what if a prover was implemented in a proof? then if we could verify the proof its contained it, we could know that the prover didn't cheat in proving its own proof. 

for zk I think we use mask poly and reduce to both data poly and mask poly evaluations, but then we can combine them into a single poly and that is the poly sent to the next prover. now do we actually have to send a poly? i think so, because otherwise not just the prover but the proof would need to be convinced with a sublinear argument. maybe the overehead of sending will be the same for ecc and lattice because ecc is only one poly but its huge, about the size of security parameter, whereas lattice is as many small polys as the security parameter. well actually for ecc we might be able to make the poly of small coefs. 


for SIS we can find regular solution to SIS in q. then we translate to a solution in the interval [-q/2,q/2]. then we move the negatives to another side, and get a collision with positive infinity norm q/2. thus we need positive infinity norm < q/2. 

well it looks like highly compressive infinity norms won't work. we can still use infinty norms with a hash tree, or we use beta norm and compressive using position information. but the question is how to reduce claims about evaluations to claims about opening. the only valid idea I had was modulo using infinity norm, and using opening to prove valid over-flow vectors. 
how else could we manipulate commits only relying on valid openings, not considering them as polys?

i think we should define sagas as reducing a 'claim about the external data', and merging these claims. its not enough to just say 'the claim exists' and I think its maybe more than enough to say 'the data exists and it has a pattern' cuz we say for overflow vectors we only need existence. So in general its just a claim. Now it happens that with a commitment scheme we will also need the prover to have knowledge about the data, so the claim about the data should include the specific condition that the prover has knowledge of that data. 
in the poly case, our claim is the prover has knowlege of a vector of coefficients satisfying the commit, they they evaluate as a poly at a particular place to a particular value. for lattices we can reduce this claim to the claim about knowledge of 01 commits. 

suppose we can do params for SIS hash so we get enough compression with bits. what can we do with that? the previous idea of transforming the base then testing for an opening doesn't apply like I thought because the opening would consist of the random powers which are large.
in fact, in order to apply the amortization we need it to be hard for about infinity form of more than the security size. suppose we work out the parameters for this, which I think should work asymptotically. then we might as well have all our basic commits with this norm which will be high enough for our purposes to use for modulo. so I think we can construct a proof system that works asymptotically. 


for SIS commits, what if we do a transformation of an input to one under a different base, perhaps even newly randomly generated by the verifier. then the verifier is only left to verify under the new base. the verifier takes two commits, each from a different base, and we'd like to join them to be under a new common base. suppose we just add them. then the new common base could be the addition of the two previous bases, which remains random. this new format might help us reduce some of the data. but now i'm struggling as to how we'd do that. maybe continually transform this combined data into a form such that the verifier can efficiently reduce the data by itself. 


i think we just go ahead with the impractical but asymptotically meaningful lattice construction. 

we need a proof of the root-relation-intractability of subset sum or the SIS hash. i think we can reduce the problem to finding inputs for subset sum, high density (not one way necessarily), such that it yields a root to the poly described by the inputs. in reality the poly will be another related poly, but first lets try this. for now lets consider the case of sumcheck where we're considering a low-degree univariate poly. for simplicity lets express our poly in factored form, so the inputs are scalars, some of which are roots. then the goal is to find inputs such that the output is one of the inputs (a root). consider the further simplified problem that all inputs are roots, and output can be any input. maybe even consider the further simplified density 1 problem of finding an input that yields itself as output. notice the representation of input and output is different. what's important about this relation is that its a permutation (identity considering the relationship of the representations). if we can prove for a permutation, lets try to prove for a non-injective relation but one where every target has about the same number of sources. actually we should consider this first because the case for density 1 likely has no solution. 
suppose with noticible probability under the choice of constants and coins, one can find the identity relation as above for k inputs in binary form (subset sum). use this to solve subset sum. I think we'll need to do this by constructing our own constants related to the relation and the instance of subset sum we are given, and hand that new problem over to the subroutine.
consider the equation with the constants and the input in bit form on one side, and the powers 2 and the same bits on the other side which are equal to the answer. then consider the constants for the input that will be the output. subtract the powers of 2 from those constants, yielding new random constants, but still multiplied by the same bits. then a solution to the resulting subset sum (target 0) solves our problem. but for a particular subset sum we may need more randomness, more options, to make sure the noticible probability has opportunity to show. above we only have as many options to change the problem as we have there are inputs (depending on which input is the output), though I suppose we could multiply by random values, but then ratios remains the same. we need more instances that we can take advantage of, and they should be uniformly distributed. lets just go straight to the final goal of unknown poly relation, because that may offer opportunities not seen here with the idenitity relation. 
so there is the invisible poly, then there is the submitted poly, and the relation is the submitted, input poly to the roots of the difference of the two polys. so really the relation is determined by the invisible poly. 
maybe we can replace the powers of 2 with any constants that map the input (a source) to one of its targets. then again we subtract one side from the other, factor out the common inputs, and we have solved the problem for target 0. notice we can do this for any coefs, not just bits. the condition is that for random constants and the set of coefs, its hard to solve the instance. 
the problem is we need the relation to exist before the constants. or else maybe the prover could choose the initial poly such that the relation has a certain relationship to the constants. like the prover could just plug in a poly to the function, see what comes out, then commit to any poly that agrees with the input poly at the point that comes out. maybe we should plug the commit into the hash as well. this might be equivalent to a choosing a random target from the commit. 
or what if we use the commits to pseudo-randomly generate the constants, such that the hash is chosen after the relation is established. the constants would not be totally random, but the ability to solve the instance for pseduo-random constants and not random constants in noticable time would contradict the indistinguishability of the pseduo random number generator. so this argument works as long as we can prove we have a pseduo random generator, and also that if the constants were truly constant and chosen after the commit then the relation is intractable. 



sublinear zk argument paper
p: prime for field of circuit
N: number of gates
\lambda: security parameter
n: number of \Z_p elements in one commit.
q: ambient space for commits.

commit to N values in \Z_p. total size of commits and proof must be sublinear in N. 
hiding matrix is of size r * 2r\log_p(q)
commitment is t = Ar + Bs
so we commit from n elements of p, n\log_2(p) bits, to r\log_2(q) bits. 
set n = poly(r)
communication complexity (total size of commits and proofs) is O(\sqrt{N\lamda\log(N)}).
do this by creating N/n commits, each committing to n values, so total values committed is N/n * n = N. (I think n will be \sqrt{N}).
make a matrix of the \Z_p values (they have 3N total but only N distinct), making a commit for each row (eg N/n rows, each of length n). 
in terms of element of \Z_p, the space for committing the N values will be N/n * r\log_p(q)
they say we can set \log_2(q), r = O(\log_2(N))
they set n = \sqrt{...}

negligible means about 0
overwhelming means about 1
[N] means {0,...,N-1}
use rings \Z or \Z[X]/(X^d + 1), d power of 2
consider norm of poly rings elements as norm of coefs
use quotient ring R_q = R/qR for odd q
norm of element in quotient ring will be norm of unique representative in [-(q-1)/2,(q-1)/2].
operator norm of matrices defined as s1(A). 
they say we can reduce columns of random hiding matrix to make it computationally hiding based on LWE

make sure binding euclidean bound \beta satisfies \beta < q (can justify ignoring this), \log_2(\beta) < 2\sqrt{r\log_2(q)\log_2(\delta)}. they set \beta < N^2p^2, p < N

two sets of constraints: multiplications, linear (additions and multiplications by constants)
embed constraints into poly equation over GF(p^{2k})
then argue for satisfiability of equation
also construct product argument of commits for k commits over GF(p^{2k})

first try embedding into \Z_p then over GF(p^{2k})
N = nm multiplication gates, labelled from 1 to N
inputs and output connected to multiplication gates
mult gates arranged into 3 m*n matrices, A,B,C for left, right, and output wires respectively.
do linear constraints too keep track of wire duplication
also add linear constraints for additions and constant multiplications
somehow transform the latter linear constraints to the form of the former linear constraints so they in terms of entries in A,B,C. 
want to prove entry wise product A.B = C as well as the linear constraints on them. 
can standardize expression of the linear constraints. U such constraints.
in total N + U constraints.

reduce the N+U equations to two poly equations in Y
distinct equations will be embedded into distinct powers of Y
suppose Y is (Y,...,Y^m)
Y(A.B) = YC, so each power of Y multiplies a row. that is, \sum_i Y^i(a_i.b_i) = \sum_i Y^ic_i
now (a.b)Y' = (a.Y')b so it turns into
\sum_i (a_i.b_i)Y'Y^i = \sum_i c_iY^iY' =>
\sum_i a_i(b_i.Y')Y^i = \sum_i c_iY^iY'
(don't exactly know what Y' is)
do similar for linear constraints, making them values of coefs of poly in Y
so we end up with two poly equations. note they are univariate.
now reduce testing these poly equations to testing related polys for certain coefs being 0.



for sumcheck, the prover sends the univariate, verifier hashes it to r, then next round suppose prover sends correct answer, such that verifier is comparing correct poly at r with different, submitted poly. 
suppose we use regular hash like SHA. why not first round, prover chooses poly that on 0 and 1 sums to incorrect answer. prover hashes it and sees what the evaluation point will be, then commits to a different poly that agrees with it at r. i think we just need the commit to go into the initial hash along with the submitted poly. 
doing this means the relation is not predetermined (while the hash may be). or like said before we could use the commit to generate the hash so the relation is pre-determined but the hash is not. 
think about how expressive our map from inputs to a target can be via an inner product with constants. first notice we can map input bits to powers of 2 then add the results to convert them to natural form. then we can multiply each of these numbers in natural form by a constant then add them. what we can't do is multiply them together. so basically we want to linearly map a source to a target. of course this was easy for the identity relation. 
imagine a matrix equation Ax = b where x is the hash, b is the vector of targets, and the rows of A are sources. if we knew the relation, A,b, then we could solve for x, though the height of A and b would vary. 
for a hash, consider the subset of relation instances that hold with respect to that hash. note that each source can hold with respect to at most one target since the hash is a function. note that the function is almost-surjective, and almost-uniform. also note that the relation is about k to k for small k. almost certainly I think there will be a solution. We want to say that such solutions will be intractable to find. Maybe we can say there are few enough solutions and they will be distributed uniformly, such that solving one is like picking a random target and finding not just any pre-image for it, but a source, so its at least as hard as inverting the hash. the difference is mutiple targets can be chosen, but what we need to prove is that only a few targets will yield solutions. It seems subset-sum or the SIS hash is not relevant to the analysis and we simply frame it in terms of functions and relations with certain densities and structures, one-wayness, etc. Then we invoke that subset sum and/or SIS have certain properties so they satisfy the analysis. 

does it make sense to accumulate hashes in the fiat transform? while it may be that won't need to, I think we should, as it captures better the idea that each execution the randomness is different. i realize we would need to hash the commit on every hash on every round anyway, so it just makes sense for the commit to start a chain hash, with the inputs to every hash being the submitted value together with the output of the previous hash.
Note that considering the commit as a subset sum hash, we are making a hash tree. 

what structure does the relation solve? if the relation was two polys as sources and their points of agreement as targets maybe we could give some structure. But we are planning on the chain hash, so maybe we can justify thinking of it as a relation from just one poly to a point of agreement with another hidden poly, which will determine the hash. the 'last hash' (which is connected to the commit which is connected to the relation) can 'determine' the hash in two ways. one is it generates the random constants. another is we hash it with pre-deterined constants, but bringing it to the other side, its like it randomizes the target. for the abstract framework we don't want to consider these details, and I think we can safely say the relation is chosen first, then a random hash is determined from it. since we hash a poly representation to points of agreement with another (invisible) poly, maybe we can take advantage of some of this structure to describe the relation. 

where else will we use the transform besides the sumcheck? another place is reducing from two points to one. line is implicit. prover sends line poly, verifier hashes it to choose point on line. then we do identity test. this is again instance of prover submitting a poly that will be used for an identity test, so its exactly the same as for sumcheck. there is also the case of two points on two polys to one point, where prover sends two polys, verifier hashes both to choose single point of evaluation for two instances of identity checking. i think this is equavalent to two of the regular instances given before, but now the relation is with respect to two polys cuz they are both hashed together. 
the other instance we use the transform is from two polys at one point to one poly at that point. this is the random linear combination test, which is equivalent to a degree 1 univariate identity test where as usual one poly is invisible and the other is submitted. so I think we can focus on this general case.

suppose we have a fixed, invisible poly. when a submitted poly changes, how does that affect the point at which the two polys agree?
if the two polys share a common root, they will agree there with answer 0. suppose consider the difference poly, factor out all terms (x - ai) where ai is a common root, such that we are left with two polys of the same degree. take the difference poly and consider its roots. 
for simplicity suppose our poly is quadratic so we can use the quadratic formula.
roots are (-b \pm \sqrt{b^2 - 4ac}) / 2a where the variables are the differences of the coefficients. so now we have a mapping from representations of the two polys to the points where they agree. but actually our inputs to the mapping will only be one poly, the other one fixed. what we wanted to do was make this a linear map. this won't work. 

what is the prover's noticeable probability over? I think the relation is fixed, and the probability is over the hash and the coins? This is analogous to the lattice case where S is fixed, and randomness is over challenges and coins. So we can use the noticeable probability to find several solutions. How can these solutions for a fixed relation help us solve a problem? given the problem, we need to relate it to a relation, then generate 'challenge' hashes for the prover. our relation should be of the poly agreement form. 

our goal is to reduce correlation intractability to pseduo random generation. 

they do intractability first assume intractability against the relation of decryption. then the make it more general by taking the input, homomorphically computing the relation output, then passing that through the original hash. then if one can hash to a relation, which is equal to the decrypted input, then one has managed to find an input to the original hash that hashes to its decrypted value, which we assumed was not tractible for the original hash. now, the exact relation is unknown, so they can't actually evaluate it with the exact circuit, but rather use a dummy-circuit. the justification is a dummy circuit results in a dummy ciphertext that should be indistinguishable from the real ciphertext. if it worked for the dummy then they would be distinguishable. 
if we follow the same pattern, with out original hash being intractable against the linear relations, then we need to transform the input to something (analogous to encryption) such that a linear transformation of that something yields targets for the original input.
for input x compute transform f(x). Suppose target(s) r'(x) can be expressed as a linear function of f(x). H'(x) = H(f(x)) = r'(x) => f(x) and r'(x) are an instance of a linear relation breaking H. now we don't know r'(x) and we might not even need to know the linear function from f(x) to r'(x) if we can design f such that a linear function always exists. 
we are putting a transform on the input before it goes into H. could we also put a post-transform on it? the goal is to reduce finding a relation of H' to finding a relation on H. for general H with certain intractability, what restrictions must we put on f and g to ensure finding a relation R' instance on H' implies finding an relation R instance on H?

let R and R' be the relevant relations.
if (x R' H'(x)) = (x R' g(H(f(x)))) (that is H' is broken) then we need (y R H(y)) for some y (that is H is broken). 
--
suppose g is identity. x R' H'(x) => x R' H(f(x)).
f(x) R (x R' _)
--
suppose f is identity. x R' H'(x) => x R' g(H(x)).
x R g^{-1}(x R' _)
--
x R' H'(x) => x R' g(H(f(x)))
f(x) R g^{-1}(x R' _)
--
notice on the right hand side we have sets, so we mean for each element its related to the left side. 


gosh, I had dreams about this last night, and they kept waking me up in a frightful way. I was thinking how there always exists a linear transformation from inputs to arbitrary points, like the points of agreement. but i was worried the linear transform would need to be fixed for all inputs. Is that true? Suppose, given two polys for input, one calculates their points of agreement on the side, then find a (possibly different) linear function from the inputs to each of those points. this could be trivial is what I dreamed about, where you simply take a non-zero input and multiply it such that it results in the desired output point, multiplying all other inputs by 0. then one has the equation with right side as inputs multiplying constants, left side as inputs multiplying some chosen constants (most of them zero) such that it equals a target for the input. then take the difference of the sides, and we have solved the hash for target 0.
oh yes, the trouble is the linear function must be fixed for the reduction to work. this is because the prover has noticable probability of finding an instance, but we don't know which instance, and the linear function depends on the instance. thus we must find a linear function that works for many instances to give the prover opportunity to all those instances. we could choose multiple linear functions, each working for a separate set of instances. so how many instances can a single linear function cover? I think very few. 

a similar approach would be the same constants on both sides of the equation, but different inputs, such that upon subtraction one has solved SIS. this is harder, however, because prover must describe desired output by a valid input (different from main input) on the constants, which means finding pre-image for desired output. this is not convenient.

what if we mix both, where the left side has different constants and also input. the constants would be fixed, powers of 2. since these are fixed and not random, the constants for the solved problem are fixed and remain random. an instance would be found, then the input to the left side would be the bit representation of that instance. oh, but then subtraction doesn't work because neither constants nor inputs can be factored out. 


well suppose we were given the two polys as input. we could calculate the roots of their difference (idk how efficiently), ie their points of agreement, then pass those points through the hash. 
we could pass to the hash anything (f) such that each point of agreement can be described by a linear transformation of what we pass. but this doesn't seem much help because because whatever f creates, it could just as well append that linear function at the end of its computation anyway. 
what about post-transformation (g)? the condition is for any point of agreement, the pre-images of that under g must be a linear transform of the input. Not sure how to use this, and I think using f may be sufficient if the current idea holds.
notice that we don't have the need for a dummy-relation due to not knowing the exact relation. given the inputs, we can compute the exact relation. 
now our transformation (f) need not have an explicit form. we could say that given inputs it finds their points of agreement. now how we will do this is have the prover compute them on its own offthread, present them, then we will verify them, and also verify none are lacking, and then we will have the info to compute the non-explicit transform along with the explicit original hash resulting in the final hash. 

maybe we could also 'conditionalize' the fiat hashes. prover gives answer, verifier later batches them giving them to another proof and then verifying that proof. but that proof itself requires fiat hashes, but fewer than it will verify, thus compressing them. these proofs may provide side-data that the verifier doesn't want to evaluate so it goes into the amortization box. note this is only worth doing if there is side-data. 

so it seems only lacking is way to verify a set as being the complete set of roots of poly. maybe one way is the prover doesn't give roots but rather factored form (roots implicit), then verifier can do identity check (I suppose manually to avoid generating random points). but we must still verify that a term is irreducible. may be easier just to provide roots, test that each is a root, and make sure none are missing.

now a little tricky may be to form the hashes such that we can calculate the exact relation. actually I may have been mistaken that we can always do this. just like they chose a dummy relation, maybe we chould choose a dummy-invisible poly. they justify saying ciphertexts should be indistinguishable, as to which belongs to the real relation versus the dummy-relation. but we're not encrypting, but computing in the open, so one may distinguish so that cannot be justification for dummy-invisible poly. 

can we reduce hashing to agreement points with a particular poly to doing so for random polys? suppose we take two random polys that sum to the particular poly. one must declare weights on the decomposition, eg 1/2 each. then find points of agreement with both with the weights. then one has found a point of agreement for the whole.
well given a poly equation and we want to find points of agreement, suppose we add a random poly to both sides, then this preserves the points of agreement. but it may not yield randomness. for better randomness, maybe we also add or subtract parts based on the original equation. 

maybe take the approach of reducing linear relations (eg identity) to other relations. then we need only transform to another relation. 

maybe we can use that our hash is a universal hash function. so we imagine the constants are generated after the relation is determined. its this that should make it work against aribtrary sparse relations. 

consider arithemtic circuit that computes a target from a source. and suppose the target is also computed from the source via the hash. Notice that arithmetic circit is multivariate polynomial, and hash is also one but of degree 1. note the hash is fixed, and so is the circuit (though implicit). challenge is to find point of agreement (input) for the two polynomials. in the case that the circuit is also a linear poly, we know this is like solving SIS. in general we assume the inputs to the polys must be small compare to the coefficients. 
I'm wondering now if the related problem of finding short input to a multivariate poly (eg multilinear) that evaluates to 0 is hard, because we know its hard for degree 1 (SIS). even if this was hard, this doesn't completely translate because the circuit will be of high degree than the hash, so subtraction won't render a completely random instance, cuz only the terms of degree 1 will be randomized. 
actually calculating roots might not be possible by a pure arithmetic circuit, and if it was it would be very high degree.

in the distinguishability approach, the dummy is indistinguishable from the real (and thus so is anything computed from it). 

why not just take a dummy, accept purported points of agreement, verify them, then encrypt them so they are indistinguishable from the real ones. then pass them through a decryption intractable hash.  

suppose we generalize the identity relation into cycles, eg a two cycle is H(H(x)) = x. this also is intractable. but how to reduce it to finding a relation? maybe compare to finding poly cycles, ie treat the roots of a poly as coefs of a new poly, then find roots again. now there are always <= roots than coefs. 
the task of solving original sum problem is reduced to finding short cycles. think of a directed graph, with a node for each number. there is an edge from A to B if the binary rep of A hashes to B. Since hashing is a function, each node has only one-outgoing edge, but maybe several incoming edges, and if not then there are some nodes with no incoming edge. this means wherever you start, you will end up in a cycle.
suppose we generalize and allow compression. then there is an edge from A to B if A can be anywhere in an input that hashes to B. there would be a lot of outgoing edges. to have less, only allow a few bits b of compression. then there are about 2^b outgoing edges. 

try to reduce from particular invisible-poly to random invisible-poly. 
we have to be able to say more about what it means structurally to find a solution for our 'points of agreement' relation. the identity relation is clear with structure, so thats why it allows us to reduce the original problem to it. but we don't know much about structure between 2 poly representations and their points of agreement. one reason is there is no simple algorithm for calculating those points (ie calculating roots). this seems the limiting point necessary for any instantiation at the moment. 


perhaps best for us now is to suggest a hash, and show how other more expensive ones, even the intractable proven ones, can be computed via pushing to another proof and putting the side-effect data in the amortization box. specify when this would be worth it. we also need to show that appropriate compression is possible. 


maybe probabilistically check fiat transforms by random linear combination, only works if hash is linear transform. reall what this means is just reducing it, and never having to evaluate directly.


for constants, maybe prover computing in additive form, then mapping to multiplicative form for verifier is faster than computing in multiplicative form. could compare costs, and show algo for isomorphism. given an element of q its easy to identity if by its index with respect to generator 1. thus I think we just need to have a generator for the multiplicative group and be able to calculate the index with respect to it for any element. oh no, I think this means discrete log over the multiplicative group. so the isomorphism is easily computed from q to the field, but not vice versa. So suppose prover computes commit in q, then maps to field. keeping track of the q values, as long as any new value is derived from those already had, the prover never needs to compute in the opposite direction. adding commits is fine, as is computing overflow polys. what might be a problem is the challenge. given the challenge as plain binary, prover can compute new commits in q, then again map them so it seems no problem. the question then whether it would be worth it. I think so because we know multiplications in general take more time than addition, but we should compare exact costs. Note the generator for the multiplicative group can be pre-computed and chosen for optimization. 
http://www.lix.polytechnique.fr/~ionica/IonicaAfricacrypt.pdf


lets figure out the parameters for lattice setting
we must choose an infinity norm \alpha that implies a euclidean norm \beta. thus \sqrt{m\alpha^2} <= \beta. we will choose equality. thus we have \beta = \alpha\sqrt{m}
\alpha is what we want binding. we will use \alpha/2 for our d-values. 
I suppose both n and q are mostly free, but we want enough compression between n\log(q) and \alpha/2*m. Note these are our four parameters. now we must choose \alpha/2 as the field size such that it is at least twice the degree of the multilinear polys, which is log(m), thus \alpha/2 >= 2\log(m), for simplicity now and later we choose equality. now we can choose to express in only m or \alpha, and I think we should pick \alpha because it will appear in more places. then m = 2^{\alpha/4}. now \beta = \alpha*2^{\alpha/8}.
it would be helpful to establish a relationship now between n and q, reduce to only one variable, then leave open the relationship between that and \alpha.
well the constraint for lattice reduction relates \alpha to n\log(q) so that imposes no restriction on the relationship of n and q. actually the combinatorial attack also only relates to n\log(q). 
But we must have \beta < q. but I think this inequality can be tight and need not involve n.
so I think we choose q > \beta, which then fixes q relative to \alpha. then we're only left with n. thus I think we can have a free relationship between our only two parameters n and d (we should analyze wrt naming d rather than alpha). 


maybe use the word 'tail recusion' to better capture the 'folding' and differentiate it from regular recursion. its tail recursives in the sense that proof A verifying proof B doesn't know whether proof B is valid, yet the answer to proof A gets folded with the answer to proof B. We view the recursion as vertical with the top layer being the top proof, ie A on top of B, ie A calling B.
Sagaproofs: scalable proofs via tail recursion 


Lets try to really figure out the LWE relation intractable hash and see if we can simplify if
The relation must be fixed. this is done by considering the commit. in our cases, the relation consists of visible poly (for now consider one), invisible poly (also consider one), and points of agreement between them. so its a ternary relation. 
We consider the 'real' versus 'fake' cases. we only see the visible poly. we refer to real or fake based on what we don't see, that is the invisible poly and thus the points of agreement that depend on it (as well as depend on the visible poly). That is, we can only see one corner of the ternary relation.
Upon any instance, we assume the invisible poly corner of the relation is fixed. this leaves free one to find a visible poly and points of agreement that complete the relation.
Graphically let us assign the top left corner ('left') to 'visible', the top right corner ('right') to 'invisible', and the bottom corner ('bottom') to 'points of agreement'.
The goal of the adversary is to find a left that hashes to a bottom such that they complete the relation for the fixed right.
We design the hash such that the left (input) interacts with the right and bottom to produce a bottom (output). Now we also have the right encryped, or more general represented in an unrecognizable way but such that it can still be interacted with.
Now we are perhaps giving too much structure. we are assuming the left is input, the right is encrypted, and the bottom is output, but maybe there could be different organization. we are not assuming anything about how one is to arrive at a bottom given a left and a right. one could explicitly compute one. or, a bottom could be proposed and then validated. 
The real right will be unknown, so we use a fake right, that is again encrypted. We assume the two encryptions are indistinguishable. Now when computing, or verifying, bottoms, they will be with respect to the fake right not the real right. The goal of the adversary, however, remains to hash to a real bottom. Thus while it be remain instractible to hash to a fake bottom, it may be feasible to hash to a real bottom. By contradiction, suppose the adversary has noticeable probability of doing this. The given an encrypted right, the adversary can try to hash to a real bottom, and he will succeed with noticeable probability if and only if the encrypted right is real, thus yielding an ability to distinguish.
I think the only way to make sure the relation is fixed, via the invisible right, at the time of hashing, is to include the commit in the input and treat it as a parameter to selecting a hash from a family. Thus if the relation changes, the commit changes, and so does the hash. Thus the hash cannot be determined before the relation because the hash itself is unknown without a relation.
I still don't understand exactly how the encrypted data must be used. Satisfying the condition of intractability with respect to the real data doesn't require the data to be encrypted. I suppose not just encrypted data but the entire scenarios must be indistinguishable. Thus if data is unencrypted, it must be the same data in both the real and fake cases. We cannot include the real data in the fake case because we don't know what it is. We also cannot include the fake data in the real case meaningfully because it is irrelevant to the real case. Thus it seems any data that varies between the real and fake cases must be encrypted.
Suppose we make an encryption scheme such that decryption is a linear function of a secret vector. Then hashing from an encryption to a decryption means solving SIS. This means we must submit encrypted roots as input. How are we to compute the encrypted roots? If they are also submitted as input, we need a way to validate them and reject them, that is abort the hash, if they're incorrect. But since they're encrypted we can't read the internal state and decide whether to abort.
We need a way such that in the real case, if an unencrypted target value is hashed to, then we found a solution to SIS. I think this means the target value must be the output of the hash, rather than having the output as, say, 0 and then adding the unencrypted target value (which we don't know). Thus we need a target output to yield an SIS solution. As said, this can be done if the targets are fixed linear functions of the inputs. 
Unfortunately there is no straightforward way to compute roots.
Notice that even if we validated the roots and aborted, they would not pass for the fake case. This suggests we don't want aborts. But without aborts, the adversary could submit incorrect roots in the real case. Then we would end up with the wrong encrypted roots, which may pass through the hash to the decrypted targets without violation. 
Though it won't resolve our problem, we are able to efficiently compute the number of roots (but not the roots themselves). 


I realize like I did before, two important things, and now I realize how to put them together. The first thing is we can reduce an evaluation of some data to an evaluation of a one-to-one data transformation. The second is that we can only 'shed' entropy by direct computations by the verifier. Why not take a 01 vector specifying values in powers of 2. Then add the two polys to shed information, and the result is an expression of the addition, except that we have not performed the carrying yet. Then is a one-to-one transform to the binary form of that number ... actually there's not as there are multiple ways to carry and end up at the same value. But we've already shed most of the entropy so maybe we can make it work. 

Consider an n-digit number with radix d. There are d^n possibilities. Upon addition of two there are (2(d-1)+1)^m = (2d-1)^n possibilities. Suppose we add the numbers without moduluation. I think we can do this for partiuclar radix bases and particular number of digits. But we will still need to eventually do modulation and lose more entropy.

But remember, we can allow supplementary data from the prover as long as the total cost is sublinear. And recall that we can use this data in anyway we want. The verifier manually reduced from d^{2n} to (2d-1)^n. If we want to compute modulo, we need to reduce from (2d-1)^n to d^n. Comparing these ratios it appears we were able to jump a bigger entropy gap manually, and so we will only need sublinear auxiliary info from the prover to jump the second smaller gap.

In fact, could we just keep dropping our entropy by this addition, together with transformations, with not need for auxilieary info?


multiplication is the challenge. it drop no entropy but we can't rely on prover. 
remember we have small field, so many multiplications will be needed. maybe we can amortize them.
we must review our strategy for handling polys over a small field. we will execute the sumcheck multiple times for each, at the end needing to evaluate each poly at a number of places. beginning the sumcheck we have the answers for each sumcheck all in agreement, and the sumcheck says if the answer is incorrect then with a certain probability the final evaluation will be incorrect. if the answer is incorrect, then all answers are incorrect, so with high probability at least one of the evaluations will be incorrect. oh, i was hoping they would all be wrong. with only one wrong, we must spread that error around. Consider a wrong evaluation. Then when connecting it with another poly through a line, the returned line poly will be wrong if it agrees with the wrong evaluation. It seems spreading keeping the error spread is difficult.

Suppose we use the small field, but we do evaluating on a large field, like an extension field. Then sumcheck is not a problem, and reducing from two points to one is also not a problem. But reducing from two polys to one we will not be able to simply multiply the commit by an extension element. We can do two equivalent things. One is select an extension element and multiply the commit by each coef separately and consider it a decomposed scalar by the extension element. Or we just select multiple random elements and multiply each commit by it, and I think soundness is the same. Now we need to see whether we can maintain this without blowup. Notice that multiply constant by extension field element is equivalent to just multiply component wise by the constant. We can regard the group of commits as referring to a single poly with extension field coefs. So given new polys with only scalar elements we first reduce it to evaluation at a single point (probably extension), then we connect the two extension points to arrive at a single point (note we can do all this using virtual polys). Now we have the task of evaluating the old poly with extension coefs and the new poly with scalar coefs at a single extension point. We can't multiply the extension poly by an extension element because that's more complicated, but luckily the new poly has only scalar coefs so like we did before we generate a new commit for each component of the extension element multiplying the scalar coefs. Now we have two groups of commits, and our task is to add them as extension elements. I think this can be done in pairs with regular addition using overflow polys. Using this technique, we can always keep our poly commmits to only 2, like with DLP. But we must still figure out how to manage the overflow polys. 

Gosh, now I'm worried the regular SIS amortization doesn't actually compress. 
We start out with l bit commits of length n, so entropy (2^n)^l
Then we end up with s commits of length n to values in l, so entropy (l^n)^s. 
The former equals (2^n)^l. The latter equals (2^n)^{log(l)s}.
Our s is fixed, and our amorization will be l/(log(l)s) = factor => l/log(l) = s*factor. So two half information we set factor = 2, and for security s = 128, we need l/log(l) = 256 => l is about 3000. 
So actually it does compress, but for a large commit q and thus commit size, 3000 commits to boolean vectors is a huge overhead. 
One option may be to have prover return z values in l form rather than bit form, thus submitting s commits rather than log(l)s commits. Then instead of putting them back in the amortization box we treat them and polys and reduce their opening to their evaluation at a point and merge them with others polys. 

A technique we want to use in general is to compress as much as possible to avoid many commits and thus large circuit size. its more efficient to have a single large commit (even with much larger q), such that the commit is smaller, but still gives 'virtural' access to the same data.

Suppose we compress the boolean overflow polys. This means instead of many commits each to boolean polys we have one big commit to field size elements. For the challenge we usually compute random linear combinations of the commits. But in this case we don't have access to the individual commits. Another possibility is not to receive a compressed commit to the boolean vectors themselves, but rather a commitment to the commitments. Then ...

A problem now is that our infinity norm must be as large as l! 

A best option may be to start our hardness assumption in term of the not-yet-well-studied infinity norm. Our commits use bits. Then we choose each challenge limited to k 1's such that the z's should have infinity norm at most k. Hopefully this only comes at the expense of reasonably more challenges. 

In a binary extension field suppose we have a bit poly. Then we want to multiply by an extension element. Following the approach outlined previously, we have two tasks. One is to multiply a bit poly by an extension element. The other is to add two extension polys. The latter can be done simply by xor. The former we have to think about. What is special is we don't need to multiply extension elements, only multiply by bits or add extensions. 

I will generalize the idea. Suppose all goes well using an arithmetic circuit over p with sumcheck changed to k dimensional extension field. We ask that the polys providing data be encoded with bit coefficients. When it comes time to evaluate one of these polys, we multiply it by a random extension element. To do this, we duplicate the commit k times forming a row vector. We can visualize the data of the commits forming columns of a matrix. Then we multiply the commits component wise by the extension components which are p values. We can now visualize each row of the matrix as a coefficient of the poly with extension elements. We have thus randomized the poly without help from the prover (under the strong assumption it was in appropriate bit form to start). Then we will add it to another poly with extension elements by adding the commits component wise modulo p. How we do this is still undetermined. Note that characteristic 2 may be optimal. 

We want error in the beginning to persist until the end. One kind of error is invalid commits. If the prover doesn't have an opening for a commit, we want to make sure he doesn't have an opening for any derivative of it. In our model, a opening means a bit vector. The first derivative of a commit is multiplication by a p value. 
Suppose a prover does not know a bit opening for a commit. In particular suppose some values are non boolean. Upon multiplying the commit by a p value, the result will not a vector of 0 and that p value but will contain other values (though they may still be p values). 

I'm worried the auxiliary data necessary for addition will not be sublinear in a bit vector, which is the amount of data it accounts for. 

Note that we can use normalized result of addition as data, but it drops some entropy.
Consider adding two vectors modulo p. There is p^{2n} data. Upon addition there is (2p-1)^n data. Now our target data is p^n. This means our remaining data to drop is (p/(2p-1))^n data. We want to drop as much data as possible by addition, and as little as possible by overflow. In total we must drop p^{2n}/p^n = p data. Well the first gap is p^2/(2p-1) and is about p/2, so linear in p. Our second gap is (2p-1)/p = 2 - 1/p is basically constant. So the bigger p is, the greater the percentage we can drop by addition. 

Lets work with this and see what works, considering our plan about as a subcase that may or may not work.
We will consider addition of polys, disregarding whether the two polys are different or not, or what level of correlation they have.
We want to consider a valid way to add polys such that if the input is valid then the output is valid and correct, and if the input is invalid then the output is invalid. For a moment we disregard the entropy costs of doing this, and only focus on completeness and soundness. Our approach has been to take the commits, add them, then consider information provided by the prover to compute modulo. For completeness, if commits are valid and the overflow data is correct then result is correct. For soundness, it could be either that the commits are not valid or the overflow data is not correct. ...

I'm worried we have the law that any data given by the prover must be directly manipulated by the verifier, and if the verifier asks for more data from the prover it must be a request with sufficient randomization. For example, the following exchange would not satisfy this law. Verifier has two commits to add. Verifier adds them and asks prover for the overflow commit. This request has no randomization. It would only be randomized if there is enough randomization as to which commits the verifier will add. 

Before we made the assumption that we start out with valid commits. Then we said if a boolean overflow poly is given the if its wrong the result will be an invalid commit. We knew this leaves the need to verify the poly is boolean and we were planning to use the challenges for that. But what we didn't realize would be so limiting is the initial assumption that the commits are valid to begin with. Our commits will come from the prover and there is no reason to believe they should be valid. Thus it could be the case that the initial commits are invalid, but the addition is valid and thus so is anything else that follows. 

We need a method to immediately randomize the commits of a prover. With DLP its exponentiation. 
One possibility is like the challenges where we wait until we have enough commits and then we randomize by adding some of them, and perhaps asking for the overflow polys. 

Suppose that figured out some way to randomize with only additions modulo p, and that each one only required an bit overflow poly. Upon our randomization we would be able to reduce a certain number to another number, at the cost of a certain amount of new information. It only becomes practical when the amount saved, ie entropy dropped, outweighs the cost, ie entropy added. I think we can establish a threshhold. This is the same situation as for the 'heavy' challenge. 
What we have on our side is the high entropy drop in addition, and the low entropy drop for overflow. 
So at this point I think we should rethink the 'heavy' challenge. To be clear, we will accept a number of commits from the prover that may be flawed, and by our random challenge we expect the flaws to persist. Now different from the regular heavy challenge, we don't just want use this as a proof of knowledge of opening, but also as a reduction, where there is a meaningful polynomial relationship between the initial commits and the randomized ones derived by the verifier. 

I think the general scheme will be that we have a collection of commits, each claimed to be a valid commit along with a claimed evaluation value at a particular point. We can reduce all evaluation points to the same. Then we choose random linear combinations (how many and how many in each combination subject to analysis) of the polys, and reason about what their addition evaluations should be. Then we perform the addition, asking the prover for the relevant overflow information, which should be sublinear in the total. We can assume this data is correctly formated because that can be checked, but what we can't assume is that the data is correct. We would like to show that if the data is incorrect then the result after adjusting for modulo will be incorrect, in which case it the error will probably continue to persist. I think this is indeed the case because given an addition of two p polys, one will only end up with a poly in the p range if correct modulation is done. 
Another approach is that an 'over p/2' poly is provided for each commit before any randomization. This extra data is sublinear and is the 'cost'. Then after randomization the verifier can manually compute modulation. But with only the 'over p/2' information, the verifier won't be able to compute exact modulo but only approximately and only for a limited size linear combination. Therefore I think the previous approach is better. 

Keep in mind we may like to keep the idea of randomizing a single poly, but allowing the random linear combinations to include scalars other than 0 and 1, like 2, and maybe even all scalars. Maybe our scalar vectors could be limited to a certain norm like euclidean such that each new poly only requires a certain number of overflow polys. 

Remember the general proof technique for the heavy challenge. Previous analysis on line 1091. There is a matrix of instances. It has 0 and 1 entries depending on whether than instance was a success. We assume there are eps > 2^{-s} fraction of filled entries. Thus we can invoke the subroutine such that in expected time 1/eps we find a solution. Then we manually (without the subroutine) search for a second solution in the row. If it's a heavy row we will probably find one. If its not a heavy row, we will abort. Enough of the time we will land in heavy rows such that we should eventually find a solution. Once we find two solutions in the same row, we can use them to solve the problem. 

I suppose our scenario should take the same general form as the heavy one.
A: n*m
S: m*l
T: n*l
C: l*j
Z: m*j

Uh oh, suppose upon addition before modulation, infinity norm exceeds bound. Then prover could find a collision maybe, and send the overflow poly for that one, and then verifier ends up with a valid poly but one that is incorrect. Thus we must make the theshold high enough, and limit the number of additions performed before a modulation. Even this might not be safe. Suppose prover knows all that you will add together, finds a collision, computes the non-boolean modulo poly, splits it into parts, and each round sends a part. Then in the end the verifier ends up the same, with a valid but incorrect commit. Thus we need the threshold to be high enough to account for the number of additions we'll do before re-randomizing. Note that we can add a fixed number, like 2 or 3, do modulo, then re-randomize and repeat, which will only cost more in terms of overflow polys. 

So regarding the above we won't have C as a random boolean matrix like the heavy challenge. Instead we'd like to limit the 1-norm of each column of C to, say, B. Then each vector of Z should have infinity norm at most B*p. The overflow poly will have entries in [0,B-1]. 

Regarding the high amortization cost, maybe the network can be layed out so that commits are gathered a few passes ahead, then challenges are sent back and parties worth together to generate the new polys. This means a sublinear number of polys must be sent in total, but it requires more 'liveness' to the network and enough trust among the community. 

Actually the method considered last night of bit vectors and extension element would work, because we could check the initial commits as being bit vectors. then we only need some overflow commits for the additions. In this case our challenges wouldn't be over p vectors but just bit vectors. 

I think it is sufficient to consider a single 'row'. We want to show that if some elements in this row are outside the valid space then a random linear combination of them will probably be outside some corresponding space. 
We want this to be a statistical argument, not one of hardness.
I actually think the randomization doesn't require an amortization threshold, that's only required to make amortization worth it. 
We will our random scalar vectors are bounded by a 1-norm B. 
We may be able to get better soundness if our random scalar space is larger than our coef space. In particular, we may be best if our coef space is bits. 
We want to know the probability of soudness error, that is
\sum_i c_i*r_i <= \sum_i b*r_i = b*\sum_i r_i
when max_i c_i > b.
It depends on the distribution of the c_i. We need to relate the probability of different tests, because there is a lot of conditionality. That is, passing one test gives information about the distribution. 

Suppose we can end up showing that for a certain number of random tests the error will persist. We then wanted to say that if the wrong overflow vector is provided, the error will also persist. I think this part has no soundness error, and its not randomized anyway. So really the request for the overflow vectors by the verifier is a randomized request, a function of the random linear combination. We assume the overflow poly is correctly formatted, because we will verify that. 

Given C and Z, can one solve for an S such that SC = Z? Since j < l, maybe we can using some kind of optimization. If this is the case, then given a Z that satisfies the challenge equation, one can obtain an S such that SC satisfies it. Of course we are assuming different norms on Z and S, and solving for S must take this into account. The contrapositive is that without knowing a valid S one cannot have a valid Z, because if one did have a valid Z one could solve for a valid S. But actually this is not sufficient because need not only ASC = TC but AS = T and C probably has no right inverse because j < l.
Suppose we put a certain restriction on C such that we can recover particular columns of S and T that fully satisfy As = t. In other words, while we can't ask for a right-inverse to C, maybe we can ask for something else. Remember we can write the right side TC as \sum_i t_i*c_i^t. Similarly we can write the left side SC as \sum_i s_i*c_i^t.  

Using the adversary as a subroutine is one approach. In this one we assume nothing about S and only assume adversary is able to find Z, and we use that ability to recover an S as best we can. The other approach is the statistical one where assume the adversary knows an S such that AS = T (because a trivial one is easy to compute), and we show that if S exceeds some bound then so will Z for random C with high probability, which contrapositively states that if adversary knows a Z that satisfies a bound then with high probability adversary also knows S that satisfies a bound. But the proof techniques seem very different. 

We have been assuming S is fixed, but the prover is free to choose S after receiving C. In the end, the prover must find Z with two conditions: first that it satisfies a bound, second that AZ = TC. 

Suppose we show as considered before that given a Z that satisfies its bound and AZ = TC, one can solve for an S that satisfies it bound and Z = SC. Thus passing the test implies finding an S that satisfies its bound and ASC = TC. We would like to show that feasibly finding such an S implies AS = T.
Cheating means that for random C over fixed A and T, one finds valid S (meaning an S that satisfies its bound) such that AS-T != 0 but (AS-T)C = 0. 
Somehow we need to take advantage of the bound on S, but S multiplies A yielding nothing special relative to T. 
Actually S need not satisfy a bound. The prover passes if and only if he can find an S such that SC satisfies a bound and ASC = TC. 


We should probably conditionalize on the invisible knowledge of the prover regarding A and T, before C is given. I think we should set some threshold on the knowledge. If the knowledge exceeds the threshold then we reduce to Z = SC for valid S with high probability, or finding some collision. If the knowledge does not meet the threshold then we reduce to finding some having more knowledge after C than before which makes no sense. 
The main challenge is that the target and the bound for input changes with C. 

One theshold is how many S vectors prover knows prior to C such that they all satisfy some bound, though not necessarily the proper S bound. Since we assume intractability of collisions for bound Z, we can assume if the bound is not above Z then prover knows at most one vector for each target. 
Suppose prover has knolwedge of s vectors all satisfying Z bound, call this matrix Z', prior to C, such that AZ' = T. Now suppose after C, prover is able to find Z'' satisfying Z bound such that AZ'' = TC. 
	Now suppose we conditionalize on whether Z'C satisfies the Z bound.
	Suppose Z'C satisfies the Z bound. We will need to show this has small statistical probability.
	Suppose Z'C does not satisfy the Z bound, thus Z'' != Z'C. Suppose we set the binding high enough that a challenge multipling a Z bounded vector remains collision resistant. Then Z'C and Z'' yield a collision.
Suppose there is a t in T such that prover has no knowledge of a z satisfying the Z bound such that Az = t. We must show it is infeasible for prover to find Z' satisfying Z bound.
	The indented following I don't think is correct, but I keep it just in case.
		Well actually this is not the case. What we can argue however, is that prover cannot find response satisfying Z bound for any challenge that involves t in its linear combination. This means we will have to modify the argument so that every challenge includes at last one target for which prover has no knowledge of a pre-image. Since we want't to prevent against even a single instance of a target without a pre-image, we either must include all targets in all challenges, which wouldn't be achieve anything sublinear, or we must adjust the argument for the above scenario. Maybe our overall condition will have to be variable rather than fixed on the bound Z. Proceed as we would.
		Now for any challenge c with witness z and target t, suppose prover has knowledge of pre-images z_i' for targets t_i' and does not have knowledge for targets t_i''. 
		A*z = \sum_i' A*z_i' + (A*z - \sum_i' A*z_i') = \sum_i' t_i'*c_i'^t + \sum_i'' t_i''*c_i''^t
		=> A*z - \sum_i' A*z_i' = \sum_i'' t_i''*c_i''^t
		From the subtraction I think we can reduce this to proving that if prover doesn't know any pre-images for a set of targets, then prover won't be able to find pre-images with same bound for a random linear combination of them. I think we can use a variant of the heavy argument for this.
	I will work on this part below starting line 1550.

In the final part of the heavy argument, suppose we look for an index of the two different challenges that are the same. Then upon subtraction we don't get t but rather 0, yielding us a collision of not 2B but B. Now we have to bound the probability the two challenges are different eveywhere, ie have no index with common values. Consider the binary case. If we add rather than subtract then on the left we get the sum of the two witnesses, and on the right we get the matrix with all columns as t. 
Maybe we could just take one index of the challenge being focused on and group it with the other challenges in the heavy matrix, forcing it to be the same. We could repeat try for every index since there are only as many as the challenge length. 


Consider the possibility of doing binary addition with overflow polys that are sublinear so we don't need the challenge scheme. Binary is the only case when the overflow poly carries 1.5 info, and for any other base it carries much closer to 2. The idea is that even though upon any addition any number of carries may happen, if we add in a tree format where we use the results of what we have already added, then we know that in total the amount of information about carrying is sublinear. But one problem I see is a lack of randomization. Suppose we do as before where we wait for a large enough batch then we randomly pair. Remember we can't add successively, so I suppose we only do one round of random pairing. In return we request the carry info for all pairs, and I think we can expect this info to be sublinear in the information lost. Maybe we can request a single bit for each commit on whether its proper interpretation should be the negation of the commit or not. Then provers can always commit with no more than half the bits filled. Now it takes n+1 bits to encode 2^n data. Now at most n/2 bits of two added commits should overflow. We are unaware of their positions, however. And accounting for that information makes the whole thing worhtless.

Disregarding randomization, and only considering information capacity, we could add together 2 d-value vectors, get a (2d-1)-value vector, then do a transformation of it to a d-value vector of length between n and 2n, thus saving information. Take the more general case of add k of them together. Then we start out with d^{kn} information. Upon addition we have 
(k(d-1)+1)^n, which we'd like to convert back to d^m for some m. What is the relationship between m and d and k?
(k(d-1)+1)^n = d^m => m = n*log(k(d-1)+1)/log(d). The larger k and d are the greater the total savings. 

Suppose we gather enough polys such that we have sufficient randomization to add them and get soundness regarding poly evaluations. I suppose we have the same challenge in this scenario as the previous. We can imagine the prover directly sending us the results, but we don't know how to extract valid initial commits from valid linear combination results. 

In fact, is it a falsifiable assumption to say the prover cannot cheat? Maybe not, and just like other falsifiable assumptions like KOE we can still partially accept it. Falsifiable means I give you a problem and assume you can't solve it, but you can give me data, ie a witness, to prove that you did solve it, eg SVP, DLP. Here the problem is, I give you random A. You choose T. I give random C. Then you must find Z below bound such that AZ = TC. Now the question is whether you can do this without knowing S below bound. You could pass the test easily by choosing any S below bound and then choosing responding with T and Z accordingly. But suppose you don't know a bounded S for the T. Can you still find a Z? 


Consider taking the limit in the batch size. then a random linear combination, even just a adding two together, gives an almost-random target. Prover is unable to find Z with bound for a random target. 

Regarding randomization, which vectors get added must be randomized, but the transformation need not be. 

I'm thinking a new method for randomization would be to generate random vectors then perform addition with that and one given by prover. This, however, will cost information. 


Revisiting relation intractable functions, suppose we try 'composability' towards resistance against any relation expressed by a 'series' or 'compositions' of linear functions. What we mean by composability can vary. 
Suppose we take the output of the hash, transform it in some standard way, then feed it back into the hash. I would like to consider any circuit with each gate a linear transformation, but for now just consider a single gate at every layer.
We want to make the statement that if at every layer, the prover can hash to some intermediate value such that at the end a valid target is achieved, then the prover can break SIS. 
For the case of a single layer, if prover can hash to a target then prove can also pass hash through linear transform, subtract sides and find that the input is a solution to SIS for the related constants of the originals minus the transform constants. The security reasoning is the following. The relation, though unknown, is fixed relative to the hash. For a fixed relation, we consider the noticeable probability over the chosen hash and adversary coins that the adversary manages to break it. Suppose we are given an SIS basis and we'd like to use adversary subroutine to break it. We are free to try multiple relations. For each relation, we compute the random linear combination, add it to our constants, then give that hash instance together with the relation for the adversary to break. Upon receiving an answer, the answer should serve as one for the original SIS. But we give the adversary all linear transforms corresponding to all targets. 
Note the target could be any injective function of the output of the hash, still yielding a break. Mapping the output to binary form for input is just one example, in particular a permutation. 
Note that while our hash may not be injective, it behaves as such because collisions are intractable. 
Now consider two layers. Unfortunately, it could be the prover achieves the final output with different intermediate values than the circuit. These intermediate values would be unknown to the outside and could not be used in the reduction.
Even if we could prove the circuit we have is a lower bound in complexity, prover could find a negligible fraction of instances that map with fewer steps (or in our case with the same number of steps, but with different intermediate values). 

Note that hashing to a relation means locating a single-hash pre-image for it, inverting the injective function, then finding a single-hash pre-image for that, etc, until finding a pre-image for the top hash, which must also satisfy the relation. 


What we need for the heavy challenge is to have two columns the same. Then we have
AZ1 = TC1, AZ2 = TC2 where C1 and C2 share a common column j. Then A*z1_j = T*c_j = A*z2_j. If z1_j != z2_j then we have a collision. Oh but its the exact same challenge so with high probability z1_j = z2_j. 
Consider the regular challenge again. All rows are the same except row i. Suppose the two rows have an index j that share a common value. Then column j is the same, and if z1_j != z_2j then we have a collision. So assume for all common j, z1_j = z2_j. Remove all such columns, left with a challenge matrix where rows i differ in all indices. Then we must subtract, to get
AZ1 - AZ2 = t*c'^t where c'^t is a row that contains no zeros. 
Thus every every column j we have A(z1_j - z2_j) = t*k for some constant k!=0. Now the regular argument ends here, assuming there is a k=1, then using z1_j-z2_j as the solution. I'm wondering if we can use the different columns to do better.
Note that none of the columns in Z1 and Z2 are the same. Now k must be in the range [-(d-1),d-1]\0 if the challenge entries are in the range [0,d-1]. If we have at least 2d-1 remaining columns then at least one constant will appear twice. But subtracting could yield a 0 pre-image. similarly if there is a constant c and also its negative -c then adding might also yield a 0 pre-image. 

Again, we want to prove that if there is target t for which prover has no knowledge of a z with the Z bound such that Az = t, then prover will be unable to respond with Z' with all columns bounded by Z bound such that AZ' = TC. In particular, the prover will be unable to find appropriate columns for Z' corresponding to challenge columns that involve t. I think we can thus reduce this to the problem of finding these particular columns. 
We will need to conditionalize based on what other targets the random linear combination contains, and whether the prover has knowledge of pre-images for those targets. From now on consider the Z bound as the only relevant bound, though I guess we will have some kind of bound on the challenge, I think 1-norm. 

Suppose we set up system of equations with C as coefficient matrix, Z' as right side, and Z as unknown to solve for. ZC = Z'. with dimensions we have (m*l)(l*j) = (m*j). Consider each row of Z as a separate unknown vector. Each column of C serves as a linear combination of these unknown fectors, and the right sides of each equation is the corresponding row of Z'. Thus for each system we have a vector of l unknowns, and j equations. With j<l the system is underdetermined. We would like to solve for an appropriate Z such that ZC = Z' and AZ = T. 


Think about soundness for adding poly evaluations. Think of zippel lemma. Our polynomial coefficients consist of a subset of the targets. The degree is 1 (maybe changing this would help as it decreases randomness necessary). So for each target in the subset there is a random point coefficient. We must select the coefficients uniformly and independently from a subset. This means our challenges might not always have the same 1-norm, but we can calculate the 1-norm (maybe this can be improved). Suppose our subset is [0,d-1]. Then the probability two different polys evaluate the same is 1/d. Suppose we must repeat the test t times to be negligible. If the`re is only a single wrong evaluation, then we must then that wrong evaluation must appear in t tests. We think there will be at least as many challenges as the security parameter s. With 1/d^t = 1/2^s we have t = s/log(d). A simple possibility is to have all targets are the subset, thus choosing a completely random [0,d-1] matrix. I'm confused because if two coefs are different but they both get multiplied by 0 they disappear from the equation, so can we really count this? Well by the lemma if our poly is really all targets then it indeed seems valid by the lemma. For the case d=2 we would do the random 01 matrix and the number of challenges must only be s, so the regular heavy challenge would work. 
This covers the soundness of poly evaluation. What remains is preventing the prover from not knowing a commit for some poly but then knowing a commit for their addition. If there was no randomization, prover could easily create invalid commits that sum to a valid commit. Therefore we must show that our randomization is sufficient not just for evaluation soundness but also for commitment soundness. Remember we don't need to worry about modulo because that is a separate issue that we may not even need to do soundness analysis for. 

How about the analysis regarding the probability of a vector above bound S multiplied by a random linear combination not exceeding bound Z. Well maybe consider the 'closest' vector that does indeed satisfy bound S, with distance measured by how many evaluations differ on the challenge space. We want to test via random evaluations whether this distance is greater than 0. Let's use schwartz zippel to figure out how these must differ. All we take into account for zippel is that the two polys are different, not minding how different or in what ways. We want one of our random choices to yield different answers. Whereas above our polys for zippel were the evaluations of the polys, now our polys are the committed vectors themselves. In particular, each poly will be a row of the commitment matrix. We will again interpret them as degree 1 and draw our random samples from [0,d-1], so I think we'll get the same soundness as above. An important point I mentioned that justifies this approach is the valid poly has coefs that are less than or equal to the committed poly.
But of course we don't have access to the 'closest' poly. We only have access to the evaluations of the real poly. We know a bound of each evaluation for the closest poly, and our only way of detection is to see if an evaluation of the real poly exceeds this bound. 
I realize this method is flawed because we can't compare against a fixed virtual poly, even if the closest poly. The evaluations may differ yet the evaluation of the real one does not exceed the bound, and in fact could be less than the evaluation of the closest poly. Instead we need to ask whether there exists any poly in space of bound S that agrees with the real poly on all the particular challenges issued. Thus this virtual poly, if it exists, is not determined until all challenges have been issued, so we are far from comparing against a fixed poly. 

Can we do analysis and conditional reasoning on the commited vector by how the few random evaluations relate? It an under-determined system of equations. The degrees of freedom is l-j. Maybe we can relate this to an l-j variate linear function, ie a hyperplane in dimension l-j. 

I think we can make the following lemma. Given a set of l vectors satisfying targets all 'close' to bound B, given a set of random challenges j < l, one has negligible probability of producing j satisfying vectors all bounded by B. We need to clarify the 'close' condition more, the concept regarding 'maintaining' the bound.
But for sake of contradiction, suppose there is noticeable probability of maintaining the bound for one round. Then one can repeat at least a constant (if not log or linear or poly) number of times and with noticeable probability the bound will be maintained. But within a constant (or maybe need more) number of these transformations of the targets, we will arrive at one uniformly distributed (via the randomness of the challenges). Thus the adversary is supplying a vector satisfying the same bound for a random instance, which is considered intractable.
Complications include not just the 'closeness' issue, but also that the challenges for which the adversary succeeds and thus proceeds upon may not be uniformly distributed, thus affecting the distribution of the final target. Note each round there are fewer columns. 

I realized another way to randomize it to take advantage of every commit being a matrix with multiple rows. We could randomize the constants by replacing each row of the previous set with a random linear combination of them. This means every constant gets replaced by a random linear combination of the other constants in the same column. If the prover is honest, he will have commited the same valid vector to all rows. Then a random linear combination of rows will yield a commit for which the same vector is a witness. Presumably, we would generate just as many rows as before to keep the same commitment security, the only use being the randomization, whereas for the randomization of commits the result is sublinear. Also note that the number of rows involved in the combination, or the range of the coefficients used, have no impact as they do for the randomization of commits. But note that if q is composite we may not want to multiply such that it sends constants into cosets, but either multiply by constants c such that gcd(c,q)=1, or simply add. 
Now how might this randomization be used? Note that when adding two commits they must have the same randomization of constants. If prover commits same vector to all rows this randomization will have no effect, even if the vector is invalid. Thus it seems this may only be useful for catching the prover committing different vectors to different rows. For constants A and B that have different value x and y commited, upon their addition the prover will be forced to solve for z such that
Ax + By = (A + B)z
and due to the randomization, z may well not be a valid value, even if x and y are.
I suppose we should speak of the above as columns instead of rows when visualizing it in committed form.

Yet another way to randomize would be linear combinations of rows, equivalent to replacing each coefficient of a poly with a random linear combination of other coefficients. This may replace the need for many commits before randomization can take place. But either we need a meaningful way to translate this transform to a polynomial evaluation statement, or we need to reduce our polynomial statement to statements just about the existence of an opening. 

It may actually be more efficient for some networks to batch a lot together because in total I think it requires less circuit work, and that may outweight the extra cost of communication and trust. the bandwidth would be about the same. 

Back to the regular random linear combination of commits.
I should be able to establish some inequality about probabilities of exceeding thresholds. 

We were thinking its important that a vector meet a precise infinity bound. But is that really necessary? Remember we just need to be sure that the prover cannot change initial commits, and cannot use the randomization process to obtain a commit other than the proper randomization of the initial commits. So suppose we can be sure the prover knows commits (unique via collision resistance) for each target. Then we need to make sure the only witness for the challenge the prover knows is the random linear combination of those initial witnesses the prover already knows. Well the regular heavy challenge can solve the first part, with bound 2B. Then suppose prover can find witness Z with bound B for challenge, and that Z is not the corresponding linear combination of the prover's already known witnesses for the targets. Show this is infeasible.
Note that the linear combination of the initial witnesses may not yield a valid witness Z, because it might be as large as 2B and then with low probability satisfy the B bound. If it did, then we could say if the prover provides that for a Z witness then there is no problem, while if the prover provides another Z then we have a collision. But we cannot say this.
Also, if we could say the Z witness is the linear combination of some witnesses for the targets, those witnesses must be the original ones or else we have collisions. But unfortunately, it could be the prover finds a Z witness not a linear combination of target witnesses. 
Here is an ugly solution. Make the constants so big that there is collision resistance even for a random linear combination of 2B values. Then the extracted vectors bounded by 2B can be multiplied by the challenge and yield a solution to the challenge. Then as argued above, either the prover used the same ones and there is no problem, or the prover used different ones and thus found a collision for the challenge under the new bound. This can be what we do if we can't find a way to improve these constants.

I'm confused how the heavy challenge is not sensitive to the format/density of the challenge matrix. Maybe we will need to compensate for the smaller space of each challenge with more challenges. To the extreme, suppose the challenge is 01 but with only a single 1 for each challenge. Intuitively, this won't work, but where does it fail for the heavy challenge? Each column of C has a single 1. Thus there may well be rows that are all 0. There are j challenges, and each challenge has l possibilities. But we need to enumerate not the number of column possibilities but the number of row possibilties. The number of row possibilities is precisely 2^j when its completely random. So our first challenge to use the heavy test is to construct a challenge matrix with enough structure that the row space is clear. 

Suppose we build a matrix by selecting a column at random, then selecting k rows at random and filling those entries. We then randomly select one of the remaining columns, and again select k random rows. We do this k times. Then we continue selecting random columns, but now to prevent more than k entries in each row, only certain rows are available for selection. I think something like this would be appropriate. Perhpas this is equivalent to filling the top left k*k submatrix with entries then randomly suffling both rows and columns. The latter is more elegant and the parameters, like the filling of the entries, and the size of the submatrix, are straightforward. Also the amount of randomness is easily calculated. 
BTW, is unshuffling a matrix an easy problem? 
I shuffling the columns is equivalent to multiplying on the right by a permutation matrix, and shuffling the rows by multiplying on the left by a permutation matrix. But matrix multiplication is associative so it doesn't matter which happens first, and since permutation matrices form a group, it also doesn't matter how many different permutations are applied.
Actually these formulations are not the same. 
If we want challenge vectors all with the same 1-norm, it doesn't need to be the case that each column have the same number of entries. Thus we could go through each row i, selecting the the k columns i through i+(k-1), modulo the column number. We end up with a matrix where each row has the same number of entries, while some columns may have more than others. Now we have to decide how to fill the entries. If we ensure that all columns have the same 1-norm then the row space is not easily enumerable. Maybe we select row values at random making the row space enumerable, at the cost of different challenges having different 1-norm which yields different Z bounds for each response vector, yet these bounds are still computable by the verifier. I think it would be better to have a fixed 1-norm, and deal with more complicated enumeration of rows and columns for the heavy matrix.

If the lowest witnesses the prover knows for the targets is Z bound, ie they are all higher than Z, then we may use an argument suggested before to say that with negligible probability the prover can obtain an witness for a random challenge also satisfying the Z bound. What's most important about this theorem is it does not assume how the prover finds the witness, like it doesn't assume its a random linear combination of the other witnesses. 
This means prover can only likely pass if knows witnesses to targets below Z. This is better than the 2Z bound. 

Suppose we set up a system of equations SC = Z, with S unknown and independent from C. I think we won't get far until we assume a lower bound for S, because of course if S is very small, eg containing many zeros, yet exceeding the infinite norm, it will pass with high probability. I suppose Holder's inequality is useful, saying SC bounded above by the p norm of S and the q norm of C when 1/p + 1/q = 1. But here we are actually considering the rows of S, not the columns which are the value vectors themselves. 
Note that if l columns of S each have p-norms n1,...,nj then concatenating them in any way yields p-norm n1+...+nj. Now one way of concatenating them is by rows, so we can say that adding the p-norms of all rows equals the same. In other words, the sum of p-norms of columns of a matrix equals the sum of p-norms of rows of a matrix, which are both subcases of the p-form considering every matrix element as a coordinate. I think this is the case for all p-norms except infinity. 
We can take the holder inequality and say that the p-norm of the S row is bounded below by the response (a 1-norm) divided by the q norm of the challenge, both of which we have access to. Thus we can explicitly calculate a lower bound for the p norm of each row of S (actually just the entries involved in the challenge). For example let p = infinity and q = 1. Then the response divided by the 1-norm of the challenge is a lower bound for the max element of the S elements. This isn't too helpful, but maybe it would be more helpful to invoke for p other than 1 and and infinity. For example, for p=2 we can find calculate a lower bound for the euclidean norm of the S elements. Maybe given all challenges for that row we can estimate the lower bound for the whole row, then the sum of these lowerbounds is a lower bound for the sum of the euclidean norms of all S columns. 

Above we are trying to make sure prover has knowledge of S bound commits so that they can be used as a linear combination for the Z witness, then argue that if the prover used another one for Z then there's a collision on bound Z. But this is just one means to an end. Our end is not to make sure prover has knowledge of S bound commits but just commits low enough for collision resistance, eg Z bound. But our end also is ensuring that the Z witness is the linear combination of these commits. So another means would be to show that its infeasible to give a witness Z that is not some linear combination of known vectors, disregarding the bound on these known vectors. Then show that with high probability of the challenge, for a linear combination of these vectors to be a valid Z, they must be below a certain bound, and that this bound is collision resistant. Therefore the Z witness is a unique linear combination of vectors known by the prover. 

Now for the first task above, that is to show Z witness must be linear combination of existing witnesses, even if these existing witnesses don't satisfy a bound (eg the Z bound). We suppose Z is obtained by some function of the targets and witnesses for them (may not be valid), and that this function is not a linear function. Perhaps we can formulate that this function is a circuit, representable by a multivariate polynomial. To be precise, we assume there is a probabilistic, uniform algorithm representable by a circuit that accepts randomness, the targets, any witnesses for them S', the challenge, and the original matrix A and outputs a valid witness Z, and that the circuit does not output SC where S is a subset of S' containing an instance of a witness for each target. 
I dont know how to continue.

For the second task above, that is to show if a valid Z witness is a linear combination of other witnesses, they must all satisfy the Z bound. 

Maybe we should just put together as many lemma as powerful as possible, independently, then later consider how they can compliment each other.
One lemma should be the one about the infeasiability of 'maintaining' the bound. if all commits are greater than or equal to Z bound then its infeasible to find in anyway answers to a challenge that also meets the Z bound. The benefit here is we don't assume how the adversary find the answers, but the drawback is no commit can be below the Z bound. 
Another lemma should use holder's inequality. It should consider the 1-norm returned by all challenges, add them together to get a 1-norm for the whole row combination, divide by the q norm of the challenge, then assert the p norm of the row is lower bounded by this. add up all these bounds then assert the sum of the p norms of all commit vectors is lower bounded by this. we then compare this with the valid upper bound for the sum of all commit vectors. this may be most helpful with p=q=2. the translation from rows to columns doesn't work for p=infinity, q=1. For p=1, q=infinity we are wasting our knowledge of the challenge unless we set all challenge entries equal to the same, which means no randomization. Note that just as for the euclidean norm we can also calculate an upper bound on other p norms for an appropriate vector, then calculate using corresponding q on the challenges. I think taking advantage of this lemma means performing these extra checks for each norm. Actually we can also do checks before we translate from rows to columns by only checking the rows. we bound what each row must be for any norm, then compare with our calculations for the challenges of that row. In fact, this is the original check for p=infinity, q=1 (the case that doesn't work for translating to columns). I'm not quite sure whether adding up the answers for a row then calculating or calculating separately for each answer is better, maybe both. See if one implies the other. Oh wow, I think Holder's generalized inequality holds for 'norms' in (0,1) too. All checks the verifier makes will probably be of this holder form. 
Another lemma should be the heavy extraction for our challenge format.

Luckily I think the only thing that changes without our security reduction is our security parameters, including the number of challenges, and we can hopefully keep the basic scheme of randomization. But the particular checks the verifier performs may vary, though I think they will have similar form. 

To recollect the entire merging scheme, the verifier is to take the responses, check them, then if they pass use the coefs and evaluate at a particular point and compare with a claim. But we want to translate the data back into a commit form suitable for re-merging. So maybe the prover submits new commits for each response (the verifier already holding a commit for each), but these commits claim to hold the same data but in a renormalized form. Since this is a one-to-one transformation, the verifier only needs to process the data in the new commit. Now the verifier can perform the checks regarding either via the old data, then checking the old data with the new data, or do a direct translation and checking from the new data, and I think the choice depends on gate count optimization. 
Unfortunately, I realized, to transfer from the old to the new commit, one must open the old commit, and this must be done in the circuit. This means taking the new data, translating it to the coef form of the old commit, then computing the commit and checking it against the target. But computing the commit inside the circuit means explicitly computing it (rather than using the isomorphism) and it will take a lot of gates. This computation is analogous to DLP exponentiation, which is costly, especially with all the polynomial modulations. One, still costly, solution is to make q prime and make it the field size for the circuit. Then computing the commit becomes trivial. Another solution is to try the overflow poly. But this costs more information and may not maintain sublinearity. Also worth noting we plan on hashes and fiat transforms to be based on the same kind of explicit computations, so maybe we should focus on optimizing it or amortizing it. But amortizing means merging old commits but those commits commits have already reached the threshold. Another possibility is we delegate the commit computation to its own circuit which takes place in q, which means verifying this q circuit in the main field of much smaller characteristic. but the conversion is simple enough, basically linear, it might be able to be done with a simple proof system. 
Anyway, suppose we renormalize. Instead of trying a clever way to relate the decomposed poly to the old poly, we explicitly compute the coefs of the old data and evaluate it at the relevant point. Then we are left to treat the new data as its own poly, with no ties to the old poly. This requires a long explicit computation, and it means computing the same coefs as if we are opening the commit, execpt we are not opening it so we don't need to exponentiate.

Proof system for commit conversion. We want to accept two commits, and old and a new, and verify the data in the new maps in a certain way to the data of the old. It should be an isomorphism. I thnk we may have to split the new data into multiple commits because it is large, but for now ignore that and assume a single new commit contains all the data. We don't access the data in the old commit and only access the new data. 
We take each value in the new data and scale it by some constant, add together segments, then multiply each by one of the constants, then add the whole thing together. This is a single linear function of the new data. The proof statement then doesn't even need the old commit and it just says that the data tied to the new commit, when reassembled, yields a particular commit (which should match the old commit). Thus the statement really just consists of two q values, the new commit and the supposed old commit). So supposed we have a fixed linear function. 
c(x)*(q(0)*d(x + 0) + ... + q(n)*d(x + n))
probably a single sumchek should work, and the q data sent by the prover would be translated to the multiplicative group in the main circuit. 



What is the sagaproofs model?
Use the concept of on and off-thread. 
offthread computation sends meaningful messages to onthread computation, which sends back randomness.
complexity-preserving
integrity, not privacy focused
practical, rather than theoretical approach
regular recursion of proofs is where one proof computes its statement by using another proof as a subroutine, like regular function recursion. tail recursion is where a proof only computes its role but leaves the answer open ended by delegating to a condition on other proof answers. this is like functional tail recursion where control is passed entirely to the delegated proof and doesn't need further assistance upon return from the parent proof. Technically, tail recursion is when a tail is called recursively, and a tail call is when a subroutine is called at the end of the main routine. 



What about zero knowledge?
Can we add a final layer that does nothing, not even merging, but mask the previous layers? In this case it may not need extra data, so it doesn't add to the payload. How can we mask a poly? If we keep the same point of evaluation, the last layer can generate a random poly that is zero at this point. 
I suppose all polys should be zk, especially in lattice setting where they will travel between parties. Maybe each transcript poly is accompanied by a commit to a random poly. We execute on them as in Libra, and at the end we must evaluate both. This means more merging is required for the same amount of meaningful data. I think we can also make commits zk by adding an extra constant. I think this is both necessary and sufficient. Upon adding the commits all info will be gone so revealing is then ok. Oh, but a problem is that for the sake of randomization we cannot deterministically add the transcript and its mask. But we also cannot use the gaussian noise for zk because we need the random linear combination to be a meaningul poly combination, not one with noise or else the poly evaluation assertions won't hold. If we must commit real data, and the opening of those commits must be added to that of other parties, then zk seems unlikely. Only way I see is that each party merges all its data, which can include masks, and then send the randomzied merges to the next party. But this requires large cost for each party, only worth it if computation is very large for each party. 
for DLP we can deterministically add the transcript and mask due to our ability to randomize one. 

can we specify how to do zk for the general case? maybe we can try the last layer masking method. The data for the last layer could be a supposed isomorphic randomization of the real data, but the randomness that determines the isomorphism would have to be hidden. But through the verification of the last layer we would need blockers.
ZK is not such a problem for the protocols that don't do tail recursion because the full evaluation by hashing or interactive proof or whatever is completed, whereas with tail recursion the data will get passed on.
The libra Zk requires the verifier to evaluate by oracle access the mask polys. But as said, with tail recursion we evaluate polys directly. So if the verifier is to evaluate the mask without looking at it, then it must be randomized with other ones.
I suppose we could just some general mask polys that can mask any other poly. these polys would be used in the randomization, so only their combination with others would be sent. We would need to prove appropriate hiding, which depends on the randomization, ensuring at least one random poly appears in each combination, and that a poly doesn't appear in too many combinations to leak info. Soundness holds because all poly commits are determined before we choose random evaluation point.

So suppose we hide all commits, and use appropriate masking polys as in libra, and also have some generic masking polys that will appear in any combination of polys one sends to another party. I think this is the general technique we can formulate for the polynomial method, including DLP. we can describe this in the general poly construction section.



How to prove the heavy challenge for our matrix format? Ideally we'd like all columns (challenges) to have the same 1-norm, and all rows to have the same too. Also we'd like all challenges to involve about the same number of commits, and each commit to appear in about the same number of challenges. One way to achieve this is with the number of rows (l) a multiple of the number of columns (j). There are multiple ways to then fill the entries to achieve same numbers in each row and each column, eg diagonals, checkerboard. Then we shuffle the columns and the rows, ie multiply on each side by permutation matrices. It remains to choose the values for the entries.
Best for security might be that each row contains a high variance of entries, maybe even one of each value, and there are probably no more than j values. Such a design would also let us (almost) enumerate the rows. So maybe we could formulate that each row contains exactly one of each value in a set like [1,d-1]. Not quite sure about columns. 

Precisely what requirements must our challenge matrix meet to work for the heavy argument?
We have our heavy matrix H. It contains 1 in entries for success. We assume an eps fraction of the entries are filled. We would like the row space to describe the coins and all rows of the challenge matrix C except row i. We would like the column space to describe row i of C. 

eps >= 2^{-s}
j*eps/2 >= k => j >= 2k/eps
j >= 2k*2^s >= 2k/eps

a problem with the heavy challenge in our format is that with non boolean entries, subtracting two rows may not yield a vector containing a one but rather a nontrivial divisor of q, and thus we must divide both sides by that constant, but such a divisor doesn't exist. this leaves us with a Z witness for a scalad target. Hmm, but remember one thing driving up our constants is that we take a linear combination of the extracted witnesses. Well maybe that linear combination can be precisely the one we find in this extraction process, solving both problems at once. To review, the context is that the prover found a valid Z witness that is not a linear combination of known targets. Well we extracted targets in the same collision resistant space, so we know the prover has knowledge of these targets. So supposedly, despite knowing these targets, the prover gave a Z witness other than this linear combination, thus yielding a collision in the Z binding space. So our task is to take the extracted target witnesses and multiply them by the same challenge to obtain a valid Z witness. So can we do this linear combination? Well each target must appear in multiple combinations, each with a different constant. Well if our challenge rows contain one of each scalar in a random order and spacing, then subtracting two of them should yield a vector with entries in the range [-c,c]. Consider breaking a constant up into parts. Doing this would exacerbate a potential issue alread not addressed and that is knowing an extracted witness of a scaled target does not imply knowing a witness for the target itself. Do we need to have that knowledge? Well we thought we did, but maybe its infeasible to have knowledge for witnesses for the same target at multiple scales without having knowledge of a witness for the target itself. 

how does holder's inequality help us in any proof of security? 
maybe we should just prove security for the case of a fully random 01 challenge matrix, yielding large constants, and then suggest the other possibilities that may yield smaller constants, by using more structured challenges with more variability in the entries, and making more general use of holders inequality. we shouldn't guess the most efficient challenge matrix anyway, and leave it as an open question. 
Whatever challenge matrix we choose, recall what role it must serve. One role is in the context discussed above, where we want to know the prover has knowledge of target witnesses who's linear combination is the Z witness. The other context is poly evaluation soundness, where we proved security thinking of the evaluations as coefs of a poly, then each of the j challenges corresponding to evaluation on a random point in the challenge space. For 01 this this is {0,1}^l, giving soundness 1/(2^{l*j}), when we only need 1/2^j assume j > security_parameter.
Anyway, we specify the protocol which will use the general holder inequality and what must be prover, then we leave as a separate issue which we approach separately what distribution is best for C.


what proofs will we show in this paper?
proofs for the definition
proofs for the general poly construction
proofs the EEC scheme satisfies the definition and the poly construction
profs the lattice scheme satisfies the definition and the poly construction
maybe proofs about application schemes, like network topology


can we generalize the schwartz zippel lemma where we choose uniformly from a different set for each variable? The univariate case is simple. Suppose it holds for case n-1. Now show it holds for case n. Take the new variable x_n and a poly with that as the variable and the other polys as the coefs. This univariate poly is of degree at most d_n.

What about even more general where we pick the n random points not independently but from a joint distribution. The motivation is that our distribution of interest is the challenge space where we don't want the coordinates independent. In fact, our poly evaluation soundness relies on the zippel lemma for this so if we can't rid the need for independence then we may be forced to have our challenges with coordinates independently. In particular, we would like to choose uniformly from the distribution of F^n where the vector is bounded by some p norm bound. The usual case is when p=infinity, because in this case each coordinate can be selected independently. We measure the norm in Z not in the field F, and we use the standard representation by cyclic groups and their polynomials for extension fields. For now just consider the prime field case, and picture the selection domains as a subset of the n-dimension domain in Z. In fact, just consider the case of n=2 for now. So with the infinity norm we pick from a square. With a 2-norm we pick from a circle, and with a 1-norm we pick from a diamond, the lower the p the smaller the smaller the space. Intuitively, if I imagine a 2 variate poly over this space then it seems the exact shape of the space doesn't matter. The entire domain is a square, but can I select randomly from a subset that it not a square? I don't think our proof can use induction on the dimension since the dimensions are no longer independent. Here our variable selections are dependent. One way of uniform selection is uniformly selecting the first variable that meets the bound, then restricting the remaining variables. 
(x1^p + ... + xn^p)^{1/p} <= B
x1^p + ... + xn^p <= B^p
x2^p + ... + xn^p <= B^p - x1^p
So we can just change the bound. This implies we can actually use induction.

Any more general distribution?

Organizing the challenges by column is suited for the poly eval soundness, and bounding the combinations all the same. Organizing challenges by row is suited for the heavy argument, and intuitively makes a certain amount of sense for soundness in that every commit should receive equal attention.

Hmm, I guess I didn't think of this, but even though polys in their initial form are best bounded by infinity norm, so they can best represent data, once added and only considered in the amortization box, we could forget about their infinity bound and only focus on euclidean bound. This would work for the sublinear proofs of knowledge of opening. By the triangle inequality we would need to support 2\beta for a bound. But remember our response is treated as data again and processed, so actually data doesn't stay in the amortization box but rather follows a loop. Thus to close the loop we need a way to reduce evaluating a virtual poly at a random point to evaluating its representation in euclidean space at some related point, still viewed as a poly. we should still view it as a poly because I don't know another way to take advantage of the additive homomorphism. 

What if we just did all commits with respect to the euclidean bound, then do an isomorphism to the poly. The justification is that indeed we are making more efficient use of our constants n and q.
A beta (radius) bound gives possibilities of the volume of an m-ball, while if we use infinity b with beta = b\sqrt{m} then we have information b^m = (beta/\sqrt{m})^m. 
Turns out we are comparing the expressions pi^{m/2}/gamma(m/2 + 1) for the euclidean space and 1/\sqrt{m}^m for the infinity space. 
Well using the full euclidean encodes far more information. 
Before our plans was to commits to polys in infinity bound, add them, then receive a new infinity commit and compute an isomorphism from the new commit to the sum of the original commits. Suppose instead we have an isomorphism between infinity and euclidean polys. Prover chooses infinity poly then computes euclidean and commits to it. Proof is executed with respect to virtual infinity poly. Then before (rather than after like the previous scheme) the addition of commits, we compute an isomorphism from the commit to the virtual poly, perhaps in a different field. This means the euclidean poly is our new data and with that data we compute the coefficients of the virtual poly, multiply by the random point(s) and thus evaluate the virtual poly as needed. Then the problem reduces to evaluating the euclidean poly at a random point. This is where we add together euclidean poly commits. Oh, but we will still have to do an isomorphism after, like the previous scheme, where the prover gives new commit to a renormalized version of the data in the computed sum. Maybe we can do this all in one field. I already touched on this before I think, but prover can give the new data multiple formats. One option is in a new virtual poly, forming a data cycle. Then the task is then to compute the isomorphism to the euclidean version (opposite direction as computed before), then evaluate that euclidean poly at the random point. The other option is present the new data as a euclidean poly but one of appropriate bound. Then task is to compute a (different) isomorphism from the smaller norm larger dimension poly to the larger norm smaller dimension euclidean poly. Then evaluate the latter at the random point.
It seems using the euclidean version will same some data but have basically the same complexity for a protocol, actually a little more complex.


I think a continuous map from the euclidean to the infinity for the same volume is
scalar*Norm2(x)/NormI(x)*x where scalar is a constant to make the volumes the same
In general to map from a p-sphere with volume vp to a q-sphere with volume vq take the point in p space, multiply by its p-norm, then divide by its q-norm, then scale by volume. 

I think it will take substantially less gate count to do checks in the infinite rather than the euclidean setting. Also, the infinity problem is thought to possibly be harder than for euclidean, but its not yet well understood. 

wondering again if we should only stick with the 'original data' concept. I think we can classify by a 'rate' at which the entropy can be reduced, ie a ratio of how much data is dropped compared to the original amount (maybe use the word 'molding'). Probably better to consider the range [0,1] than [1,infinity]. For DLP we can do ratio 1/2. Ratio 1 means all entropy is dropped, which means fully verifying. In coding theory, the rate is k/n if there is n total information, and k of it is useful. Thus these rates are in the range [0,1]. Rate 0 is useless for codes in both, so I think the analogy is useful, as while they use the words 'code rate' we could use something like 'mult/molting rate'. Another reason to use the word molting is it suggests something periodic. 
I think we need to compare the molting rate with the ratio of the amount of redundancy in the transcript. Rather we could consider 'code' rate, which is basically regular code rate, ie how much of the total is useful. For example, in the poly div construction, I think code rate is well below 1 due to h. So code rates are in [0,1]. Lower code rates and lower moting rates are worse. Suppose we have code rate r, and molting rate m. We need to calculate the threshold that serves as an upper bound, above which r and m will be small enough that data will blow up. 
For a second suppose r = 1. Suppose we have x total information. Then after k molt cycles, regardless how the data is split (sequential or in tree form) we will have x*m^k data left. Well its not so simple because, remember, we need a certain amount of information before molting can take place. So x must exceed this theshold. Actually I realize the rate is not constant but may depend on the amount of data amortized. 
Oh I think the threshold is when we start with original amount of data x, the we have x*1/r data to evaluate, then after molting we have x*1/r*m data left that we must consider as having full entropy. If this exceeds x then we have added extra costs and saved nothing.



oh gosh, the computation model I've been imagining, map reduce, I think has a lot of redundant data in the transcript. But at the same time I know intuitively it should still work despite this low code rate. Perhaps this is a good indication that the code rate doesn't really affect molting. I think this is because molting allows us to keep the amount of data to process at a constant, regardless how big that constant is and how much redundancy we will need to process. 
I wish I could remove the redundancy necessary by allowing for agreement tests somehow. Maybe there is no way to get around this except recursion itself, in that we want to process arbitrary depth computation with processing only dependent on the width. The redundancy in our regular model of computation is proportional to the depth, so does not achieve this. 
Maybe we can describe an equation that holds between the code rate and the number of rounds. It seems this tradeoff translates to one between the communication cost between parties (more amortization) and the gate count (more rounds). 

Remember the most crucial verication costs are fiat transforms. For efficient fiat transforms we want to use subset sum/SIS, but in this context the input and output are in different formats. we definitely want to avoid checking prover provided data for correctness or manually decomposing it, eg operating in field for the constants of the hash, and checking the input is in the input range, or instead manually decomposing the input to the appropriate format. We want input and output in the same field, and I see the best way to do this as operating in the input field, and using the multiplicative groups for finite fields for the constants. The price paid though is that hashing is no longer scalar multiplication but instead scalar exponentiation. 

What if instead of the poly transformation we do a fully evaluated snark using hashing commits. Suppose our hash is the SIS type. Can we amortize the hash openings? I don't think we'd save any cost. And I think the cost of opening all of them manually may well exceed the amortization costs, but the benefit is due to full evaluation communication cost is much lower. 

I have the chance to prove secure an abstract definition of tailed recursive proofs.  

The hologram paper talks about 'universal simulation' and saying its concretely inefficient, and that holographic proofs are better. By this they're talking about proof systems that are uniform where the verifier simulates the basic steps that are applied over and over, eg in STARKs. 



for correlation intractability with SIS consider the non-compressive version first. suppose take the input and pass through a fixed affine transform to obtain each coeff of the difference poly. Then suppose we use plain auxiliary data, the roots, to output all the roots (verify them). Rather, actually instead of the roots we would output the diff poly in factored form, which includes roots. Now how to we achieve compression? I was thinking we reduce the problem of mapping poly to roots, to problem of mapping roots to roots. So first we do the non-compressive transform from the input poly to the difference poly in factored form, which is non-compressive. Then we map the factored form to output, and if output contains a root it will match one of the inputs, satisfying the identity relation. Lets think in 3 steps. First take input and map to diff poly. Then take diff poly and map to factored form. Then take factored form and map to output. Only the last step is compressive, but we know since its a linear relation that is ok. The first two are non-compressive.

Consider if just the first step was the full hash, so a successful break is mapping input poly to correct diff poly. We have a fixed mapping from the input to each diff coef. Then a break means break all the induced hashes. So actually a break means breaking the hash (one of the those induced). Thus if the prover does not break the first hash then we assume all coefs it computes are incorrect. 

I think our basic problem is we still can't prevent the prover from taking an alternative path. Prover could map to a totally wrong diff poly, thus not breaking the first hash. Then correctly map to its factored form. Then map to a real root that is not an element in its factored form, thus not breaking the last hash. In other words, inability to break the first hash and inability to break the second hash does not imply inability to break them together.

I thought it would work, with the non-compression of the hash giving us control over the input to the last hash. But what I didn't realize is the invisiblity of the first step takes away this control.

Note that in sumcheck for GKR we plan to send quadratic polys. By the quadratic formula roots are explicitly computable, but require sqare root and division. If we could compose one intractable for this relation I think that suffices for GKR, though not the other fiat transforms. So for a quadratic we have 3 coefs. Hmm, don't know how to implement this.

Basically we have the ability to do any invisible permutation intractable hash. And we can also do a compressive hash for linear relations. But we don't know how to combine the two. 

So I think we leave this for now.



to review, for lattices we haven't yet chosen a distribution for the challenge matrix. we were thinking of proving the zippel lemma for p-norm distributions. This would be for the context that our challenges are chosen uniformly within the p norm space. But if we choose our challenge matrix in such a way, eg with respect to rows, that challenges are not uniformly distributed in some p norm space, then we may have more trouble proving poly soundness for the randomization. 
Remember how crucial it is to work in a native field. To do this I think we want to take the normalized poly and since it is above the field size, we want to do modulo on it before doing the final evaluation. We will still open the summed commit before modulo.
But I realize checking p-norms is costly, and generating that distribution is as well. Only uniform distributions are easy to generate, and the larger the range check the more costly (bit checks are minimal).
Maybe a random 01 matrix would serve us for our current best solution. It suffices for poly soundness in randomization exactly, with the regular zippel lemma. 


We don't need a field. I think an integral domain is sufficient. With zero divisors a term may be zero when it does not contain a zero, so zippel won't hold. Like integers or rationals, even complex rationals. I'm thinking of this in the context to avoid modulo to make euclidean analysis more convenient. We still want finiteness for precision, and we want to avoid blow-ups. 
before we were limited by fields, but I think integral domains can have many more forms, like analytic functions. 


Suppose we limit the 1-norm of each challenge to the field size d, such that the result of each challenge should be bounded by (d-1)^2. Then maybe instead of prover submitting renormalized version of sum, prover can encode same length commit with entries in field, and submit this as overflow poly. Then verifier multiplies it by constant, subtracts it from computed sum, and should end up with new commit. The submitted data could be anything in the range of the native field so it need not be checked. But we need to prove soundness in that if prover submits incorrect overflow, the resulting commit will be invalid. 
So we have the following formula for any 1-norm k
k*[0,d-1] = d*[0,k-1] + [0,d-1]
and this is is optimal, but we are not concerned here with optimal entropy but optimal overall efficiency and for that we'd like the prover to return a commit of the same length with native elements, that is in [0,d-1]. So we'd like to set k=d, but multiplying by d is the same as 0. So we'd have to do k=d-1 but then the overflow poly is restricted to [0,d-2]

Oh, hah, if we use the overflow method then we still need to evalute the old poly after the modulo is performed, now in addition to the overflow poly so it only adds cost. 


Ok, to figure out holder, a first place to differentiate is whether each row of the commits is only bounded by infinity or something else? The only way it can be something else is if we relate it to the bound of the sum of the commit columns. 

Rows bounded by something else.
Suppose we use the euclidean bound of each column. Actually we will then need to relate this not to the bound of a row but to a bound of the sum of all rows. 

Rows bounded by infinity.
Now we know the bound of a row for any p < infinity is greater than the infinity bound. We want to compute the q norm of the challenge then compare the received one norm respose with the p norm and q norm. If x is a row, the since we can only assume it meets an infinity bound b, all our p norms of x will be (l*b^p)^{1/p} = b*l^{1/p} for l the length of the row. So the larger p is the smaller the p norm. But the larger p is the smaller q is, and thus the larger the q norm is for the challenge c. So for fixed x and c, how does the right side of the holder inequality change for p and q. It is
b*l^{(q-1)/q}(\sum_i x_i^q)^{1/q}
= b*l/l^{1/q}(\sum_i x_i^q)^{1/q}
= b*l(1/l*\sum_i x_i^q)^{1/q}
= b*l(\sum_i (x_i/l^{1/q})^q)^{1/q} = b*l*qnorm(x/l^{1/q})
well this function goes up with q. as q approaches infinity b*l^{(q-1)/q} approaches b*l and (\sum_i x_i^q)^{1/q} approaches max(x_i). This means the tightest bound we can get with respect to a c and an x assuming nothing except its infinity bound is for the smallest q. But remember holder inequality holds for non-norm values p and q, so we could take q to be less than 1, oh but this would make p negative, which is not allowed, so this only works when 1/p + 1/q = 1/r and r > 0. But our linear combination only allows for r = 1. 

Upon adding the range [0,d-1] times we get the range [0,(d-1)*k]. It only makes sense to have k < d. We want to figure out for a given k and d how to represent the result in division form. This means for a number x we want the form d*y + r. with 0 <= r < d. We want to know the possible values of y, and from that the possible values of r. The max value of y is the max number of multiples of d in (d-1)*k = d*k - k. Well since k < d I think the answer is max for y is k-1. Then max value of r is d-k. So to represent max value (d-1)*k we'd have d*(k-1) + (d-k) = d*k - d + d - k = d*k - k = k(d-1). This means overflows can take place in range [0,k] and residues in range [0,d-1]. But note this gives extra information, because
(k(d-1) + 1)^n > (k+1)^n * d^n. 

Any finite integral domain is a field, so we are stuck with fields. 

So i'm struggling on how to conveniently represent the summed commits in noramlized form such that no checks need to be performed on it. Checks, however, may be unavoidable, because if challenges don't have the same 1-norm, then k is variable, so the expected max of each challenge is different. Maybe we add together the 1 norm for all challenges, k'. Then we are left to represent the range [0,k'(d-1)]. But I think we may need to perform each challenge check by itself. Theoretically a way to do this is plug in the regular d range and allocate enough space to decompose each element. Then go through, make sure the encoded value does not overflow the space it should, despite already fitting in the allocated space. 
For a given k, each element will be in the range [0,k(d-1)] but each slot in the allocation space has range [0,d-1]. So we need to represent k(d-1)+1 possibilities with a power of d possibilities. The power will be Ceil(log(d,k(d-1)+1)) = Ceil(log(k(d-1)+1)/log(d)).


I'm thinking in a distributed network with high liveness it may be cheaper to do random selection of addition between polys. That is, we wait until enough polys have accumulated for evaluation, then we randomly select which polys should be added with which others.  


Suppose we have s (security parameter) random mask polys in the randomziation, such that for each challenge the probability is 2^{-s} that none of them appear in a random challenge. Well remember s is about how many polys we need for amortization. Or what if we just choose a single mask poly and add it to the randomized sum. But we might need the poly to have entries as large as the summed poly. For hiding, I think we cna say adding this random poly to the summed poly yields no info about the summed poly. Now for soundness. The summed poly is basically randomized prior to addition with the mask poly. Thus I think soundness holds. 
I think this must be done in the last layer. So in previous layers prover gathers its own polys and also the polys of others (which are zk). The polys of others, though hiding the data of others, cannot be leaked to the next party because that shows the next party which previous parties the prover interacted with, eg transaction history, so that leaks. So upon aggregating own and previous party polys, in the last layer the prover generates random mask, then randomizes the other polys, then adds the two. Actually we may need a mask for each output poly. 
How would this work for DLP? Prover takes the two or more polys from previous proofs, generates a single new mask poly, then randomizes the others, then adds the mask.
Now does the last layer leak data in its computation? Suppose all commits are blocked. Yes, it can leak by how it verifies previous proofs, so we need a way to block its sumcheck. Hmm, here unfortunately the prover will need to pass on these mask polys


It may be too complicated to do a single molting cycle involving among parties, and instead do one cycle or more cycles per party. But I can mention how the former is worth considering, though it makes zk and trust more complicated, but gives the benefit of smaller computations per party. 

note can pack two polys into one by concatenation then adding one extra variable, then evaluating at the same point. Thus we can commit to multiple small polys via one large poly. We reduce evaluating the small polys to the same point, then we reason about the evaluation of the concatenation poly at that point with added variables, which will be instantiated based on the claimed evaluations of the polys. For example, this would be useful for the mask polys in the sumcheck.

Maybe we can actually do one or more molting cycles per party regardless how much data that party does, and at the end each party always sends to the next the same amount of data. Remember the data of one party has two parts, the data that party received from others, and the data it generated on its own. For zk it can't send its own data in isolation. I think the prover provides all commits, which can be blinded. Then based on those a challenge is formed, and the prover computes the response using its own data together with previous party data. The input of the challenge may be smaller than the output. Then the prover adds a mask poly to each response poly, and sends those polys to the next prover. Note that the entries sizes of each of poly depend on the number of input polys to the challenge, so the fewer inputs the smaller the output, but only logarithmically. So all this logic is really happening in the circuit of the next party, and the circuit of the current party is oblivious.
For DLP this means the prover has the single poly accounting for past parties, its own single poly, and a mask poly. The challenge randomizes the former two polys, then the last poly is added. Now for full zk, the number of commits one party transfers to the next should be the same. We would like party to submit to the next the same number of polys and have each poly with the same max bound, and have that bound be tight. ...


Maybe it would be more efficient to encode polys in bits instead of the native field. Then upon randomization (and limiting the max number of bit poly inputs to the challenge) the max bound of the response polys can be in the native field. 

Realized one of the rows (eg the last) of the challenge matrix can be all 1s as far as poly eval soundness goes, because this corresponds to the constant term of the randomn point evaluation. 

Consider the equation b^x = 2^{security_parameter}. Suppose our challenge matrix contains x collumns and entries in b. Then this satisfies the poly eval soundness. Consider one of the rows to be 1s. So we will have at least 2 rows. For DLP we have 2 rows and b=2^{security_parameter} and x=1. But for lattice I think we need to restrict b to be polynomial in the security parameter. Since 2^{security_parameter} is exponential in the security parameter, we will need x to be super constant (maybe poly, or maybe just log) in the security parameter. 
Now we want to consider the knowledge soundness. 

Given a set of l commits. Consider a random challenge with infinity bound b. We could later consider viewing this as a random point of evaluation for a multivariate poly, to decrease the randomness needed, but for now for simplicity just consider a vector all with random entries, of which there are b^l. Suppose adversary succeeds with probability eps > 1/b for a random challenge. This means prover succeeds on eps*b^l of the b^l total space. How can we use this ability as a subroutine? Note that we can invoke it poly many times, and don't need to limit our usage according to how many challenges we issue in the real protocol.
Suppose we could find two challenges that are the same in all l entries except entry i where they differ by 1. Then subtracting the Z witnesses we get one for the i target. 
Suppose we use the subroutine to find a solution. This takes expected time 1/eps. Then focusing on i, we see if we can find another challenge that differs only in index i and only by 1. There are at most 2 such possibilities. We invoke the subroutine on these too, and see if either works. 
Suppose we consider the matrix H with boolean entries depending on whether adversary succeeds for corresponding position. Rows are enumerated by possibilities of all challenge entries except i, together with the adversary's randomness. The columns are enumerated by the possibilities of entry i, of which there are b. So upon landing in an entry in the matrix, we seek an entry adjacent to it. 
Now we are answering the question of whether the adversary can pass the test. But we will already have closed the door to the question of collision resistance. Suppose we land in two different rows of H, and then find entries in two pairs of columns that are the same distance apart. Then we have two witnesses for the multiple of the target corresponding to gap. If the witnesses are different we have a collision. But nothing here guarantees they will be different.
Note that if we obtain witnesses for two multiples of a target that differ by 1 we can subtract. This is not a re-statement, because above we are considering a gap in the challenges. This gap will determine a single multiple, and in this new context we are considering gaps between multiples. If we obtain more than b/2 gaps then at least one pair must be adjacent. 
Let's try to use the heavy argument. Then half the initial landings will be in heavy rows. Then we will execute in search of secondary landings on all b columns. If the row is heavy then at least b*eps/2 >= k of the entries will be filled. This requires b >= 2k/eps thus eps >= 2k/b. Thus we could have 1/b < eps < 2k/b yet no extraction method. Therefore we need to assume prover succeeds not with probability 1/b, but 2k/b, so we will need more challenges to reach the same soundness, and b > 2k. Thus the number of successes in a row will be less than half the row size.
So for every heavy row we hit, which will be half the time, we will obtain at least k-1 gaps, and likely more. We would like to continue until we have b/2 gaps or we find two adjacent gaps, or we find a single gap of width 1. What if we set k >= b/2? then b > b. So we must have k < b/2. Now the trouble is nothing seems to prevent us from getting the same gap over and over. Suppose we have multiple pairs of challenges, where for each pair all entries in the challenge are the same except index i for which there is there is a gap of width w. Both challenge pairs yield witnesses for a w multiple of the target, and we assume these witnesses are the same. Denote the tuples (ca1,ca2), (cb1,cb2) with witnesses (wa1,wa2) and (wb1,wb2) such that ca1-ca2 = cb1-cb2 and wa1-wa2 = wb1-wb2. Then ca1-cb1 = ca2-cb2 and wa1-wb1 = wa2-wb2. And these are not zero. This leads to a solution to the challenge ca1-cb1. I don't know how to take advantage of this.
Note that subtraction may end up with a challenge or witness containing negative entries. We are working modulo q, not modulo b. 
Note if we obtain two multiples m1 and m2 such that m1*c = m2 for constant c, then for witnesses w1 and w2 we must have w1*c = w2 unless there is collision. 
It seems if given two different multiples its always possible to use them to obtain a 1. Given multiples m1 and m2 we want small x and y such that
m1*x + m2*y = 1
The idea for Euclidean algorithm that is also why I think only two is necessary is given two distinct multiples, upon subtraction we will obtain a multiple that is less than the larger of the two. Then we may continue obtaining smaller multiples as long as our two multiples are distinct. So actually it can only yield 1 if the multiples are relatively prime. 
Intuitively, the problem is prover may be able to pass test only knowing witnesses for scaled targets, ie targets mapped into cosets. I think with enough challenges we could with high probability issue challenges such that the set of scalars for a target can yield 1, ie there is a pair for which gcd is 1. Suppose we establish this. Does this make it harder for the prover? 
Why can't we reason that if prover can pass with high enough probability then it can answer enough challenges (more than issued in the real protocol) such that we can solve for S given Z in the equation
SC = Z
thus AZ = TC => A(ZC^{-1}) = TCC^{-1} = T
If we can establish that C has a right inverse with small entries with high probability then S can be of the bounded form ZC^{-1}. 
Suppose prover succeeded on a challenge with probability 1/b. At this point the number of commits, ie the height of C, becomes relevant. For now just suppose some constant l. Now for poly eval soundness we need s/log(2,b) challenges. Thus the probability of passing all challenges is (1/b)^{s/log(2,b)} = 1/2^s. So we've lower bounded the number of challenges via the poly eval soundness rather than knowledge soundness. Note that in this approach we are not using a heavy matrix. Instead continually issue random challenges to the subroutine and we find a solution in expected time 1/eps. We will need eps large enough that we can solve for enough challenges that we can invert C. The number of challenges to invert C depends on the height of C. The number of challenges also depends on b due to poly eval soundness. Now if the number of columns induced by poly eval soundness, s/log(2,b), is to be sufficient for knowledge soundness then we must ensure that the adversary cannot pass a challenge with probability more than 1/b. This means that if the adversary can pass a challenge with probability more than 1/b then we should be able to extract from it. So it seems we are asking too much.
Maybe we could combine this with a heavy matrix, where upon finding a solution, we can probably find related solutions. But this only helps if we can recover more challenges this way than if we just randomly sample. 
The goal is to be successful with enough to establish a challenge matrix of rank l. We need to recover not just enough to pass the test, or not just enough to make a square matrix, but a little more than square. For each heavy row, we could collect k witnesses, this would happen almost half the time, so for k' extractions we'd have k'/2 + k'*k/2 witnesses. But for a heavy row to always have at least k means eps >= 2k/b thus 2k/b (> 1/b) must be the expected success probability, so this dictates a lower bound for the number of challenges. Recovering this number of challenges will be done with probability eps, and we need to take advantage of the k heavy rows to recover enough extras. 
The intuition is that by using larger scalars we can achieve the same soundness but with fewer challenges. This works for poly eval soundness, and I hope it works for knowledge soundness. 
So subroutine succeeds with probability eps for a challenge then it takes expected 1/eps samples to find one the subroutine succeeds on. The probability the subroutine succeeds on all of j challenges is eps^j, so soundness is multiplicative. What is the expected time for the subroutine to succeed on j different challenges? Clearly for j=1 it is eps. Suppose after finding j' of them we freshly sample, not avoiding those challenges already solved, but instead we just account for the probability we encounter one already solved. Then is seems it takes expected total time j*eps, thus recovery is additive, not multiplicative. This seems a huge advantage over the regular heavy argument that considers challenges in a single set, setting j=1 so that additive and multiplicative are the same. So the technique of finding a right-inverse to C may succeed just fine. Continue solving challenges past the number of challenges issued in the real protocol, until C has rank l with high probability. Suppose b is a prime. We would like to establish that the right inverse has small coefficients, say also up to b. 
We've been considering the heavy argument for solving two challenges C1 and C2 that differ in row i. We then subtract them hoping they contain adjacent constants in the same index. But what if instead we find a vector v such that the dot products of the ith rows of C1 and C2 with v yield adjacent constants. Actually, suppose we solve as usual to obtain
AZ1 = TC1, AZ2 = TC2
A(Z1 - Z2) = T(C1 - C2)
but instead of just choosing one of the indices that differs, hoping its 1, we instead hope at least 2 indices differ in such a way that we can find a small coef vector v such that the dot product of the ith row with v produces 1. Then we have
A((Z1 - Z2)v) = T(C1 - C2)v = t_i so the vector (Z1 - Z2)v is our solution. 
Another approach in this direction is to do the per-challenge version of the heavy argument, try to solve several scaled targets. Then instead of just hoping they can be added in such a way as to produce 1, we instead hope there is a vector v with small coefs such that the dot of v with the scalars produces 1. Then v multiplying the witnesses is a solution.
Of course this method gives a larger witness up to a factor as much as the coefs of v and also a multiplicative factor of the number of terms in the combination. We actually already partially considered this solution when looking for coprime multiples. 
The benefit of the right-inverse is the challenges are uniformly distributed, whereas for the heavy argument when we seek in a heavy row its not uniformly distributed so we can't statistically guess how the challenge may differ, only that it somehow differs. On the other hand, the benefit of the heavy argument we only need to normalize one row, whereas for the right-inverse method we must normalize all rows. 
There are b possible values for index i. If prover solves on whole challenge set with probability eps > 1/b then prover can solve on eps*b > 1 of those possible b values. It could be the prover always succeeds for 0 and just one other value. For any two indices i, j there are b^2 possibilities. If prover solves on whole challenge set with probability eps > 1/b then prover can solve on eps*b^2 > b of the b^2 possibilities. For the total of l indices there are b^l possibilities, so prover can solve on eps*b^l > b^{l-1} of all b^l possibilities. 
Suppose prover can solve on 0 and one other constant for a particular index, and maybe all constants for the other indices. This means solving a challenge with probability > 1/b. I think the problem we're having is prover can pass with non-negligible probabability as just said only knowing witnesses for scaled targets, when we need witnesses for non-scaled targets. This may justify not using scalars and just doing addition. 
A problem with the right-inverse method is that since our challenges non-negative, a vector to normalize them will require negative entries. This vector with negative entries will multiply the witnesses which may result in a witness with negative entries. Well I guess this happens with the regular heavy argument anyway. One possibility that may be helpful is allowing negative scalars in the challenges. This way a Z with negative entries is expected. In fact, the S vectors can also have negative entries, and in fact can be any integers, as they are all just different equivalence class representatives. 
Solving for a right inverse is equivalent to solving an l by j b-ary lattice basis C for a short lattice vector that results in one of the standard basis vectors. If we could phrase this as a reduction that we know is too good to be true then we don't need to present an algorithm for solving the reduced form, ie finding a right inverse. But one complication is we are solving for a relaxed form, ie a multiple of Z bound, not the Z bound. Another complication is the C lattice not actually a b-ary lattice because we are not doing modulo b. Its not necessarily the case that a solution to the modulo version means one can find a solution to the integer version, thus reducing to the modulo form may not be possible. 
The heavy method retries of a vector of bounded euclidean length for a target. But this is not a solution to SIS or ISIS, because targets are not necessarily random. But maybe we can reduce random ISIS to non-random ISIS. Given random ISIS, multiply both sides the constant c such that for random target r we have rc = t where t is the non-random target. The constant c is entirely dependent upon r and t which are both independent of the constants for the original problem, thus the constants in the derived problem remain random. If q is prime, then once we find a solution for the derived problem we can divide out the constant. In general, we want gcd(c,q) = 1 => gcd(t/r,q) = 1. So t cannot be 0. Maybe we should consider each row of A separately. 


Note we could even do 'independent' testing, where we issue one random test, then upon the response we issue the next test, using the fiat transform.

What if we make prover commit to not just a vector s but powers of it, eg s^2 and s^3. 


A big question is whether or not to use scalars. If we do, we would still need a nontrivial number of challenges. But compression would be better. 

b^l*eps/2 >= k
eps >= 2k/b^j

Now i'm thinking again about using a structured matrix, but now I'm not going to try to make it sparse. Instead we focus on a structure that will help with the knowledge proof, and the price will be that columns are not uniformly random and thus we must modify the poly eval soundness. We structure rows such that any two instances of a single row have at least one index that differs by 1, rather by a set such that gcd of that set is 1. 

One argument we could make is that when sampling a second row uniformly, there are many more 'helpful' rows than unhelpful rows, so the probability we select one find a successful unhelpful row before we either find a helpful one or abort is small. In this method we could possibly still select our columns as random evaluation points, which will probably be just fully random infinity-bound vectors. Since rows have indepedent entries we can analyze them index-wise. We will count the probability of each absolute difference. Then we need to calculate the probability of not gcd set.
Consider just getting absolute diff of 1. There are b^2 possibilities for 2 elements. There are b-1 differences that are 1, and b-1 differences that are negative 1. This is more than any other absolute difference. There are b differences of 0, 2(b-1) absolute diffs of 1, 2(b-2) absolute diffs of 2, ... 2(b-k) absolute diffs of k. So out of b^2 possibilities, 2(b-1) diffs are 1, so that happens with probability 2(b-1)/b^2. Now we compute the probability no indices of two random samples diff by 1. This is (1 - 2(b-1)/b^2)^{128/log(2,b)}. To make this negligible we need to do it multiple times. But I'm concerned our samples are not random enough. I think we would need to ensure all random secondary rows we draw would be valid, this way regardless which of them is successful for the adversary, it will work for extraction. So for two random samples we need the probability their diff would not work to be 2^{-s}. Then upon drawing n random samples the probability not all of them work for extraction is (1 - 1/2^s)^n. This remains <= 1/2^s as long as n is about less than s. Well we need n to be on the order of 1/eps, so this won't work.

Regarding ISIS and collision resistance. If one can find a collision for a target t where the witnesses w1 and w2 may have negative entries, we can subtract the witnesses to get an SIS witness but with twice the bound. So recovering a witness for a target with negative entries is not a problem. 

For a random challenge with b^l entries, suppose eps >= 1/b fraction, ie b^l*eps >= b^{l-1} of them are filled. Then b^l*eps/2 of the entries are in heavy rows. We haven't even decided the matrix format. But each heavy row has at least a fraction of eps/2 >= 1/2b, ie j*eps/2 >= j/2b entries filled. 

I think we already stated intuitively why scalars may not be sufficient. Suppose there is only 1 target prover doesn't know witness for, but prover knows witness for a multiple of it, c, that sends that target into a coset. Then prover succeeds with probability 2/b, which is more than we want. We are worried the prover only has knowledge for multiples. We I think we can account for these multiples by increasing challenge numbers. On the other hand, the more distinct multiples a prover has knowledge for, the greater the probability the set has gcd 1. Thus we may be able to balance this trandeoff. For example, suppose prover only has knowledge for the multiple k. Then the probability of passing is about 1/k > 1/b. This only means increasing the number of challenges by a factor of log(1/b)/log(1/k) = log(b)/log(k). But this already accounts for all multiples of a 1/k of the possible multiples. When we account for more multiples, and what multiples they can produce together with previous multiples, our challenge number will need to further increase, but the probability of gcd 1 will approach 1. The next step is to figure out how many targets a prover can know without that set having gcd 1.

What if we multiply both sides by a multiplicative inverse of the multiple in b? Then the scalar of the target becomes m*b+1 for some m. 
If we could have knowledge for a multiple that divides b, we could have knowledge for b, then we could subtract m*b above and end up with the target. Oh, but b is prime so nothing divides it. Actually b doesn't need to be prime though and could be anything, because that still works for zippel. But if b is not prime then not all multiples will have inverses. 

What if we choose b=4 (only a little better than b=2) so any distinct subtraction in the set {0,1,2,3} results in multiples of the set {1,2,3}, which all have gcd 1. But what is to guarantee that if we get multiple 2 we will also get multiple 3 or vice versa? 

I think the technique of accounting for multiples that don't have gcd 1 is too costly. Even just accounting for the multiple 2 means increasing the challenge number by a factor log(b)/log(2), which is 2 in the case of b=4 meaning we must double the number of challenges, eg from 128 to 256. More challenges is not what we want and defeats the point of scalars. 

And now I realize the right-inverse method won't work for the same reasons. A row in C could have gcd > 1. In other words, there's no guarantee any two, much less l columns will be linearly independent. 

So before I abandon scalars, I want to leave one prime example of why this doesn't seems to work. Suppose prover knows witnesses for all targets except for one, knowing only a witness of multiple 2. Then prover passes in (at least) b/2 out of the b possibilities for that index, and all possibilities for other indices. This means passing with probability 1/2. Thus the number of challenges is no better than if b=2. And at the moment we don't know any way to take advantage of knowledge of a scaled target.

So then it only remains to go with binary challenges. What if we also test scaled targets with binary challenges? Is this the same as scalar challenges? I think its a bit different because we can fully choose the scalars rather than leaving it probabilistic. Suppose we choose our scalars, then do binary challenges. The binary challenges should prove knowledge of the targets, which are specific scalar multiples. It seems there is no benefit, because we must still employ the same number of binary tests. 

I think by using the regular binary heavy argument, we can do at least one molting cycle per proof. 
I don't think the witneses need to be checked using holder. instead we just need it inside the binding space, and we can designate a fixed amount of memory for that such that anything that fits in that memory is valid. 
I'm wondering about the concept of optionsal zk. This is where a prover decides whether or not to do the extra costs of zk, and it doesn't much for verifier. I think it should work and be a meaningful concept worth defining.
For zk we need a fixed number of commits for each molting cycle. If prover has more and wants zk then probably needs to do multiple cycles. If prover has fewer than prover can create some extras that are zero vector or one already had, but with a different mask. Then despite different looking witnesses, they shouldn't matter because random poly mask can compensate for them.
So the standard zk delivery would be fixed number of witnesses of fixed bound. This would introduce a standard conversion to normalized form. Suppose our normalized bound is b, and our standard witness bound is m*b. Since log(b,b) = 1 it takes 1 memory slot (normalized form) to encode a b value. Since log(b,mb) = log(b,m) + log(b,b) = log(b,m) + 1 each witness element will be placed into log(b,m)+1 memory slots.


Its worth thinking about the programming logic. Maybe its part of the model. We'd like to think of it as computer programs. Recursion is possible because we use meta data to describe the logic, and we can also amortize those evaluations. Otherwise we would hard code everything and verifier's circuit would have to simulate the proof circuit which is itself a verifying circuit. The other papers had to deal with this partially, but we avoid it by amortization. I'm now wondering if we can use our method with non-holographic proofs, eg with STARKs. Here verifier circuit must know the (uniform) logic of the proof. I suppose this is possible by the recursion theorem. 
But for the holomorphic type I'm wondering how our meta data can efficiently express our logic. The parameters, like sizes of data, and number of instances, will affect the polys so we need a way to handle this variancy while still only having a small set of polye to evaluate to keep the eval amorization possible.
Our basic operations are addition and multiplication, and above that we can define more complex operations, which are basically polynomials, together with abstract operations like map and reduce. We want to create a language generic and efficient enough that we want to make it a standard for the GKR type proofs. 


is there any use in proving the zippel lemma for alternative distributions, like p-norms? if we use p-norms for challenges then the rows of the matrix are not enumerable for the heavy-argument. 


suppose we use {-1,0,1} for scalars instead of just {0,1} such that we either subtract one direction to get 1 or we add to get 0, which contradicts SIS. Then we have j >= (s+1)log(2)/log(3) + log(2)/log(3) = (s+2)log(2)/log(3), which seems a good improvement, and very luckily it seems adding -1 as a scalar doesn't increase the amount of information in each witness like 2 would for example. Let's recalculate amortization.
Suppose standard commits are in b. Length m stays the same so we leave it out. Then we take l of them and amortize. So we start out with entropy b^l, and after amortization we have (bl)^j. So breaking the threshold means b^l > (bl)^j => l*log(b) > (s+2)log(2)/log(3) * log(bl) => l > (s+2)log(2)/log(3)(1 + log(l)/log(b)) => l/(1 + log(l)/log(b)) > (s+2)log(2)/log(3). 

But now we should probably employ an actual holder-inequality check. actually, not really. we could just keep track of 'offset' and otherwise we just use a slot of an l-multiple. The offset will be one of l values I think, so it has size bit size log(2,l). 

recall how we will performing the opening? the witnesses will be encoded in normalized b-form but provided to a circuit in the additive q-field. The statement to be proven is that the witness commits are valid openings for the derived target commits. In fact, could we do all q-operations in this circuit? This means multiplying TC. I suppose so. Then the circuit statement is that new targets T' represent valid witnesses for T in terms of C, and maybe even C can be local to this side-circuit where it is computed from T. The witnesses of T' are to be evaluated as polys at the points proposed for T, but instead of doing this in the circuit it would be nice to just derive a new evaluation claim directly, and this may be possible by encoding the normalized witnesses such that they represent appropriate polys. So we will want to compute each term t_i*c_j multiplied by a random element r, as part of a sum. But t_i*c_j is not normalized so we need to split it into normalized parts and each part is a term multiplied by r that belongs to a different poly. The challenge is normalizing so the noramlized polys evaluate correctly only if the unnormalized would evaluate correctly. Doing it this way may not be so important, because we will be going through all terms t_i*c_j for opening anyway, so we could also probably evaluate them as we go with little extra cost. 
we want the verification of this proof to be simple, and we hope to achieve that because it should involve few multiplications, and without the need to compute TC I think it involes zero multiplications, and even with TC that fact that C is probably of {-1,0,1} may help in someway to do metamultiplication. 
We can have our new polys W' with targets T' and with encode polys K' and zero poly z and constant poly c and divider constant m we can set up a comutations with arbitrary additive constraints. So with . meaning dot product we'd have something like 
w1'.k1 + w2'.k2 + ... + c = m*z
and the degrees of k_i and z signifies the number of constraints. Before we think about the metamultiplication, what about the ease of verifying this? Verification means taking the commits to the transcripts and m and choosing a random point of evaluation. We are in a big field q (q is prime) so we should only need 1 point of evaluation. (In fact, choosing q to be this big, eg 128 bits, we may be able to improve the amortization in some ways, and hopefully a larger q means a smaller worst-case lattice dimension.) Giving a point of evaluation we are to evaluate both sides and compare. Of course doing this is more work than manually computing the whole thing. The benefit of this form is that we can use crypto or amortzation.

For crypto using DLP we secretly choose random eval point r then evaluate each encode poly k_i then encrypt it using DLP. Prover exponentiates each encryption then multiplies them together. z will also be evaluated at secret point and encrypted. QAPs uses multiplication so is pairings based, and now I'm wondering if this simpler version without multiplication would still work. I suppose so.
Could we make this work with lattices instead? Encrypting a value would mean splitting it up into multiple pieces for each constant rather than a single constants like DLP. We would add and multiply encryptions. I think making sure prover used encryptions requires consistency checking that requires multiplying encryptions, which we don't know how to do with lattices. 

For amortization we can take a random linear combination of multiple transcripts and they should form a valid transcript. Now for DLP this is easy, but for lattices remember we need quite a number of instances before we can do sufficient randomization. In particular, given any number of instances we will need to generate not just one randomized instance but many. But actually we know this is sustainable because the output size only grows logarithmically with the input size. So maybe we just verify by this randomized amortization, but notice its this side-circuit we are trying to verify that was designed for computing randomization and amortization operations in the first place. But this is not circular dependency because a later instance will verify a former instance. In fact, the randomization for this can probably be taken care of via the regular randomization for poly evaluation, because that poly randomization doesn't depend on the random point of evaluation for them. So for poly evaluation we have a random point of evaluation, and for this circuit we have a different kind of evaluation, but in both cases amortization happens by taking random linear combinations of the transcripts.

Now regarding communication between the circuits for efficiency we can encode a q value via multiple b values, and then have the q circuit construct its inputs and deconstruct its outputs via linear combinations. So all statements are expressed via b values. Bits work too, but b values are  ore efficient. 

Now we can talk about metamultiplication. This means we have a transcript value c in {-1,0,1} to multiply another transcript value v. We can't directly multiply them. Instead we will need to multiply v by one or more constants and c must dictate how we do this. We are limited to linear combination so this kind of case-dependent reasoning, like branching, may not be possible. Forget 'multiplication' and instead think of it as addition dependent on data rather than fixed constants. 
If we can't figure it out then it appears we must compute TC in the main circuit, and if q is prime we can't use the isomorphism but must do long arithmetic.

I think we could generally define our poly molting scheme by a single amortization round that shows how entropy is dropped. It consists of additively homomorphic commitments, and the zippel lemma, and reducing evaluation to a single point then randomizing sufficiently. This describes both DLP and lattice just with different params along a spectrum. 

If we assume q is prime, which we need for the side-circuit approach, can we get any advantages? Intuitively, making q prime should make the problem harder. For any non-zero target there always exists a unique scalar (maybe large) that yield any other arbitrary target. We're interested in hitting an arbitrary target by a linear combination of small scalars 

We'd also like to use this side-circuit for fiat-shamir transforms. Can we justify that SIS type hashes are sufficient, maybe with more than one layer? 

We still haven't figured out the side-circuit. Instead of trying a QAP-type structure, we could try to take advantage of the uniformity of the SIS computations, using STARK-like structure, which is a different version of poly-div. Like
w(x)k(x) = h(x)z(x)
but now I realize this is not good for summing. Maybe we can create a custom proof structure.
Or maybe we just use regular map-reduce sumcheck, which would take place over the prime field q and would be able to take care of the multiplication in TC. The challenge is performing the verification for this proof, but maybe even this can be delegated to a future side-circuit. Suppose we were just doing SIS, maybe multiple times and multiple instances. Then we take a random linear combination of each. We iterate log(m) many rounds. At each round we should receive a quadratic poly, one variable coming from the transcript poly, the other coming from the consants poly, and they multiply each other. At the end we evaluate all polys at a single point. But what I described so far is only a linear function which could be done in QAP structure. So its only worth it, and won't be much more complex, if we do multiplications. With this simple method we could incorporate the multiplications of TC by the verifier (delegated to future side-circuit) calculating the expected output, the TC values. Note there would be a long linear combination on the order of n*s, the worst-case lattice dimension and the security parameter. 

We could maybe make q prime and still have an isomorphism via the multiplicative group of an extension field, make q a Mersenne prime so q = 2^d - 1 so that its cyclic and the same order (and thus isomorphic to) the multiplicative group of the extension field of order 2^d. Other bases don't generalize to primes of this form, buy maybe we could find a small prime base b such that we can use a subgroup of the multiplicative group of an extension field, which is always cyclic. So any subgroup is also cyclic and to waste as little as possible we want the divisor to be small, ideally 2 (a single bit wasted). This means we want (b^n-1)/k = q for small k to be prime. If we use subgroup we want minimal effort in checking inputs are in the subgroup. One way is to present a natural number for the cyclic position (no restriction here), then for the verifier to generate it, but this requires log(2,q) extra multiplications.
By the way, doing the poly computations should be done by computing the respective polys, then for modulo both divisor and remainder are submitted as auxiliary, and it should be easy to check. A question is how often we do modulo, extremes being once for every multiplication and once for every final answer.

So if we can indeed use the isomorphism then we can use the linear QAP structure. Can we do layered hashes this way too? Yes I think so. It means we add a constraint to destructure an output to a new input. This are intuitively span programs, were we have a target vector and we have a set of basis vector and any valid witness is a set of scalars such that a linear combination of them and the basis vectors is the target poly. 


Map to another diff poly that is incorrect but shares at least one common root with the real diff poly. For quadratics there are about |F| of them. Then breaking means mapping to a root of one of these. We must compare the |F| with q. If |F| is significantly less than q this becomes hard, but we will need |F| to be large.
The cost of making q big is large hash output, and since all of it should be used this means large field size, so |F| and q grow with each other and are not independent. 
The way we planned to do hashing is make q and n large enough for security, such that it allows for compression, then set the field size to about this output size. So actually q and |F| are about the same, at least proportional. 


Maybe we make q the main field size. It can still be small enough for efficiency. But then commit entries will need to be smaller than q, which shouldn't be a problem. In any case, we use the commit entries to do our computations and only require that they are in the binding space, which reduces to our randomization check. Normalization should work too. 

We need to figure out how to split our logic and theorems and schemes into modules. For example, using q for field size or a custom field size both end up with basically the same lattice scheme so we'd like to describe the lattice scheme independently. 
Maybe we can describe molting schemes independently of recursion, leaving parts like the vierfier abstract (a person or a circuit). We could require that a molting scheme be a complete cycle, closing the loop. So for lattices it would also involve the normalization. Another option is for us only to require a sublinear amortization argument, which may be cleaner.
We need to decide how to model a sagaproof. Prover and verifier are not so clear roles. Every proof has a prover, a circuit, input and output, a proof string. One or more parties can verify this proof. 

For lattices, what if we try designing together with a network to help with efficiency regarding the high amortization costs. I'm thinking of doing the amortization offline, which means the witnesses for a single amortizaiton round are those for the whole network. Proofs are passed up the tree, the commits are committed to, which is not the same as a fiat transform. At the top we actually do a network wide interactive proof random response to avoid fiat transform. But in the next network round circuits would still need to compute the new witnesses and do normalization. So really the only use is preventing the fiat transform, and constants wouldn't be better. Just as much if not more hashing would be necessary, and parties would still have to prove to each other knowledge of valid witnesses requiring high communication costs. 


So we'd like to describe how proofs can be composed via tail recursion with a sublinear amortization scheme. Then we show our sublinear amortization scheme for polys, and their particular versions for DLP and SIS and their zk variants. Then we show the details of normalization, side-circuits, circuit communication, etc. I think we'd like to describe a sagaproof as one for an abstract proof setting where we reduce verifying computation to verifying a property of data in a commit, and the sublinear scheme has the formal purpose of reducing verifing the property for commits to some computation (not necessarily doing the same verification) that requires sublinear data in the commits. Sagaproofs necessarily require the notion of commits. We can formally define what a proof scheme is, what a commit to data, and a 'propert' of the data is, and how they must interact, and then show how our poly scheme satisfies this, where our property I think is evaluation claims per poly, which means that eval reduction belongs particularly to the sublinear scheme part of our sagaproof instantiation. Our sagaproof defintion would be two fold, with the general defintion involving the definition of a sublinear argument, which will be defined prior and an independent definition. Commits will be defined prior to that. I think we would define the 'rate' of a sublinear scheme and then in the sagaproofs definition relabel it in accordance with a notion of molting. 

notice our sublinear scheme is necessarily dependent on commits or else it would not be sublinear. we shoud call it a sublinear property argument (SPA). and note that since commits rely on owf, we must use computational soudness, not statistical so we must say 'argument' instead of 'proof'. 

apart from the introduction where we give some informal descriptions, I don't want to talk about polynomial details until section 3. In fact, section 2 should be completely abstract. 

Maybe instead of using ROM we don't accept it doesn't work for our recursive proofs and instead state our assumption about fiat transforms. So we define V as a deterministic I think, not probabilistic, while P remains ppt. 

I don't think we need to involve the logic and/or recursion theorem in the model. 

should we have a compliance predicate? Remember we need a finite amount of logic, equivalent to a codebase with many functions. New functions can be added, but dropping or modifying is harder. Can we design this abstractly as part of the model? If our proof system treats code like data then verification with regard to the code should reduce to verifying the property of the data for that code. But the code is static data, so we only need to reduce evaluating two possible properties for the code data to verifying one property. We could work this, and the codebase, into the model if its indeed the only way we can efficiently implement logic. The method I have in mind depends on the 'property' being ealuating a poly a random point, so QAPs related structures won't work. 

A downside of the FRACTAL preprocessing is it requires the code to be fixed. We can say how theoretically this is not a problem because of universal machines, but it is not efficient. 

Maybe we could do a QAP/aurora type structure where use linear amortization for the matrix satisfactions, and we employ something else for the multiplication. Maybe we could use sumcheck for the multiplication. So start out with a witness commit w. Also make commits x,y,z purportedly such that x = Aw, y = Bw, z = Cw, and x.y = z. Remember commits are just vectors. Given multiple witnesses w_i we just take their random linear combinations (and corresponding claims) for the linear amortization. For multiplication with sumcheck we will need to evaluate the polys at a random point. For eval reduction we will treat them as multilinear polys. We will use the same randomization for both the linear and multiplicative amortizations. 
Actually there seems to be a simpler way that I could call my own, and it may work for both DLP and SIS and be a general non-uniform proof system, whereas sumcheck is suited for uniform. We have a witness w for each proof together with a 'divisor poly' h. Each proof consists of these two polys in commit form. Suppose w is of length n. Suppose we put m constraints on the witness. Suppose we do selections via 'code' matrices Ki. We will speak of 'the poly induced by Ki' to be the vector of coefficients Kiw. We can consider the witnesses as consisting of multiple parts, yet still have it packaged as a single commit. 
For a moment just consider a quadratic system K1w.K2w = K3w. To prove this we will use the poly div method. This means we will show that (K1w)(x)*(K2w)(x) - (K3w)(x) = h(x)*z(x), which means evaluating (K1w), (K2w), (K3w), and h at the same random point, which really means computing scalars rK1w, rK2w, rK3w, rh, rz via matrix arithmetic. For now only consider those with w. We can pick a random 'point' for eval reduction (eg 1,s,s^2,s^3,etc) and instead of evaluating the poly for each Ki we take the respective linear combination and evaluate that, ie 1*rK1w + s*rK2w + s^2*rK3w = r(1*K1 + s*K2 + s^2*K3)w. So we reduce the verification evaluating (1*K1 + s*K2 + s^2*K3)w and h and z at the random point r. 
But for amortization we will do this a little different and keep the code matrices separate rather than merging, such that they can instead by merged across instances. So a proof consists of w and h, which induces the single random point r for evaluation in a fiat transform, which prompts the prover to include in the proof claims of rKiw, rh and rz. Verification means checking the evaluation equation and the claimed evaluations. But an amortizing circuit will only do the former, and amortize the latter. For two proofs (wi,hi) and corresponding eval points ri and corresponding claimed evaluations, we first reduce to evaluation at the single point r which means interpreting as mutilinear polys. Then we choose random s and reduce to computing \sum_i s^i*rKjwi = rKj(\sum_i s^i*wi), and also \sum_i s^i*hi, both of which are acts of taking random linear combinations of the witness commits. So our conditions regarding h polys will be a claimed evaluation, while our conditions regarding w will be that an induced poly for each Ki has a claimed evaluation. We we are doing regular poly eval reduction but now we are cleverly doing it with respect to induced polys. So unlike Aurora and such, we have avoided the need to commit to any encoding of Kiw, instead only w. 
I wish we could take more advantage of how the same r is used across different Ki. An alternative to poly div that may not leave this lost opportunity, and operates in linear rather than quasilinear time, and avoids the need for h but incurs additional rounds, is sumcheck. We perform the sumcheck over the invisible induced polys pi something like \sum_x (p1(x)*p2(x) - p3(x))u^x, where u is a random point multi-point to ensure with high probability the sum being zero implies all terms are 0. So we are summing over the constraints. But in fact, this too yields evaluations for each induced poly at the same point, the difference is its a multi-point not a uni-point. 
Hmm, maybe we could in fact merge in both ways, across both Ki and across instances, by waiting to choose the random linear combination for Ki until we have both instances. So we begin with claimed evaluations with respect to all Ki for all instances. From that we choose a single s that will serve for Ki for both instances, so we are left to evaluate ri(1*K1 + s*K2 + s^2*K3)wi. Then we reduce these two polys to one evaluation point r (before we had to do this protocol for each Ki). Then we choose random t and reduce to r(1*K1 + s*K2 + s^2*K3)(1*w1 + t*w2 + t^2*w3 + ...). Oh, but this only works for one round of merging, because we have the same problem, that once s is chosen we can't do it again. 


It appears 2^d amount of auxiliary data is equivalent to d rounds. Since any auxiliary data in our model must be sent between parties, we want as little as possible while still keeping the round number poly log. Note that round complexity is proportional to randomness complexity. 
What if we increase degree of index variables in sumcheck, reducing rounds? Then the intermediate polys get bigger, and soundness decreases. Consider the extreme of only one variable in which case it must take on all enumerable values. Then prover will send integrand as expanded poly and prover is to enumerate it at all points, verify final sum, and then is left to verify the expanded form is correct by evaluating it at random point and comparing with factored form. How does this relate to the poly div method? 
I think the compression of the fiat transform also relates to its security and round and randomness complexity. But we should first consider the above relationships in ROM. 
Beginning this paragraph I was thinking of how h is interchangable with more rounds. But h is a certain kind of derived data, different from intermediate values. I think we can say that all computation data, input, output, and intermediate values, must be processed, either in rounds or in final poly evaluations. The case for h is its not part of computation data, so it can be replaced by rounds. Our amortization is suited for reducing the processing of many polys to that of fewer polys. I'm wondering now if it would be worth the large number of commits to have commits commit to less data, in effort to minimize communication cost. But the commits must be sent, so we now have an optimization problem.
First consider DLP where commit size is constant with respect to commit load. Communication will consist of all commits and a single poly. The total data is n, and say there are k commits each of length n/k. So communication is about n/k + k. Taking derivative with respect to k and setting to zero we get 1 = n/k^2 so min occurs when k = \sqrt{n}. 
SIS is more complex both because commit sizes grow with commit load, and because the amount of info sent depends on the number of commits. For oversimplification, suppose commit size is constant with respect to commit load and we must send s commits each containing the same about of data as regular commits. Then communication is sn/k + k where this time optimizaiton yields k = \sqrt{sn}. 
But note that minimizing total communication will increase circuit communication due to the commits. 

How about zk for the non-uniform case. I think prover could choose a single masking poly for the witness, which could be used across all Ki. Prover first makes eval claim about mask then its randomized with witness.

What about the non-uniform sumcheck version, is that suitable? basically the integrand involves the transcript and the code polys and we enumerate over the constraints. A difference is this is matrix based non-uniform model is more friendly towards linear combinations rather than just additions, which is important for our hashing, so I suppose we stick with that, and we can use the sumcheck for multiplication but mention poly div is an option too. 

Maybe we could characterize proofs that can be proven in linear time. What's special about linear is the functions are closed under composition and don't blow up. So what does it mean to 'compose' in this context? The input (variable) is the statement size, and the output is the time it takes to prove it. So upon composition we assign the time it took to prove the first statement to size of the second statement. So suppose, say, the second statement is that the first statement was proven in a certain way, ie the second statement doesn't just care about proof of first statement but also about process prover went through to prove it. Now we know we can compose a constant number of times, therefore upon composition prover time remains a linear function, but of course with bigger constants than the sub-times. Note we can also add linear functions to get another, which corresponds to the times for two parallel proofs. 
We have the ability to generate a proof that a witness is valid in no more time than it takes to verify the witness by itself. I'm not sure what kind of characterization we're looking for because we already know GKR can do low depth statements in linear time, and we know how depth can be increased at the cost of increased verification time. 

What about the STARK-type structure? This is like the non-uniform except polys are not induced, but are used directly so the circuit is more complex. And actually this would require the recursion theorem probably. The recursion theorem is not needed for the non-uniform case because verification, apart from input and output, is independent of the logic. 

Would the non-uniform version work for lattices? we don't have to compute the induced polys in the commit, we just need to take random linear combinations of the witnesses. Due to overflow eventually one must be submitted and analyzed by the circuit directly. But unlike with GKR the analysis not evaluation at a random point, but rather evaluation of the induced poly at a random point, which means computing the induced poly which requires the recursion theorem. But I realize this is not the only way to analyze directly. Instead, maybe we could renormalize. But actually there is another problem and that is that submitting witnesses requires putting them inside a new witness, thus witness size blows up. Could there be a separate circuit specifically for renormalization? It would take a commit and its claimed normalization commit(s), which would consist of multiple vectors, namely the divisors and the residues. So the witness size for this circuit would be multiple times the normal witness size. This side-circuit would not require any multiplications, but it could and we could use that to compute TC, but this would introduce more complexities. Suppose the side-circuit uses no multiplications. Then there is not need to evaluate induced polys at a random point. Instead we can just take random linear combinations of witnesses to amortize. But witnesses are in b, so we'd only like {-1,0,1} scalars, so I think this randomization could be exactly the same as the regular normalization and take place in the main circuit. Thus the side-circuit serves only to open a normalized commit, a purely linear operation that requires reconstructing the witnesses to their unnormalized form then multiplying with constants. Soundness relies on correct formatting, that the witness contains small (though not exact) coefs, and we are already taking care of this with regular randomization. In fact, I think the side-circuit code would be A*D where A is the SIS matrix and D is the reconstruction matrix, but we'd also incorporate the expected outputs and subtract so a valid witness returns 0. Now do we need q to be prime? We want soundness to rely on linear combinations of witnesses, and this works but the problem is we plan on renormalizing these linear combinations, and maybe a linear combination results in non-zero (invalid) before normalization but zero (valid) after. Suppose E is our code matrix. We're wondering if E(x mod b) = 0 => Ex = 0. Ex = E(x - x mod b) + E(x mod b). In other words it is possible Ex = E(x - x mod b) != 0? It q was prime we could divde each vector element that is non zero to get E(x*x^{-1} mod b) = 0 where x*x^{-1} mod b will be a bit vector, and I think we can show this is close to solving SIS so its not feasible. But not sure about composite q, which is necessary for isomorphism.


For bit checks in the non-uniform model, the 3 induced polys will all be the same, because the constraint is x*x = x. I don't know how to optimise with this except that after sumcheck we only need to evaluate one of these instead of 3. If we mixed these bit constraints with the others the polys would not be the same so we couldn't do this and we'd be wasting space. 

For hash in DLP setting we should use SIS rather than DLP because we don't need homomorphic property and its easier to compute as can be done with n linear combinations. 


Could we amortize bilinear pairings? 
e(a1,b1)=c1, e(a2,b2)=c2
Now lets try to reduce to a single point. First construct lines a1*(1-t) + a2*t and b1*(1-t) + b2*t
e(a1*(1-t) + a2*t, b1*(1-t) + b2*t) = e(a1,b1)*(1-t)*(1-t) + e(a1,b2)*(1-t)*t + e(a2,b1)*t*(1-t) + e(a2,b2)*t*t
So to evaluate this prover could basically do interpolation by providing additional claims e(a1,b2) and e(a2,b1). 
Then verifier chooses random point r on line, and we are left to compute a single evaluation. 
Is this sound? Correctness holds. Keep in mind this is not a polynomial so lets write again in multiplicative form.
Our lines are a1^(1-t)*a2^t and b1^(1-t)*b2^t
e(a1^(1-t)*a2^t, b1^(1-t)*b2^t) = e(a1,b1)^{(1-t)(1-t)} * e(a1,b2)^{(1-t)t} * e(a2,b1)^{t(1-t)} * e(a2,b2)^{tt}
Suppose this equation does not hold for all t. Is it like a poly meaning that then it does not hold for most t, enabling soundness by sampling random t? Well suppose we have claims c1,c2,c3,c4 of which at least one is wrong.
e(a1^(1-t)*a2^t, b1^(1-t)*b2^t) = c1^{(1-t)(1-t)} * c2^{(1-t)t} * c3^{t(1-t)} * c4^{tt}
We know both groups are isomorphic to each other (because they have same prime order and are cyclic) and thus isomorphic to a prime cyclic group which is a field. While infeasible to compute, we can consider the iso map f that maps to elements of the field modulo p. 
f(e(a1,b1))*(1-t)(1-t) + f(e(a1,b2))*(1-t)t + f(e(a2,b1))*t(1-t) + f(e(a2,b2))*tt
	= f(c1)*(1-t)(1-t) + f(c2)*(1-t)t + f(c3)*t(1-t) + f(c4)*tt
So we comparing two quadratic polys, so indeed zippel should hold. 
Suppose we want to reduce from 4 to 1. Doing it 3 times means computing 2*3 extra claims. Doing it 1 time means computing 4*3 extra claims. What about 3 to 1? Two times means 2*2 extra claims. One time means 3*2 extra claims. In general consider reducing 2^n to 1. 2^n-1 times means 2*(2^n-1) extra claims. One time means 2^n*(2^n-1). So doing pairwise is more efficient generally but requires more randomness and fiat transforms.
Note that this amortization is just another case of our general poly eval amortization.
Using this scheme means instead of computing pairings in the circuit we will compute exponentiations. 

Now how could we use pairings? The point is to reduce communication. Ideally we could perform poly evaluation with pairings. I suppose with a setup we could encrypt a random evaluation point, but then we would need the protocol to be single round, like poly div. Consider the non-uniform system with poly div. Prover computes induced polys and evalutes them, and does same for h (z is preprocessed). I realize this won't work unless we make it circuit dependent otherwise nothing is preventing prover from using difference circuit.
How can we reduce evaluation at an arbitrary point to evaluation at a fixed, secret point? Suppose we have secret points s_i (a log number of them), and we need to evaluate at points r_i. Suppose we make the standard line
r_i*(1-t) + s_i*t
and ask prover for evaluation along this line. Knowing evaluations for all subsets s_i the prover can compute this in the exponent and return coefficients for the univariate in the exponent, and I think this can be done efficiently. Then verifier chooses random t = r' and evaluates both line and uni poly. But from here I don't know how to do final evaluation using pairings. 

Looks like we'd like an equation that relates a poly evaluated at 2 different points (r and s) such that we can compute f(r) via f(s) and r. Apparently one can construct polys q_i such that for any r f(x) - f(r) = \sum_{i to v} (x_i - r_i)*q_i(x). Thus we can compute f(r) by f(s) - \sum_{i to v} (s_i - r_i)*q_i(s). This can be made zk. 
To review the protocol, prover evalutes poly on secret s and that is considered the poly commit (different from our usual commits). Then we determine r and then prover commits to q_i. Then verifier verifies the final equation with pairings, which requires 1 + l pairings. 
For the moment suppose the target group is one of the cyclic groups (i'm not yet clear on this) such that amortization can be efficient.
So how do we use this new commitment scheme for our amortization. As usual we reduce to evaluating polys at certain points. We could just do this using the pairings and amortize them. But suppose we try to do better by reducing to evaluation at the same point r for all polys. Then we'd like a commitment to a random linear combination of the polys, such that we only need to evaluate that. Well our commitment is not the same as usual, but the commits can undergo the same homomorphic manipulation as with the usual commits to obtain this desired new commit. Then verification is left to pairings for a single evaluation. Unclear at the moment which way is more efficient, the latter or the former. 

Well it turns out the target group for the pairing is a multiplicative group is an extension field, of the field containing the elliptic curves, which the native fields thus allowing the right hand side for our amortization equation to be computed efficiently. 

This construction is a bit different from our others because the amortization doesn't make it sublinear (it already is) but rather avoid the need to compute the pairing inside the circuit. So doesn't quite fit our model for sagaproofs built from sublinear schemes. Maybe it should go as a last (or first), as an aside.

Does this eval scheme work for the non-uniform system? It we commit to the witness then we are set to evaluate that, but we instead want to evaluate the induced polys. If we commit to the induced polys then there's no guarantee they are induced correctly without circuit-dependent encryption. So I don't think it will work.

Maybe lattice scheme can be described by itself, like DLP scheme can. In particular, I'm thinking how we have the pattern in both uniform and non-uniform case that we must renormalize. In the uniform, upon renormalization we test the property, whereas for non-uniform we want to delegate the property to the renormalized form. Such delegation doesn't help in the uniform case because the property is direct evaluation at a random point and that comes at little extra cost, whereas in the non-uniform case computing the property directly means computing the induced poly, and then doing multiplications on it, which means a circuit bigger than the regular circuit, and then we must verify for this bigger circuit etc so we must use the recursion theorem. 
I'm wondering if we can do better regarding renormalization in general.
What prevents us from scaling into cosets before re-normalization. Suppose b divides q. Given an unnormalized commit we scale it by q/b. What was the problem before when we tried something like this? It was that we thought prover could then submit the normalized target and we'd scale it to make sure its correct, but this won't work because many targets could properly match the correct one due to the correct being in a coset. So what if the prover provides the normalized form and we directly examine it, scaling not the target but each coef appropriately, so we are doing an opening as before but now with less entropy. Is this sound? lol no because prover could have large coefs and upon scaling by q/b they would all appear in the valid coset. 
Another idea was having prover split a commit into entries that are even and odd. Then prover gives normalized version (scaled bye 1/2) of the even one. Verifier multiplies by 1/2 and compares against previous. This seems dangerous because we are asking for info from the verifier without randomization, and doing so seems bound to offer the prover an opportunity for planned cheating. In the end what I was visualizing is equivalent to the prover just providing a list of commits, each with distinct indices, such that \sum_i commit_i*2^i = target, eg commit_0 would contain all entries that are odd. But an even simpler solution I should consider first is a list of b commits with distict indices such that commit i contains coefs of only i. These kinds of ideas may be sound but a problem I realize is they are not modulo operations so the claim of a poly evaluation will not be preserved. The best candidate I could think of related to those above, one that would preserve the eval claim, is splitting into residue layers by multiples of b, eg for the range 0,2b-1 we split into 0,b-1 and b,2b-1. Then we subtract b from the upper layer then add them again. I'm concerned about lack of randomization, but lets find how this can be attacked. Lets try to analyze per index. Suppose an index value z is out of range. Prover must split it into two parts, x and y, one for each layer, such that 0 <= x + (y - b) < b. We also need x + y = z. This means 0 <= z - b < b or b <= z < 2b which means z is inside the range. The contrapositive means if z is out of range then prover won't pass. Actually while this might be sound splitting only allows for multiplication, not subtraction, because we don't know which indices to subtract from, so this won't work.

What about an entirely different randomization that requires little amortization overhead but more computation for the verifier, analogous to the DLP inner product argument. An idea, though this also would not preserve the eval claim, is we randomize the basis. 
Another possibility is we view the coefs in blocks and extension field elements, and we change the basis so as to multiply each block by a single random extension element, thus the equivalent of a fully sound randomization. I don't think this is possible, and we are limited to scaling individual indices, not blocks.
If we take the approach of considering a way of randomization first, the only way I can think of is randomizing the basis. Maybe we could make b so big (and q sufficiently big via extension field, maybe thousands of bits) we'd only have to randomize a few times (even once) for sufficient soundness. But this basis change then wouldn't allow us to add multiple commits. What if instead we did like the DLP inner product and randomized so as to evaluate at a random modulo b point (b still sufficiently large)? This could likely work but I'd need to look again at how it works. One concern is verifier would need to compute base changes using the isomorphism which is costly, but maybe I suppose we could do it with an additive side-circuit. Actually the isomorphism would not work anyway because to change basis we need inverses for any b value which means q has few if any divisors, and those divisors are not b values. After we determine the changed basis we are just looking for a valid opening with respect to that basis. In this case we could try the split-and-re-scale approach. 
Just consider the simple case we add two binary commits then split into {0,1} and 2, then ask for a rescale of the 2, then add them back together to get a binary commit. Prover provides 3 commits in addition to the original s0: s1, s2, and s3. Verifier checks that s1+s2 = s0, s3*2 = s2, and s1+s3 is a valid commit. Is it possible s0 is invalid but s1+s3 is valid? Well consider an index value z in s0, that prover splits into x and y in s1 and s2 respectively. To pass the first test we have x + y = z mod q. Suppose this index in s3 has value w. Then we have w*2 = y mod q by the second test, so x + w*2 = z mod q. The final index value is x + w. 
x + y = z
w + w = y
x + w = 0
=>
w = - x
- x - x = y
- x = x + y = z
0 = x + z
---
x + y = z
w + w = y
x + w = 1
=>
w = 1 - x
1 - x + 1 - x = y
2 = x + x + y
2 = x + z
---
eg for any z let x = - z, and y = z + z, and w = - x = z. 
First test: x + y = - z + z + z = z
Second test: w + w = z + z = y
Third test: x + w = - z + z = 0
so indeed this won't work without some kind of randomization.

I've looked at the bulletproofs inner product argument and it looks like it relies on automatica modulo being performed in the exponent, a property we don't have with b inside q. There will be two different representations of the same number modulo b, call it t. We will have one set of numbers modulo b a1,a2 such that a1*a2 = t mod b, and another set b1,b2,b3 such that b1+b2*b3 = t mod b. Thus the multiples by which these two differ is variable. Having the prover show the two are modulo equivalent is equivalent to our original goal, one we haven't solved. 


can we do a deterministic base change for SIS as an isomorphism? verifier computes on new commit, opening with respect to the old basis to verify its the same. Suppose we take a commit as a poly, break it into (say 2, could be more) parts. Prover provides the split commits, as well as the commit under the new basis for one of them. The random point of evaluation is the same for both polys, so they can immediately be randomly combined. We could compute another isomorphism to normalize this, but if beta is big enough we may not need to. Then we begin the process again for a poly commit of half the size. And in fact, the renormalization may especially be unnecessary because m is now half of what it was before.
b\sqrt{m} = x\sqrt{m/2} => x = b\sqrt{2}
this equation indicates the infinity norm can increase by \sqrt{k} upon slicing the poly into k parts and adding them up, which gives infinity norm b*k. |\sqrt{k} - k| is closest at k=2. also, multilinears are best suited for k=2. Any power of two works well for DLP and mutilinears. Oh, the part I'm forgetting is that the randomization increases the bound by a lot.
For lattices, the randomization is the challenge, whereas for DLP, the base change is costly. 
Actually, for DLP base change, we could consider the existing commit as belong to the new basis, and there is a fixed relationship between the old and new basis. Thus there is a fixed constant that will multiply each coef with respect to the old basis to get the new coef with respect to the new basis. An alternative to computing the base change which involves exponentiation is to instead compute what the eval claim about the old poly implies about a claim about the new poly. Not sure exactly how to do this. 
The DLP base change requires naively I think n + logn exponentiations, which is better than bulletproofs inner product argument, but they optimize in an obvious way, and I'm not sure if we can do the same because our has different structure.
Note that changing the basis means computing on the data in the new basis, which means evaluating at a random point, and this point will have to be incorporated with eval reduction. This means one line, because both old polys are to be evaluated at the same point, we connect this point with a new random point for both polys. 
Can we work with sets of polys in lattices? suppose we have a set to evaluate, we split each into two, the compute new basis for that half, then randomize that half of them with as many tests as there are polys (> s), then add them component wise to the other half. Then we would need to renormalize. Instead maybe we make the one half into bits when we change the basis, so upon later randomization its about regular b size. This means that we will double the infinity bound for each round. with log(data) rounds and initial bound b this would mean a final size of data*b, but taking advantage of the smaller m in each round this can probably be reduced something like data*b/\sqrt{2} about data*b*0.7. I think we could tolerate this with a large enough q. If not, we could renoremalize entirely after every round. 

How would this work in the traditional prover-verifier setting, ie without proof composition? Consider for DLP. For base change I think they have to engage in another proof. The data for the base change proof is the poly for which to change the base. We iterate through the poly opening the old commit, the verifier is left to evaluate poly at random point, but now it can be with respect to the new basis. I think we could still consider this an IOP because the subproof just involves responses and randomness.


It would be great to reduce evaluating a poly at a multi point to evaluating at a uni point. We can view both in the case of a multivariate poly, no need to consider uni polys. Is this theoretically impossible? Well I suppose we could just use a univariate proof like poly div to evaluate it at the multi point at the expense of evaluating at a uni point. Something like univariate sumcheck would work but have restrictions on the field, like composite order. 
Consider just two variables. Suppose we replace each with a 2d plane such that each goes through its respective variable at the same point, and the two planes intersect on a line. Prover sends back d degree bivariate poly consisting of d^2 coefs. Prover verifies consistency, but then prover can't choose point along the common line but instead must choose randomly for both variables, thus yielding an unstructured point for the original poly. To prevent this we would need to have the polys replacing the variables such that upon any uniform evaluation it yields the structured point we want. But this structure prevents from testing for consistency because that requires an evaluation that results in the original unstructured point. 
For a univariate based proof, how could we most efficiently express evaluation for a multilnear in terms of constraint to minimize witness size? I think this could be done, eg in STARK form, but it would probably still mean a big witness, and it comes at the expense of h as well as having to evaluate the witness at multiple points (which can't be reduced to one). We can still mention this as a possibility if one wants to avoid cryptography and use only CRHF as an assumption. 
For STARKs something like p(x)*r(x) + p(x+1) = p(x+2) but this is not complete and its unclear how to finish.


I was wondering if it might make more sense to try to set up an open source way to show and discuss these concepts they are best through of from multiple perspectives. This would suffice for presenting for admission, and also be good enough foundation for my thesis, just needing to translate to a paper format. One reason not to do this is I'm not up for maintaining this because I want to work on other things after I finish this project. 


mention how all CRHF are kinda costly by hashing, and require univariate proofs which require quasilinear prover time. Mention how we could get linear prover time but still CRHF if we could reduce multi point eval to uni point eval, but we don't know how to do this. 


In general, if the amortization scheme permits, we can recurse on it to have a succinct proof. So maybe I can make it part of the general model.

What proof systems do we have that should work? For GKR we can use DLP and lattices, and succinctness is available. For non-uniform, DLP amortization is available but for lattices normalization is tricky. For non-uniform, evaluation at a point is not performed, so the succinctness doesn't apply. 


For the succinctness make a few lemmas. Error persists if prover splits commit into two. Error persists if we do base change via proof. Error persists if we randomize one side then add together. We'd like to be as abstract as possible, leaving open as much room for optimization as possible, like how and when normlization is done.
But as an asside, I'm now thinking normalization should done length wise, increasing the size, of the commit, rather than horizonally, speading one commit into multiple. We had to do this before because commit sizes could only grow, but now that we have a way to reduce their length, I think we can forget horizontal normalization. But this only applies where succinctness applies, which is only the property testing can be broken into parts, which is not the case for the non-uniform case. 

We want our model to be meaningful, not just abstract language wrapping around our specific case. I think the case of succinctness is meaniful enough. It says that when we can split the property testing for a single instance into multiple smaller instances we can by recursivity achieve succinctness. This is because by the general amortization model we will achieve sublinearity, so this property that allows for succinctness is a subproperty. 

Can we achieve succinctness for the non-uniform in any way? 

One way to model to capture a lot is by proofs that reduce to evaluating polys at random points. This actually captures everything except the non-uniform case. 


I was thinking again about scaled targets. If q is prime and we divide by the scale, then we can obtain an appropriate witness for the original instance multiplied by the inverse scale. If the max different scales we need for gcd 1 are smalle enough (or just making b small enough) then we can get witnesses for a few problem instances related to the original problem instance. Can we prove these related instances (by inverse scaling) are also hard? 
Note if a power of b equals q but all scales are gcd 1 with q we can still use the multiplicative field isomorphism. 
This scheme would enable us to prove knowledge of a witness with respect to one of the few instances. Unscaled knowledge for the unscaled instance implies k-scaled knowledge for the k-scaled instance. And k-scaled knowledge for the unscaled instance implies unscaled knowledge for the 1/k-scaled instance. In general, k-scaled knowledge for the k'-scaled instance implies k*u-scaled knowledge for the k'*u-scaled instance. Basically we want to know that if prover has k-scaled knowledge for k'(!=k)-scaled instance then prover either also has knowledge for k-scaled instance or prover will fail later. So suppose prover only has scaled knowledge for unscaled instance. Well actually I see no reason this will make prover fail, because in the end after randomization we only require knowledge with respect to random linear combinations. I've already analyzed how accounting for scaled-knowledge is not worth the extra challenges. 
Let's calculate once more. If we use scalars {-c,...,0,...,c} then for poly eval we choose from the set of size 2c+1, so we need n = s*log(2)/log(2c+1) challenges. The total info if we do this number of challenges will be (1+c*b*l)^n, not accounting for the extra offset cost that grows with c. So plotting s*log(2)/log(2c+1) * log(1+c*b*l) it appears it shrinks with c, implying higher c is better.
While c=1 is fine we'd still like to prove for higher c, which may not be possible. 

note that if the poly to evaluate has zk then we can proceed through the succinctness argumemnt with no more zk overhead. 

we could present the sumcheck non-uniform which reduces to point evaluation. 
say we show other non-uniform case in order to show how we are not entirely dependent on point evaluation, but the homomorphic feature allows for general linear relations. Also for DLP it is low cost and simple. 

should we still use the molting model? our ability for succinctness closes the need for tail recursion, but I still want to use that model, and its still necessary for the matrix non-uniform. 


so how to structure our model?
proof system that resorts to property testing
property testing amortization
special case of decomposable property testing
we present our 3 models of computation and describe what property testing they resort to, and which are decomopsable.
then we describe amortization for each
we describe how to achieve succinctess using decomosable property testing, and how to amortize pairings and achieve succinctness for the matrix non-uniform. i'm wondering if we should also present pairings for the succint poly eval. 

to make sure the model works, maybe we can conjure a NP proof system based on another computation model than circuit/constraints. It needn't be a natural computational model. We would find a property testing algo for it, and it only needs to be efficient asymptotically. We could use commits as a black box, ie leave the scheme unspecified and model with an oracle.
Note that the original NP verification is efficiently computable, and we are reducing it a verification of a property, which is also efficiently computable. The point of the reduction is to arrive at a property for which a sublinear testing algo exists. I would like to develop a model and then separately exert the sublinearity condition. For example, the simple model might be a reduction from verifying an NP witness to verifying a property of the witness. Then the extra condition would be that we can perform the property verification sublinearly. But is this separation appropriate? Instead of oracles it may make more sense to use commitments, which can model oracles. Indeed, the oracle for sublinear algos is used to look at particular parts of the object, and we can prove how this can always be done via CRHF trees. The proof would be that after we lay out the data describing the parts of the object, we an always map that topology to a line, and then given an address for any part we can map that address to a point on the line. Then we can map the line to tree form. Again, oracle access to parts is not sufficient because an amortization technique may involve manipulating the oracles/commits. 
We would like our model to have efficient provers too (not restricting to linear). 
Note that property testing is a decision problem, though verification in general is. Verifier must be randomized, and only probabilistically correct.

mention how we know many sublinear property testing algos, but we don't know how to turn them into proof systems. But I'm thinking of property testing algos for single instances, whereas what we care more about is amortization, which depends on the commitment scheme. we know of few commitment schemes, and CRHF by itself provides nothing for amortization. To what extent can be take advantage of the isomorphism theorem? It means we can transform the representations of the data. So given commits to two objects, we can compute a commit any object from which there is an efficiently computable function to the original objects. This gives us substantial flexibility. But I formulated the isomorphism theorem when only thinking about original data, but the larger the computation the deeper or the wider so the more auxiliary data will exist which must be treated as original data as far as we know. 
Suppose we have the isomorphism theorem. What must we reqire for the property testing and commitment schemes? Dropping the entropy comes from direct computation by the verifier, so it would seem we require malleability for the commitment scheme. But we know from CRHF schemes which are non-malleable that verifier can always ask prover for values and need not compute any directly. Instead, in these schemes the verifier opens are random places and checks for consistency. This is like a 'look back' approach where as manipulating commits is like a 'look forward' approach. Both are approaches to dropping entropy. For look-back, I think the actual looks can happen after all other interaction. 

If two graphs are 3 colorable, that does not imply a particular join of the graphs is also 3 colorable. But if a graph is 3 colorable, it implies any split of the graph results in two graphs that are also 3 colorable. Suppose we prove that for a random cut, knowledge of a coloring for each graph implies knowledge for a whole coloring. 

A problem we know we can probabilistically check is matrix multiplication. Maybe we can recast this problem in terms of binary relations, both CNF and DNF. Well actually the matrix problem is a sub-normal time way to do a single instance, whereas we are looking for amortization of multiple instances. 
Remember a binary relation is specified by a binary incidence matrix, where (i,j) = 1 if i is connected to j. We can compute CNF and DNF for two relations by relevant matrix multiplication. 
I think there are multiple ways to make this into a proof system. A witness would probably be a vector or a full relation. The computation would be multiplying in CNF or DNF or both the witness by multiple relations, including itself. Suppose we don't do any addition or subtraction of matrices, only the multiplication. 
To verify that a series of vectors as the columns in a matrix W satisfy a target t for a matrix A, we could multiply AW and compare with t, but instead we choose random r and compute A(Wr) and compare with tr. 
In fact, we could even do SIS with binary relations, because the group q can be represented by a group of permutations with DNF composition. But the size of the relation may be huge, having q nodes. But if q is highly composite, it would require much fewer nodes, eg if q = b^d it would require b*d nodes on each side. 

I think it may be too difficult to find another example besides circuits, and if we used the above it would be quite similar to a poly based scheme and it would be complex to fully formulate. 

Maybe we should incorporate the notion of 'holomorphic' into our model, where to allow for recursivity, verification regarding the code doesn't require knowing the code directly, but rather having as a reference an object that represents that code, and verification involves testing a property for that object. Since the model will already require amortization towards testing, it applies too to that object. 
We could formulate our model in multiple ways. One way is property testing objects. Another way is functions and querying at random 'points'. Another is boolean functions, ie relations. Maybe functions would be the best along with abstract names for the domain spaces, eg commit space. We could formulate algorithms like this too, and probabilistic as receiving randomness. 

Maybe we just ask for a sublinear property testing (a boolean function) algorithm that work sublinearly for some number of instances. If this threshold is more than 1 than its amortization, if 1 then sublinear, and if close to 0 then succinct. We will also define the 'rate' as the ratio of the size of the original computation to the size of the needed computation, and it will be expressed as a function. 

I'm realizing in DLP changing bases either requires computing directly, or requires auxiliary data for each exponentiation, eg a field element as an inverse, and this may be more data than we are saving for, making the conversion pointless. For SIS, however, we luckily don't require auxiliary data and computation is easy directly. Is it possible to amortize base changes?
We will know the prover has a witness V with respect to the new basis B' with target t' and we want to verify that this same witness with respect to the old basis B yields target t.
<B',V> = t', <B,V> = t => <B'+B,V> = t'+t
<B',V1> = t1', <B,V1> = t1
<B',V2> = t2', <B,V2> = t2
Suppose we know those with respect to B' and want to confirm those with respect to B. Can we reduce to a random linear combination? In particular, we know <B',V1> = t1' and <B',V1> = t1' and we consider r*V1 + V2. Does <B, r*V1+V2> = <B, r*V1> + <B, V2> = <B,V1>*r + <B,V2> = t1*r + t2 imply <B,V1> = t1 and <B,V2> = t2? As a random linear combination, we can see that yes, the implication holds. Afterall, I suppose this is just a linear relation we want to verify, namely <B,V> = t, so we can do it with random linear combinations of candidates. 
So what does this mean for the succint protocol? Hopefully it means no need to open. For lattices, we request the commits with respect to the new basis. Then we treat these new commits as commits to the data vectors for which we already have targets (other commits) with respect to the old basis. We will end up randomizing these new commits anyway, but that can also serve as the amortization for verifying consistency with the old basis. But this amortization actually takes us back out of the succinctness setting. Can we make this succinct too? This is a general linear relation, and just like we don't know how to make the matrix non-uniform succinct, we have the same problem here too. So this amortization doesn't help. Its not necessary for lattices, and for DLP it doesn't decrease amortization. So I'm thinking we don't use it at all, though we could mention it.
So I'm thinking for DLP succinctness we could use the pairings for mutilinears. I don't know anyother way to make it succinct. The recursive inner product argument won't work for the same reason as our base change, that is it requires non-deterministic information that must be treated as full entropy and in the end saves nothing. 

So maybe I present, matrix non-uniform, sumcheck non-uniform, and sumcheck uniform all with amortization. Then for each I show how it can be made succint. For matrix non-uniform its amortized pairings. For both sumchecks with DLP its amortized pairings for multilinears. For sumchecks with lattices is the base change. So I'm wondering if we should present matrix non-uniform using lattices, because lattice succintness doesn't apply there, and even amortization is complicated by normalization, and even if it wasn't amortization requires many instances whereas this is no longer a problem with the sumcheck schemes. I think the sumcheck non-uniform may be better than the matrix non-uniform. Maybe I should present the matrix non-uniform as an introductory example to show how the property is not always poly eval like it is for our sumchecks, how succintness can take other forms (in this case QAP style) (and I can forward reference when I mention amortizing pairings), and how amortization can require a large number of instances, and how the recursion theorem can be tricky. For all these reasons I can justify the value of the sumcheck schemes. 
So we have to decide whether to divide the sumcheck by the uniformity or by the crypto assumption. Maybe we don't need to divide by either. Instead, following the model, we present the proof system and the sublinear property system separately and in that respective order. This forces me to get definitions and modules straight. 


For the side-circuit suppose we do it in a prime q using sumcheck involving multiplication by a poly encoding C, which consist of coefs only in {-1,0,1}. This might make verification by the main circuit eaisier (though we could still delegate to the next side-circuit). Suppose we received all the inputs for the verification in field q, but we use some injective, almost surjective, map to an extension field of the main circuit field, then interpret values and perform all verification arithmetic there. Well I suppose even completness wouldn't hold because the fields differ, and no field homomorphism holds between different characteristics. 
Suppose instead of sumcheck we use the matrix non-uniform scheme. Let A be the SIS constants, and S be the reconstruction (un-normalization) matrix, T the targets, C the challenges, and W the noramlized witnesses. (AS)W - TC = Z. Could continue but I'll leave this for the moment.
What if we simply compute TC with long arithmetic in base b? Is modulo really that hard? We could split two q value representations into bits (do bit checks), then add them, then inspect the leading term and decide whether or not to subtract q, then translate back to base b form, or back to bit form for another addition. What if we perform all additions at once. This would require that the number we add together at once be less than b to prevent overflow. We've seen other reasons before that its desireable to make b about equal to the security parameter anyway. Oh, but remember we are doing subtraction as well, so its more complicated.


I'm remebering that the sublinear lattice argument with succintness involes a proof system so maybe its tougher than I thought to separate proof systems from sublinear arguments as I planned.

Maybe I should merge the two sumcheck protocols, uniform and non-uniform, into one general framework. They can easily complement each other, and they are both built on sumcheck, so instead of separating them just because one checks constraints and the other computes derived values, we treat them the same as they deserve. 
Our DLP and lattice versions will use different version, the former with a big field and the latter with a small field such that the random points sent by the verifier must be in an extension field. So maybe we can prove in the model of the latter, for which the former is a special case where the extension field matches the primary field. 


what if we make the first succinct IP from worst case assumptions, with efficient provers.

it would helpful to have model be naturally non-interactive, unlike interactive proofs, to capture how the chain really is non-interactive. One way may be to model the fiat-hash as some ideal oracle or function, that both prover and verifier access. This oracle receives the prover message together with record of the transcript. Suppose this record is just the last oracle response. 
Maybe we can cover fiat-shamir in the model, and later only try to choose a suitable hash. 
Note that even an ideal hash is not secure for an unbounded prover who can query it unbounded times. In fact, maybe we should just assume a certain kind of hash from the beginning, rather than some abstract oracle. This forces us to decide what we require from the hash. I think we want relation intractability where the hash is chosen randomly and there is a fixed relation. This is tricky because in a sense the hash does not observe a whole instance of the relation, only the visible part. But remember we consider this visible subrelation as the relation itself, but now it is not fixed but rather chosen after the hash. Before we thought about just saying the hash is chosen after this relation is fixed. 

Suppose we found suitable conditions for a hash. How to complete the model? Maybe we model randomness not just as an input, eg random coins {0,1}^n but a function that a probabilistic algo can query. afterall, this is how it works in the real world, with libraries. Assuming the random function is completely random and non-deterministic, there's no reason we can't assume all parties use the same function. So we still use two algos, prover and verifier. Prover is probabilistic (for zk) and verifier is deterministic. But as adversaries (prover against soundness and verifier against zk) they are both probabilistic. The problem with randomness as a function is a function is deterministic. Maybe we give it special notation, such as marking the calling parethases in some way, like a line over them, to signify its impure. This is close to conjugate notation in complex analysis but we are far from that and in complex analysis all functions are pure. 
Now since we conditioned on a hash for the fiat-transform there is no interaction, we can model the prover as a single algorithm. It receives the instance, a witness, the random oracle, the hash function, and outputs the proof, all in a single invocation. The verifier receives the same inputs except the witness, and outputs a boolean decision. 
Say this non-interactive model is necessary for us because interaction is inherently non-verifiable. Whereas IP can indeed be done with interaction so the modelling is appropriate, whereas for us this is not possible. 
A hash is Correlation Intractable for a relation if the hash is randomly chosen after the relation is determined. I think full correlation intractability still means the relation is determined first then a hash is randomly sampled from a family. But of course we can't randomly sample. But suppose we consider the simple relation of (m_i,t_i) -> v_i being an instance where m_i is prover message, t_i is current transcript, and v_i is verifier response, and an instance is where (m_i,t_i) is doomed but (v_i,m_i,t) is not. In this case the relation is determined along with the proof model, so we can choose the hash after, thus having a correlation intractable hash. 
We should probably leave little comment on which hash to choose, only pointing to the research done for GKR. 
I realize SIS is not suitable for hash because output size is so large. Thinking more about SIS I'm worried q must have large prime factor, and cannot be a power of a small number, ie highly composite, because that is isomorphic to q being the small number and the worst case dimension multiplying a factor of the power. Oh, this is not the case because the q-group must be cyclic, and this additive group I'm considering is not cyclic. 

for hash, what if we use SIS with q=b, and n such that output can be interpreted as element in extension field. Then we could take bit strings about the square size of b. 

literature says if hash family is correlation intractable for efficiently sampleable relations, then it works for fiat shamir for pv-SNARGS. 


Maybe instead of the multiplicative group we could use the galois group for an isomorphism. Actually the galois group only has order n for q = p^n. 

In a finite field an irreducible poly f of degree n. 

(\sum_i a_i*x_i)^p = \sum_i a_i^p*x_i^p


(\prod_i a_i^x_i)^p = \prod_i (a_i^p)^x_i

I know its more complicated, but suppose given a single root of the poly in the extension field, we can find all others by the p'th power map. Then suppose we hash to a root with SIS. Taking the p'th root of both sides, assuming all inputs are presented in the base field, the same input yields a solution to the next root with respect to the p'th power of all constants. 
The degree matters. For us, in sumcheck, we will have quadratic polys, so degree is 2. This means our polys can have at most 2 roots. 

Suppose we could handle quadratic polys. This would work for sumcheck, but could we also make it work for poly eval reduction? Theoretically we could just use the sumcheck again, where we take a random linear combination of the claims, and perform the evaluation of both in the integrand, indexing over the coefs, then at the end we will have a single point of evaluation for both. In this sumcheck we would only need to handle linear polys, easier than quadratics. What about the more traditional eval reduction, where this time we only join a line for k variables at a time, and receive a k-degree univariate poly in return. 

Would it be possible to do multiplication with sumcheck only with linear intermediates? Before I had thought about 'currying' where we don't have two, but only one input vector at a time. For source vector S and target vector T we'd want something like T(z) = \sum_x f(S(x),z) where f is a linear function in its first arg, and splitting z into its component variables, f must be linear in each one. Suppose we let the arguments of S also depend on Z, ie S(g(x,z)) where g is linear in all args. But then we couldn't let z be an arg of f.
It seems to do anything non-trivial we need a quadratic integrand. 


again consider the task of reconstructing a multlinear poly to a univariate poly of same size. there is a spectrum, where it will probably need to transform first into a higher degree multivariate poly, at every step reducing variacy by 1 and doubling the degree. 
first suppose all out polys start out as univariates, we transform to multivariates only to reduce evaluation to the same point (a multipoint), then we want to return that point back to a uni-point. Suppose r1 is the uni-point for poly one and likewise r2 for poly two. We connect these two with a line. 
r1*(t-1) + r2*t
Suppose we pass it in for the first index, and for the second we pass in the same line squared. 
(r1*(t-1) + r2*t)^2
Prover returns the degree-5 univariate poly, we check for consistency then choose random t, and are left to evaluate the polys at the points
(r,r^2,r1^4,...)
(r,r^2,r2^4,...)
Now we can consider the first two indices as transformed from two variables of degrees 1 to one variable of degree 3.
Now suppose we continue for the rest of the indices in pairs. For example, the next would mean the line
r1^4*(t-1) + r2^4*t
and its square, passing them in respectively for indices 3 and 4. We will choose a random t different from the one for the first two indices.
After all pairs are done we should be left to evaluate the two polys at the same point of the form
(r,r^2,r',r'^2,r'',r''^2,...)
where we started with v variables of degree 1 (ie 2-1). Now we have v/2 variables of degree 3 (ie 2*2-1). Basically cutting our variables in half we have doubled our degree.
(r,r',r''',...)
Can we continue this process recursively? I think it breaks down the higher the degree each variable, because the univariate polys returned by the prover will have corresponding high degree. Even if it worked, I'm realizing there is little reason to have this goal. 
The point of reducing from a multi point to a uni point was to use the uni commitment scheme of CRHF. But if we commit to a poly using the uni commitment we can only open that poly, not its linear combination with another. But I suppose we can still concat multiple polys together and treat as a single poly, and we concatenate to them as a single one. When the polys were commitmented to from multiple parties, before its determined they are put together, we can still cocatenate commits by muliplying a response from each commit by an appropriate monomial, but this doesn't help actually because the query complexity remains the same.
With this general approach of 'connecting point' I don't see a way to get around high degreeness.  


Now supposing all fiat-shamir transforms for us are wrt the relation of quadratic poly points of agreement, can we say anything helpful about the necessary hash? One thing we could say is that it has compression factor 3, mapping 3 elements for the quadratic poly to one element for the random point. For any input there are at most 2 solutions, and they are efficiently computable, though not so friendly to circuits. 	

-(b2-b1)/2(a2-a1) +- \sqrt{(b2-b1)^2 - 4(a2-a1)(c2-c1)}/2(a2-a1)
(b2-b1)^2 - 4(a2-a1)(c2-c1) = (b2^2 - 4a2c2) + (b1^2 - 4a1c1) + (- 2b1b2 - 4a1c1 + 4a2c1 + 4a1c2)

suppose we have the diff poly encrypted with SIS. prover knows the diff poly and computes roots for it. verifier takes claimed root, mutiplies powers, and scaled each encrypted coef appropriately, then adds up encryptions and should get a zero equivalent encryption. 

when verifier sends only randomness its called 'public coin'

since we want compression factor 3, maybe subset sum won't work. We can still consider SIS with q=b, and inputs decomposed into binary form. but given this constant compression we could just leave the hash uspecified, and we assume it will be calculated in the main circuit. 

what if we map to both of the quadratic roots, then reduce the two points of evlaluation to one, using a fiat transform that only requires a linear (rather than quadratic) poly as input. In this case the quadratic hash has compression factor 3/2 (or 3/1 if irreducible), and the linear hash has compression 2/1. 
But this would mean changing the sumcheck protocol. Consider after receiving i'th prover poly, and checking for consistency, we don't immediately plug in a random r_i for index i, and request the next univariate over the (i+1)th index. Instead we have the two random values r1_i and r2_i. We request the univariate over variable i connecting these two points. But actually that univariate can be obtained by the verifier just composing the line with the i'th univariate. Suppose verifier then chooses random t online and computes relevant r_i and expected relevant output. This seems too convenient to be possible. With no extra prover interaction, we reduced from computing at r1_i and r2_i to just r_i. By round-by-round (RBR) soundness suppose the next univariate returned is correct. Then after adding up evaluations at 0 and 1, we obtain the correct value of the ith univariate at r_i. We wanted to know the correct value at r1_i and r2_i. Instead of evaluating on those points we evaluated on the line connecting them. But without the verifier observing the valid evaluation on this line, the verifier can't perform consistency checks for r1_i and r2_i. Oh, the problem is we are trying to reduce a quadratic from 2 points to 1, but this was only assumed to work for multilinears. If we interpreted the quadratic as a univariate then we would reduce the problem to reducing eval from a multi point to a uni point. In other words, we wouldn't need the regular evaluation at some r_i for the next poly, but rather an evaluation treating the i'th variable x_i as two variables u_i and v_i where u_i = x_i^1, v_i = x_i^2. Suppose the prover actually treats it this way, plutting in v_i for x_i^2 and u_i for x_i when computing the (i+1)th univariate. The integrand will be a quadratic multilinear poly, but upon treating every squared variable as independent from its regular form, we will have a multilinear but with twice the arity, and actually I don't think this is a full multilinear judging from below where we start with two multilinears with variables (a,b), and upon multiplying them we replace aa with u and bb with v. 
(1,a,b,ab)(1,a,b,ab)
(1,a,b,ab,a,aa,ab,aab,b,ba,bb,bab,ab,aba,abb,abab)
= (1,a,b,ab,aa,bb,aab,abb,aabb)
= (1,a,b,ab,u,v,ub,av,uv)
I think this is because with u and v we have enough to represent a cubic poly, but ours is only quadratic.
Not yet sure if this technique works, but if it did could we generalize? Suppose our integrand was a multivarate poly of degree 2^d-1 in each variable for some d. Upon receiving a univariate we map wish to evaluate the next poly at all 2^d roots. Breaking it up into a multilinear with d variables, we reduce recursively to one multilinear point, then the degree-d variable for the current index gets broken up into d distinct linear variables taking the values of the final multilinear point. Eg, for d=2 we could be multiplying together 3 multilinears in the integrand, and in the end we would end up with a full multilinear, no variables partially wasted. 
I would need to formally prove this works because with the current sketch I'm not even intuitively convinced. Its still odd that there is no interaction when reducing the multipoints on the current univariate. This means the this reduction process to arrive at the final multi point from the multiple uni points should be considered part of the fiat transform. Our plan was to consider this reduction as a set of additional fiat-transforms for linear polys, but the prover already has these linear polys and they are not provided by the prover. Could we still carry-out the linear poly fiat-transform as usual?
Another question is how to handle the destructuring of the integrand. We saw before how we can restructure as long as the degree mains low, eg for the d=1 case we could restructure pairs and again obtain a quadratic multivariate. Now we must translate an eval claim about this multivariate into claims about each of its components. Well I suppose this is what we normally do with the sumcheck anyway, where the prover just provides claims, we check for consistency, then reduce to the claims. Actually I realize now we only know how to do restructuring between two structured points, not for our destructured point.

In IOP it can be non-interactive but prover still queries oracle. Thus soundess against state restoration is enough. But with the oracle replaced by a CI hash, then we can't say the same. 

To do the above scheme we need a root-relation intractable hash. If I can't do this as I thought then we should just entirely ditch finding explicit fiat-transforms. The invisible input to the relation can be thought of as the function for the subrelation it induces. In our case the invisible part are the coefs of the real poly. The invisible operation is obtaining coefs for the diff poly. For input vector x and real coefs vector y, and hash H we need it to be infeasible that y - x = H(x) or x - y = H(x) where both sides remain vectors, so H will have to hash x multiple times or in multiple parts. 
For SIS/subset sum we can rearrange as y = H(x) + x = Ax for some matrix A and x in decomposed form. Thus solving means finding a witness x for target y. But y is not random so this is not ISIS. We would need to enforce that A is random with respect to y, which means choosing H randomly after y is determined. y is determined by the commit or however the final witness is held. But this requires interaction to choose A, or requires fiat-shamir, a circular problem. The original scheme I imagined required the function to be a linear function, but here we are doing subtraction, an affine function. 

Suppose we randomize y to make it indistinguishable from a random c. If one iteration of SIS was enough, then the hash
H(x) = A(x - c) = Ax - Ac
would suffice because when we replace c with y and we successfully hash to the diff poly x-y then we have
A(x - y) = x-y => (A-I)(x-y) = 0
Now suppose we must go through two iterations of SIS, A and A' (composition denoted by $), to make y indistinguishable. Then we have hash
H(x) = A'$A(x - c) = A'$(Ax - Ac)
and
A'$A(x - y) = x - y
but the latter I don't think implies breaking either A' and A, so this idea of more than one randomization iteration won't work.

I suppose we don't know a way to make y indistinguishable from a random c. 


I realize the sumcheck is a kind of poly eval reduction, where the poly is the integrand, and we reduce from the two points 0 and 1 to one point r_i for each variable i. When we have a product of multilinears as the integrand, suppose instead of asking for the integrand along the single line, we ask for each multilinear along that line (in fact we could later generalize to have different lines). We receive two linear polys back and we check for consistency, choose random values on each line, then reduce to testing the new product. The benefit here is we only need relation intractability against linear polys, not quadratic ones. The cost, which is no cost for us, is the two polys in the integrand are evaluated at different points. 

But do we have the ability to do a linear relation intractable hash with SIS, or do we end up with the same problem as above where the 'function' is affine, not linear. We could still theoretically apply the FHE hash if not. It might be feasible because our function would be (a,b) -> (a-c)/(d-b) for constants c and d. This means two additions and one division. Hashing compression would always be a factor of 2. 
Computing the diff poly means doing the two additions, which are affine again. For random SIS instance A we don't want prover to maniciously choose c so it knows an 'a' that hashes to a-c. Prover could choose an 'a', hash it, call the output a-c, then solve for c. Similar for b and d. 
Actually to reduce two polys to one, we must choose a single common point on both lines, meaning this random decision must be a function of both lines, meaning we have compression factor 4 rather than 2. I think there may be a way around this, doing it in two iterations of a 2 factor compression. First ask for the eval of each poly on the point of the opposite poly, receive back two values, then hash to get a value and use that to take be an extra variable so as to join the two multilinears into one. Now are are left to reduce this new multilinear from two points to one, which can be done for each variable (no need for the last) with 2 factor compession.
Now it may seem at first thought that a random linear combination is a different relation for a fiat transform than poly agreement. But in fact, when two linear combinations agree that is equivalent to two polys agreeing. So in our case both relations are the same and can therefore be treated with the same fiat-hash function.
And on first though, fiat-hash functions cannot be amortized because amortization requires further fiat transforms. But on second thought, the purpose of amortization is to reduce the total number that must be calculated, thus allowing them to undergo tail recursion. This may indeed be possible for our fiat-hash functions. 

Now we are left to formulate a hash function with compression factor 2 that is intractable agains the relation (x,y) -> (y-y')/(x'-x) for possibly maliciously chosen constants x' and y'. 

What if we instead represented these lines in polar form? The form xt+y is where the vertical dimension is a function of the horitzontal dimension, or it could be vise-versa. For polar form the radius is the angle, or vice versa because for lines both of them are comptabile I think. 
Polar point (r,l) corresonds to cartesian point (r*cos(l),r*sin(l)). Thus x*t+y becomes r*sin(l) = x*r*cos(l)+y and we can solve for either r or l. For r we get r = y/(sin(l) - x*cos(l)). Now the point is change representatino, so instead of having y and x in this form I think we have u and v such that r = 1/(u*sin(l) + v*cos(l)). 
Now two lines intersecting in this form means 
1/(u*sin(l) + v*cos(l)) = 1/(u'*sin(l) + v'*cos(l))
=> (u*sin(l) + v*cos(l)) = (u'*sin(l) + v'*cos(l))
=> sin(l)(u - u') = cos(l)(v' - v)
Note this is similar to x*t+y = x'*t+y' => t*(x-x') = (y'-y) so we'd need division here too.

If instead we use representation x*t-y then we have x*t-y = x'*t-y' => t(x-x') = (y-y') which is more convenient.
H(x,y) = (y-y')/(x-x')
Note we can't just have affine functions from x -> x-x' and similar for y because we need an injective function from the outputs of the hashes to the final hash output.

Could we in general amortize hashing? If so, we could leave the hash completely unspecified, and instead focus on amortization which is more in the vein of sagaproofs. Well hashing is a computation like others, so while it may come at high auxiliary data cost, we can reduce it to poly evaluation for additional polys containing this auxiliary data. But we have to make sure the amount of hashing in the proof exceeds the amount of hashing necessary to verify the proof. Suppose we do the succint lattice poly evaluation. Suppose we input l hashing computations. We would like to verify with something like l/2 or less hashes. Suppose each computation requires s variables for sumcheck. Then we must add log(2,l) variables to enumerate the instances. So I think the sumcheck evaluation only grows logarithmically with l. Then similarly, it requires log(2,l) rounds in the poly eval reduction phase. Actually this is not amortization because a circuit must compute the hashes. We only know amortization via homomorphic commitments, and that won't work here unless we use SIS as our hash. 
Is the FHE hash friendly for circuits? Its impractical because it will involve homomorphically computing inverses. Its done with an SIS modulus q, so it would best be done in the side-circuit, which further complicates circuit communication, but if we figure out an isomorphism it should be doable. 

note hashing is made more complex that we have imagined because its over extension field elements. this makes it more complex both for FHE and SIS. 

consider again SIS hash, trying to disguise and use a linear combination to cancel the disguise. disguise real z' with c, and having z as input. we are thinking of z and z' as decomposed representations of scalar values, eg bit vectors. the dot operator means concatentation. There is only one place we will split for concatenation.
Our input will be 0.z, but I suppose we could replace the prefix 0 with something else, maybe tying z to its past state.
A(0.z - c.z') = (0.D)(0.z - c.z')
where D reconstructs the vector z - z' into a scalar value, which might be tricky but leave that alone for now.
Upon submitting input 0.z - c.z' that satisfies this equation, it also serves as a solution to the SIS problem for 
(A - 0.D) which is random if A is random because D has a standard fixed form.
Now we can't actually submit z' as input because it is invisible. The purpose of c is to disguise z' such that we don't rely on it and it can be replaced with something else that we know. For whatever z'' we choose to take the place of z', our hash will have the explicit form
z -> A(0.z - c.z'') = A(0.z) - A(c.z'') = A'z - A(c.z'') where A' is the second part of A.
We showed that if z''=z' then unless z=z', hashing to z-z' means breaking SIS.
We also will make the assumption that A(c.z') is indistinguishable from A(c.z'') for z''!=z'. What we mean is we have an adversar who provides us with z', then we choose random c and either return A(c.z') or A(c.z'') for z''!=z'. We assume for any z' adversary can only distiguish with negligible probability over choice of c and z''. In fact, we could just set z''=0. 
Suppose we have a subroutine that can hash from z to z-z' for some fixed z'. We will use this adversary to win the distinguishing game above. We take the z' for which the subroutine works, provide it to the challenger. The challenger replies with a vector M and we must decide whether it is A(c.z') or A(c.0) for random c. To do so we invoke the subroutine. We create the hash function H(z) = A'z - M. We test whether the subroutine can hash from z to z-z' for some z. If so then we know M must be A(c.0) because if M was A(c.z') we would have
A'z - A(c.z') = z-z' => A(0.z) - A(c.z') = z-z' => A(0.z - c.z') = z-z' = (0.D)(0.z - c.z'). 

any immediate concerns with the above? maybe use subset sum instead of SIS for more comptabile output. Suppose we replaced 0 prefix of z with something else, say r. Then hash is
H(r,z) = A(r.z) - M
which indeed randomizes the output. We could choose r (and thus c as well) to be the size of an output, this way a previous output can be part of a new input.
this is all exciting, but i feel nervous because I don't know if it will work.
Let us try to generalize to linear affine functions. 
we are not done. use separate SIS instance for each affine. then argue if correct ratio is hashed it means hashing this ratio between two SIS instances and that is infeasible.
this concrete fiat-hash is a bonus feature, and others features don't rest on this, but this rests on other features like the ability to do sumcheck with only linear fiat-hashes. Therfore we should construct other things first and leave this as a separate bonus feature. 
whatever hashing we can do with SIS, maybe we can do the same with DLP.
before finishing the hash for roots, we could prove the above hash for linear affine functions, which would be useful even if it doesn't end up working for roots. 

what are the input/output spaces we'd like for this affine (affine includes multiplication, so no need to mention linear) intractable function? Our main circuit is for small prime b. To verify it we will use elements of a large extension field. These are the elements we will be hashing, and we want them both as input and output. They are each representable as a vector of b values. Note this extension field may be different from the one used for the primary commits with SIS, which must be the size of q, whereas the current extension field has size for standard soundness. We could try either performing the hashing inside the main circuit or the side circuit as be explore below.

Suppose we would like to perform this hashing inside the main circuit, and suppose we'd like to do this using SIS. We would need to use a multiplicative extension field group, and I think the size of the constants must only be such that SIS is hard for our input parameter choices. To hash b values we will need to break them into bits anyway in order to do appropriate exponentiation. Now suppose the output of the hash is to be interpreted as an element in the sound-extension field. 
...uh oh, pause for a moment. I'm worried the hashing argument might have a conflict in that the SIS group is different from the group for the input/output affine structure. I think the requirement is that the inputs go through an affine function where the constants must be no larger than the SIS group order. In other words, the affine function is taking place inside the cyclic group isomorphic to Z_q. Since we are doing affine operations, and not multiplicative operations, we should be able to do operations for extension fields as well as prime fields. But we do an operation on the input for each of the n dimensions, so the output is the direct product of n large cyclic groups.
...to continue. Well the output of the hash is n elements of the multiplicative hash-extension field. Suppose we set the sound-extension field to be the n'th power of the hash-extension field. Then we could interpret the output of the hash to be an element of the sound-extension field where all n components are non-zero. This might be ok, because the sound-extension field should be larger than the hash-extension field anyway. But with a large sound-extension field we will need more hashing, so this means more operations in the hashing-extension field, which will be expensive. So it may make more sense to try to use the side-circuit for hashing.

Suppose we would like to perform this hashing inside the side-circuit, and suppose we'd like to do this using SIS. Well the side circuit is made with prime modulus q, so this group has no subgroups. Suppose we are to use a commit-extension field for an isomorphism with q, eg with Mersenne prime. Suppose we set the sound-extension field equal to the commit-extension field. We would like to interpret the output of the hash, ie n hash-extension field elements as one element in the sound-extension field. Several reasons this won't work. For one the sound-tension field has order q and is not divisible, and if we fiddle with the off-by-1 due to multiplicative groups I arithmetic somewhere will go wrong. Also, we will be doing much more arithmetic with hashing than with other verification, ie more in the hash-extension field than the sound-extension field. Thus it makes more sense to try setting the hash-extension field equal to the commit-extension field. The only trouble here seems to be that the commit-extension field will already be large, so doing this then interpreting the output as an element for the sound-extension field means a huge sound-extension field. This could be solved if we set n=1, ie only one SIS row. In that case I think all field extensions (commit, sound, hash) could be identical. On the other hand, what if did the commit-extension as a regular SIS with many rows, and we do sound- and hash-extensions as SIS with only one row, but the same modulus. This may be secure because the load for the hashing SIS is less than that for the commit SIS, so many one row is sufficient. This also means the modulus would serve for our soundness. I think this is the best option so far. If one row for the hashing-extension is not enough, I suppose we could add more, but only as many as we need, not as many as the commit-extension. Then the soundness extension would be a further extension. 

What about completing the fiat hash? Suppose we use x without use of disguise, x does not hash to x-x' and y does not hash to y-y', but we still have
x*H(y)/H(x) - y = x'*H(y)/H(x) - y'
H(y)/H(x)(x - x') = (y-y')
H(y)/H(x) = (y-y')/(x-x')
This involves multiplication, so it cannot be dealt with using SIS, especially not extension field multiplication.
I thought we might argue H(y) = k(y-y') => H(x) = k(x-x') where k is also an element in the hash-extension field which causes an offset because the group is cyclic. Lets write this additively
H(y) - H(x) = (y-y') - (x-x')
H(y) = k + (y-y') => H(x) = k + (x-x')
H(x) := Ax - r
Suppose we use the same SIS instance for both x and y.
(Ay - r) - (Ax - r) = (y-y') - (x-x')
this implies either
Ay - r - y + y' = Ax - r - x + x'
or
(Ay - Ax) - (y - x) = (r - r) - (y' - x')
(A - 1)y - (A - 1)x + (y' - x') = 0
(A - 1)(y - x) + (y' - x') = 0
but this is not even making use of our intractability for H.
We need to say its infeasible for H to have two outputs with a certain ratio. 
Or we could put it through the outputs through another hash and make a similar statement.
But I don't know how to do these.
Suppose we use different SIS instances for each instance x and y.
(Ay - r) - (Bx - r) = (y-y') - (x-x')
(A - 1)y - (B - 1)x = -(y' - x')
so we have the same problem.
This implies we may need to make the SIS group multiplicative and do the subtraction for the diff poly 'in the exponent' with representations translated from the additive to the multiplicative group.



what if we don't even do isomorphic arithmetic in the main circuit and instead do all verification in the side-circuit. The field of the side circuit has order q which is prime and the same as the multiplicative order of an extension to the main field. Adding q elements is equivalent to multiplying extension elements. The q elements represent only the non-zero elements of the extension field. Thus 0 in the main circuit has no counterpart as a q element. This could be problematic. Also, the side circuit can only do multiplication in the extension field, not addition, so in fact we will surely need the main circuit for that.

remember that theoretically the relation is fixed before we choose the hash. thus if we want to use the same SIS constants for all hashes we must somehow modify the problem, eg with a new target every time, and prove this is just as hard as a new instance. if we choose a random new target, then it functions as the hash key, and will have length smaller than the input, and does not grow with the size of the relation (ie the affine function). for this reason it would be called 'compact'. The ability to use a sufficintly 'random' new hash for each relation without an entirely new SIS instance may be what prevents us from completing. 
I think our rerandomization will come from taking the outputs of previous SIS hashes or commits, decomposing them, then feeding those to the fixed SIS. We need to prove this way of 'composing' SIS instances is sufficient. Maybe we say something like out of all the possible targets, only for a negligible fraction of them can adversary find a solution. Then we say of all the outputs from the first SIS that the adversary can generate, only a negliglble fraction of them will map to the said vunerable targets. I think this should work. 

Before we finish formally organizing all these into theorems, etc. figure out how to complete the fiat hash, which we did above only for the special case of taking the difference of two hash outputs. We can take the hash outputs and do any isomorphism with them. Take the case of computing quadratic roots. The outputs from the affine hashes are elements in the multiplicative extension field and represent the coefs of the diff poly. We can multiply together ac and bb but then we'd need arithmetic in the additive field for bb-4ac and 2a, and the back again to multiplicative for \sqrt{bb-4ac}/2a and -b/2a, then back to additive for final +-. This is huge overhead even if the compressions are somehow safe in the additive field. Remember we don't know how to prove relation security for composing SIS hashes.

As far as relations with multiple outputs, maybe we can also use indistinguishability. The idea is before we disguised the function, and now we have multiple functions to choose from and we disguise whichever we choose. We know adversary cannot solve for the one we chose, but due to disguise the adversary can't tell that apart from the others or the fake one, so adversary can't solve any of them. Using this we can prove for not just affine functions but affine relations. But a relation is like a collection of functions, so if intractable for all functions then its also intractable for collections of them and thus relations as well. It seems a little too good to be true that there only needs to be one function for which the adversary is bound to fail, and that we can disguise any other functions along with it and suddenly the adversary must fail on all of them as well due to indistinguishability. This shows its really the disguise that prevents against all structures really. What's important is that the structure searched for is chosen prior to the random hash instance. 

I'm wondering if we can also have a provable hash constructed in a similar way for the SIS commit randomizations. The reason this may work is we also use linear functions, albeit they are multivariate. 
...


thinking about finite fields to determine what is best groups to organize various isomorphisms. take q = p^d the order of a finite field. what would a subfield look like. i assume it would be of the order p^d' for d' < d. for any two k1,k2 < d' we would need k1+k2 < d' to be closed under multiplication, but that doesn't seem possible without changing the irreducible poly, but that would change concistency with the ambient field it seems, so idk. Oh, just like we take the extension of the prime base field by choosing an irreducible poly and making elements of the base field coefs, we probably do the same for extensions of extensions. This means multiplying two elements of a double extension of degree d2 of a degree d1 extension means taking d2-1 polys each composed of d1-1 coefs from the base field and multiplying them together. The amount of multiplications in the base field I think has grown quadratically. 
For our hashing to work, the output of a hash must be a single sound-extension field element. This is because we must divide two such outputs for the linear relation.
A most desireable setup I suppose would be one of those mentioned before, namely selecting a prime q large enough for the hashing-extension and the sound-extension and having SIS for hashing have n=1. Then we use the same q for the commit-extension but with n large enough to support the extra load. This means we don't use extensions of extensions. 

If we are to use extensions of extension then I need to understand subfields, and my description above with extension of extension may be flawed. Afterall, a subfield should inhereit the operations of the ambient field. 


As far as using b > 2 to make q prime we will need to a use a multiplicative subgroup, and testing subgroup inclusion seems costly. But maybe we can strategically choose the subgroup. The subgroup of squares is what I think of. Considering the map x |-> x^2 we want to find the size of the range. each element k^2 in the range has inputs k and -k which are different because b>2. Thus there should be (q+1)/2 such squares, but they include 0, so our subgroup has order (q-1)/2 which divides (q-1). This subgroup is maximal as we would like. Also, and for the reason we chose it, its easy to verify membership, because any element k^2 can be given as either k or -k and the verify squares it obtain the intended value. The point of this was to have an isomorphism where b>2, since there are no primes b>2 and q and power n such that b^n = q-1. For this solution to work we need primes b and q and power n such that b^n = (q-1)/2 => find b and n such that 2b^n+1 is prime. eg 2*3^4+1 = 163 or 2*11^3+1=2663, or 11 with 9. But I don't know how many exist for other b and much larger n. We can treat the case b=2 as a special case where every element is a square. We would need a special way to translate representation via b powers into representions in the extension field. But the circuits don't need to understand this translation process because its done offline by provers. So there just needs to be some efficiently computable isomorphism. And it would be much harder anyway to solve the above with q also a power of b. 



lets finalize the general poly method. we decided to combine both uniform and non-uniform sumcheck, then we realized sumcheck is just poly eval reduction. So we need to finalize the building blocks. I think we consider 'low' degree multivariate polynomials. Then we present how we can compose a variable with a univariate poly, and reduce evaluation anywhere on that univariate to a random place along it, and how this is a one-round protocol. We want these lemmas to imply how we can reduce for a poly made as a circuit of other polys. we ask for the uni poly of each poly separately. if these are all correct we can evaluate the main poly as desired, so we reduce to checking that what the prover sent is all correct. 
we also need as a building block how we can deconstruct poly into parts or put them back together so that we may work with virtual polys and our program can be independent from the way the data is organized in the physical commits. 
when we want to evaluate the same poly at two different points instead of regular poly eval reduction using univariates, we could just ask for claims then take a random linear combination. Would this even work for sumcheck? Suppose we start with
\sum_{0,1}^v f(x_1,...,x_v)
so first we want to evaluate for x_1 at 0 and 1, and we receive claims c1 and c1' respectively. Then we hash to random scalar r1 and are left to compute the linear combination
r1\sum_{0,1}^{v-1} f(0,x_2,...,x_v) + \sum_{0,1}^{v-1} f(1,x_2,...,x_v)
= \sum_{0,1}^{v-1} r1*f(0,x_2,...,x_v) + f(1,x_2,...,x_v)
Suppose we try again, receiving c2 and c2' and hashing to r2, leaving the combination
r2\sum_{0,1}^{v-2} r1*f(0,0,...,x_v) + f(1,0,...,x_v)
+ \sum_{0,1}^{v-2} r1*f(0,1,...,x_v) + f(1,1,...,x_v)
= \sum_{0,1}^{v-2} r2*r1*f(0,0,...,x_v) + r2*f(1,0,...,x_v) + r1*f(0,1,...,x_v) + f(1,1,...,x_v)
Continuing in this way we can see that at the end we will end up needing to evaluate the multilinear at the point (r1,...,rv) defined with the coefs as the entires of f. Thus this achieves the same as the sumcheck. 

Is there any fundamental difference between reducing via a random linear combination versus reducing via the univariate? for a second consider the special case of the same multilinear poly at two points. With univariate case prover sends two elements defining the line, and verifier sends back random element as a point on the line. In the random linear case prover sends two elements as the claimed evaluations, then verifier sends back random element for the combination. Suppose there are k points. In first case prover sends k points defining k-1 degree univariate that interpolates the k points. prover responds with a single element. In second case prover sends the k claims and verifier returns with 1 or more random points for a combination (the more points the better the soudness). Actually I've been forgetting that in the first case the degree of the univariate sent by the prover doesn't only depend on the number of points or the degree, but also the number of variables. In general the second way requires less communication, only the claims and the response and these can have constant size apart from the number of points, but the cost is the resulting random combination poly will be more complex and perhaps less convenient to work with.
We should present both techniques. 
we should also show along with this how to make prover linear. 



hmm, how large must beta be for a solution to exist?
one paper says \beta > \sqrt{nlog(q)}
but another paper says \beta >= \sqrt{m}q^{n/m}
the latter is clearly much bigger
if we use the latter we need
\beta = b\sqrt{m} >= \sqrt{m}q^{n/m}
=> b >= q^{n/log(b,m)}
=> b >= b^{n/m}
and in the case of hashing m=2nlog(b,q)
=> b >= b^{2/log(b,q)}
=> 1 >= 2/log(b,q)
=> log(b,q) >= 2
=> q >= b^2
oh, so no worries. for the other we get
b\sqrt{m} >= \sqrt{nlog(q)}
=> b\sqrt{2nlog(b,q)} >= \sqrt{nlog(q)}
=> b^2 >= nlog(q)/(2nlog(b,q))
=> ~ b^2 >= 1/2
which is too small to even make sense.
I think the first formula is correct, and the second is wrong and is in fact rather an estimation of the total number of solutions.
More generally the second source says \beta > \sqrt{m}(#G)^{1/m}
for hashing with density 1 this implies
b\sqrt{m} >  \sqrt{m}(#G)^{1/m}
=> b > (#G)^{1/log(b,#G)}
=> b > b
but this won't be a problem with higher density

understanding an overlattice Z^2/L
lattice cell points: (0,0),(1,3),(3,2),(4,5)
(1,1) + (2,2) = (3,3)
(1,1) + (3,3) = (4,4) = (1,2)
(2,2) + (3,3) = (5,5) = (2,3)
(1,1) + (2,3) = (3,4)
(1,1) + (3,4) = (4,5) = (0,0)
this has order 1 + 1 + 1 + 1 + 1 + 1 + 1 = 7
its indeed a group and supposed to have order about the area of the cell
cell base is 3 and height is 3 so area is 9. this makes sense because we should only count 1/4 the parimeter points because a cell has 4 sides. but in higher dimensions this grows much slower than the volume.
this group is prime, but apparently this factor groups can be any finite abelian group.
For any finite abelian group G, denote by L_{G,m} the set of all lattices such that the integer overlattice is isomorphic to G. A lattice belongs to the set iff it has rank <= m and there exist (g1,...,gm) that generates G and also the orthogonal lattice L_g which is isomorphic to L. 
Note this is a special case because Z^m is a lattice and could be replaced by any other integer lattice. 


in general we will want finite abelian groups whos prime decomposition is powers of b. in the case of \Z_q^n we have decomosition (b^log(b,q),...,b^log(b,q)) with n entries. To use the side circuit for this special case we set b=q. But how could we generalize? We will need the group to be composed of prime powers if we want a full side-circuit from it. And we would actually need a separate side circuit for each distict prime. In the case above we have only prime q, suggesting it may in fact be the best option. 
If we want the output of an SIS hash to be an extension field element, and we want the group to be the multiplicative group for the intractability argument to go through, then we need the group to have prime order p^d-1 for some prime p and power d. And we need to somehow represent such prime values with b values. Before we were thinking p=b or p=b^d' for some power d', both of which allow us to do extension field arithemtic in the b circuit by representing values with d or d'*d elements and interpreting as any non-zero element of the extension field. So either way this forces a prime group order and only very particular primes. 
If we want to do base change in a full-circuit we also need a prime group order but it need not have the form p^d-1. 
So it seems we only do extension field arithmetic in the main circuit if q is not prime and a power of b, otherwise q is prime and we leave all arithmetic to the side-circuit. 

Now our only hope that may not be met is choosing q as a prime with large enough value that we may let n=1 for hashing. Seems like paper abstract https://arxiv.org/abs/1505.06429 says cyclic lattices (which means n=1) are the most common, which is evidence for this choice. Solving for a given group is a reduce problem for many lattices, and the above says if we choose a cyclic group then we can solve for the most lattices. 


for commits we should encode b values around 0 rather than above such that we can avoid the need for offsets. and remember its still collision resistant because upon a collision they can be subtracted for an SIS solution of twice the bound. and since we use euclidean norm by encoding around 0 I think we achieve twice the b size for the same bound. encoding above 0 we get bound \sqrt{b^2m} while encoding around 0 we get \sqrt{(+-b/2)^2m} so indeed. and since b is even (except for 1) encoding around zero is precisely symmetric, not off by 1. Oh, haha, I see why this doesn't actually achieve anything. Its because when encoding above 0, upon subtraction we get the same benefit where the bound doesn't change. So we get the same result in either case, just in different places. But for the initial reason above encoding around 0 is still desireable.


a possible problem in practice is that for security the hash group must be so large, and thus the sound-extension field must be so large that the prover has trouble doing all the poly evaluations, which involve multiplications in this giant extension. This is a good reason to choose only linear hashes, this way \beta can be small. In fact, maybe for SIS we should select parameters according to subset sum. 
Or maybe we should investigate using the ring versions for SIS. 
So I suppose today I will be investing how the more size-friendly ring version (though still costly) could be used.
It seems we could probably do subset sum with compression factor 2 for hashing with a group of size b^d-1 and treat it as a multiplicative extension field. The extension field would still have to be large, eg > 1024 bits but this is less than if we used SIS. We might still want to do this in a side circuit which means b^d-1 must be prime or a large subgroup of a prime additive field.
In this case we only need to use SIS, and try ring SIS, for the commitments. Ring SIS basis change is not different from regular SIS and would be done over an extension field, doing a linear combination as usual. My question is how we can treat these extension field (b^d-valued) polys as base field (b-valued) polys. 
For the ring SIS, I think the Michanccio version is more suitable as the Peikert one has some special distribution for commit values that is not too natural. So for our version the ring is polys modulo an irreducible poly of dimension n modulo an integer about n^2, and values as polys with some infinity norm. Clearly we could make this into an extension field choosing modulo a prime power. So the output is about size 2nlog(2,n). 

Also, the randomization process for commit knowledge might be different. 

Another possibility is we put the main circuit in an extension field.

Hmm, SIS params doesn't seem that bad if I consider nlog(q) and the analysis I did in the stackexchange post. 




The fiat hash chain may be more problematic than I thought. Theoretically inputs for the hash should have not just the new input but also a tuple of all previous prover and verifier messages too. Then we have binary relation for all failures. This assumes the prover can continually choose the tuple inputs, which is true, but doing so is much more costly than choosing the new message because a valid new tuple means going through all previous parts of the proof again. This is where we might have an advantage and assume the prover only samples a small polynomial of left tuples, which in our case means the prover can only sample a small polynomial number of hash keys. Then we consider the probability the prover breaks the intractability relation against the new input with respect to any of these hash keys. That means our soudness error is multiplied respectively. How could we formally bound the number of keys sampled?
The hash chain we plan to use means the prover need not repeat the whole protocol to sample a new key. Since we do it sequentially the prover only needs to resample the last hash. We assume sampling the last hash its output will not break the intractability, but that output might be a key that helps the prover break intractability for the next hash. It would be nice to have something to make it expensive for the prover to sample new keys. Interactivity with a verifier is one very effective way to do this, and only allows sampling once. But we are trying to do non-interactive.
I wish we could make 'interaction' with the verifying circuit. But this requires the verifying circuit to respond in some unpredictable way, or for it to interact with the next verifying circuit, etc, a system for which we don't know proofs. 
What if proofs in a chain had counters of their position and each one is expected to use a distinct source of randomness in its fiat hashes, generated by the verfier (or the network) particular for each fiat transform. Suppose the prover followed along honestly, never rewinding. Then it would be just like interaction. But of course the prover could wait for several 'blocks', see the randomness again, and continually try and then rewind. But maybe we could make such rewinding very expensive, by having the prover compute and submit many parts at once independently for a single hash transform. Then the transform would have to accept many inputs at once, but rewinding would mean re-generating all those inputs. But I don't know how to structure a proof like this. 

Maybe we say prover can only sample a negligible number of keys. If prover samples a key and succeeds, then from our reduction of relation intractability to SIS, we know prover has sampled a key for which it can solve SIS. So maybe we can relate this to the complexity of solving an ISIS problem when the target comes from sampling another SIS (or the same SIS) instance. This itself may be an interesting problem. 


So what about fiat-shamir for the lattice challenge randomizations? but the randomization is for two purposes, knowledge and poly eval soundness. For poly eval, each challenge can be thought of as sampling a random point on an l-variate linear poly. Now unlike our linear univariates that have just one root to avoid, these polys have many roots, far too many to compute. But we are still above to hash to the difference poly with affine intractable hashes. However, computing we can't just compute the roots by dividing the difference poly coefs like for linear univariate. Instead we must compute the kernel of the diff poly. Consider for 3 variables diff poly a,b,c, finding x,y,z such that
ax + by + cz = 0
Suppose we iterate the hash, where we hash for x and y, and then apply the univariate linear intractable relation hash for the last variable z. I think this could work to the same extent as our regular poly eval hash works, only leaving the concern about selecting hash keys. 
But we are still left with intractability for knowledge soudness. Does the knowledge argument rely on complete randomization? It relys on challenges being sufficiently uniform that at least one lands outside the set for which the adversary can succeed....


We are sampling hashes by just passing part of the input to SIS as the key. Assuming the key is random, is this hash secure? This can be reduced to asking whether ISIS is secure upon changing the random targets. 

SIS can't be a random oracle due to linearity. Even if we decomposed the inputs, it retains linearity as long as there is no overflow. Also, of course, 0 -> 0. 

Breaking the relation without the key is breaking SIS for a related problem. We would like to reduce breaking the relation with the key to breaking the related SIS with the key, which is equivalent to just breaking another SIS. 
In fact, suppose we include key in the regular input for the regular affine-intractable hash. In the affine function we just cancel it with 0's. Now the augmented relation includes the key as input, and in some non-trivial way the key then determines the polys, so this augmented relationship is not an affine one.  
We need to think more generally because we not all relation will be affine anyway, eg when reducing two points to one across two polys cuz we will need to choose the same point for two univariates. 
I was thinking before we could handle quadratics and other relations by hashing all but the last input then doing an efficiently computable function with an intractable hash. But I realize for many cases the resulting function is complex rather than the simple linear one. For example, with a quadratic poly suppose we hash two of the coefs. Then two of the coefs in the quadratic formula become constants, and we are left to compute the resulting function, which will involve a square root. But theoretically we could use sumcheck to do this reduction and still only need linear polys.
As far as multivariate polys, after we set all variables but the last we are left with an affine function so it might work in this case.
But in univariate constraint check we use multilinear polys but the poly coefs are not determined until the end, which is after the random point is chosen. Instead, the random point must be chosen by the commits of the polys in the integrand. This could be thought of as the problem of committing to polys via SIS such that the outputs are taken as the random points directly (with rehashing), and doing so such that the random points form a root for the sumcheck circuit. This is highly non-trivial to compute, so it seem difficult to make provably relation intractable.

Consider the simplified case that all relations will be the linear ones, and we are only left to determine whether key sampling is secure. The key sampling technique we'd like to use is decomposing the previous verifier message and feeding it as partial input to the hash. Interesting is that the moment the previous verifier message is output, both the relation and the key are immediately determined. This is in contrast to the regular notion of relation intractability where the relation is determined first then the key. The point of this is that the key be independent of the relation. When the relation and the key are determined simultaneously by the same input, of course they are theoretically not indepennt. The goal is to show that the way the key is chosen yields a hash which seems independent of the relation from a computational perspective.

I am very interested in giving up on provable relation intractable hashes for fiat-shamir. I'd like to just model with interactive proofs and use some sufficiently complex hash as a black-box for non-interactivity which is necessary for recursion. I would instead probably formally model with interactive oracle proofs. But I'd still like to show the linear intractable hash I've created, but I don't know how to frame it such that it shows any progress. Even if the key sampling was secure I could only use it with some sumcheck, not constraint checking-type sumcheck, not lattice randomization, and not poly eval of multiple polys the simple way. I suppose I leave this as an open problem now, that is how to make use of the linear intractable hash I have so it can contribute to the paper. 
I could have a section on efforts toward proving intractability, maybe mentioning my hash if I can frame it as progress, because at the moment I've reduced the probably to key-sampling which seems just as hard as the original problem. In this section I could also suggest the possibility of trusting a third party to act as an oracle, accepting inputs, choosing random responces, and signing them with a publically verifiable signature that has friendly verification for circuits. This would capture interaction at the cost of a semi-trusted third party. If the party cheats the most it could do is reduce the oracle to using the signing algo as a hash function, which itself may be hard to break. 
Without the need for the linear hash I had, our prime q can correspond to a subgroup of a multiplicative extension field, letting b be other than 2, and if we want to do randomzation in the main circuit we must have primes b and q such that (b^n-1)/2 = q where q would be isomorphic to the multiplicative subgroup of squares in the extension field. 


hmm, for the basis change technique for DLP, though changing the basis requires much auxiliary info, that info could be committed under the new basis, and could be folded into the next amortization round. Like lattice, could perform evaluation on previous part, not just basis change. When we do this it is clear we are reducing enitrely to new data poly evaluation, so we are not relying on circular security.


hmm, with the above in mind I think we could also generalize to having a means of entropy-dropping. that is an efficient way to compute representations of commits such that they can be combined probabilistically to drop entropy. 

to get a better understanding it would be great to find another example, which are already tried but didn't achieve. matrix multiplication would be a candidate and we also identified others but the problem was they were all based on algebra and even most of them on finite fields. 

I'm wondering whether I can get better motivation for writing up the method, maybe even improving it, by first working on the math want to continue with anyway after I finish. I already tried motivating myself with applications, but despite long effort (in another paper) I didn't achieve anything. In fact the negative results for applications made me a whole lot less excited, even disappointed, in the power of verifiable proofs. In fact, even if we achieved what I thought would be an ultimate goal, that is computing on shared secret data (homomorphic encryption), that still falls short. For a primary reason, big data on shared data requires so much computation (even if the data was not private) and lots of bandwidth and high liveness. This would not just be problem if we used everyday devices, but even personal servers would suffer. We need big server clusters, which means reverting to that externally-closed centralized autonomous systems solution. So party of my lack of motiation is a feeling of loss for my ambition I've long thought I had.
personal snippet follows. I thought an adventure would motivate me because thinking about going somewhere cool gets me going. so I came here to xiamen and I'm already exhausted after 24 hours and I've seen I think the coolest neighborhood I ever have. Thought that give me hint of excitment at this moment: travelling more (but I know I'm lying to myself), getting fluent in chinese, a chinese gf who's not obsessed with her phone, creating visual presentation/web platform for the math I plan to continue with, hearing back from weizmann (actually i just feel this dragging-on and they should have replied by now), getting excited (yes even the thought of getting excited gets me excited).  


hmm try to amortize SIS hash, and of course this requires hashing, but maybe in this case the relation is well defined and is in fact an affine relation, therefore we can use SIS again as an affine intractable hash. 

proof size would be big given all the commitments, of which we take random combinations multiple times. Maybe we can reduce proof size by commiting to them and then doing a proof on them. Doing this I think could also save a constant amount of commits. can we roughly estimate the optimal size?
it will be at least \lambda*q. 


maybe could argue that entropy dropping is information theoretically necessary to combine a large number of proofs and keep the size constant. and to keep soundness this entropy dropping must be randomized. 


think about the possibility of doing proof generation in the browser using wasm. 


try to improve the lattice randomization to only as many challenges as necessary for poly randomization. currently we need 2 extras. maybe do this by improving the heavy argument. 


what about fourier instead of polys. in polys we have a function over a domain, we ask for its value at a point in that domain. with fourier we could have a distribution over the domain of frequencies, and we ask for its amplitude at a frequency. I suppose this is just a dual of polys and is no more useful unless used together with polys. 

another idea is with expander graphs, a way to drop entropy would be taking two commits as walks with directions that encode the data. The commitments are the landing positions. For each commit we want to verify knowledge of data that forms a path there, as well as a property of that data. We want to reduce two such verifications to one verification. We need some kind of randomization. Maybe we could randomly choose an element then offset the path by that amount and derive a corresponding claim. But we need a randomized way of combining. Maybe like lattices we need a large number of commits first. Since expanders are bidirectional, we could randomize a new path out of the collection. Maybe we do one path of all commits, where we randomize the order and direction of each commit. Or maybe we do several randomized paths from subsets of the commits. Then we are left to reduce from a longer path to a shorter path. Maybe we can pass the group through a homomorphism to reduce the size.
suppose we use a cayley graph thats undirected and the problem is path-collision resistant. 
we could consider the transform of the new generating set being the square of the existing generating set. but this doesn't decrease the information needed for the path, as it decreases the number of steps at the cost of increasing the choices between steps. well actually there are nk/2 edges in first graph with n vertices. Then in second with k^2 generators we have (k^2)n/2 edges. This means the vertex number is the same, and the edge number is multiplied by k, and path length has decreased by half. 
suppose we pass the group through a homomorphism, say squaring all elements. Suppose this approx halves the number of elements. the vertices would now be n/2, but k may stay the same. 
a,b,c
aa,ab,ac,ba,bb,bc,ca,cb,cc
aaaa,abab,acac,baba,bbbb,bcbc,caca,cbcb,cccc
so we have randomization earlier above, but we also need entropy-dropping. well technically concatenation is entropy dropping, its just we don't know how to ask the prover to deal with a concatenated path. 


prove zk for simple poly eval, not for special case of sumcheck.


multiply using cyclic isomorphism



try lattices for an algebraic structure. Looking for analogues of the schqartz zippel lemma, which may hold here for multilinear polys.  
eg take lcm and gcd for a lattice, and consider the function
f(x) = lcm(
    gcd(a), gcd(b,x)
)
I also tried min and max.
disjuctions and conjunctions also.
or(
    and(
        or(a)
        or(b,x)
    )
    and(
        or(c,x)
        or(c,x,y)
    )
)
but in all cases it seems that absorptions, not idempotence, is the problem. I tried multilinear because of idempotence. How can we get around absorption? Well maybe its not absoptions thats problematic. I guess a necessary condition that isn't satisfied is irreducibility. Regarding OR and AND, suppose b is 1. Then suddenly maybe the whole expression is determined. But hmm, this doesn't happen for min/max or gcd/lcm. 
I don't think lattices will work.

what about non-abelian groups? what something as simple as x^0ax^1bx^2cx^3d
x^0ax^1bx^2cx^3d = y^0ay^1by^2cy^3d
=> x^0ax^1bx^2cx^3d(y^0ay^1by^2cy^3d)^{-1} = 1
=> x^0ax^1bx^2cx^3dd^{-1}y^{-3}c^{-1}y^{-2}b^{-1}y^{-1}a^{-1}y^{-0} = 1
=> x^0ax^1bx^2cx^3y^{-3}c^{-1}y^{-2}b^{-1}y^{-1}a^{-1}y^{-0} = 1
=> x^0ax^1bx^2c(xy^{-1})^3c^{-1}y^{-2}b^{-1}y^{-1}a^{-1}y^{-0} = 1
=> x^0ax^1bx^2c(xy^{-1})^3c^{-1}y^{-2}b^{-1}y^{-1}a^{-1}y^{-0} = 1
now suppose xy^{-1} is in a normal subgroup N. Then (xy^{-1})^3 \in N. Then c(xy^{-1})^3c^{-1} \in N. 
now lets write the following where we replace the single element of N with the whole subgroup.
x^0ax^1bx^2Ny^{-2}b^{-1}y^{-1}a^{-1}y^{-0} = 1
=> x^0ax^1bNx^2y^{-2}b^{-1}y^{-1}a^{-1}y^{-0} = 1
=> x^0ax^1bN(xy^{-1})^2b^{-1}y^{-1}a^{-1}y^{-0} = 1
this continues, we replacing the element with the set wasn't justified
try a mutilinear version
now use ' for inverse
ax1by1cx1y1 = a'x2'b'y2'c'x2'y2'
=> ax1by1cx1y1y2'x2'c'y2'b'x2'a' = 1
=> a(x1(b(y1(c(x1(y1y2')x2')c')y2')b')x2')a' = 1
maybe try working from the outside in instead of the inside out as above.
consider the simplified problem. for any element a, for what z does
aza' = 1 => az = a => z = 1
this then reduces the equation to 
x1(b(y1(c(x1(y1y2')x2')c')y2')b')x2' = 1
so consider for what z
x1zx2' = 1 => z = x1'x2
then it reduces to
b(y1(c(x1(y1y2')x2')c')y2')b' = x1'x2
now we must think about probability of an equation like this holding

the simplest question we can answer is the probability that for arbitrary target t, random x and y are chosen such that
xy = t
suppose this probability is negligible. what can do with this?
I don't know how easy encoding will be. Its easy with a field because 0 cancels entire terms. We need something similar, like mapping to the kernel. any kernel N is a normal subgroup that corresponds to a homomorphism f:G -> H for some subgroup H. f(g1g2) = f(g1)f(g2). encoding means sufficient cancellation, and using kernels that seems to mean passing through a homomorphism, with the cancellation proportional to |G|/|H|, but the larger that value the smaller the use of randomness, ie many different inputs could result in the same output. 
I think interpolation reduces to solving equations which often may have no solution. eg choosing x and y to find c means solving the equation
axbycxyd = c
=> xbycxy = a'cd'


try matrices. constraints would be in form probably of matrix equations. I think we can use the homomorphic property to reduce many multiplications to one. 
I've looked at matrices before and came up with that system that didn't allow for succinctness. Let me try again from a different approach.
Disregarding how we encode the computation, just consider the task of multiplying matrices at random points, and we want to reduce from two to one across both matrices and points. Consider the same point for two matrices. we could multiply one matrix by a random value. but this won't work for lattices. note that each matrix claim is a series of linear poly claims. we can randomize these claims across both matrices, even across just a single matrix, obtaining new claims for a single matrix of maybe different dimension. 
consider a poly as a row of n coefs and take p points of evaluation expanded into monomials as columns. the result should be a row of p evaluations. suppose we randomize by multiplying both sides of the equation by a random column of height p. this corresponds to taking a random linear combination of the claimed evaluations. we already know about this method. but before we'd reduce from two points to one with the intermediate poly. here we reduce with just multiplication, but the cost is that the resulting final point is a column that doesn't correspond to an expanded input point and is rather the linear combination of previously exapnded points. 


maybe we can avoid elliptic curve cycles by simply taking a single field and a single curve with prime subgroup q not equal to p the field size. we would need to reduce a claim about a poly mod p to a claim about the poly mod q. this may not be an entropy problem and can be computed as an 'isomorphism'. but hmm, I don't know how to do this.


maybe the field {-1,0,1} would be better for computation than the binary field {0,1}. 


try to prove that its impossible to probabilistically reduce two eval points to one on univariate.

think about fourier. is linear just like coefficients. shifting poly evaluation by a constant corresponds to multiplying fourier evaluation by an exponential. 
generalized fourier exists, with w an nth primitive root of unity. let t be time domain and s be spectrum, or frequency domain. 
s_i = \sum_{j=0}^{n-1} t_jw^{ij}
t_i = (1/n)\sum_{j=0}^{n-1} s_jw^{-ij}
putting together we see these are correct as
t_i = (1/n)\sum_{j=0}^{n-1} (\sum_{k=0}^{n-1} t_kw^{jk})w^{-ij}
= \sum_{j=0}^{n-1} \sum_{k=0}^{n-1} t_kw^{jk}w^{-ij}/n
= \sum_{j=0}^{n-1} \sum_{k=0}^{n-1} t_kw^{j(k-i)}/n
= \sum_{k=0}^{n-1} \sum_{j=0}^{n-1} t_kw^{j(k-i)}/n
= \sum_{k=0}^{n-1} t_k\sum_{j=0}^{n-1} w^{j(k-i)}/n
now if k-i != 0 then \sum_{j=0}^{n-1} w^{j(k-i)}/n = 0
while if k-i = 0 then \sum_{j=0}^{n-1} w^{j(k-i)}/n = 1
thus we have in total \sum_{k=0}^{i-1} 0 + t_i + \sum_{k=i+1}^{n-1} 0 = t_i

now how do we use this transform? 
we have to be clear whether we are transforming the coefficients or the function point-wise (ie points). We'd like to do the points, but then since the function is defined over the whole field, then so must the spectrum. this would be an enormous fourier to compute or invert in the first place. 
maybe we can take advantage of our function being a poly, in particular assume a univariate poly of degree m, call it f(x) = f_0 + f_1x + f2_x^2 + ... f_{m-1}x^{m-1}
s_i = \sum_{j=0}^{n-1} f(j)w^{ij}
= \sum_{j=0}^{n-1} w^{ij}\sum_{k=0}^{m-1} f_kj^k
= \sum_{k=0}^{m-1} f_k\sum_{j=0}^{n-1} w^{ij}j^k
so we must consider the term \sum_{j=0}^{n-1} w^{ij}j^k for fixed i and k
when 0 <= i <= n-1 and 0 <= k <= m-1.
suppose we have some efficient way to compute these 'monomials' for any i and k.
then we end up with a spectrum of size n, each frequency a linear combination of these 'monomials' with the poly coefficients. 
now consider the shifting property. consider we wish to evaluate the spectrum for two i values, and we want to probabilistically reduce to one (later consider the dual which is what we actually want). For u and v we have
s_u = \sum_{k=0}^{m-1} f_k\sum_{j=0}^{n-1} w^{uj}j^k
s_v = \sum_{k=0}^{m-1} f_k\sum_{j=0}^{n-1} w^{vj}j^k
suppose we pass in the line connecting u and v as a function of z. (1-z)u + zv
s_{(1-z)u + zv} = \sum_{k=0}^{m-1} f_k\sum_{j=0}^{n-1} w^{((1-z)u + zv)j}j^k
= \sum_{k=0}^{m-1} f_k\sum_{j=0}^{n-1} w^{(1-z)uj + zvj}j^k
= \sum_{k=0}^{m-1} f_k\sum_{j=0}^{n-1} w^{(1-z)uj} w^{zvj}j^k
idk what I'm dong. I instead want to consider the shift property.
Suppose we want an offset on time domain by constant j'. what is the frequency function for j-j' (a function of j). and call the new spectrum s' (a function of i). according to the theorem we have
s'_i = w^{j'i}s_i
To evaluate the original function t at time i we have
t_i = (1/n)\sum_{j=0}^{n-1} s_jw^{-ij}
Now to evaluate the shifted by j' function t' at i we have
t'_i = (1/n)\sum_{j=0}^{n-1} s'_jw^{-ij}
= (1/n)\sum_{j=0}^{n-1} w^{j'j}s_jw^{-ij}
= (1/n)\sum_{j=0}^{n-1} s_jw^{j(j'-i)}
so we'd like to probabilistically reduce these two evaluations to one
suppose we take a random linear combination of these claims.
t_i + rt'_i = (1/n)\sum_{j=0}^{n-1} s_jw^{-ij} + r(1/n)\sum_{j=0}^{n-1} s_jw^{j(j'-i)}
= (1/n)\sum_{j=0}^{n-1} s_j(w^{-ij} + rw^{-j(i-j')})
we'd need some way to iterate over all n frequencies to convert back to time, and above we'd need to do it with a transformed monomial. no idea how we do either, and due to what I'll explore below I think there are better solutions that fft.


see matter labs for implementation examples in rust, in particular the zksync repo.


I've realized the beauty of relying on PCPs instead of crypto because crypto assumptions become so much simpler. like the use of expanders for crypto is hard, but for PCPs its just some algebraic props needed. 

reading about second PCP construction. remember randomness is log and query complexity is constant. it uses a natural strategy of starting with constant randomness and query complexity but with high error probability. then we iterate through log rounds improving the system at each step, maintaining constant query complexity, but adding constant randomness, thus ending with desired complexities.


I'm going to start reading FRI and STARK and IOP related stuff, and I need to make sure I know why. I want to understand ways in which to encode computations for FRI commitments. I want to better understand FRI, and I want to understand whether all these schemes rely on the same FRI. 

note the fudanmental diff between PCPs and crypto directly is while both are forms of commitment, PCPs rely on redundancy for the benefit of no 'special effects', while crypto relies on the special property, in our case, of being homomorphic. is there are further way to characterize this difference? are homomorphic properties and redundancy related? 
f(x + y) = f(x) + f(y)
PCPs allow us to look into parts of it, while crypto does't and looking inside means opening the whole commitment.

PCPs connect naturally to locally testable codes and thus coding theory and notions like codewords and distance.

use univariate sumcheck for reducing multipoint eval to univariate eval. or actually use multivariate sumcheck in order to access constants, but formulate input to coefs poly as univariate input. but this may not be possible. the domain of the multivariates is much larger than the domain of the univariate. indeed, this won't work. maybe take the dual approach where we expand the constants into a poly, then we do something like univariate sumcheck, then we evaluate coef poly with uni commit as usual, but to evaluate the constants poly we take advantage of its expanded redundancy and reduce it later with something like multivariate sumcheck to manual computation involving no additional commits. maybe we can improve this by not expanding all at once, but expanding like a square root number of times, thinking about the matrix formulation of multivariate evaluation. alternative to univariate sumcheck we could use poly div. 

alternative to hamming distance, or rather hamming closeness, is the inner product which may offer better analysis. normalize it and still end up with fraction. they call it epsilon-orthogonal rather than epsilon-far. 0 means orthogonal, 1 means identitcal. so this corresponds to 1-distance.
notice that a q-ary lattice is a linear code defined by othgonality to the constants and its this orthogonality (inner product of 0) that allows for the homomorphism. but note that the input elements are small compared to the constants so we are taking the inner product of two vectors from different distributions. 
an interesting question related to both is inner product and orthogonalities of polys in a finite field. just as its hard to find a lattice codeword, maybe is hard to find a poly that is orthogonal to some fixed function (maybe from a different distribution, ie not a poly). 

affine. 
f(x) = ax+b.
f(x) + f(y) = (ax + b) + (ay + b) 
f(x + y) + f(0) = a(x + y) + b + b
has good test but each element encoding requires an independent variable, so huge domain. 

try using fourier to analyze distance, where an arbitrary function can be represented in fourier. 

I think a good lemma, maybe even optimal, is that if a test accepts with probability delta then it agrees with a proper function on at least a delta portion of the domain. clearly there is a tradeoff to be optimized in terms of sampling size.

I'm thinking the core of FRI is the ability to effectively sample a bivariate poly by sampling a univariate poly at multiple places. 

note exapnders can be permuted. this enables randomization via choosing an isomorphism by applying a random element. I still don't know how to do entropy reduction though. maybe like FRI its not a bad idea to use squares and cut the domain size in half, because like poly degrees halving, so would expander path lengths. but one advantage is expanders may support direct commits. 

maybe use expander as domain for commit to reduce both randomness in sampling but more importantly hash cost for verifier. forming a commit here is more challenging, enforcing consistency, but could be done using a proof tree. proof trees would mean many small proofs, so crypto amortization rather than succintness would be a great advantage here. maybe another way would be using PCPs where entropies to be evaluated are gathered and put into a single tree. we could probably do more with proof trees for commits than enable commiting to an expander domain. eg the tree could testify to the low degreeness itself, at each layer doing something like interpolation, joining child polys to a new one. an alternative is just to construct a commit as usual and have another proof testify to its low degree. both of these seem to spit out more entropy than they consume so are probably not worth it. 

PCPs of proximity put the input in an oracle too. previously verifier always was given input. 

Robust PCPs are where the queries are far in hamming distance from being accepted by the verifier, rather than just not being accepted.
apparently these only make sense for non-adaptive verifiers. this is because the query responses are viewed as inputs to a verifier predicate that outputs a decision, and adaptiness would mean these inputs are dependent on one another, so the signature isn't well defined. 

Supposedly composition is great for an outer robust PCP and an inner PCP of proximity. this is because the output for the outer PCP is far from being accepted, and this is the input to the inner PCP which is being a proximity proof reject inputs far from valid. We don't need to worry about the how many queries the outer PCP makes because the inner PCP can handle those. We also don't need to worry about the encoding the inner PCP uses. we just need to make sure the distance param of the output PCP is at least as big as the distance param of the inner PCP, and decision complexity (always upper-bounding the query complexity) of outer is no more than input complexity of inner. 
thus robust PCPs of proximity can be recursively composed.

worth to note in thesis writing how many related definitions there are to things like PCPPs, IOPs, IPs, property testing, hologaphic proofs, PCP spot-checkers, locally decodable codes etc. and decide whats important to us in our definition, which may be yet another definition. 


first draft of FRI-stuff, called Simple PCPs with Poly-log Rate and Query Complexity
proof length npolylog(n), query complexity polylog(n)
Give reduction of satisfaction to verifying closeness to RS code. Then give PCP for RS code closeness, I think this is an LTC, which only relies on the bi-variate low-degree test of Polischuk and Spielman. Thus we get ultimate PCP from an LTC, whereas previous works go from PCPs to LTCs. 
Motivation is using PCPs for positive constructions, thus focusing on shorter and simpler PCPs for explicit use.
PCP(r(n),q(n)) is class where fundamental variable, n, is size of instance.

1. INTRODUCTION

Theorem 1: (Efficient PCPs)
SAT has PCP verifier with r(n) = log(n*poly(log(n))), q(n) = poly(log(n)), proof length n*poly(log(n)), and running time n*poly(log(n)), ie SAT \in PCP[log(n*poly log n), poly log n]

so query size is larger than recent results but proof size is smaller. 
PCPs show existence of witness. PCPPs show closness of oracle to being a witness. 

Theorem 2: (PCPPs for SAT)
For every \delta \in (0,1) there's verifier ppto V_SAT that on input circuit \phi of size n, tosses log(n polylog(n)) coins, and makes polylog(n) queries to assignment oracle \alpha and proof oracle \pi. \alpha is of some length n' and \pi is of length n*polylog(n). Completeness is 1 and when input encoded in assignment oracle is \delta far from satisfying \phi then regardless the proof, verifier rejects with some constant probability. 

the above is confusing because \phi must be at least as big as \alpha in order to accept \alpha as input, so then what's the point of making \alpha an oracle? 
PCPs are similar to LTCs and go hand-in-hand. The rate is important, that is ratio of message over codeword size, a parameter contained in (0,1]. Another param is \delta, the proximity parameter, and query complexity. I would think randomness is also relevant. 

Theorem 3: (Efficient LTCs)

previous constructions start based on multivariate polys. core ingredients they use are
1. low-degree test
2. self corrector
3. zero-testers
4. reduction from satisfiability to zero-testing
we do the same but without need for a self-corrector. 
we describe our constructs in reverse order

reducing SAT to univariate zero-testing
given a formula \phi we give two polys P1 and P2 and corresponding domains H1 and H2 and constraint C such that \phi is satisfiable iff there P1 and P2 exist and vanish on H1 and H2 respectively and C(P1,P2) holds. 

univariate zero testing
reduces to two univariate low-degree testing problems with a consistency test.
query complexity is a constant independent of degrees. 
reduce directly from zero-testing to low-degree testing without self-correction needed.
both this reduction and the reduction from SAT to zero-testing only have constant query complexity. 

Reed-Solomon codes and proofs of proximity
central problem is given finite field F and degree-bound d and oracle f:F->F, test if f is close to a univariate poly of degree at most d. 
If f is \delta-far that means it must be changed on a \delta fraction of its domain. 
direct testing would require d+2 queires or more, so we instead turn to auxiliary proof \pi. This proof is the PCPP (or the assignment-tester). 
Oracle pair is (f,\pi). 
previous work shows this is possible but with large constants and design complexity.
Give an O(n*polylog(n)) size PCPP for RS-codes over field F of characteristic 2 and cardinality n. 

(
hmm, can we try to reduce poly evaluation between fields. for efficiency this would require a relationship between the fields, both the characteristic and the extension degree. well apparently homomorphisms between fields of different characteristic don't exist. this incompatibility is worth mentioning.
Forgetting extensions for a moment, suppose we are supposed to evaluate over prime field p but we want to reduce to q. We have a claim for p. Then suppose we evaluate over Z, not Z_p. We get an answer, confirm that modulo p it is as claimed. Then we take modulo q for the new claim. Since the poly is finite and inputs are finite, in [0,p-1], evaluation should be finite and within a certain range that should be handleable but it will still be large because remember each multiplication doubles the bit-length and the number of multiplications we'll have is log(poly_size). Note we are doing two evaluations, one over Z then one over Z_q but this is because the final evaluation for succiness is best done over Z_q, not Z. We want to say that if the claim to p is false, then the claim to q is false. Well if the claim over Z_p is false then evaluation over Z will not end in correct remainder and therefore be false. Then we want to reduce eval over Z to eval of Z_q. That is, if the claim over Z is wrong, then the claim of Z_q will be wrong. But hmm, this could go wrong by giving a wrong answer that agrees with the right answer modulo q. So idk.
How can we extend this to extensions? Maybe only option is extending to extensions over Z.

also, I'm thinking it may be worth mentioning in the paper how constructions go from complex to simple and my contribution is further simplification.
)

Organization
Section 2 is definitions and overview of PCPP for RS
Section 3 is zero-tester
Section 4 is reduction from SAT to algebraic problem
Section 5 put together other parts for final PCP
Section 6 generalize zero-testing to multivariate case

2. PROOFS OF PROXIMITY FOR REED-SOLOMON CODES

Distance between x,y\in\Sigma^n is noramlized hamming distance \Delta(x,y) = |{i : x_i != y_i}|/n = Pr_i (x_i != y_i).
C, the code is a subset of \Sigma^n. \Sigma is a field, and C a linear error-correcting code (since its a poly).
Distance for x to C is \Delta(x,C) = \min_{y\in C} \delta(x,y). If C is empty then \Delta(x,C) = 1 because it will evaluate differently to another element with probability 1.
Say "x is \delta-far from C" if \Delta(x,C) > \delta, and "x is \delta-close to C" if \Detla(x,C) <= \delta. Thus for fixed \delta, every element is exclusively either \delta close or \delta far from C. 

Proofs of Proximity
Make few queries to both instance and proof.
Soundness is a function of proximity parameter \delta.

Definition 1: (PCPP)
C\subset \Sigma^n has a PCPP over \Sigma of length l(n) with query complexity q(n) and randomness r(n) and soundness s(.,n), if there exists verifier V that tosses r(n) coins, makes q(n) queries to (x,\pi)\in\Sigma^{n+l(n)}, and satisfies the following.
completeness: x\in C => \exists\pi\in\Sigma^l(n) st. V accepts (x,\pi) with probability 1.
soundness: \Delta(x,C) >= \delta => \forall\pi\in\Sigma^l(n) verifier rejects (x,\pi) with probability >= s(\delta,n)

Consider C\subset\Sigma^n to be a property. PCPPs are defined for any property, but we only care about one in particular.

Definition 2: (RS Codes)
For polynomial P(z) over F and S\subset F, define its evaluation table over S to be <P(z)>_{z<-S} := <P(z):z\in S>. The RS Code of degree d over F, evaluated at S
RS(F,S,d) := {P(z)_{z<-S} : P(z) = \sum_{i=0}^{d-1} a_iz^i, a_i\in F}
The fractional degree of the code is d/|S|.

Field over p^l can be thought of as a vector space of dimension l over p. 
S\subset F is 'linear' if its a subspace of the vector space. 

Theorem 4: (Binary RS PCPP)
There exists universal constant c>=1 st. \forall F of characteristic 2, every linear S\subset F with |S| = n and every d<=n, the RS code RS(F,S,d) has a PCPP over alphabet F st.
1. l(n) <= n*log^c(n)
2. r(n) <= log(n) + c*log(log(n))
3. q(n) = O(1)
4. s(\delta,n) >= \delta/log^c(n)
and a verifier for RS(F,S,d) can be generated in time polynomial in |F| given (F,S,d). 

Notation has <w> = {w^0,...,w^{n-1}} where n is the order of w in the cyclic group F^*. 

Theorem 5: (Multiplicative RS PCPP)
There exists universal constant c>=1 st. for w\in F^* of order n a power of 2, S=<w>, and d<n, RS(F,S,d) has a PCPP over alphabet F st.
1. l(n) <= n*log^c(n)
2. r(n) <= log(n) + c*log(log(n))
3. q(n) = O(1)
4. s(\delta,n) >= \delta/log^c(n)
and a verifier for RS(F,S,d) can be generated in time polynomial in |S|+log(|F|) given (F,S,d).

n suffices to be poly(log(n))-smooth, all prime factors of n are at most poly(log(n)). And if we have characteristic <= poly(log(n)) then we use the additive group instead. Above is the case for multiplicative case of 2-smooth n. I don't understand why smoothness matters. 
We can boost soundness to an arbitrary constant < 1 by more sampling and averaging. This means trading query complexity for soundness.

Corollary 6: (Arbitrary Constant Soundness)
There exists universal constant c>=1 st. \forall \delta\in(0,1) we have a PCPP for RS(F,S,d) over F as in either Theorem 4 or 5 st.
1. l(n) <= n*log^c(n)
2. r(n) <= log(n) + c*log(log(n))
3. q(n) <= log^c(n)/\delta
4. s(\delta,n) >= \delta/2
and a verifier for RS(F,S,d) and \delta\in(0,1) as above can be generated in time polynomial in |S| + log(|F|) + k when given (F,S,d,\delta) with k being the bit-length of \delta. 

Sketch of Proof for Theorem 4
Consider P(z) with degree < n/8 evaluated on subspace L of dimension l and cardinality n. 
Create Q and q such that P(z) = Q(z,q(z)) with both Q and q of degree about \sqrt{n}. 
Let L_0' and L_1' be subspaces with same dimension l/2 and direct product L.
Let q(z) = \prod_{\alpha\in L_0'} (z - \alpha)
Apparently q is linear over L.
The kernel of q is clearly L_0'.
Image of L under q is subspace L_1 with dim(L_1) = dim(L_1').
Let T = {(z,q(z)) : z\in L}.

idk the rest.




FRI
Block-length N, prover complexity 6N, verifier 21*log(N), query 2*log(N), soundness min(\delta(1-o(1),\delta_0). 



1 INTRODUCTION

Algebraic coding theory.
|S|=N. S called 'evaluation set'. 
\rho\in(0,1] is called 'rate'.
RS(F,S,\rho) is set of functions f:S->F such that f is evaluation of poly over S of degree d < \rho N.
RS Proximity Problem gives verifier oracle access to f:S->F and verifier must determine whether f is \delta far from the code, doing so with large confidence and few queries.

RS proximity testing: 
When no additional data is provided, d+1 queries are enough, and we say prover gives zero effort to a make a proof of length 0 involving 0 interactions. 

RS proximity verification - PCPP model:
RS Proximity Testing but with access also to oracle proof \pi. 
Proof length, |\pi|, is measure over alphabet (F) length. 
PCP Theorem shows this can be proved in poly N time with constant query complexity and log randomness. 

RS proximity verification - IOPP model:
Generalization of IPs and PCPs. Interactive proof where verifier only queries prover messages at a few random places. 
Prover messages are \pi_1,...,\pi_r for r rounds. 
Query complexity to total number of alphabet elements read from f and \pi_1,...,\pi_r.
Prover is given f (no need to produce it itself) and prover complexity is time to generate all prover messages. 
Proof length is |\pi_1|+...+|\pi_r|.
IOPPs can be used to reduce proof length of PCPPs with no compromise to query complexity or soundness. 


1.1 Main results

New IOPP for RS codes, called Fast RS IOPP (FRI).
Analysis relies on quasi-linear RS-PCPP of BS08. 
Strictly linear prover complexity, strictly linear verifier complexity, constant soundness.

IOP
An IOP system S consists of algorithms (P,V). Input is x, with |x|=N, rounds r(N), and proof length l(N), query complexity q(N). Note this not a proximity proof so x is given to verifier and query complexity to x is not counted. 
Usually q(N) << l(N).
Every round starts with the prover sending a message in oracle form, and verifier responds with message. I think first message by prover is not f, but \pi_1. I think final message by verifier is decision.
For FRI, q(N) = O(log(l(N))).
<P<->V>(x) is output of V, either accept or reject. 
IOP is 'transparent' if all verifier messages x are publicly random and all queries come from these public coins (like AM family). 

IOPP
Generalization of PCPP to IOP model. Limit definition to codes for this paper.
Again, (P,V). Input is a code specification C = {f:S->\Sigma} for finite set S and alphabet \Sigma. Also give prover f^(0):S->\Sigma and verifier oracle access to it, where this function may not be in C. 

Definition 1.1 (IOPP)
An r-round IOPP S=(P,V) is a (r+1)-round IOP. Say S is r-round IOPP for C with soundness s^-:(0,1]->[0,1] wrt distance measure \Delta if the following holds.
1. First message format: first prover message is f^(0) is claimed codeword. 
2. Completeness: Pr(<P<->V> = accept | \Delta(f^(0),C) = 0) = 1.
3. Soundness: \forall P^*, Pr(<P^*<->V> = reject | \Delta(f^(0),C) = \delta) >= s^-(\delta)
Proof length is sum of all prover messages except f^(0). Prover complexity also disregards f^(0). Query complexity, however, accounts for f^(0). (Remark 1.2): Verifier decision complexity is, say, arithmetic complexity which accepts problem input and queries and query responses. 

Main Theorem
S is additive coset of F_q if coset of additive group, ie additive shift of additive subgroup. 
Additive RS code family is codes of form RS[F,S,\rho] for additive coset S. Called binary if F is characteristic 2. 

Theorem 1.3 (Main - FRI properties)
Binary additive RS code family with rate \rho = 2^{-R}, R>=2, R\in\N with \rho*N > 16 has an IOPP called FRI with following properties where N=|S| is block-length. 
1. Prover: Arithmetic complexity less than 6N. Proof length less than N/3 field elements. Round complexity at most log(N)/2.
2. Verifier: Query complexity 2*log(N). Decision arithmetic complexity at most 21*log(N). 
3. Soundness: Exists \delta_0 >= (1-3\rho)/4 - 1/\sqrt{N} st. \forall f \delta-far from C is rejected with probability >= min{\delta, \delta_0} - 3N/|F|. 
4. Parallelization: Each prover message can be computed in time O(1) on PRAM with CREW, with each arithmetic operation taking unit time. 

Remark 1.4
A multiplicative group H\in F_q is 'smooth' if |H| is power of 2. Family of smooth RS codes is when S = H. Theorem 1.3 with smooth RS codes has smaller constants.
Theorem 1.3 for generalized rate has larger constants.
Also can be generalize to groups of order c^k for constants c and k. 

Soundness for Theorem 1.3 is nearly tight for \delta <= \delta_0. 

Conjecture 1.5
Soundness limit \delta_0 of Theorem 1.3 approaches 1-\rho. Forall \delta <= 1-\rho rejection probability for all f that is \delta-far with rate \rho is at least \delta - 2*log(N)/\sqrt{|F|}. 


1.2 Applications to transparent zero knowledge implementations



2 Overview of the FRI IOPP and its soundness

Overview of IOPP for smooth RS code.
Section 2.1 about completeness.
Section 2.2 about soundness.


2.1 FRI overview and similarity to the Fast Fourier Transform

Let w^(0) generate a smooth group of order N = 2^n denoted L^(0) in field F. 
Prover claims f^(0):L^(0)->F \in RS[F,L^(0),\rho]. That is, f^(0) is evaluation of poly P^(0)(X)\in F[X] with deg(P^(0)) < \rho*2^n, eg assume \rho = 2^{-R} for R\in\N. 
From IFFT we know f^(0) = P^(0) implies there exists polys P_0^(1),P_1^(1)\in F[Y] with degrees < \rho*2^n/2 st
\forall x\in L^(0), f^(0)(x) = P^(0)(x) = P_0^(1)(x^2) + x*P_1^(1)(x^2)
Let Q^(1)(X,Y) := P_0^(1)(Y) + X*P_1^(1)(Y), and q^(0)(X) := X^2.
Then P^(0)(X) = Q^(1)(X,q^(0)(X)) = Q^(1)(X,Y) mod (Y - q^(0)(X)) ?
deg_X(Q^(1)) < 2 and deg_Y(Q^(1)) < \rho*2^n/2.
x -> q^(0)(x) is 2-to-1 on L^(0). Output is multiplicative group of order 2^{n-1} denoted L^(1). 
For every x^(0)\in F and y\in L^(1), one can compute Q^(0)(x^(0),y) from two entries of f^(0) since deg_X(Q^(0)) < 2. These two entries are the roots of y - q^(0)(X). 
Verifier samples x^(0)\in F and requests f^(1):L^(1)->F st. that is oracle of Q^(0)(x^(0),Y) on L^(1).
There is a 3-query test for consistency of f^(0) and f^(1) called the 'round consistency test'.
1. Sample y \in L^(1) and compute roots of y^2 s_0,s_1\in L^(0). 
2. Query f^(0)(s_0), f^(0)(s_1), and f^(1)(y), denoting answers \alpha_0, \alpha_1, and \beta. 
3. Interpolate line p(x) passing through (s_0,\alpha_0) and (s_1,\alpha_1).
4. Check that p(x^(0)) = \beta.
(Would make sense to commit to element of f^(0) in pairs like (s_0,s_1) since they will be queried together). 

In first round verifier sends x^(0) and prover responds with f(1):L^(1)->F. Testing consistency of f^(0) and f^(1) requires 3 queries. So we reduced testing f^(0) on L^(0) to testing f^(1) on L^(1) and the rate \rho did not change. We repeat r = log(\rho*2^n) = n - R times. f^(r) is a constant, or a code evaluated on domain of size 2^R, so prover just sends constants.
This protocol has completeness 1. 

Differences between informal and actual protocol
F is finite and binary.
The evaluation domains are additive cosets. Think of these as affine shifts over vector space over F_2. 
q^(0)(X) = X^2 is the Frobenius automorphism so not 2-to-1. Use another poly q^(0) that is many-to-one on L^(0) such that L^(1) = {q^(0)(x) | x\in L^(0)} is also an additive coset with |L^(1)| << |L^(0)|. q^(0) will be an 'affine subspace polynomial' from the class of 'linearized polynomials'. 
q^(0) is of degree 4, not 2, so round count is halved and query complexity stays the same. 
All queries are performed after prover sends all messages f^(1),...,f^(r). The COMMIT phase takes r rounds. In the i'th round, verifier chooses x^(i) and prover sends f^(i). Then in the QUERY phase verifier does consistency test for all r rounds. Query on L^(i) tests both consistency with f^(i-1) and f^(i+1) to save queries and apparently increase soundness. I think this means there is only one choise to make in the QUERY phase and all other queries deterministically follow by squaring. Well I suppose thats the case when using X^2 but for higher degree more randomness may be introduced at each round.


2.2 Soundness analysis - overview

Every round, ie instance of proof composition, incurs two costs for verifier in addition to the randomness. One is query complexity for consistency checking. The other is reduction is distance, ie soundness. 
Assume f^(0) is delta^(0)-far from all codewords. Then we need to prove f^(1) is \delta^(1)(\delta^(0))-far from all codewords. Larger \delta^(1) is better.
With high probability in FRI, \delta^(1) >= (1 - o(1))\delta^(0), ie distance reduction is negligible. Previous constructions have a multiplicative loss, limiting the number of composition rounds to log(N) thus requiring a high degree poly like square-root degree instead of constant degree like X^2. 
Suppose \delta^(0) < (1-\rho)/2, which is the unique decoding radius (for some reason). Then whp sum of round consistency error and \delta^(1) is at least as large at \delta^(0), meaning \delta^(1) is \delta^(0) minus a small loss (the consistency error).  



3 FRI - detailed description and main properties







What if we require the univariate poly in a certain form, eg many factors, and then use this special form. 
Take advantage of how rational polys are maximally far from a poly. 
Eg for a poly commited in root form \prod_i (x - a_i), randomly split it into two parts, and ask for the eval of the original poly divided by one part, and compare the result to the other part. 

Encoding the poly in an arbitrary way is our unique benefit so try to take advantage of that.
So consider using another form of univariate functions, like Laurent polys. 


In an extension field of characteristic p we have (x + y)^p = x^p + y^p.
Of course we also have (xy)^p = x^py^p. 
p(x) = \sum_{i=0}^{n} x^{pi}
p(x + y) = \sum_{i=0}^{n} (x + y)^{pi} = \sum_{i=0}^{n} (x^p + y^p)^i != p(x) + p(y)
so this won't generalize to large univariate polys.
So we can only have linearized polys with as many terms as the extension degree. 
But suppose we try multivariate polys.
p(x,y)^2 = (a + bx + cy + dxy)^2 = a^2 + b^2x^2 + c^2y^2 + d^2x^2y^2 = p'(x^2,y^2)
p(x)^2 = (a + bx + cx^2 + dx^3)^2 = a^2 + b^2x^2 + c^2x^4 + d^2x^6 = p'(x^2)
where p' is p with coefficients squared.
Let p(x,y) = a + bx^2 + cy^2 + dx^2y^2
p(x+x',y+y') = p(x,y) + p(x',y')
This means we can obtain a claim at a sum of points by taking the sums of claims. 
This is the same as what I was thinking about where we do a small univariate poly but replace coefs with claims of other small polys, etc, branching out. 
So I'm thinking this might be an alternative to reducing two points to one on the same poly. But we just take random linear combinations because multiplication in extensions is not repeated additions. Thus we can only make randomized sums. 

But also apparently a linearized poly can be constructed by the zero poly on a subspace. But these polys are not user defined, so seemingly unhelpful. 

What if we did analsis with respect to high degree polys. Then for the consistency test we use two domains F and L. Commit happens on L, so consistency soundness is |L|/|F| if we sample on F, but we didn't commit on F. The solution I'm hoping for is interpolating on values on F by checking multiple values on L. I don't think we can do this. 


For univariate sumcheck to reduce multi to uni eval, use domain (which is subspace anyway I think) that forms a tree of roots so we can plug into the points poly to select monomials without the need to expand that poly then re-contract it. This will mean as high degree as the data poly but multiplying only doubles degree, so they can be split into two if we'd like to maintain degree. 
Actually I think we can do multivariate sumcheck, but for prover univariate sumcheck may be quicker. 

An alternative to the lattice scheme I have is to do a hash tree using homomorhic hashes, eg lattices, then operming the homomorphic operations upon opening. This effectively enables opening many trees by opening a single one. Maybe another source of randomness could be reversing a trees evaluations. In fact, maybe many more manipulations are possible. All are possible because we still end up walking down a tree, but by homomorphism each step that would be necessary on many trees can be reduced to one tree. 
Maybe we could use this method to instead efficiently open a single tree in many places at once. Really you'd end up opening a random sum of leaves.
We'd still store multiple values in each leaf.

Note that hash trees only involve the entropy at the base and all else is derived on top. And note that a hash tree is only of log depth. Thus it can be calculated in a proof with only the entopy of the leaves. 

note that every pair in tree i+1 will be mapped to from two pairs in tree i. therefore it comes at no extra query cost to check both of these sources. this can be generalized. so it makes sense to structure tree i appropriately. but note that of course this deterministically-dependent query has not randomness. 

we will try to simply and be consistent with notation by using only 'polys' instead of 'polys', 'codes', and 'extensions', as well as all their side-notions like 'interpolents'. 

think about decomposing a poly with model where coefs are linear polys. 

maybe we can get more power by making the new poly requested be a function of the queries we see. currently each new poly is decided with just a random element irrelvant of the semantics of the last prover message.


i'm wondering now if the homo hash openings will be possible with only doing one opening. when we open we want to make sure that if prover has opening for the sum, then prover has openings for all parts of the sum. if we use an extractor, then given a prover that knows openings for the whole with non-negligible probability, we must also have it find openings for each part with non-negligible probability. I'm afraid that like the heavy argument this require many tests. 
note this is homo approach is only real contribution to univariate stuff and the only motivation to understand it. 

if the homo won't work then the only thing we can do with univariate stuff is give a reduction from multivariate and delegate the rest to other research and saying it may be used when wanted. we will only know which is better between lattice and univariates in terms of prover time and proof size by implementing.
if homo requires many tests then that destroys the benefit for the most part unless we can reduce the number of those tests enough which is already a problem we'd like to solve for the lattice setting. 
if it turns out homo does not require many tests then we still have the question of whether proof size will be sufficiently shortened. the worry is that the verifier still has to hold a large number of trees before the random selection can begin. but whereas with a regular hash tree each path must be traversed separately, with homo all selected paths can be traversed at once. 

the problem for homo is a bit different than for lattices. for lattice we want to ensure preimages for all targets so we randomly select subset sums to test. for homo we want to ensure preimages only for a randomly selected subset. 
For lattices we also need a certain number of tests to satisfy zippel. we are considering a linear poly, ie of degree 1. So for a fixed subset, the soundness error with coefs in {-1,0,1} is 1/3. But this randomization does not count the randomization of selecting the subset in the first place. 

I think there may be a problem with homo in that you may only be able to traverse the same route on all trees. this is ok if we are amortizing across independent trees, but subtrees of a single tree representing a single function are far from independent. therefore we would need to do this by splitting the poly into many and for each a different tree, then performing all queries that a tree of that size usually needs but doing so simultaneously across all trees. now we need to perform many queries as usual but they will all be of smaller size. this may make the need for multiple tests no longer a problem. each test could correspond to a query, and that query would be performed on a random selection of the trees. Would this querying satisfy the tests soundness? suppose there are as many queries as tests. think of each set of vertices across trees for each vertex position. each set would require many tests but the further down the set in tree depth, the less likely we are to touch it so the less tests it will get. this seems a problem.

I realized we can in fact traverse a single tree with many different routes simultaneously, but at some extra cost. Also, the question of multiple tests still remains. Actually this scheme I'm thinking of would save no space and require just as much data traversing each path separately, because randomizing the direction of descent involves accepting all children, then partitioning them by direction. 

try the heavy argument using a witness multiple times.


I think we can state and prover all necessary proof of knowledge only with respect to 'entropy dropping' mechanisms. this gives indication that this concept is special and deserves separate attention and analysis. our two entropy dropping mechanisms for now are homomorphic commits and hash trees for univariates. we need to prove that all proof of knowledge can indeed be reduced to proof of knowledge for entropy dropping. 
It would be great to also isolate zk proofs to a particular module. Maybe we could also reduce it to the entropy dropping phase (maybe I'll still call it 'molting'). 
Apart from entropy dropping, I think we'd call the other part 'arithmetization'. 
It'd be great to create a modular structure where all modules are proved and thus putting them together in any way yields a proof of the whole thing. 


can we make a relationship between ways of 0 testing. multivariate vs univariate. multivariate has log rounds but linear prover. univariate has one round by nlog(n) prover. maybe the relationship could be that multiplying the number of rounds by the prover complexity yields nlog(n) optimally. note that this distinction is important because we may often want to use univariate because extra prover time of univariate can be cheaper than the fiat shamirs for multivariate in many cases. 
we'd like to prove this relationship. also, we'd like to obtain a way to place ourselves anywhere on this spectrum. I think the latter can be done by some variation of multivariate sumcheck and univariate sumcheck. and I'd like to explore if theres any other way in the finite field setting to do this without sumchecks. 
actually we need to distinguish between vanishing vs summing. multivariate sumcheck is find for both. univariate sumcheck is for summing and I think only takes linear time (but for summing to 0). poly div is for univariate vanishing and takes quasilinear time. 
remember we can reduce vanishing to summing. We can also reduce from summing to vanishing by having constraints forming an addition tree. 

try composing univariate sumcheck, like with multiple rounds like the multivariate sumcheck.

for code-base could use 'indexing' like fractal where we use univariate polys for the code and thus these polys can be pre-evaluated into a tree, so verifier can obtain evaluation without need to tail-recurse. this may only be helpful sometimes. but hmm, this means committing over all of F, which is too much. DEEP-FRI allows out of domain sampling but this isn't determined during pre-processing. so we may need to evaluate only over the pre-commited subset of the field many times for enough soundness. 

still to figure out fully, but they can arithmetize a matrix, or really any data, so that verifier can figure out the function at a random point. 


use alternative basis for univariate commit. 
take advantage of how function with small range, eg predicate, can still encode a lot of information. also take advantage of how upon composition a small range is a small domain. 

I think we could outsource finite field multiplications, because they can be probabilistically checked by Zippel. But this is unlightly worth the bandwidth and the verification cost. 

maybe it would be worth trying other representations of fields. for example, I woudn't be surprised if other forms of polys, like multivariate polys of fixed degree could represent a field. 

maybe could say entropy dropping requires some homomorphism, eg for polys its f(x) + g(x) = (f+g)(x) which as a homo is
eval(f) + eval(g) = eval(f + g)

try using machine language as the native language for a snark. translate the crypto problem into that language. the constraint is we need a language with randomization and holographic properties. we tried the language of lattices and couldn't find holographic properties. 

consider alternative homomorphic commits, because anything homomorphic drops entropy.
On is something like H(S) = \prod_s h(s) or something similar with sum or any other commutative and associative operation. h is any hash. security then is not just reliant on h. suppose h is collisian resistant. must also be resilient to relations, such that one cannot find two subsets such that their hashes accumulate to the same value. suppose h was a truly random oracle. 

note that instead of vector commits I think we could also use homomorphic hash trees. but we may still have trouble with randomization. the benefit, however is that q maybe smaller and enable smaller proofs. maybe we just do the same testing before but with vector commits replaced by tree commits. so given a set of tree roots a single test entails taking a random sum (or linear combination with small coefs) of them to derive a new root. This could be it, just like with vectors, and opening would require opening all branches of each tree. But maybe we could take advantage of the tree structure. For each new root of each test, prover could submit children. I think all these child trees would have to be re-normalized. But then with now two children for every root, random combinations could be taken again. Repeat a lot number of times to finish. So it seems this could be an alternative to basis-changing, each with own benefits and drawbacks. 


secret cryptography seems the dual of hash based, like SIS is dual of LWE. with secret crypto one starts with a small secret and computes a larger output (maybe composed of multiple parts) from it that are public. with hash crypto, one starts with a large input and computes a small output. with secret crypto the task is to compute the small input part given access to the large output part. with hash crypto, the task is to compute a (there are many possibilities due to the assymetry) large input part given access to the small output part.
like for hash inversion, for secret crypto there are many ways to map a small input to a larger one. this is where the randomization of encryption comes in. 
a mapping (or rather a relation) cannot serve both purposes at the same time, because each requires efficiency in one direction and hardness in the opposite.
note how we could compose such systems. one direction gives a mapping from large to large first by contraction then by expansion. the other direction gives a mapping from small to small first by expansion then by contraction. its interesting to consider the hardness of these composed relations. 
I suppose we should restrict to binary relations that are partitions, ie equivalence relations among the larger set according to what they map to in the smaller set. this is because we have functions both ways. Hashing as we consider it is deterministic so a large will only map to a single small. Secrecy as we consider it is like encryption where a large maps to only one small. Note these two constraints are identical.
suppose we consider category of sets with such binary relations. for morphism composition to hold, a set must be partitioned the same as a source and target.

if we'd only like to rely on ad hoc crypto functions with no structure, I think we need to consider more structures with homomorphic and also holographic properties. Both of these seem hard to establish, especially simultaneously. 

note how differentiating a function reduces degree and sheds one coeficient, ie drops entropy. integrating requires specifying a new coefficient, increasing entropy.

Other than the question of entropy-dropping, I think its interesting to consider reducing sequencial computation to parallel computation. testing properties and entropy-dropping usually happens in parallel, whereas the computation we want to reduce from is sequential. it should be interesting to compare trading entropy for parallel computation. the simplest formula that comes to mind is increasing entropy by a factor of n enables cutting sequential length by a factor of n. 
this goes along with the question of benefits of reversible computation. a simple way to use reversible computation is take the trace, fold it in half, then proceed from both ends ending at the middle where that is the entropy at the top. I don't know how to extend this to more than halve the trace length. 


what about lambda calculus where each bounded variable must be used. maybe say each must be used exactly once. then recursion is not possible I think. the goal is introducing enough structure like a group where things don't just disappear, ie no absorption. I'm sure this limits the power of the language. Further, exactly one use of each variable bounds the size of a term with a certain number of variables so maybe we could think of the space of terms with no more than n variables. 
\x.x
\x.(x \y.y)
\x.(\y.(x y))
\x.(\y.(y x))
but composing two, for examle the latter with itself, increases from 3 to 4 variables. 
\x.(\y.(y x)) \x.(\y.(y x)) = \y.(y \x.(\z.(z x)))
can we expand it further? I don't see how.
In fact, it seems the above terms all together are closed under composition. But its not a group because there is no identity. This means it may be interesting. \x.x is a left identity but only a right identity for the first two. Here's the full table. Entry (i,j) is (i j) that is j applied to i.
__1_2_3_4_5_
1|1|2|3|4|5|
2|1|1|1|2|4|
3|1|2|3|4|5|
4|2|*|*|5|*|
5|4|2|4|5|5|
------------
1 (lambda x.x)
2 (lambda x.(x (lambda y.y)))
3 (lambda x.(lambda y.(x y)))
4 (lambda x.(lambda y.(y x)))
5 (lambda x.(x (lambda y.(lambda z.(z y)))))
Actually I was wrong, they are not closed, and the entries with the star are outside this set. But it seems in all cases its invariant that each variable is used once. And the number of variables of terms with n and m variables is at most n+m-1. Notice is only 4 as the function that expands, just waiting for an input to serve as the next function.
(4 2) is \x.(x \y.(y \z.z))
(4 3) is \x.(\y.(\z.(y z)))
(4 5) is \w.(w \x.(x \y.(\z.(z y))))
Notice that the latter is the first with 4 variables and arises in the same way that 5, the first one with 3 variables, did. 
Of course these operations are not commutative. And the example below, it is also not associative. 
((2 3) 4) = (1 4) = 4
(2 (3 4)) = (2 4) = 2
Counting reptitions in the table we get:
Count of 1's is 5.
Count of 2's is 5.
Count of 3's is 2.
Count of 4's is 5.
Count of 5's is 5.
So maybe there is enough variance for randomization.
We'd like to reduce evaluation at multiple points to evaluation at one point. The standard method involves plugging in an interpolation function of the same variable for all variables, thus breaking the property of only having one instance of each variable. Hmm, but this may not be a problem because we know feeding one of our terms to itself results in another term. Given the below it appears that even when we plug in all for one variable we may not be able to reduce until we know that variable. Thus the standard technique wouldn't work.
((x a) b)
((x c) d)
((x e) f)
\y.(y \x.(\z.(z x)))
= ((x a) b) \x.(\z.(z x)))
= ...
But notice that we only need one variable to keep an arbitrary amount of info, in the form ((((x a) b) c) ...). What if functions were always (supposed) to be of this form. But still, how do we go about reducing evaluation. We still have problem of irreducibility. Like polys being highly parallel, maybe we can choose a special format for our encoding also parallel allowing reducibility. For example, a tree form like (((x a) (x b)) ((x c) (x d))). Forget about interpolation and how prover chooses these constants, focusing on reducibility. When we plug in an interpolation poly for x we still won't be able to reduce until we know the value of x. So this won't work either. Maybe we should treat the variable as the argument, allowing reducibility, disregarding the further complication of prover interpolation, thus (((a x) (b x)) ((c x) (d x))) or (a (b (c x))) though the latter may not retain all the info because its partly reducible before knowing x. Well actually it seems out disassociativity and uncommutativity leave us a contradiction in our goals. We want the function with unknowns to be irreducible, but then we plugging in an interpolation poly that still contains unknowns we want it to become reducible. If we can't use the techniques of low-degree polys, how about those for high-degree univariate polys. Like a poly, we can break evaluation down into evaluating two parts. But can we then do the the equivalent of a random linear combination to reduce those two back to one? What means to we have to randomization anyway? Randomization requires changing part of the structure while keep other parts invariant, which seems hard in a system with little invariance properties. 

Groups in general may be easier becuase we have associativity. Or maybe action on a group. To try standard reduction version, we need a function that upon plugging in an interpolation function can be reduced. One property we probably want that's probably not hard to achieve is upon plugging in for one variable and the reducing, we get smaller version of the same function pattern. Something else we want, probably worth thinking about now because its the problem that stopped us above, is randomly combining two into one. randomization seems possible simply by randomly selecting an element and hitting on the left or right. Now we'd like a function form where two can be combined to one of the same size. Actually I think we might have tried this before and failed. Indeed, this is even the problem we had with expander graphs. Lets look for patterns for which two can be reduced to one.
xax' is one as (xax')(xbx') = x(ab)x'
But thats about it for what I can think of.

Another option may be a lie algebra. This would be similar to a field except for multiplication. We'd probably have the equivalent of multilinear polys. We can subtract sides of an equation and from commutativity of addition and distributivity I think we can arrive at asking the probability a single poly evaluates to 0. I don't think there's a universal answer to this like for fields, and instead depends on the lie algebra. Oh, and since we need multivariates to encode function we can't use hash trees for commits but will need direct crypto. Maybe expander graphs, but even that is risky. 

gosh, I don't know, maybe I should just stick with finite fields and stop looking for alternatives. the goal for alternatives to get more examples so that standardization of abstract ideas has more than one example as evidence. Necessary properties we've been looking for so far are being holomorphic and homomorphic, that is respectively that two different instances examines at random places will be different with high probability, and given two instances they can be reduced to one. Many examples have one property or the other but not both. I think lambda calculus is holomorphic, groups are holomorphic, lattices are homomorphic, and lie algebras homomorphic. The homomorphic property is used to reduce the examination of two to one. Along with the homomorphic property I think we need sufficient randomization. I think both these concepts can be formalized in an abstract framework. 


note assuming one way permutation, provers chosen values can be transformed to random values at no cost to entropy. 

multivariate crypto for signatures looks interesting because it compresses input, and although not linear, it operates over the whole field and thus doesn't need normalization like lattices. it may be a good candidate for tree-based constructions.
since we operate in the whole field a source of randomization can be just multiplying each claim by a constant, corresponding multiplying all coefs by the constant.

question shift from prover having coefs and point chosen by verifier, to prover having point and coefs chosen by verifier. 

verifier chooses random point of evaluation of multivariate. prover passes values through hash to get coefs and computes eval and submits claim. Verifier can choose random line passing through evaluation point and ask for small univariate poly representing that eval. verifier checks uni as usual, chooses random point on line, computes new claim, then prover is left with new point to prove. this means prover must find a poly that evaluates at this new point to the new claim. this task requires the prover to solve a relation whence input coefs after transformations yield a poly with a fixed evaluation at this point. this is a relation-intractable problem which may be hard to prove. I'm wondering if we could reduce the relation intractability to that of the permutation function which could be arbitrary. Suppose yes. Then for multiple commits we can reduce their point to a common point. But we can't take a random linear combination and preserve knowledge, unless we use a homomorphic hash which is our original goal. 

I'm realizing the advantage of multivariate crypto over just a hash is that the function can inverted given secret info. Thus unless we can operate in a secret setting this doesn't seem of use. but this doesn't throw out the idea above which is independent of multivariate crypto, and doesn't even assume its hard to find input given output. 

Assuming the function is hard to invert, we have a hash with the structure of a multivariate poly. can we use this structure? we know how to use it to probabilistically with ineraction reduce evaluation from two points to one, but this is just as costly as evaluating the two points separately.

Return to where the prover has the point of eval. Using implicit change of coefs, verifier can randomly change point of evaluation which should result in the same claim. This can be combined with multiplying the claim by a random constant. The point is we can achieve full randomization with one commit unlike lattices requiring many commits. 
But alas, we still can't find a way to reduce two to one. One attempt would be by means of first achieving a common point but we don't know how to do that. 




Maybe I should try other math objects only for the final test of oracle property testing. must reduce poly evaluation to testing another object. of course we need whatever the object is to be representable efficiently in hash trees, which basically just means I think that there is one query variable. So maybe its most helpful to map directly from univariate polys. I realize this is really the question of codes by way of focus on redundantly encoded data, but I don't understand what properties of codes we want, like is decodability good or bad for our purposes?

linear codes can be resenting by matrices, mapping small space to larger space. Can think of input vector as data and output vector as oracle to query. So for input u and output v and matrix H we want to verify that given v, there exists u such that v = Hu. Note that H is left-invertible, that is there is H' (not unique) such that H'H = I (but probably HH' != I). If there is satisfying u then we have
v = Hu
=> HH'v = HH'Hu
=> HH'v = Hu = v
Now if for all u we have
v != Hu
=> v != H(H'v)
Thus testing whether there exists u such that v = Hu, is equivalent to testing whether HH'v = v. Hmm, it doesn't seem like this is dependent on H', like H' need not be random with respect to v. Actually we can have a 'check' matrix M = (HH' - I) such that v is valid iff Mv = 0.

Note how the general idea of mapping from a small space to a large space is like the encryption kind of crypto, the dual of the commitment kind. 

it may be easier to work with groups than finite fields. for most simplicity we work with abelian groups. suppose the group is known. suppose prover data is a number of group elements. 

I think the framework in terms of the recursive linear codes we're interested in could be simplified in terms of homomorphisms, reflecting the linearity. Suppose we have a space G, and a homomorphism to a smaller space H. An element in G is valid  if it maps to the kernel of the homomorphism. Our goal is to reduce testing an element in G to testing an element in H. Suppose we are testing g in G. Suppose there are two element in H, h and h' such that g = h + x*h'. For now we are assuming these sets have two operations, but maybe later we restrict to one and have something like g = h + h'^x. For random r, prover commits now to h + r*h'. With homomorphism f, note f(h + r*h') = f(h) + r*f(h'). Now suppose there is a lemma that if either of two elements of H are far from the kernel then with high probability, a random combination of them is about the same distance. so if g is far from the kernel, then so is the supposed new commit. Now to test that the commit is as supposed, verifier does a consistency check between the oracle on G and the oracle on H. Suppose that by querying g in multiple places, verifier is able to query h and h' individually, thus computing the supposed h + r*h'. Verifier does it multiple times, then passing the result through the homomorphism, and checking that with the commit on H. Suppose that if two elements in H are different then they are different almost everywhere. So in total it appears we have 3 conditions for this to work. The first is the distance, the second is homomorphic property, and the third is the holomorphic property. But note that like in FRI, the holomorphic property may be conditioned on distance.

In FRI, G is the space of functions on a space S, a subset of a finite field. H is the space of such functions but on S^2, and |S^2| = |S|/2. Suppose M is the check matrix for the linear code. Then the homomorphism is multiplication by M. Decomposition is g(x) = h(x^2) + x*h'(x^2). For DEEP-FRI, the spaces are not deterministically related, and the poly goes through a small transformation before the homomorphism.

If we operate over a finite field, then we know that with probability 1/|F| over r, if Mv != 0 or Mv' != 0 then M(v + r*v') != 0. This is an absolute statement, however, and not in terms of hamming distance. Its the hamming distance that is hard to prove. Is there some way to commit and query such that we don't need hamming distance? This means a new approach to consistency testing. 
One possibility that still uses the tree format but tests absolutely rather than querying randomly is one that is probably inefficient in practice. We will use the 'isomorphism theorem'. The new commit is with respect to a poly of half the degree of the original, ie h(x) + r*h'(x). To test for absolute consistency we'd like to query the poly on every element in S^2 and from that compute g at that point and test for equality. Can we do this? Note g(x) = h(x^2) + x*h'(x^2). Querying the new poly we can get h(x^2) + r*h'(x^2), but we need h(x^2) and h'(x^2) separately. So alas, this only work when querying the new poly at r^2 thus computing g at r. 

Suppose we try for an abelian group. 




On possibility for performance, though it requires bandwidth, is a pool of provers using an iso to randomize their data for zk, not losing entropy, then sending the vectors (high bandwidth) to a super prover who joins them together and recursively reduces them, then outputs a proof that somehow can be combined with a proof of each verifier for a proof for that single verifier. 


I was thinking a way to drop entrop using the concatenation-homomorphic hash is by the many to one mapping of how the ordered (non-commutative) output could be split into an ordered list of ordered input strings in many different ways. Eg output abc could be made by either (ab)c or a(bc). I think this an associativity based analogue of the commutativity based scheme where {a,b,c} can come from either {a,b} + {c} or {a} + {b,c}. So we are effectively dropping the entropy of choices on where to cut the string. So the information is not encoded in the final string itself, or any of the characters of the input, but purely the cutting of the input. Note that the ordering of the input is not even relevant because that ordering is determined by the output. And note that characterizing in terms of commutativity and associativity is only partly true, because in neither case are we dropping the full entropy of commutativity or associativity, in particular only the entropy of how the inputs are combined, not any entropy within the inputs themselves. So in both cases the entropy used is the organization of data, but I'm not sure how to extract this entropy for use. This is because the entropy data is not explicitly given. So maybe this is unusable because we are operating in a scenario of verification where non-determinism is needed. If the data was given in whole to the verifier rather than via commit, then the verifier combined them as output, the input to the verifier would clearly be of higher entropy than output and verifier would have an easy way to extract the entropy. But in our case the verifier is not given the data explicity but rather given commits to the data, and it would require non-deterministc extra data by the prover to specify how to open the commits, exactly cancelling any benefit. So to take advantage of this entropy we'd need a verifying strategy where the data can be presented in any form (for associativity in particular the input can be presented in any sliced form) without requiring non-deterministic help from the prover to interpret it. Note that the data itself need not contain entropy, and could even be a fixed block of data. 
Let us focus on the associative case. In this case its the length of each slice that matters, not the contents of the slice. Somehow the verifier must infer the length of each slice. Variable slice count gives more entropy, but for simplicity we could first focus on a flixed slice count. Actually in the case of expander graphs I'm thinking this won't work at all because a commit size is log size the number of vertices in the graph, but what we are commiting to is a natural number of the length of a walk, which assuming no loops is upper bounded by the number of vertices, thus the commited value size is upper bounded by the commit size. Actually it couldn't work for any hash function for the same reason. The commit size must be at least the security parameter, and for the committed data to be at least as large as the commit size, and using natural numbers, the range of numbers must go up to exponential in the security parameter, which of course is too much to commit to. The commutativity case would have for potential but we don't know any post quantum assumptions for that.

It seems the above won't work for associativity with expander graphs. It could work with other hardness assumptions like commutativity for discrete log but I want established post-quantum assumptions, so for expander graphs I need to try another way to use its structure. 
Maybe we could use expanders for oracle commits, where the thing committed to is the random walk taken, and it must be checked for certain redundancy. But actually this is an instance of using crypto properties, not just algebraic properties.


One kind of code that comes to mind is the coefs of the poly taken by exponentiating the source poly by n. With N coefs, the rate is N/(n*N) = 1/n. There could be many variations of this. So the code is the coefs of a poly. Reducing this recursively would seem most naturally done by testing that a poly is small product of a source poly, then testing the source poly etc. Thus the problem reduces to small products. Consider quadratics. 
(a + bx + cx^2)(a + bx + cx^2) = (aa) + (ab + ba)x + (ac + bb + ca)x^2 + (bc + cb)x^3 + (cc)x^4. 
Suppose you are given full access to the source poly. Then you could quickly calculate any coef of the target poly and check it again the commited target poly. Note we're not really using polys, just binomial expansion.  


For oracle proofs I think we should stick with objects that are functions over a reasonable size domain, such that a single query corresponds to checking a single evaluation. This is because I don't know any more efficient way to commit for queries other than hash trees. Eg if we used graphs, a query would probably mean traversing at least one edge, and there's no clear way to commit to a graph such that each query can be efficiently opened. Well some representations, like adjacency matrix, but that only supports one kind of efficient query.


The lemma about hamming distance is easily solved when considering hamming distance in 'norm', ie distance from zero. Since linear codes allow us to work in a normed, rather than just a metric space. Note this isn't an official norm since we don't have notion of signs and order. Use |v| for norm. Suppose |v| = d and |v'| = d'. What is expected norm of f(x) = v+x*v' for scalar x. Well if a pair of entries of v and v' are both zero then they remain zero. If a pair has one non-zero and the other zero, then as long as x is non-zero, the result will be non-zero. If a pair has both non-zero then for random x the probably the result is zero is 1/|F|. Thus with high probability the size of the union of indices with non-zero entries. 
Now instead of considering the hamming distance of a vector from 0, we'd like to consider its distance from the code. Given a vector v, let c be (one of) its closest words. Suppose we have a homomorphism H with the code as the kernel. We would like to reason that if v is delta far from closest word c and v' is delta' far from closest word v', then H(v + r*v') = H(v) + r*H(v') will be far from 0 because H(v) will be at least delta far from 0 and H(v') will be at least delta' far from zero. To use this reasoning, we need the property that if v is delta far from the code, then H(v) is at least delta far from 0. 
So what linear maps H have this property? Let us express the property mathematically. Let c be the closest codeword to v.
We want: |H(v)| >= |v - c|
For dot product '.', what can we say about |v.u| in terms of v and u? Well first, what can we say about the norm of the point-wise product of v and u? An entry is non-zero iff both u and v have non-zero entries. The rest gets more complicated depending on the relationship of u and v. 
Can we characterize c in terms of v and H? Consider the generator matrix G that given a source s maps to codeword v, ie Gs = v. G is left-invertible, that is there is G' such that G'G = I. How does G relate to H? I think H = (GG'-I) such that Hv = 0 iff (GG' - I)v = 0 <=> GG'v = v. Remember there are many possibilities for G', so H is not unique. Suppose we have a G' such that G'v = G'c. Then GG'v = GG'c = c and thus we characterize c, the closest codeword to v. In this case our desired inequality would become |GG'v - v| >= |v - GG'v| which is actually an equality. So can we have a G' such that for all v and corresponding c, G'v = G'c? If not, can there be some probabilistic solution?
The equation G'v = G'c is an algebraic relationship between v and c which have a lexicographic (hamming distance) rather than algebraic relationship, because slightly changing lexicographic property may radically change algebraic property. Well actually since v - c is of small hamming distance we need a G' that can map every small weight vector to 0. This is not possible because suppose we have two vectors with hamming distance 1 that have the non-zero entries differing by one in the same index. Then it must map these two vectors to different values for that index, so both can't map to 0. One reason this doesn't work is because small weight is not closed under addition. 
Suppose for fixed v and thus c, there exists a G' of right dimension such that G'G = I and G'v = G'c. Then GG'v = GG'c = c. Thus we have |Hv| = |GG'v - v| = |c - v|. But note that H is dependent on v. 

Lets start again and be clear about our local goal. If v or u is far (from a and b respectively) then is v + r*u also far with high probability over r? In order to consider norm instead of distance, consider a homomorphism H with the code as kernel. There are many ways to pick H. Above, under an existence assumption, we can show that for an H depending on v, |Hv| = |c - v|. Then for closest codeword c to v + r*u, and H corresponding to v+r*u, we have 
|c - (v + r*u)| = |H(v + r*u)| by theorem 2
|H(v + r*u)| = |Hv + r*Hu| by homomorphism
|Hv + r*Hu| = func(|Hv|,|Hu|) by theorem 1
But we can't say much about |Hv| or |Hu| because H is in terms of v+r*u.
GG'(v + r*u) = c
Hv = GG'v - v = c - r*GG'u - v
Hu = GG'u - u = (c - GG'v)/r - u
idk how to use this.

Trying the other way with H's for v and u first, we have
GG1'v = a
GG2'u = b
GG'(v + r*u) = c
H1v + r*H2u = (GG1'v - v) + r*(GG2'u - u) = (GG1'v + r*GG2'u) - (v + r*u) = (a + r*b) - (v + r*u)
which gives us
func(|a - v|,|b - u|) = func(|H1v|,|H2u|) by theorem 2
func(|H1v|,|H2u|) = |H1v + r*H2u| by theorem 1
|H1v + r*H2u| = |(a + r*b) - (v + r*u)| by above
so we have related |a - v| and |b - u| to |(a - v) + r(b - u)| (probabilistically). 
We could get our desired final expression |c - (v + r*u)| if we can establish that c = a + r*b. It could be that a+r*b is much farther away from v+r*u than c is, in which case it wouldn't be surprising that |(a + r*b) - (v + r*u)| is large.
Suppose that a,b are unique.
What is closest codeword to v + r*u? I think it might not be c. Suppose on half the domain v differs from a and on the other half u differs from b. Then v+r*u could be different than a+r*b on the entire domain. Now this may be unrealistic, and a more realistic case has a smaller distance than 1/2. Suppose the minimum distance is d. Suppose v differs from a on d/2 of the domain and likewise for u and b but a different part of the domain. Then their combination I think would differ on at most d of the domain, which is more than d/2 so it could well be closer to another codeword than a+r*b.

Maybe we have enough freedom to construct an H that works for multiple points at the same time.
Suppose we got an H0 that works for both v and u. Then we could get farther then the second try above. We'd have
H0(v + r*u) = GG0'(v + r*u) - (v + r*u) = GG0'v + r*GG0'u - (v + r*u) = a + r*b - (v + r*u)
which is actually exactly as far as we got before, not farther.
So we'd need a G' such that G'G = I, G'v = G'a, G'u = G'b, and G'(v + r*u) = G'c.
Note that this means c = GG'(v + r*u) = GG'v + r*GG'u = a + r*b as we hoped for above.
So can we find a G' for 3 arbitrary points, and if not, can we take advantage of how c relates to v and u?
The freedom of G' depends on its dimension n and m where G is m by n, which forms the rate as n/m. The lower the rate the more freedom. Potentially this means we could do a longer linear combination, matching more than 3 points.
How do we match a point? Note that G has full column rank m. My naive assumption would be that for each of the n rows of G', n elements must be reserved for the inverse such that G'G = I. This leaves m-n elements free in each row. Suppose we want to match the point G'u = G'b => G'(u - b) = 0. Hmm, my naive assumption would say this only requires one more element for each row. In fact, since in all cases we multiply to the right of G', we could just consider the problem of solving for a G' such that for any target n by k matrix T we can solve for m by k matrix S such that G'S = T. In our case S will be of the form [G,v-a,u-b,v+r*u-c] which is of dimensions m by n+3 but we can consider for m by n+l for arbitrary l. My native assumption is that we could go up to n+l = m. Seems my assumptions are correct as long as the system is not inconsistent. We can view the equation G'S = T more generally as S^*G'^* = T^* where ^* is for transpose. To determine inconsistency we must compare the rank of S^* with that of [S^*|T^*]. The first n rows will be devoted to the inverse and shouldn't be a problem. The last l rows have unpredictable entries in S^*, but they will all have 0 entries in T^*, and thus should not be a problem. The only danger is the combination of the first n rows with the last l rows. The first n rows of T^* will all be 0 except for one which will be 1. So it is only this single line that could conflict with another. It is problematic if and only if this S^* row can be expressed as a linear combination of the other S^* rows. This can then be framed as another equation on whether Ax=b can be solved where b is the transpose of the special row of S^*, and the columns of A are the other rows of S^*. A is m by n+l-1 and b is m by 1. The system will be overdetermined with m > n+l-1 but a solution may still exist and this may be a problem. We may be able to solve this probatilistically because r is chosen randomly after v and u are committed. ... to be finished.
Note that finding G' does not enable efficient decoding, because it relies on knowing the closest codewords. 

Suppose we can finish the proof above and its valid. The theorem would then say something like for generators G of dimensions m by n with rank n, distance between elements is basically preserved under random combinations, modulo some details. How could we use this? We'd like to have a way to split a function (in the range) into (say two) parts, with the ability to query each part from an oracle to the original function. One way to approach this is to view the source as defining a function, and each row of G as an unfolded query. Maybe instead of formulating some explicit function for decomposition we could just think of matrices and matrix decomposition. I think we'd need the matrix to be structured, like a univariate poly matrix is. 
Alternatively, we don't need a structured matrix, and instead we start with multiple oracles of the decomposed parts, and we ask for a random linear combination, then do a consistency test. Actually all the oracles could be combined into a single tree, each leaf filled with multiple corresponding values. It is harder to argue that this reduces entropy. Clearly it can if theres enough decomposed parts to start and verifier fully opens the combined commit. Or rather the resulting poly is submitted to the verifier as plain data. But of course the verifier need not literally receive the poly, and could instead use a virtual poly and things can return to as normal with entropy dropped. So in this case we are not recursing directly, where the result is returned as an oracle which must then be decomposed somehow, eg by structure matrix queries. Instead, its returned as a virtual poly that can be decomposed by any proof means. What if it was returned as an oracle, but destructured in the same way as the original poly? Maybe this could be an alternative direct recursion technique than used by FRI. Direct recursion may have lower soundness than when using virtual polys, but that's yet to be analyzed.
Note that we don't need to use polys in the indirect recursion scheme and can instead use a random linear code. I think we could still reduce poly evaluation to general linear code evaluation, but I'll leave that for now. Can we take advantage of the computational infeasibility of decoding random linear codes? Such tasks are best suited for the secrecy setting. Well maybe it can contribute to soundness in that while prover won't be able to decode a random linear combination of noisy words, even knowing how to decode each of those words. it would be an interesting reduction to prove this. if this works it means the prover can't submit the closest poly because the prover can't find it, thus increasing the probability that whatever poly the prover submits will fail in the consistency test. LWE may be relevant. 

Maybe expander hashes, even with no homomorphic property, would still be useful for their algebraic computation over the field and also their flexibility in the length of input and consistency in length of output, and also by the simplicity of the structure of computation, probably just matrix multiplication over the field, such that verifying hashes in bulk could well be suited for delegation to a specialized proof. Note how the input is probably in large field elements whereas the lattice version has input in small field elements, and each is beneficial depending on the scenario of native circuits and what is implemented where. 

Searching, it appears multivariate crypto is best for signatures, maybe encryption, while isogeny crypto is best for key exchange, while code based crypto is best for signatures and encryption. But we are not concerned with secrecy. Thus none of these are best for our purposes of collision resistant hashing. Only expanders may be useful, which may involve isogenies, for friendly hash functions. 


Suppose given a witness we have a set of vectors to which it should be othogonal, forming a non-uniform program. Actually suppose we view this as taking the dot product of the witness scalars and a vector of polys. The resulting poly should be 0, and this can be tested by choosing a random point. Maybe the best poly is linear, ie a random linear combiation. Then we evaluate this poly at a bunch of random places to form a random matrix for the random linear code PCP protocol. 
Use LWE to reduce need for randomness. Oh, forgot LWE needs errors, so idk about using it.


For randdom codes, split each row into 2 (more generally k) parts, and think of each vector as the coefs with monomials of the same random multilinear poly (or small degree multivariate) but evaluated at 2 random points. suppose for now that concatenating these vectors will appear like a random vector. the coefs and the two eval points are kept secret. after commitment to a word, reveal secret points and go through interactive protocol to reduce those two points to the same for the two different polys (which have coefs the element wise product of the input and the secret coefs). Now upon having the same eval point, the monomials are the same, so we can think about adding the coefs. Actually we should think about absorbing the secret coefs into the input by element wise product, such that upon revealing the secret coefs of the two polys the prover can form a new input. now we add the two parts of the input together, or take a random linear combination, to get a new input.
After each round send a random scalar to prover and prover is to take this combination of the parts and now commit to that. 

what if instead of committing an input to a code matrix we just split the input into two and commit each separately, noting the total output size is the same if we keep the same rate. upon two commits, we then ask for a random linear combination as input, and do consistency checks by opening random entries of the two commits, computing the expected linear combination. this is so simple, would it work? the reasoning is that if either of the commits is not a codeword, then with high probability a random linear combination of them is also not a codeword. then we are just left to compute the probability a non-codeword agrees with a codeword, ie the error probability. 
To pass, the submitted codeword must agree with all initial testing, including those with errors which occur with probability distance. We fail if we sample at least one error, and all consistency tests pass (including agreement with the error). Note that if we sample an error, the codeword cannot be the combination of the commits without errors, or else the initial testing wouldn't pass. It must be another codeword, which may be more than min distance far from the combination of the commits. Now we are left to figure out how much the codword can agree with the combination, both on more error indices and non-error indices. 
Maybe we can reason that the prover knows the nearest codeword to the combination, ie the combination without errors. But from above this codeword won't agree whp with error samples in initial testing. therefore the prover must submit another codeword. if the prover submits a closest codeword to the known codeword, then prover has knowledge of min distance, which has hardness results. But to use this hardness we need a random matrix, and we still lack an arithmetization to reduce to it. 


for efficiently, optimize where prover may only do part of the merkel tree. after constructing, say half, prover goes through and completes proof and hopes that opening will occur on that side of the tree. if not, then it must construct the other half and begin again (though not constuct the first half again). but this may not be worth it because we perform multple checks so probability of starting over increases exponentially. 




I'm now wondering more about the idea of doing an IOP where the oracle is encoded as two instead of one. 
We go through the same interaction type as FRI, where prover sends oracle, verifier replies with randomness, and only at the end does the verifier look at the oracles, performing consistency tests. 
For any linear code, eg univariate poly, encode multiple of them in an oracle such that each leaf is its opening of all them at the same particular location. Then verifier sends random coefficients for linear combination. Prover sends new oracle that should be random linear combination of previous oracle parts, but splits this new oracle into parts as well. The process then continues. For a consistency check verifier chooses random location, opens first oracle, takes parts and computes previously chosen random linear combination. Verifier then opens second oracle parts at that same location and combines them to compute what the whole oracle would render at that location. The the process continues. 
For this to work for a code, we need a random linear combination of codewords to be a code, so presumably we use a linear code. Second, we need the ability to split a codeword or its source into multiple parts such that upon evaluating the parts the verifier can compute the whole. This works for univariates via the even-odd or offset technique. Third, we need the codes to be the same to the extent that upon opening the parts of the second on the domain defined the verifier can compute the evaluation of the whole along any point on the support of the previous oracle, making use of the linear combination coefficients chosen prior. This works for univariates because of the power structure and how cyclic subgroups work.
Now to what extent could a random code meet these conditions? A random linear code is of course linear, but it doesn't satisfy the second or third condition for the same reason that we don't know a reduction to such a code anyway, that is no structure to the matrix. 

Now does this relate to the FRI-style direct crypto? They seem to be opposites to an extent and I hope mine is more flexible. 






Any attempt to structure a random looking matrix would require a setup.
One attempt is using LWE for each entry with the same hidden vector s. To compensate for the errors, I think the underlying vector, and thus the errors would have to be revealed at the end. But this may be possible because all oracles can be submitted before any verification needs to happen. With the hidden vector revealed, the errors can be corrected. Or rather just reveal errors and subtract them. Disregarding errors now, what could we do with this structure?
Dot product of a row with the commit vector x would look like
x1(a1*s1 + ... + an*sn) + x2(b1*s1 + ... + bn*sn) + ...
= s1(a1*x1 + b1*x2 + ... ) + ... + sn(an*x1 + bn*x2 + ...) + ...
but this still has too much randomness for structure it looks like.

Instead of univariate commits we could use multivariate restricting each variable to a few options. 




what about multivariate crypto again, but for oracles not homomorphic commits. the source is an input points, the code is the polys, the output is the evaluations. 

actually a hard on average problem is given the polys finding an evaluation point to evaluate to zero. Thus this presents a hash function with good structure (though not homomorphic) but with limited compression. actually not necessarily because we don't have linearity, so finding a collision doesn't translate to finding 0. but nevertheless, such hashes exist and we could make use of them. 
I thought of this before but forgot why it doesn't work. try connecting two eval points with a line. there are two evaluation points the verifier doesn't know. prover returns poly with supposed evaluation of those points connected by a line. its a low degree poly. verifier then selects random element, plugs into univariate, and obtains new claim. however, the input for that claim is the unknown line evaluated at that randomly chosen element. Since the input points are the data we assume verifier has virtual access to them. Note that by a line we mean a linear poly of the form ax+b for each of the variables. In the last step the verifier must check that all these lines evaluated at the random t result in the new data. I don't think this is doable given the bijective translation reduction (BTR) theorem. What we've reduced to would easily suffice for a solution on its own. We're asking that given two virtual vectors [a] and [b] we choose random t, receive claimed result [c] and want to verify its correct. But doing so clearly violates BTR, ie is not a BT reduction.

how could we use this problem for a code? one possibility is as above, commitment via just evaluation, but unlike a hash in this case its expanding the source. Now this code is not linear since p(x) + p(y) != p(x + y) for non-linear polys. But suppose we try to connect x and y with a line l. the problem is the dimension of this line is the code size. 
we should instead think from the view of what verifier manipulation we can exploit. i see little opportunity except the lines method but the lines are too big for the verifier to touch. 
Surprised if it works because it throws away a round of interaction, but what if the verifier chooses a point on the implicit line and asks for a codeword at that point? Oh, but for constistency checks verifier must open previous oracle and compute expected eval at the point on the line, and this isn't doable due to nonlinearity. 
Asymptotically sending back the univariate is only as large as the number of polys. So suppose the prover commits to this instead of the original oracle. That is, instead of committing to two separate evaluations, prover commits to evaluation on a line such that at specified points, eg 0 and 1, the line yields data evaluation points. Then verifier chooses a random t, and requests the new evaluation (maybe folded in with another), or for now we can just assume prover provides new evaluation point in plain. For consistency check, verifier chooses random entries to open. For each, the opening is a univariate poly, and verifier plugs in the randomly chosen t, and compares the result with the evaluation at the prover provided evaluation point. Another random option is instead of commiting to each univariate, we join them together to form a single univariate and prover commits to that. then for consistency verifier must compute evaluate all polys (not just some for some openings) and from that compute the joined univariate. 
For security, we hope that if the initial commits is not an evaluation, ie not a valid codeword (actually family cuz of the t variable), then it will be sufficiently far from any valid codeword.
Note upon choosing t, the commitment really reduces to a single claimed codeword. So first we'd like to prove that if the commit is not a valid family of codewords, then upon a random t the collapsed commit won't be one whp. 
Second, we'd like to prove that its hard for the prover to find a source that evaluates to a word thats close in hamming distance to the collapsed word. Consider a single entry that agrees. Note that upon choosing t we pick a semi-random evaluation point. So for an entry to agree means that semi evaluation point and the prover's new eval point agree on the corresponding poly. 

Suppose for polys p_i you find two points, x and y that evaluate the same. Upon subtracting the polys, for a quandratic coef ci you have
ci(x1*x2 - y1*y2)
suppose you subtract and add points as so below. For the same coef ci you'd have
ci(x1 - y1)(x2 + y2) = ci(x1*x2 + (x1*y2 - y1*x2) - y1*y2) = ci(x1*y2 - y1*x2) + ci(x1*x2 - y1*y2)
So upon combining and plugging as above, all linear terms would vanish, and all quadratic terms would become like
ci(x1*y2 - y1*x2)
and they would sum to zero.
to really understand we just do an example with more than 2 variables.
b(x1*x2 - y1*y2)+ c(x1*x3 - y1*y3) + d(x2*x3 - y2*y3) + e(x1 - y1) + f(x2 - y2) + g(x3 - y3) + h
now suppose we use first variable (x1 + y1), second (x2 + y2), and third (x3 + y3). Then we have
b(x1 + y1)(x2 + y2) + c(x1 + y1)(x3 + y3) + d(x2 + y2)(x3 + y3) + e(x1 + y1) + f(x2 + y2) + g(x3 + y3) + h
= (b(x1*x2 + y1*y2) + c(x1*x3 + y1*y3) + d(x2*x2 + y2*y3) + e(x1 + y1) + f(x2 + y2) + g(x3 + y3) + h) 
+ (b(x1*y2 + y1*x2) + c(x1*y3 + y1*x3) + d(x2*y3 + y2*x3))
but the left over can't be split such that its the sume of two evaluations.


Suppose instead we just start with the assumption that one cannot find a source for a fixed target with respect to the random polys. however this reasonable assumption doesn't transfer to commits so easily because of nonlinearity. 
In our scheme above, the target is only semi-random with respect to the polys. 

the basic task for the prover is the following over a number of poly simultaneously. 
given a poly, output a univariate such that for a randomly chosen eval point, you can find an input to the poly that evaluates to the same as the univariate. For a single poly this seems simple. for any target (including some random evaluation of a univariate), it shouldn't be hard to find a proper input. A formula for completeness is the prover choosing some line and evaluates the poly and submits that as a univariate. then for any chosen t, the evaluation of the univariate target will match the evaluation of the poly on the line at that point. 
so of couse we generalize the above problem to multiple polys at once. the task is to submit univariates below a certain degree, eg the total degree of the poly. then for a random evaluation, one must then find an input that matches on all polys. the same formula applies for completeness.
Now I'd like to connect the hardness of this problem with that of matching a pre-chosen fixed target, eg 0. But our model is closer to that of a random target being picked for each poly, which is reasonably hard. The difference is we move towards less randomization. In fact, for n polys we move from choosen n random values (targets themselves) to choosing 1 random value to plug into the univariates. In order to connect these two models, maybe we consider submitting multivariates for increased randomness. This corresponds to a changed formula for completeness where parts of the line are with respect to different variables. note that which parts have which variables is not enforceable and prover is free to choose. all the verifier can enforce is the nature of the submitted polys, ie their variacy (degrees of randomness) and their degree. 

But the above task is only partially relevant and doesn't account for codes and errors. In this case prover must only find an input that matches on majority of polys. 

In terms of the relvance of decoding hardness, its not that the prover is ignorant of the errors and must decode, because the prover is the one who chooses the errors. Instead, the relevance is the hardness of finding a valid codeword sufficiently close to a semi-random non-codeword. 
In all cases, ie this protocol, the above one, and FRI, the protocol is the prover submits a function that should collapse to a codeword for all inputs. verifier chooses an input. prover submits codeword as close as possible to the randomly collapsed word. Now I think this is where analysis must branch. 
In one case the prover basically starts with a valid function but introduces some known errors, and upon randomization the provers intended function (without error) at corresponding evaluation is the closest codeword to the randomized word. Here I think we just analyze the distance and hardness is not relevant. 
In the second case the prover starts the same, but after randomization, the collapsed word is closer to another codeword than to the prover's intended word. Here we consider the hardness of the prover finding the closest codeword. Doing so means prover has knowledge of two closest codewords, ie min distance. 
In the third case the prover submits a function for which he doesn't have an intended family of words. This covers all other cases and is complex. I think here we assume, xor to the first and second scenarios, that the prover has no knowledge of closest codewords to (one or both of?) the commits. Then we prove that without this knowledge the prover can't have knowledge of the random linear combination of them without contradiction. Contrapositive is if prover has knowledge of closest codeword to random linear combination of two strings whp over than randomn combination, then prover should have knowledge of codewords closest to (one or both of?) the original strings. Then, assuming this holds, we reason that wihtout appropriate initial knowledge, prover must submit codeword farther than the closest, but this means distance analysis and not necessarily leads the prover to find min distance. Note how prover for this third case is really decoding, so maybe hardness of that is more useful than just finding min distance. 


I've already considered this but I'll try again. Suppose we have a permutation between prover data and poly coefs. for a commit we choose a random point and ask prover for evaluation on it. we could choose different points for different commits. we could reduce to a single point. when we have a single point for multiple commits we take a random linear combination of the claims. for completeness the prover sends that random linear combination of the polys. 
for security, suppose our randomization is a random oracle. upon submitting data the prover receives a random poly in return. ohhh, I remember the problem. its that the one wayness of the permutations, ie the unstructuredness of the oracle, gets in the way of the homomorphism. 
what if we use a homomorphich hash, eg SIS. then we must use a small random scalar, reducing randomness. 


What about the possibility we use rings instead of fields. The smoothness of the characteristic would affect soundness.
The necessary analysis is schwartz zippel for ring polys. 
In fact, I'm wondering if we can work with rings in general (multiplication maybe noncommutative). 
well I think neither of these will work. the reason is the prover has complete control over the poly begin root-tested because its the difference between the real poly commited and the virtual poly the prover is faking. So the poly can be strategically chosen. 
but maybe we can make a condition that the poly degree is sufficiently less than any zero divisor. 
oh, a limiting problem is that, supposing we do mod the product of two primes. Then many elements along the domain will be multiples of either of those primes, and so will monomial expressions of them, so when strategically multiplied by the other prime the product will go to zero. 
so i'm leaving this idea. 

however, one possibility for avoiding the flaw above, that is the user choosability of the poly, is to put the user's input through a random bijection. using this technique failed directly above because it didn't work with homomorphisms. for it to work in this case we need to end up evaluating the outputs. now note that its one-way, so even though its a bijection we can't take the output and invert to get the input as the real data. 
what if we wait until we get final claims for real data, then we randomly choose permutation, doesn't even need to be one way. then via BTR prover submits permuted data. verifier maps to previous data and evaluates, and is left to evaluated new data. the problem now is validity of mapping, ie of obeying the random perm, is dependent on the evaluation of the new data, but the soundness of the evaluation of the new data is dependent on the validity of the mapping, that is a circular dependence. usually this doesn't happen with BTR because the evaluation of the new data is not dependent on any computation on it. 
also, not to mention intermediate polys also would need the random mapping. 

i think its worth dropping the whole concept of not using fields. even if we chose strategically from a subset of the ring for evaluation points, it doesn't work eg for composition of polys cuz we're unsure of the output, like in the poly reduction algo. 

consider the multivariate in hash form, ie commiting a line to represent committing two polys. upon choosing a random t, you have chosen a random linear combination of the two plolys and that new poly is what the prover sends to open the commit. difference between hash version and oracle verion is hash version doesn't deal with errors and prover must solve satisfy all equations rather than a fraction. 
can this form be nested to allow for more compressed commits?

address the difficulty of not just finding a collision or false response, but one that will evaluate correctly for the random combination of claims. 

try other codes with better params

try randomizing the extension basis, ie the irreducible. maybe it can only be sufficiently randomized up to associates. 


maybe we can relate finding a codeword with small norm in a random code to finding short lattices. 



what about treating codeword polys as field elements, and multiplying them. since the oracle has all evaluations, only need to multiply entries. actually, would need q as well. 
p1(x)q1(x) = q(x)pi(x) + r(x)



instead of using subset of roots of unity for evaluation, use a whole field for evaluation but make the field small enough. 

instead of starting with two polys with their own distance start with one poly with a distance, and split into two, and that distance should hold whp for their random linear combination. 



try randomizing via the frobenius automorphism, ie f(x)^{p^i} = f(x^{p^i}) or the like
(a + bx + cx^2 + dx^3)^p = a^p + b^px^p + c^px^{2p} + d^px^{3p}
in fact, if we restrict the coefs to a subfield and choose i to fix that field, then given a claim on x we can obtain claims on its conjugates x^{p^i}. Note that only a log number of such conjugates exist in the field. 
actually maybe don't restrict coefs and thus poly gets slightly randomized.
also consider using the log \alpha^{p^i} values as a basis (if possible) for any point in F, so you query a subset of them and multiply by basis scalars to query at any point. 

note another source of randomization is adding a constant. in fact, to generalize this we add an entire random poly, the sparser the cheaper, but the less the randomization. In this case we address the question of the distance of
f(x) + r(x)
for a random r assuming a lower bound on the distance of f(x).
The cost of doing this comes from generating the randomness of r after commitment to f, because r can't be fixed. 
actually, quite surprisingly, this doesn't help and is equivalent to just adding r after taking the random linear combination. this means the two polys can still cancel errors and then the addition of r does nothing. 

consider a circular hash tree for shifting. there would be the same number of vertices for each step of the tree. This means the number of vertices/hash computations would go from 2l (for l leaves) to l*log(l). this is better than doing l different trees, which costs 2l^2
note the top would have to be hased like a tree to give something small enough to the verifier.
the benefit is the verifier can now easily choose any root along the cycle. 
. . . . . . . . 
 . . . . . . . .
. . . . . . . . 
 . . . . . . . .
. . . . . . . . 
but i realize this isn't the cyclic property I was looking for because what we're looking for is randomizing one hash with respect to another. the only way I see to do this is to commit to two hash trees separately, and randomly cycle one of them.
omg, actually this doesn't cost any more than combining the leaf pairs first. 
with combining the leaves first we have nl leaves so a total of 2(nl) hashes
with n separate trees there are l leaves in each tree so n(2l) hashes
so the hashing cost is no extra, but the verifier cost is twice as much. this may still be worth it for randomizing cyclic shift of one of them. 
as we considered before, we could also just do one tree and randomize the pairs once they are all opened but the reduces randomization and the math is more complex. 

how else can we randomize besides cycling?
ideally we could do all permutations and preserve the codeword.
one permutation that seems viable is flipping direction, ie suppose we evaluate on the cyclic group of order n generated by g. then we treat a query at index i as a query at index n-i. That is, f(g^i) -> f(g^{n-1-i}). Eg f(1) <-> f(n-1), f(n/2-1) <-> f(n/2).
any other permutations?
when a poly is even, ie f(g^i) = f(g^{n-1-i}) maybe we can flip each side because this keeps the 'continuity'.
f(g^i) -> f(g^{n/2-1-i}) for i from 0 to n/2-1
f(g^i) -> f(g^{n-i}) for i from n/2 to n-1
Note that even really means f is the same on an element and its inverse.
forget this, even if it works, its just another single transformation and can't be extended.


suppose for a moment we could reject distance delta with probability delta. how many tests would be required? 





the distance of a vector v is the same as its distance when scaled by non-zero r. 
|u - b| = |r*(u - b)| = |r*u - r*b|

suppose v and u are in unique decoding radius.
|v - a| < (n - k)/2
|r*u + r*b| = |u - b| < (n - k)/2
then with both we have
|v - a| + |r*u - r*b| < n - k
and by triangle inequality we have
|(v + r*u) - (a + r*b)| = |(v - a) + r*(u - b)| <= |v - a| + |r*u - r*b|
then putting the inequalities together we have
|(v + r*u) - (a + r*b)| < n - k < distance
so while (a + r*b) may not be closest to (v + r*u) in this case, they are within the min distance. 

suppose c != a + r*b
since both are words we have
|(a + r*b) - c| >= n - k + 1
from triangle inquality we can also say
|(v + r*u) - (a + r*b)| + |(a + r*b) - c| >= |(v + r*u) - c|
and putting this together with that from above gives
n - k > |(v + r*u) - (a + r*b)| >= |(v + r*u) - c| - |(a + r*b) - c| >= |(v + r*u) - c| - (n - k + 1)
|(v + r*u) - c| < 2(n - k) + 1
giving us a rather meaningless upper bound on the distance of the random combination.

try to take advantage of n << |F|. i think this comes when considering only x and the probability it cancels two errors. 

consider b' = (c - a)/r and a' = (c - r*b) which are both codewords. maybe consider
|(c - a)/r - b| and |(c - r*b) - a|
if either of these distances are zero, we have
c = a + r*b
and our previous analysis applies.
otherwise both distances are >= n - k + 1
also consider the distances
|(c - a)/r - u| and |(c - r*b) - v|

also consider u' = (c - v)/r and v' = (c - r*u)

in the end we want a lower bound on
|(v + r*u) - c|
= |(v + r*u) - z + z - c| >= |(v + r*u) - z| - |z - c|
so one option is we try to say
|(v + r*u) - c| >= |(a + r*b) - c| - |(a + r*b) - (v + r*u)|
now |(a + r*b) - c| >= n - k + 1
and |(a + r*b) - (v + r*u)| >= ~max(dv,du)
but one of these needs to change direction

what's the value of a',b',v',u'? 
|a' - a| = |c - (a + r*b)| >= n - k + 1
|v' - v| = |c - (v + r*u)|
|a' - v'| = |(c - r*b) - (c - r*u)| = |b - u|
|a - v|
|a' - v| = |(c - r*b) - v|
|a - v'| = |a - (c - r*u)|
|v' - v| = |(v' - a') + (a' - a) + (a - v)|
|(v' - a') + (a' - a) + (a - v)| >= |a' - a| - |(v' - a') + (a - v)|

|(v + r*u) - (a + r*b)| + |(a + r*b) - c| >= |(v + r*u) - c|
|(v + r*u) - c| + |c - (a + r*b)| >= |(v + r*u) - (a + r*b)|
|(a + r*b) - (v + r*u)| + |(v + r*u) - c| >= |(a + r*b) - c|
send |(a + r*b) - c| to opposite |(a + r*b) - (v + r*u)| extremes

basically we're only concerned with the case when c != a + r*b
I was trying to consider a' and b' as the should-be linear factors to result in c and then investigate their distances from v and u. But a' and b' don't function that way, ie a'+r*b' != c

I think the initial distance we need to consider about v and u is the |{i : vi != ai or ui != bi}|, that is the size of the union of indices where either v or u differ. this is of course well defined when both are in unique decoding radius.
when beyond unique decoding radius, each still has its distances from codewords, and for any possibilities for a and b, we can minimize over the expression above. 

consider v with a min distance, and arbitrary u. calculate number of r such that v + r*u decreases distance.
if u is a word, then the number of such r is 0. 
suppose we have such an r, and consider one word c of possibly many words close to resulting v + r*u.
let e = c - (v + r*u)

suppose v and u are such that both are far, but there exists r such that v + r*u = c.
suppose there were two such r.
v + r1*u = c1, v + r2*u = c2 => u = (c1 - c2)/(r1 - r2)
this implies u is a word (unless r1 = r2) which is a contradiction. therefore only one such r exists.
this shows that for resulting distance 0 there is at most 1 r.
now suppose we consider resulting distance 1.
then v + r*u + e1 = c where e has weight 1.
v + r1*u + e1 = c1, v + r2*u + e2 = c2 => (r1 - r2)u + (e1 - e2) = c1 - c2
=> u = ((c1 - c2) - (e1 - e2))/(r1 - r2)
now e1 - e2 has weight at most 2, so combining with codewords c1 and c2 and dividing by r1 and r2 suggests u has error 2. This is a contradiction, so there can still be at most 1 r.
Suppose we continue in this manner as long as the weight of e1 - e2 is strictly less than the distance of u. hmm, note how the distance of v is irrelevant. 
So suppose the distance of u is >= d. then we can give e weight < d/2. To state the proof again, suppose we have distinct r1 and r2 such that for words c1 and c2 we have
v + r1*u + e1 = c1, v + r2*u + e2 = c2
subtracting we get (r1 - r2)u + (e1 - e2) = (c1 - c2)
yielding u = (c1 - c2)/(r1 - r2) - (e1 - e2)/(r1 - r2)
now with r1 and r2 distinct we have that (c1 - c2)/(r1 - r2) is a word and (e1 - e2)/(r1 - r2) is of weight < d/2 + d/2 = d
thus u is < d errors to correct to far from the codeword (c1 - c2)/(r1 - r2) and thus u has distance < d. This is a contradiction, therefore the assumption must be false. That is, for fixed v and u there do not exist to distinct scalars that can take the linear combination of v and u to within said distance of a word. 
So to summarize if u has distance >= d, then there is at most one r such that v + r*u has distance < d/2.
Besides the distance dropping by 1/2, a limitation is that we assume the error is in u, when all error could be in v. 

suppose v and u have total distance >= d. 
v + rk*u + ek = ck
where ek has weight < d.
we'd like to count how many (rk,ek,ck) combinations there are with distict rk.

suppose we create G' to map ek values to 0, thus
GG'(v + rk*u + ek) = GG'ck
GG'(v + rk*u) = ck
so |GG'(v + rk*u) - (v + rk*u)| = |H(v + rk*u)| = |H(v) + rk*H(u)|
so for any set of ek for which a G' can be constructed that sends those ek to 0
this then induces a map on v and u and we have resulting distance the combination of weights H(v) + H(u)
in fact, why not choose G' to map v and u to other source to maximize final weights.
this seems too good, which means we may not be able to map all ek as said, meaning they must be linearly dependent. this means they must share indices.
its confusing how this depends on the distances of v and u as it should.
why not just set G' for a single ek and v and conclude distance?

I think the above confusion can be reduced to this possibility. 
set it on v + r*u, equivalent on e, the use the remaining degrees of freedom to alter G' on v and r*u that maximizes the union of their errors. note that the resulting distance should match the weight of e. may not be necessary to include e though, if we just focus on v + r*u.
we can try to achieve the 1.5 johnson bound. this means assuming the max distance of v and u is above that bound, there are only a few r for which the final distance cannot be sent up past this bound. 
I'm not yet sure how the randomness of r is relevant. 

Choose G' such that G'(v + r*u - c) = 0.
Then we have GG'(v + r*u) = GG'c = c
So |c - (v + r*u)|
= |GG'(v + r*u) - (v + r*u)|
= |(GG'v - v) + (GG'r*u - r*u)|

note how GG'v and GG'u are two words, so they must differ in at least n - k + 1 indices. 

u has distance > d
then so does r*u
then |GG'r*u - r*u| > d
now we have the freedom to map v to any source, but note that any choice will change where the errors occur for r*u.

by the reverse triangle inquality, if v has distance <= d' < d we map v to its closest codeword and thus have
|(GG'v - v) + (GG'r*u - r*u)| >= |(GG'r*u - r*u)| - |(GG'v - v)| > d - d'

the problem has reduced to how to maximize |(GG'v - v) + (GG'r*u - r*u)| given only partial freedom.

consider an index i where (GG'v)_i != v_i. For this error to be cancelled we need
(GG'v)_i - v_i + (GG'r*u)_i - r*u_i = 0
(GG'u)_i = v_i/r + u_i - (GG'v)_i/r
thus it constrains that index of GG'u
in general, given a G' we can find all errors of v and u with respect to GG'v and GG'u respectively. We can then reason what it means for two errors to cancel.

an important thing to note now is that we the equality (not inequality)
|c - (v + r*u)| = |(GG'v - v) + r*(GG'u - u)|
this implies the surprising fact that regardless how we choose G' (apart from its minimal conditions dependent on G and c), the right side will always have the same weight. 
this means instead of seartching for the maximum, we could search for the minimum weight or any weight that satisfies.
so we can phrase the problem as calculating the number of zeros in the vector
(GG'v - v) + r*(GG'u - u)
suppose we choose G' to map u to 0. then we're left with
GG'v - (v - r*u) 
so we're left to count the number of indices i for which
(GG'v)_i = v_i - r*u_i
which is the distance of v + r*u from the word GG'v but in this case GG'v = c so this is just the original question.

go back to the problem of calculating the weight of
(GG'v - v) + r*(GG'u - u)

suppose we set G' on u prior to choosing r, then set G' on v + r*u after choosing r.
this means r is random with respect to the vector (GG'u - u) because GG'u is determined prior r despite G' note being fully determined prior to r. 

i don't think we can go much further without more on the relationship of v and u.
think about the capacity to cancel
what does it mean when we constrain (GG'u)_i?
since GG'u is a word, constraining k indices determines the rest. same goes for GG'v
I think constraining on an index is GG'u is equivalent to constraining on one of G'u in that they restrict the same amount of freedom.
we then would like to move to saying something like correcting an error requires constraining GG'u which means constraining an index of G'u, of which there are only k, so we can only correct k errors.

we seem to forget that the choice of G' (apart from basic conditions) does not matter.
But G' on both v and u is subject to r. 
we'd like to limit the set of possible GG'u and GG'v independent of r. then for a fixed pair we know the errors
(GG'v - v) and (GG'u - u) and we consider the probability over r that errors get cancelled.

maybe we can restict r to a subset, and in exchange we can same more about GG'v and GG'u

maybe for a fixed r consider the freedom of G' for codewords GG'v and GG'u, and note that any choice should result in the same final distance. 
think about how that r must cancel errors for all choices of G'.

note how G' is chosen to map on c arbitrarily. instead of c we could pick any other c' and reason that
|c' - (v + r*u)| = |GG'(v + r*u) - (v + r*u)| = |(GG'v - v) + r*(GG'u - u)|
leaving us to figure out the weight of the resulting vector
thus we cannot reason as hoped without taking account of how G' relates to c and taking advantage of how c relates to v + r*u.
so i'm wondering if the idea of using G' is worth further pursuit.

somehow we must take advantage of c being (non-unique) closest to v + r*u. 

maybe consider the |F| function v + x*u and a corresponding set of closest words. 
there are a total of |F|^k words.
suppose we reason about changing v or u and how that changes the resulting sets.

i think a false idea I've held implicitly is that its quite possible for a vector v to be more than the distance of the code away from a word. but in fact given any v, just choose any k entries for an information set, and set the other n-k accordingly. thus any vector v is within distance at most n-k from a word. note n-k is one less than the code distance. 
thus distance 1 is non-sensical and the max distance a function can be is (n-k)/n = 1 - rho. so we should only consider the distance domain [0,1-rho].

maybe we can make the argument that there always exist a and b close to v and u such that a + r*b = c.
once c is determined find all those indices of v + r*u with no error. since the number of errors is at most n-k (since c is closest) there are at least k such indices. for each such index i we have v_i + r*u_i = c_i. consider these indices on v and u and use them as information sets to recover codewords called a and b. we now have a + r*b = c. Since a and b have at least k indices in common with v and u, they are within distance <= n-k.
now suppose c has distance <= d <= n-k from v + r*u. consider the n-d >= k indices with no errors, ie v_i + r*u_i = c_i. use those n-d >= k indices on v and u to recover words called a and b. then there at most d <= n-k indices that differ between a and v and b and u. thus given c with distance <= d, we can find a and b with distances <= d such that a + r*b = c.

can we translate this to imply that if a and b are unique, then c is unique? actually we have the converse. if c is unique, then can have unique a and b. but note that if a and b are unique, they are independent of r and thus c. but this still doesn't imply that a + r*b = c, because in some cases c may not be unique. 

can we conclude the seeming contrapositive that if v or u has distance > d, then v + r*u has distance > d. the contrapositive of this is if v + r*u has distance <= d then both v and u have distance <= d. to say that v + r*u has distance <= d means there exists c with distance <= d from v + r*u. we proceed as above and conclude there exists a and b with distances <= d from v and u. this means both v and u have distance <= d.
worrying that this doesn't depend on r. suppose we eliminate r, and split c aribtrarily into v and u such that v + u = c. here all indices have no error. ohh, the flaw we made is assuming that if > k indices have no error they must still all belong to a single word. 
but the first proof still holds, but that is trivial.



change notation here to match paper.
u' + x*u is the function. the word is v' + x*v.
domain is D. 
distance is d. 
epsilon is e.
code distance is lambda.

maybe we should try the contrapositive as they do in the paper. that is, if for noticeable x, u' + x*u is close to a word, then u' and u must be close to words.
the parameter we'd like to focus on is 'close', captured in distance <= d.

clearly with d=0 this holds, because if u' + x*u is a word for two x we have
(u' + r1*u) - (u' + r2*u) = (r1 - r2)*u
=> u is a word => u' is a word

try by induction.
by assumption at least a noticeable fraction of instances are <= d in distance.
we partition this noticeable set into those of distance <d and those of distance =d.
consider those for which the distance is exactly d.
suppose there was an index where all of them had an error. remove this index and consider the resulting v' + x*u' which is < d from words.
now all those within original distance <= d are within distance < d.
by assumption this set has noticeable size with respect to n, and thus with respect to n-1.
in this case v' and v are close to words by induction. 
now introduce that error index again, and v and u are now at most one error farther away from words. 

note that eliminating an index is valid because that just means ignoring an evaluation point, just changing the code slightly. for this code change to be appropriate we need the hypothesis to hold with respect to both codes.
this argument seems viable because it takes advantage of the base case, and because it will be comparing the number of instances (q) to the number of indices (n). but i don't know if it will take advantage of the singleton bound.

now the assumption that there is an index will all errors is wrong.
let O be the original set of instances with distance <= d. 
by assumption |O|/q >= b(n).
partition O into S and O\S, where S is those with distance exactly d.
now suppose we find l indices i_1,...,i_l such that removing them from u' and u results in v' and v of length n-l.
consider analogous sets O' and S'.
O' contains instances of v' + x*u' with distance < d.
now partition S' into T' and S'\T' such that T' contains instances with distance = d.
now O'\T' contains instances with distance < d.
suppose |O'\T'| = |O| - |T'|, ie removing indices has not left two instances identitcal.
|O'\T'|/q = |O'|/q - |T'|/q = |O|/q - |T'|/q
suppose |O|/q - |T'|/q >= b(n-l)
by induction, v' and u' have distances < d.
we now re-intoduce the l indices to re-create u' and u. 
since u' and u have distance < d, and we add l indices, v and u now have distance < d + l, <= d + l - 1.

now for the heavy argument
Consider the set O.
Imagine the matrix columns to be the errors of v + x*u wrt a closest codeword forall x in O.
Thus the matrix size is n by |O| >= bq.
Every column has d non-zero entries.
Thus the fraction of non-zero entries is d*|O|/n*|O| = d/n.
A row is heavy if it has a fraction of at least d/2n non-zero entries.
At least half of the entries are in heavy rows.
Thus there exists a row with (d/2n)*|O| non-zero entries.
this is clearly not enough.

if we are to account for not only large distances i think it is unrealistic to remove enough indices to catch an error in each instance.
but intuitively, all errors should occur in a few rows. 

hmm, what if we cycled the entries in columns so that we can line up in at least one row the max errors possible for a single row? that would trivially finish the above problem.
But is such cycling allowed?
For a column u' + r*u, consider rotating c_r the same way you rotate u' + r*u.
This way the distance of u' + r*u is unchanged by rotation.
We couldn't continue with induction because we've changed the structure now.
But could we do anything else?

we should still picture a matrix, each column an instance of (u' + x*u - c_x) where c_x is one of many possible closest words. thus there are many such possible matrices.
for now consider all instances, thus q columns.
number of rows is n.
a row is all zero when all instances agree with their word.
our assumption is that a noticeable fraction of these instances have distance <= d.
This means a noticeable fraction of columns have <= d non-zero entries.

I think the paper approaches by arguing there exists a set of 3 columns that share zeros on a large set of rows.
The size of this set is > n(1 - (n - k + 1)/n) = n - n + k - 1 = k - 1
From these columns v' and v can be obtained that are sufficiently close to u' and u.
They say the three points are collinear, and that the line they all pass through can be v' + x*v
Call the columns x1,x2,x3.
They say (x1, u' + x1*u), (x2, u' + x2*u), and (x3, u' + x3*u) are collinear.
Oh, well of course, they are all on the line (x, u' + x*u).
The suppose we throw out all their erroneous rows restricting to those where they all agree with their words. 
Restricting to these rows, they are idential to v' + x*v.
Since there are at least k such rows, these rows can induce the full v' and v to all other rows. 
Now that v' and v are determined we want to find their distance from u' and u.
Consider where u' + x*u = v' + x*v => (u' - v') + x*(u - v) = 0.
For each row where either u' differs with v' or u with v, we have a linear field equation. Thus in such a row at most one column (x value) satisfies (maybe no column).

the claim is that if >= p(n) of q(n) columns have distance <= d(n), then u' and u have distance <= d(n)
suppose we have q(n) total columns.
suppose we hypothesize that at least p(n) of them are of distance <= d.
we can show, and assume for now, that in the worst case all p(n) of these columns of have distance exactly d(n).
focusing on the matrix of p(n) columns, we have a total of d(n)*p(n) non-zero entries.
what is the min of the max, ie the minimum number of non-zero entries in the row with the most non-zero entries?
suppose the min-max is m.
then each row has no more than m entries.
so total number <= m*n.
but total number is d(n)*p(n), so d(n)*p(n) <= m*n
thus m >= d(n)*p(n)/n
thus we are above to reduce >= d(n)*p(n)/n of the p(n) columns to having d(n)-1 errors by only removing one index.
now with the index removed, we are left with a total of q(n) columns of size n-1.
Of these q(n) columns, >= d(n)*p(n)/n have distance <= d(n)-1.
to induce induction we need at least p(n-1) of q(n-1) instances to have distance <= d(n-1)
we have >= d*p(n)/n instances of distance <= d(n) - 1.
now with d sublinear, we have d(n) - 1 <= d(n-1)
thus we have >= d*p(n)/n instances of distance <= d(n-1)
we include these in our q(n-1) total, and exclude q(n) - q(n-1) of the others, including the rest.
thus this requires d(n)*p(n)/n >= p(n-1)
suppose we make d(n) sublinear, d(n) = n^{1/s}.
then we need p(n) >= n^{1 - 1/s}*p(n-1)
suppose this holds for small enough n, large enough p, and small enough s.
now since we have >= p(n-1) instances of distance <= d(n-1) we induce indution
thus v' and v have distance <= d(n-1).
re-introducing the index, u' and u have distance <= d(n-1) + 1.
this is a problem because d(n) <= d(n-1) + 1.
even if it worked out trying out parameters for p(n) >= n^{1 - 1/s}*p(n-1) shows unrealistic constants. 

i don't think our pigeonhole about non-zero entries is strong enough and we need to reason about the matrix structure and distance more. 
we were maximizing non-zero entries whereas they maximize zero entries. 
maybe the best we can do is understand their proof, state it in our own notation, and consider what it would take to improve it.


consider the version of assuming large distance on u' and having arbitrary u.
if distance of u' + x*u is to be is smaller than distance of u', x*u must correct errors.
show that no u exists that can cancel enough errors on enough x.

focus on the tight case where u' has distance d, and u' + x*u has distance < d.
maybe show that correcting an error for a value of x puts constraints on u. 
instead of contradiction by showing that u' has distance < d, try to show that for a certain number of good x-values u cannot be static.

iteratively pick 'good' x-values such that u'_i + x*u_i = (c_x)_i for >= n-d+1 indices i in I_x.
this way a good x-value leaves < d errors.

suppose we succeed with the method to show that for each x that u removes an error, it constrains u by one element.
then u is completely constrained after n elements.
thus at most n x-values can reduce error.
if we make the x-space large enough, this probability can be trivial.
we could generalize this where each x cancel multiple errors, such that the X space can be smaller, but distance dropped will be larger.

suppose n-d+1 >= k, ie d <= n-k+1
then restricting to the I_x indices we have u'|I_x + x*u|I_x = c_x|I_x
since |I_x| >= k we can induce v' and v such that v' + x*v = c_x
but note that not all >= k indices may be valid with respect to each other
thus there may be multiple distinct ways of inducing v' and v

now for a good x-value a, inducing v' and v, consider how v' + x*v compares to u' + x*u for other x-values
note that v + x*v is always a word
v'_i + x*v_i = u'_i + x*u_i
(v'_i - u'_i) + x*(v_i - u_i) = 0
this equation has at most one solution x
thus in each row, v' + x*v = u' + x*u in at most one column

consider two good x-values x1 and x2
consider the line function connecting
(x1,c_x1) and (x2,c_x2)
the line is 
c_x1 * (x - x2) / (x1 - x2) + c_x2 * (x - x1) / (x2 - x1)
call this line v' + x*v
that is, let
v' = (x2*c_x1 - x1*c_x2)/(x2 - x1)
v = (c_x2 - c_x1)/(x2 - x1)
a third point (x3,c_x3) lands on it if
c_x3 = v' + x3*v
we'd like to consider how may other good x-values lie on this line.
for any good x-value x3 that does
u' + x3*u has distance < d from v' + x3*v

consider additive inverse pairs of x-values (a,-a).
u' + a*u
u' - a*u


maybe the idea of cycling could help us with G'. There are n ways to cycle. We could map up to n-k of them to their closest codeword, assume each cycle of u is linearly independent of the rest and v. we still have the problem that G' is dependent on r and thus G'u is dependent on r.

maybe we could calculate the expected number of crossovers (errors in common) after a random cyclic shift. we do this in terms of param d for the distance of v and u, and the expected distance of the output. this wouldn't solve the fundamental problem, but be more of a preprocessing step to increase total distance.

maybe we can reason about list decoding by noting there are q^k codewords, and each is n-k+1 from another.
one can reach at most (n choose k) other words by this min distance. 

is it the case that if you're not within unique decoding radius you necessarily have at least two words at your closest distance? suppose not, that you can still have a unique closest word. consider this word c, and the > (n-k)/2 indices which must be changed to reach v. we'd like to find c' that is close to v. i don't know about this.
|c - c'| = |(c - v) + (v - c')| <= |c - v| + |v - c'|

note one way to take advantage is to limit evaluation to a subfield, and ask that coefs are in the subfield, and then choose x from a larger field. the only way to check is that when opening the commits the result is in the subfield. once multiplied by the larger x it won't be in the subfield anymore, so an isomorphism is needed. 
the subfield could be of minimum size n, and evaluation is on its n elements (including zero).x


since we are in working in the vector space (F_q)^n why don't we instead think in terms of isomorphic F_{q^n}.
so vector is viewed as the coefs of a field element.
a field element is a word if each coef is the eval of a single degree < k poly f at n points over F_q
we can suppose n = q = p^m
this means the field element has the form
y^{n-1}*f(w_{n-1}) + ... + y*f(w_1) + f(w_0)
= y^{n-1}*((w_{n-1})^{k-1}*f_{k-1} + ... + w_{n-1}*f_1 + f_0)
  + ... +
  y*((w_1)^{k-1}*f_{k-1} + ... + w_1*f_1 + f_0)
  +
  ((w_0)^{k-1}*f_{k-1} + ... + w_0*f_1 + f_0)
= f_{k-1}*(y^{n-1}*(w_{n-1})^{k-1} + ... + y*(w_1)^{k-1} + (w_0)^{k-1})
  + ... +
  f_1*(y^{n-1}*w_{n-1} + ... + y*w_1 + w_0)
  +
  f_0*(y^{n-1} + ... + y + 1)
instead of multiplying by an element of F_q we can multiply by a small degree element of F_{q^n}
does this preserve completeness?
no, unless we only multiply by elements of F_q, whereas we were hoping to multiply by low degree elements of F_{q^n}.
I was hoping to take advantage of doing such multiplication.
This is equivalent to the regular case but multiplying by a small degree poly instead of just a scalar.
There is a standard irreducible poly of degree n. Upon choosing the low degre poly r(x), the verifier asks for the reduced version of u'(x) + r(x)*u(x).
we'd like to say that if u'(x) or u(x) is not low degree, neither is u'(x) + r(x)*u(x).
this is not making sense because we don't know what high degree means here cuz its always restricted to n.

but actually we can reduce the commits to small field elements and thus have a big field size, to increase randomness of x. 

note H(u' + x*u + e_x) = H(c_x) implies
H(u') + x*H(u) + H(e_x) = 0
or do it with GG'
GG'u' + x*GG'u + GG'e_x = GG'c = c
we want to exploit the claim that H(u') has weight at least d, while e_x has weight < d.
how?

for two good x-values we have
u' + x1*u + e1 = c1
u' + x2*u + e2 = c2
u = (c1 - c2)/(x1 - x2) - (e1 - e2)/(x1 - x2)
|ei|, |ej| < d => |ei - ej| < 2d
thus |u - (c1 - c2)/(x1 - x2)| < 2d, that is u has distance < 2d

consider again the idea of correcting errors by putting constraints on u.
for a good x-value we have e with weight < d.
u' + x*u + e = c
we'd like to say that there can be at most n good x-values.
for each such x we have
u'|I_x + x*u|I_x = c|I_x, |I_x| > n - d


maybe consider the seemingly simpler question of the distance of
x*u' + u
the distance of x*u' is the d.
for a given x, and a chosen c_x, there is at least a distance >= d from x*u' to c_x.
u must account for at least 1 of these errors.

we can consider the set codewords C of distance d from u'. we can reason about the size of C with the Johnson bound.
We then say that C_x, the set of codewords of distance d from x*u' equals x*C.
in this case we are disregarding candidates that are of distance >= d from u' which my be a concern
in this model x*u' + u has distance < d from some element in x*C
we want to show this can occure for only a limited number of x given that u' has distance d from every element in C.

the distance between any c and c' is at least n-k+1.
|c - c'| <= |u' - c| + |u' - c'| <= 2d
thus 2d >= n-k+1, d >= (n-k+1)/2
this just means u' is outside unique decoding which is obvious by definition.
suppose |u' - c| and |u' - c'| share s non-zero indices.
then the distance from c to c' is |u' - c| + |u' - c'| - s.
thus n-k+1 <= |c - c'| = 2d - s
d >= (n - k + 1 - s)/2
idk wht to do with this

think about d=1.
then the error index i is unique and fixed.
thus c is unique
x1*u'_i + u_i = c1_i = x1*c_i
x2*u'_i + u_i = c2_i = x2*c_i
x1*(c_i - u'_i) = x2*(c_i - u'_i)
and u'_i != c_i because its the index of error
thus x1 = x2
so at most 1 x-value can correct this error.

i'm worried about the possibility that even when c is unique, one can use a different c'.
its distance from x*u' will be greater than that for c.
so u will have to correct more errors.
but maybe the errors are set such that it can do this without extra constraints.

maybe we can take the contrapositive and say:
we suppose d <= (n - k)/2
now if u' had distance < d then 
there exists c (closest word, which exists by above assumption) such that
x*u' + u + e_x = x*c
for all x, with e_x having weight < d.
The contrapositive is that still assuming d <= (n - k)/2
if there is no constant c such that
x*u' + u + e_x = x*c
for all x, with e_x having weight < d.
(ie there is an x for which e_x must have weight >= d)
then u' has distance >= d.
i don't know how this helps

maybe worth exploring reductions between the various problems we've considered
if u' + x*u has distance d from c_x
then x^{-1}*u' + u has distance d from x^{-1}*c_x
so for a fiex u' and u, can we say that
u' + x*u has the same distance with uniform x as x*u' + u as long as X includes multiplicative inverses
but this account only for max distance of u' and u, not their total distance

we can transform the original problem to
x*u + e_x in C - u'
C - u' is a set of vectors all of weight at least d.
find u such that for many x-values there exists e_x of weight < d to satisfy the equation for some element of C - u'.
suppose we generalize to consider C to be the whole code.
|(c1 - u') - (c2 - u')| = |c1 - c2| >= n-k+1
so all elements of C - u' have mutual distance >= n-k+1, and they all have weight >= d, and distance >= d from C.
those are the three relevant characterizationwe have about the set C - u' we want to match for inclusion.
think of the set C' = C - u' as an additive offset of C.
find a u such that for many x-values, x*u has distance < d from C'.
maybe better to frame the problem in the x*u' + u version.
u + e_x in C - x*u'
but with C the full code (or just closed under scalars in X), C = x*C, so C - x*u' = x*(C - u')
u + e_x in x*(C - u')
find a u such that for many x-values it has distance < d from x*(C - u') = x*C'
since u can be anything we want to view this problem as one of list decoding.
almost like we're viewing x*C' as a code where each x gives new codeword. 
this is the case because we're asking for how many codewords, x*C', have distance < d from u.
that is, how many codewords, x*C', can fit a ball of radius < d.
a codeword x*C' fits in a ball if any of its members fit in the ball.
so we could ask how many xi*(ci - u') for distict xi are accessible in distance < d.

|x1*(c1 - u') - x2*(c2 - u')| <= 2d-2
x1*(c1 - u') - x2*(c2 - u') = (x1*c1 - x2*c2) - (x1 - x2)*u' = (x1 - x2)*((x1*c1 - x2*c2)/(x1 - x2) - u')
thus subtracting two codewords in distance <= d-1 gives another codeword in distance <= 2(d-1), namely that wrt
x = (x1 - x2) and c = (x1*c1 - x2*c2)/(x1 - x2)
similarly, adding codewords within distance <= d-1 gives one within distance <= 2(d-1)
x1*(c1 - u') + x2*(c2 - u') = (x1*c1 + x2*c2) - (x1 + x2)*u' = (x1 + x2)*((x1*c1 + x2*c2)/(x1 + x2) - u')
namely with x = (x1 + x2) and c = (x1*c1 + x2*c2)/(x1 + x2)

suppose m instances fit in the ball.
can we show this implies the same list decoding capacity for C?
offsetting C by u' I think has the same capacity as C.
then multiplying by x!=0 I think still holds the capacity.
suppose diameter of ball is < n-k+1, so that only one instance of each c can be in the ball.


try induction on degree
don't mind base case for now
consider poly decomposition on u' and u
now consider the codes on u' and u.
suppose you take each code and interpolate it into its parts.
we'd like to show that if u' is far then, so are its parts, similarly for u
now we have an interpolated code of each part
by induction, the random combination of parts preserves distance.
suppose we then interpolate back from parts to whole.
we'd like to show that if the parts are far, so is the whole.
then we finally need to show that the resulting string would be the same as if we just computed the linear combination without first decomposing and later recomposing the parts. 
i'm not sure about this last condition.
clearly it holds for polys, but probably not for arbitrary strings. 

consider the squared-decomposition.
suppose we start with u' and decompose into two parts u1' and u2'
u1'(x) = (u'(x) + u'(-x))/2
u2'(x) = (u'(x) - u'(-x))/2x
similarly for u
u1(x) = (u(x) + u(-x))/2
u2(x) = (u(x) - u(-x))/2x
then we do the random linear combination for all sets of parts
u1'(x) + r*u1(x)
u2'(x) + r*u2(x)
with these as the new parts, we now recompose these parts
(u1'(x) + r*u1(x)) + x*(u2'(x) + r*u2(x))
= ((u'(x) + u'(-x))/2 + r*(u(x) + u(-x))/2)
+ x*( (u'(x) - u'(-x))/2x + r*(u(x) - u(-x))/2x)
= ((u'(x) + u'(-x))/2 + x*(u'(x) - u'(-x))/2x)
+ r*( (u(x) + u(-x))/2 + x*(u(x) - u(-x))/2x) )
= u'(x) + r*u(x)
so it seems to work, but is the above really for any function or does it rely on poly structure?
supposing all x are non-zero, the functions u1',u2',u1,u2 are all defined and from there its just plugging and reducing which holds for any function. so yay!

use the notation above to clarify reductions needed.
need to show that if u' is far, then u1' and u2' have total distance also far
similarly for u
then by induction we assume if u1' or u1 is far, so is u1'(x) + r*u1(x)
similarly for u2' and u2
then we need to show that if f(x) = u1'(x) + r*u1(x) or g(x) = u2'(x) + r*u2(x) are far, then so is
f(x) + x*g(x)

the reason we can use induction is that by definition
u1(-x) = (u(-x) + u(x))/2 = u1(x)
u2(-x) = (u(-x) - u(x))/2(-x) = (u(x) - u(-x))/2x = u2(x)
and similarly for u1' and u2'
thus they can be defined on domains of half the size with the same rate
so we only need to compute u1',u2',u1,u2 on half the original domain.
now this extends to f = u1'(x) + r*u1(x) and g = u2'(x) + r*u2(x)
so when we ask about f + x*g note that f and g are defined on a domain of half the size as our composition.

now how about the second reduction
if f or g is far, what does that say about f(x) + x*g(x)?
note how this is a very different question than the distance of f(x) + r*g(x)
its important to note the condition that f(-x) = f(x) and g(-x) = g(x)
suppose we are given a lower bound on the total distance of f and g.
what this means by definition is that we consider the minimum size of the union of indices such that a subset of those indices correct f and g.
suppose f(x) + x*g(x) is close.
suppose we interpolate to recover f and g.
(f(x) + x*g(x))/2 + (f(-x) - x*g(-x))/2 = (f(x) + f(-x))/2 + x*(g(x) - g(-x))/2 = f(x)
(f(x) + x*g(x))/2x - (f(-x) - x*g(-x))/2x = (f(x) - f(-x))/2 + x*(g(x) + g(-x))/2x = g(x)
now we're asking the question of whether f + x*g is close implies f and g are close.
the other reduction tries to show the equivalent that if f + x*g is far, then so are f and g.
thus the two reductions are the same relation but in different directions.
to summarize we to show that the relation f(x) + x*g(x) <-> (f(x),g(x)) preservers distance (from relevant codes) in some way.

we're looking for a one-to-one distance correspondence as stated above.
maybe for every c with distance d from h = f + x*g we can deduce corresponding cf and cg relative to f and g.
then with
f(x) = (h(x) + h(-x))/2
g(x) = (h(x) - h(-x))/2x
suppose we define
cf = (c(x) + c(-x))/2
cg = (c(x) - c(-x))/2x
then
cf(x) - f(x) = (c(x) + c(-x))/2 - (h(x) + h(-x))/2 = (c(x) - h(x))/2 + (c(-x) - h(-x))/2
cg(x) - g(x) = (c(x) - c(-x))/2x - (h(x) - h(-x))/2x = (c(x) - h(x))/2x - (c(-x) - h(-x))/2x
similarly suppose we're given f,cf,g,cg first
then with
h = f + x*g
suppose we define
c = cf + x*cg
then
c - h = (cf + x*cg) - (f + x*g) = (cf - f) + x*(cg - g)

lets look at the second, that is c - h
we want to know the weight of this vector
wherever cf - f XOR cg - g is non-zero, so is c - h
when both are non-zero, there is at most one x value such that c - h is zero.
without further analysis, it seems it could be that for all x for which cf - f and cg - g are non-zero, x is the root.
this leaves us with c - h having errors as the XOR of errors of cf - f and cg - g.
actually to detail further, the errors must have certain form in terms of c and h, namely
cf - f = (c(-x) - h(-x))/2
cg - g = - (c(-x) - h(-x))/2x
so in this case that h is not erroneous at x, though f and g are, we see that c(-x) - h(-x) != 0 thus h is erroneous at -x.
so f and g transfer shared errors to at least one of h(x) or h(-x), maybe both.

now for the first, that is cf - f and cg - g
first for cf - f
when c - h != 0, for error cancellation we need c(x) - h(x) = h(-x) - c(-x)
for cg - g
when c - h != 0, for error cancellation we need c(x) - h(x) = c(-x) - h(-x)
for both to hold means h(-x) - c(-x) = c(-x) - h(-x) => h(-x) = c(-x)
but this implies c(x) = h(x) contrary to assumption
without further analysis, it seems it could be that never do cf - f and cg - g share an error
this leaves us with cf - f and cg - g partitioning the errors of c - h.
to detail further, take the case that cg - g is zero.
then c(x) - h(x) = c(-x) - h(-x)
thus cf - f = c(x) - h(x)
similarly, when cf - f is zero.
then c(-x) - h(-x) = - (c(x) - h(x))
thus cg - g = (c(x) - h(x))/x
so this means that when they don't share an error, we precisely know the error of the erroneous side.

suppose we start with c and h, decompose and recompose.
when we decompose the errors are partitioned.
when we recompose when none are shared, we end up with errors exactly as we started.
suppose we go the other way, recomposing then decomposing.
when we recompose we take the XOR of the errors (not union as hoped).
when we decompose we partition the errors.
thus we end up with the distict errors to begin with but redistributed.
so it seems stronger to decompose then recompose than vice versa

suppose we decompose, then do a random combination, then recompose.
errors of u' are partitioned between u1' and u2', similarly for u.
then upon taking a random combination, we'd like to get that whp the results have large XOR errors.
then these errors transfer back upon composition. 

suppose we don't start with two (u' and u) but just with one (h)
we decompose into f and g, and the errors of h are partitioned.
we randomly combine f and g, and by induction we'd like to say that since their errors are partitioned, whp a random combination preserves the error number.
then we can invoke direct recursion on this new result
this is what FRI does.
... how? I don't see the opportunity for induction here.

compare total distance vs union distance
lets use total to mean counting duplicates
total distance >= union distance
equality holds when no errors are shared
total distance = union distance + shared error count
so if something has total distance d, then it has union distance <= d
if something has union distance d, then it has total distance >= d
thus for a fixed d, it is stronger to say that something has union distance d.

we still should more formally make the argument about distance preservation.
if h has distance d, select any closest c.
splitting h into f and g, it partitions its errors relative to cf and cg (defined by c).
suppose the error count in these respects is df and dg.
we'd like to contend that this is the smallest total error count of f and g.
for sake of contradiction, suppose there are alternatives cf' and cg' such that total error count (couting duplicates).
clearly adding duplicates makes things worse for this possibility because it adds an error for both f and g, then that those errors get XOR removed in composition.
thus for any smaller total error count errors should not be shared.
upon taking the XOR for composition, we end up with the same number of errors as the decomposition, which has reduced errors, thus we have reduced distance for h. this is a contradiction. 
conclusion we'd like to make is that if h has distance d, then f and g have union distance d.

try the other way
suppose f and g are in unique decoding radius, and share all the same errors.
upon XOR for composition, h may have distance 0 with respect to the composed c from cf and cg.
suppose so.
upon decomposition, f and g now have distance 0 from cf and cg. contradiction.
try to generalize.
suppose f and g have smallest distance total error counts df and dg. this may require sharing errors.
upon composition, suppose some of those shared errors vanish.
then upon decomposition, those errors don't re-appear, leaving a contradiction.
thus it must be the case that upon composition, shared errors persist.
then upon decomposition, that error is copied to both f and g.
the conclusion we'd like to make is that if f and g have union distance d, then h has distance d.


try the base case.
this is for polys of degree 0, that is constants.
evaluation such a poly at a set of n points just means the repetition code.
since k = 1, the rate = n.
a vector is a codeword iff all elements are the same. 
a vector has distance d when all but d elements are some constant.
we are concerned with the distance of random combinations.
consider two indices i,j of the result that are the same.
this means u'_i + x*u_i = u'_j + x*u_j
if u'_i != u'_j or u_i != u_j then there is at most one x that satisfies this constraint.
otherwise it is satisfied for all x

consider the special case n=2.
there are only 2 distances, 0 and 1.
if either has distance 1, what is the probability the result has distance 0?
for the distance to be zero, we need 
u'_1 + x*u_1 = u'_2 + x*u_2
and unless u'_1 = u'_2 and u_1 = u_2, this occurs with probability 1/|F|

more generally we need a way to frame the question in order of qualifications.
a seemingly complex way would be to do separate analysis for all pairs of input distances and calculate probability of distance reduction for each.
a seemingly better way would be to first qualify the distance of the target. then we'd need to qualify possible inputs and probabilities
the last straightforward way to is to first qualify the probability, then inputs and target.
one way I'm intersted in is to first focus on what we know.
for example, we know that for two input rows to have the same output, they must either be the same or else this only occurs for one scalar.
now let's try to scale this notion.
for example, suppose we we have 3 outputs the same. what can we conclude about the inputs? they are either all the same, or at least one is pairwise different in which case it occurs for at most one scalar.
more generally suppose m outputs are the same.
then we have m linear expressions.
intuitively they are all lines and there must exist an x at which point they all cross.
note no two lines except the same can be parallel.
maybe we could imagine plotting all the rows as lines across all x values.
at every point we can count the number of crossings. if t lines cross at an x-value that means all those rows output the same at that x-value. if at an x-value there are 0 crossings, then upon plugging in that x-value the target will have maximum distance n-1, ie all entries will be distinct. 
the maximum number of crossings indicates the minimum possible distance.
we are interested in the spectrum of cross counts, and the expected cross count for uniform x. 
note two lines that are not identical can cross at no more than one x value.
now remember, the distance assumption we can make is in regard to union distance. 
maybe we should condition on the number of distinct lines.


(
regarding multivariate hash, think about using any two of the adversaries satisfying input answers to create an input line, then plug in that input line to obtain output polys, and compare those with the submitted polys. 
)
 

we should return to making an inductive argument.
suppose we have f and g with union distance d
we split into f1,f2 and g1,g2
our goal is to say the distance f + r*g is d whp
f + r*g = (f1 + x*f2) + r*(g1 + x*g2) = (f1 + r*g1) + x*(f2 + r*g2)
we want the errors to "keep their place"

we should take advantage of how we can quantify where the errors are and what they are

we consider f relative to cf and g relative to cg
with union distance d, the error indices E has size d
let us partition E into F,G, and S for shared errors.
now let EF = F + S and EG = G + S be the errors of f and g
partition EF into F1,F2, and FS
partition EG into G1,G2, and GS

consider recursively splitting all the way down to k codes of length n/k. 
this is a theoretical interpolation of degree k.
in practice we'll do an interpolation of degree t < k, eg t = 2.
the final poly will be constructed of k/t parts, each composed of the same random combination of t parts.
show that after the random combination, the union distance of the k/t parts will the same as that of the original k parts.
this union distance is then preserved upon reconstruction.

what would this full decomposition look like?
a0 + a1*x + a2*x^2 + a3*x^3 + a4*x^4 + a5*x^6 + a6*x^6 + a7*x^7
= (a0 + a2*x^2 + a4*x^4 + a6*x^6) + x*(a1 + a3*x^2 + a5*x^4 + a7*x^6)
= ((a0 + a4*x^4) + x^2*(a2 + a6*x^4)) + x*((a1 + a5*x^4) + x^2*(a3 + a7*x^4))
= ((a0 + a4*x^4) + x^2*(a2 + a6*x^4)) + x*((a1 + a5*x^4) + x^2*(a3 + a7*x^4))
= ((a0 + a4*z) + y*(a2 + a6*z)) + x*((a1 + a5*z) + y*(a3 + a7*z))
which has the form of a multilinear poly
the above is decomposed into two branches. that's why its multilinear. we could use the larger branch factor 4.
(a0 + a4*x^4) + x*(a1 + a5*x^4) + x^2*(a2 + a6*x^4) + x^3*(a3 + a7*x^4)
the larger branch factor 8 just returns to the original poly form.
if we have branch factor b, we have a poly with b coefs, each coef being a poly with k/b coefs.
each of the coef polys need only be defined on a domain of size n/b.

we'd like to consider the case of full decomposition, with b = k.
for a general vector, our decomposition should yield a poly with k coefs, each coef a sub-vector.
these subvectors need only be defined on a domain of size n/k.
a subvector is valid if its a repetition code.
call these k subvectors f0(x),...,f_{k-1}(x)
then we express the original vector as
f0(x) + x*f1(x) + ... + x^{k-1}f_{k-1}(x)
given a distance on the original vector, we should be able to infer union distance on the subvectors
then we're faced with the question of the union of random combinations.
the number of combinations is a variable we before called t. each combination contains k/t subvectors.
the subvectors in each combination are pre-determined.
first we can infer the union distance of each combination set.
then we infer the distance of each combination for the same random scalars.

in general we can have any factor b we want, as long as it divides q-1.

suppose we decompose with b = 3.
we have f1,f2,f3
suppose we are concerned with the random combination
(f1 + x*f2) + r*f3
well this is equal to
(f1 + r*f3) + x*f2
now since f1,f3 are of smaller sizes presumably already proven for, we can invoke induction
note however this decomposition only decreases code size by a third, not half.

one thing we'll certainly need to investigate regardless is the union distance of
f1,...,fb
when there is an error in h, there must be one in at least one of fi, or else contradiction.
for every subset of fi we have an intersection set, ie a set of shared errors by all fi in that subset.
when an error is exclusive, ie in only one fi, then we know the error exactly.


statement: if h has distance d, then f = (h(x) + h(-x))/2 and g = (h(x) - h(-x))/2x have union distance d
for sake of contradiction going one way, suppose there is cf and cg such that f and g wrt to them have union distance < d.
let c = cf + x*cg
show that h has distance < d wrt c,
c - h = (cf + x*cg) - (f + x*g) = (cf - f) + x*(cg - g)
show that if h has an error at an index, then so does the union.
for sake of contradiction, suppose h has an error but the union does not.
then c - h != 0, but (cf - f) = 0 and (cg - g) = 0, and thus (cf - f) + x*(cg - g) = 0
this yields a contradiction, with non-zero on the left and zero on the right.

statement: if f and g have union distance d, then h = f + x*g has distance d
for sake of contradiction, suppose there is c such that h has distance < d wrt c.
let
cf = (c(x) + c(-x))/2
cg = (c(x) - c(-x))/2x
show that f and g have union distance < d wrt cf and cg
cf - f = (c(x) + c(-x))/2 - (h(x) + h(-x))/2 = (c(x) - h(x))/2 + (c(-x) - h(-x))/2
cg - g = (c(x) - c(-x))/2x - (h(x) - h(-x))/2x = (c(x) - h(x))/2x - (c(-x) - h(-x))/2x
if h does not have an error at x or -x, then f nor g have an error.
the contrapositive is that if f or g has an error, then so does h at x or -x.
thus for every error of the union, there is at least one for h.
(
we can say more
only f has an error when c(x) - h(x) = c(-x) - h(-x) and the error is equal to this quantity
only g has an error when (c(x) - h(x))/(x) = (c(-x) - h(-x))/(-x) and the error is equal to this quantity
there is no error when the above equation(s) hold but with both sides zero
both f and g have errors when neither of the above equations holds, ie when the errors of h at x and -x are not associates.
)


I think we should return to the base question.
here we can more generally investigate the distance of a random combination of vectors given a union distance on them.
to begin, we can return to the simplest case we started before, where we have only two vectors.
I think we want to say that whp the distance will be the union distance.

a basic questions is given t distict lines, what is the probability on uniform input that s of them evaluate the same?
for a fixed 2 of them, the probability they evaluate the same is 1/|F|.
for a fixed 3 of them
    suppose all are linearly independent.
    then they cross at 3 distict points
    the probability of a crossing is 3/|F|
    suppose they are linearly dependent
    suppose there are two crossings
    the probability of a crossing is 2/|F|
    suppose there is one crossing, ie a double crossing (maybe call it second degree)
    the probability of a 2nd degree crossing is 1/|F|
we could calculate the expected number of crossings, but would this be useful?

I think we can create classes of pairs of lines according to crossing point.
for each point there is a class. a pair of lines belongs to the class at which point they cross. 
some theorems
(a,a) belongs to all classes
(a,y*a), y!=1 belongs to no classes
(a,b) and (b,a) belong to the same class
if (a,b) and (b,c) belong to the same class, so does (a,c)
we could partition the lines into independence classes.
futher, each class could be partitioned identity classes.

we need to represent our assumption of union distance in terms of lines
suppose two vectors have union distance < d. what does this mean?
it means you only need to change < d lines, whether offset, slope, or both, to obtain all identical lines.
in other words, to say two vectors have union distance d means you must change at least d lines to obtain all identical lines.
so it seems union distance translates into distance of the lines from a repetition code.
without only a union distance assumption however, and nothing about the error values, we can't assume anything about how these lines relate, eg how many in a given independence class.

for the crudest soundness analysis then, we calculate the max probability the distance decreases even by 1.
suppose the union distance is n-1.
in this case all n lines are distict.
all we need to decrease distance is for any two lines to evaluate the same.
how many is the max number of possible distinct crossing points?
whatever, this number, the probability of hitting one is this number over the field size.
without more analysis, we can assume the max possible of (n choose 2) = n*(n-1)/2
remember in this case n is the rate, so this seems reasonable for now, though this might blow up later.
but we can try further analysis, because crossings can't all be independent.
if (a,b) cross at x, and (b,c) at y, where do (a,c) cross?
a1 + x*a2 = b1 + x*b2
b1 + y*b2 = c1 + y*c2
a1 + z*a2 = c1 + z*c2
z = (c1 - a1)/(c2 - a2)
--
(c1 + y*c2) - (a1 + x*a2) = (b1 + y*b2) - (b1 + x*b2)
c1 - a1 = (y - x)*b2 - (y*c2 - x*a2)
--
a1/x + a2 = b1/x + b2
b1/y + b2 = c1/y + c2
(c1/y + c2) - (a1/x + a2) = (b1/y + b2) - (b1/x + b2)
c2 - a2 = (1/y - 1/x)*b1 - (c1/y - a1/x)
--
z = ((y - x)*b2 - (y*c2 - x*a2)) / ((1/y - 1/x)*b1 - (c1/y - a1/x))
---
however, for small n (rate), which we'll have, I think every pair of lines can cross at a distinct point.

So given vectors with union distance d, the above bounds the probability the result has distance < d.
however, also relevant to our inquiry is where the errors are in the result.
suppose the distance does not decrease (which holds whp). then the errors are precisely in the same place as the union distance. 
keep in mind, however, that the union distance errors are not unique, and we could compare to multiple word pairs for union distance d.
we are saying that in any case, whp whence the result has distance d, it differs from a closest word with errors in the same locations as the union distance. 

but we need to consider the same random variable with respect to multiple combinations.
we know the probability it preserves distance for one combination, but what is the probability it preserves distance of all combinations?
for each combination consider its distortion set, ie the set of all x-values that decreases distance.
with strict equality, the probability any combination breaks is the size of the union of all distortion sets over the field size. 
above we bounded the size of the distortion set for any combination. 
thus we use the inequality that the size of the union is less than the sum of the sizes of distortion sets, which we can bound. this inequality is equality when the distortion sets have trivial intersections.
now suppose we do the full decomposition.
we start with a code of length n.
we can do log(2,k) rounds, assuming n and k are powers of 2.
after i rounds, we have split it into 2^i parts.
so the most we can decompose the n size original code is into k parts, each a repetition code of size n/k.
we have really decomposed the n-entropy original code into a k*n/k = n-entropy alternative representation.
the bound above for the size of a single distortion set is (n/k)(n/k - 1)/2. 
there are k/2 combinations.
Using the inequality we bound the union distortion set size to be k/2 times the bounded distortion set size, that is
k/2 * (n/k)(n/k - 1)/2 = n(n/k - 1)/4
we need to make this number negligible with respect to the field size.
2^{-m} >= n(n/k - 1)/4F
F >= 2^m * n(n/k - 1)/4
log(F) >= m + log(n(n/k - 1)/4) ~ m + log(n)
which is reasonable.

we could also try induction instead of the full decomposition and see if we get the same result.
combination of two of size _: distortion set size.
size 1: (n/k)(n/k - 1)/2
size 2: 2*(n/k)(n/k - 1)/2
size 4: 2*2*(n/k)(n/k - 1)/2
...
size k/2: k/2*(n/k)(n/k - 1)/2
same result!

to intuit and legetimize our base case analysis, it could indeed be the case that each of the k vectors has max distance (n/k - 1). this is because any entry from each vector, of which there are k, can remain fixed, and all the others, (n-k) of them, are switched. this means the max distance is n-k, which matches RS codes, because any k elements in an RS code can be an information set, and all the rest may need to be switched. 

we could try to take account of the probability that the distance decreases by a small amount.
for our base case above, breaking distance 1 means cancellation of two errors, thus the evaluation not 2 but 3 different lines to the same value.
there are at most (n choose 3) ways this can occur. 
...

we could try to generalize to larger combination degrees t. 
for the base case this means we take a random combination of t vectors. 
note in general we can multiply all vectors by a random scalar and then make the argument that whp recomposing the results will not cancel any errors, such that they appear at both x and -x.
now again we have the same definition of union distance on t vectors. is the number of lines that much be changed in some way such that all lines are idential. 
so what is the probability that we choose a random combination such that two evaluate the same?
one way to do this is recursively. what is the probability the first random scalar cancels an error, and then probability the second scalar cancels. now since the scalars are independently random, we can say these are independent events.
then we calculate the probability of breaking as 1 minus the probability of niether step breaking. that is
1 - (1 - (n/k)(n/k - 1)/2F)^{t-1}
for t-1 steps involved for a combination of t vectors.
now this probability of breaking is greater than for t = 2.
the benefit is that we multiply this by k/t for the final probability, which is then
k/t * (1 - (1 - (n/k)(n/k - 1)/2F)^{t-1})
eg for t=3 we have
k/3 * ((n/k)(n/k - 1)/2F + (n/k)(n/k - 1)/2F - ((n/k)(n/k - 1)/2F)^2)
= k/3 * (n/k)(n/k - 1)/F - k/3 * ((n/k)(n/k - 1)/2F)^2
= n(n/k - 1)/3F - n^2(n/k - 1)^2/(12kF^2)
which I think is slightly worse than for t=2. I'd expect the trend to continue towards worse as t grows. 
the benefit of larger t is fewer rounds. 


note
in all entropy dropping cases, the commit handle gives the verifier the ability to randomize a new commit handle from it. in some cases this only involves the handle, like in pedersen commits, and sometimes it involves meta data like merkle paths. the point is the prover has no say, even if expected to provide metadata, and verifier has complete control over randomization. 

in fact, this leads me to realize that we can use code based commits more generally where we don't need to rely on making the core query by opening an entry. This means we can use random codes, with no need to reduce to evaluating it on some entry of that code.
the idea is we treat the commits as commits to knowledge of polys. then we take a random combination and ask for the result. the result will only pass whp if the original two commits are valid in that the prover had knowledge. the result is either raw data or a new commit, but in either case represents the corresponding random combination of the polys. thus this approach needs the codes to be linear, though not structured. 
thus like all our other entropy-dropping methods, its an approach to reduce a commit about two polys to a commit about their random linear combination. to use this, the prover will make claims about poly evaluations (maybe multivariate) prior to the verifier choosing the random combination. then those claims are reduced to a claim on the result. so its only the consistency checks that are with respect to univariate polys, while the claims can treat the poly as a multivariate. 
note, however, direct recursion can't be used for multivariate claims, because we must check the claim by calculating on the result before recursing. in the case of univariate claims, however, the code naturally already calculates the claims. what's important to us for short proofs is that the intermediate step before recursing is small enough for multivariate claims. it should really just be a quick sumcheck, with a small proof size. these extra elements are costly, so maybe only worth it if we can take advantage of random codes to decrease consistency checks otherwise necessary.

i'm thinking this may simplify the problem statement. 
in short, show that if result passes, then it is the correct random combination of fixed polys of which the prover had knowledge represented by the commits.
the contrapositive is that if the commits don't represent knowledge of fixed polys, or they do but the result is not their random combination, then the result won't pass.
design an extractor that given an adversary that can pass the test can exctract source polys which have linear combination equal to the result. 

we'd like to take advantage of the hardness results relevant to linear codes
one direction I'm thinking of is to say either the result has distance closest to the random combination of the sources, or if not the prover doesn't have knowledge of the closest word. The argument would be something like such knowledge would allow for finding two closest words, thus calculating a word of minimal weight (ie minimal distance) which we conjecture hard to do.
maybe consider an extractor that finds nearest words for a noticeable fraction of random combinations. then work with these instances and submitted results, and try to find minimal distance. 
for example, subtracting two instances gives a word a certain distance from the scaled vector. maybe calculating this work for all pairs yields enough codewords within a certain distance that at least two must be closest. 


stop and think for a bit about the multivariate hash.
there are a noticeable number of x-values for which the adversary can find inputs.
for any two of those inputs (corresponding to distinct x-values) we can construct a input lines, then pass through and obtain output polys.
we'd like to prove that the adversary has knowledge of a line that results in the commits, and that all correct inputs of which it has knowledge lie on that line. ie all inputs lie on the same line and that line results in the commits.

one approach is to take all distinct constructed outputs, for which the adversary has complete pre-image knowledge, and argue that these outputs span a noticeable amount (too much) of the target space, such that upon a random target the adversary would have noticeable probability in succeeding. 
for this would could argue that any two distict constructed outputs (suppose quadratic) agree on at most 2 points.
suppose the target dimension is t.
suppose we have p distinct outputs.
maybe we can say the outputs span a space of size p*F - (p choose 2).
then the probability of success for a random target is
(p*F - (p choose 2))/F^t
now p depends on the probability of success breaking the commit.
the larger p, the larger the success of breaking the commit, but then we can also say the larger the sucess for a random target. the threshold on this spectrum is we'd like to say when there is noticeable success of breaking the commit, there is noticeable success for a random target. 
noticeable success for breaking the commit, means that there is a noticeable number s of points for which the adversary can answer which do not lie on a line that results in the commit.
I think this means one can construct roughly p = sF distinct outputs.
then the probability of success sF^2/F^t.

what if we interpolate more than just lines to construct outputs. for example, we take 3 points, which don't lie on the same line, and construct a degree-4 output. we could do this for arbitrary l points that form a degree-(l-1) input to the construct a degree-(2l-2) output.
if we manually plugged in a degree l-1 poly we could expand it to an output in poly time, but quicker would be to just interpolate by evaluating in unexpanded form at 2l-1 points then doing lagrange construction for the output. 

actually the previous two approach don't seem to work because they would work in general and don't take advantage of adversary's ability. in general one could just choose an aribitrary number of lines, larger degree inputs, even multivariate inputs, evaluate them, then hope the results span enough of the target space. the reason this approach doesn't work is one would have to construct too many outputs to span the space costing too much time, and once a target is selected one then faces the challenge of finding the output that can be evaluated to match the target. 

try composing multivariates, where the input is linear in as many variables as the original problem such that it just transforms into a new problem instance. the extractor might transform the problem submitted to the adversary. 
think about composing in general.

note a generalization of this hash is any hash where the input is the line in one variable. this works when the output has small description with respect to this single variable. 

for a collision resistant version, consider two output polys for which the adversary has knowledge of input polys, while this may not necessarily include the original commit poly.
then find two different points for which the two outputs evaluate the same. then plug those points into the input polys and whp they should result in two different inputs, which will evaluate to the same output, thus a collision. 
but finding such a point may be even statistically unlikely.

collision resistance is a needed property, so we can use that as a security assumption.

suppose we propose a new problem, maybe reducing it to another multivariate problem.
i'm thinking of modifying where there is only one poly, not multiple. 
suppose we take the original problem with multiple polys, and we expand into monomials. 
we can think of the input as field elements, but the coefs as vectors of field elements.
I think these vectors form a ring, as they are the direct product of fields. 

if we think of the input elements as all-identitcal vectors then we can think of the equation as a polynomial in this ring.
but the original problem in expanded monomial form need not be thought of as a polynomial and can instead be thought of as a vector space with the coefficients and result as vector elements and the monomials as scalars, which are field elements.
thus I think we can generalize the problem most elegantly to finding field inputs for a multivariate vector space equation over that field.
the parameters are the dimension of the vector space, the number of variables, and their max degree.
in the original case our vector space of dimentions m is the mth direct product of the additive group of the underlying field. 
recall that for all vector spaces over a field there is an isomorphism between vector elements and vectors of the field elements (the linear combination of which equals the vector element wrt a certain basis).
thus our choice of the vector space and the underlying field is standard.

we could think of it as a linear equation Ax
A is m by n, and each column is a vector space element.
x is of size n which is the number possible monomials composed of v variables.
the result is of size m and is a vector space element.

it is worth comparing this problem to generalized SIS.
one version of SIS is a combination with small integers over an abelian group.
more specific is where the abelian group is a direct product of n groups modulo an integer q.
note that if we replace the groups modulo q with extension elements of a field of size q = p^d, this would be isomorphic to a direct product of n*d groups of modulo p.
this leads to the known trade offs between q and n.
on the other hand, in a multivariate problem setting these direct products are not isomorphic because the scalars may be extension elements, not just integers.
in particular, scalar multiplication is not isomorphic to repeated addition.
now suppose we replace the small integers with field elements, maybe of an extension field.
then since SIS is linear, we are basically left to just solve a linear system.
thus the linearity of SIS is compensated for by the small integers.
likewise, the field scalars of a multivariate system are compensated for by non-linearity.
now we could take the hard parts of both systems, whence we have a non-linear combination with small integers (ie limited field elements).

I'm interested in a second generalization of the multivariate problem.
suppose we restricted the field elements, eg to a subfield.
analogously, suppose we increased coef field size and in return reduced the number of equations.
with respect to the previous generalization, this means decreasing the degree of the vector space, but keeping its size the same relative to the input space.

suppose this worked.
in the protocol, the verifier is expeded to commit to input x*t + y where x and y are v-length vectors in a subfield.
the commit value is a univariate in the ambient field.
for opening, the verifier chooses a random t = r within the subfield.
the prover is expected to submit x*r + y which should be located in the subfield.

an analogous setup for SIS is where scalars are not restricted to small integers but rather to a subfield.
There is no subfield of a field modulo p, so we would need to switch to an extension field, allowing scalars to be anything in the field modulo p.
Then the difference between this and the above system is that here we would have linearity at the cost of multiple equations.
now given commits, one could take a random linear combination of them that should result in a new commit.
maybe we could do a ring version of this, but with carefull soundness with respect to the random combination because the ring could have zero divisors.
in this case we may not need extensions because subrings exist in non-prime modulo q.
in appears niether of these versions, field or ring, is analogous to SIS, because in these cases the scalars are closed but in SIS, regardless how we modify the problem, scalars remain unclosed.

regarding the SIS version, the only opportunity I see is to further examine the possiblity and then propose a new computationally hard problem in the version where the scalars are closed, eg a subfield or subring.

now with respect to the generalized multivariate problems, do we have reductions for them?
i'd like to focus on the latter version of a dimension 1 vector space, ie inputs in subfield.
in this case we take advantage of there only being one polynomial. 
note the ambient field will likely be very large. 
like before, we need to assume collision resistance, so we could first reduce to that.
then we must reduce collision resistance to a hard problem. 

suppose the subfield is that modulo p.
then a system of n equations is equivalent to one equation of the degree n extension field.
thus we may safely question the one equation system.
the challenge of multiple equations has now translated to the challenge of random choices of t being restricted to a subfield.

maybe show that having knowledge of enough points on the output to fully determine the output should be enough to find knowledge of a line that results in the output.
this is different that what we were saying before, where we only asked for knowledge of enough points to determine an input, which is fewer points than to determine the output, despite that input determining some (though maybe not the same!) output.
if any subset of the inputs (over)determine the right output we are done. this means knowledge of a line for the right output.
suppose none of the inputs determine the right output. that is, upon every pair of inputs that determine a line, none match the right output.
then at most one input can be on the line which determines the right output, for it two inputs were on the line it would determine the right output.
here we can't go further because gaining knowledge of a line now would lead to a collision.

suppose we're given knowledge of enough points to determine the output like above.
we want to use this knowledge to find a line that matches the output.
a basic question is whether all these known points lie on the same line.
if so this line is the natural answer.
but what if not?
i suppose there's nothing we can do.

f(x + y)
= a(x + y)^2 + b(x + y) + c
= ax^2 + bx + c + ay^2 + by + c - c
= f(x) + f(y) - f(0)
f(x + y)
= a(x + y)^3 + b(x + y)^2 + c(x + y) + d
= (a(x + y)^2 + c)(x + y) + (b(x + y)^2 + d)
= (ax^2 + ay^2 + c)(x + y) + (bx^2 + by^2 + d)
= ax^3 + ay^2x + cx + ax^2y + ay^3 + cy + bx^2 + by^2 + d
= (ax^3 + bx^2 + cx + d) + (ay^3 + by^2 + cy + d) - d + (ay^2x + ax^2y)
= f(x) + f(y) - f(0) iff x = y
so it the pattern doesn't extend via decompositions to higher degrees
but I think we have the general pattern
f(n*x) = n*f(x) - f(0) 

suppose we look at a quadratic system plugging in a line, resulting in a quadratic univariate.
we'd like to apply the rule
f(x + y) + f(0) = f(x) + f(y)
we can plug in any value for t we want, but the rule above says for certain combinations of inputs we can get identical outputs.
I don't know how to use this.


if you don't have knowledge of a line that results in the commit, and won't be able to find one, what is the probability you can find an input for a random point on the output?

if you have knowledge of preimages for an overdetermining fractions of points on the commit, can you find knowledge for all points on the commit?
intuitively, these points for which you have knowledge have a pattern, so it seems your knowledge of inputs should also follow a pattern.
A collection of p points have a pattern if their interpolation has max degree < p-1. 
A simple definition for a collision then is when the inputs don't have this pattern, but the outputs do.
Another is maybe the ratio of the input interpolation poly size over the output interpolation poly size. 

form the problem as having multivariate coefficient matrix A which is n by m (n equations, m monomials).
transform the problem to a new one by an n' by n transformation matrix T (n' equations).
to invert the transformation, T needs an n by n' left inverse.

suppose we choose a transformation matrix T that sends the coefs of the commit polys to 0.
this way any adversary input to the previous problem is an answer to 0 for the new problem.
since this transformation has non-zero kernel its not left-invertible.
but we can transform it again with another transformation S to a different problem.
now any answer for the original problem is answer to 0 for the T problem and remains an answer to zero for the S problem.
Of course the S problem is the ST transformation of the original problem.
but an important point is that T is not known until the commits are known, which is not known until the original problem is defined.


only works with cubic or above
try all pairs of inputs given to make as many lines as possible.
each line will result in a different poly, none equal to commit.
each will agree at least two places with commit.
with high enough degree, and enough lines to try, there may be enough probability one can find a third place they agree and the adversary can answer for. 
this then yields a collision.
before we thought the probability of finding this third point was hopeless.
but suppose we can establish that statistically it exists.
then we are left to efficiently find it.
suppose we take a random linear combination of commits and equations.
then we plug in as many lines as we can into the reduced equation and look for a point at which the output evaluates the same as the reduced commit. this is root testing.
the problem with this method is it doesn't seem to take advantage of our circumstances.
you might as well just plug in random lines and whp you'll still get a collision.

maybe consider that in reasonable time one can only compute a negligible fraction of outputs. thus if one is know inputs for more than a negligible fraction (regardless what pattern the outputs have) one must use some other means to obtain these inputs than from an arbitrary table.

interpolate a bunch of points in the input, enough so the output over determins the commit. 
now the new output and the commit can't agree on any more points.
now choose a new point.
if adversary's input lies on the existing interpolation without modification, then we have a contradiction because its output should differ from the output of the existing interpolation.
thus the input cannot be on the interpolation
the point is that to avoid contradiction the interpolation must have max degree, not just degree above a line.

it looks like this task is randomly reducible.
you are given the task of finding such a 'higher' collision.
you first randomly transform, via an invertible transformation, to a completely independent problem.
the adversary provides commits and a higher collision for them.
invert these commits back to the original problem to form you own commits, and your inputs are identical.
but the flaw here is that the two problems are not completely independent unless the number of equations is the same as number of monomials.
or put another way, these problem are randomly reducible between each other with respect to some basis (the multivariate coefs).
if you find a pre-image for zero, or a collision in one, it applies to all.

lets restate the problem in vector space notation.
the problem is with respect to some basis of size corresponding to degree d and variacy v parameters of the problem.
give an output basis to be treated as a univariate expression of degree d.
for a random evaluation of the output, find field elements for a pre-image.
both basis, the main basis and the output basis, span some space in general, and perhaps a more restricted space via their evaluation forms (multivariate and univariate respectively).



back to making use of random codes.
suppose we decompose a random code similar to how we do for RS.
we start with a random code B of size n/2.
then we combine with itself in some pattern for a larger code A, like for f + x*g version of RS we have
A = [A1,A2]
A1 = B + x*B
A2 = B - x*B
from which you extract two instances of code B and then take a random combination.
this is slightly better than just starting with a single n size random code, because our analysis from before still applies which guarantees a certain union distance.
but this still doesn't take advantage of any hardness of random codes.


suppose we are given a set of successful instances, those with close distance, from the adversary.
we'd like to apply some transformation that maps all the codewords to one, and the corresponding instances will surround this single word.
consider just summing them all.
then each instance undergoes the addition of all other codewords but its own.
for instance a, its codeword c, and the sum of other codewords c', we have
|(a + c') - (c + c')| = |a - c|
thus each instance maintains its distance through this transformation.
now we can compare the distances between the modified instances.
for modified instances a' and b' and central word c'
|a' - b'| = |(a' - c') + (c' - b)| <= |c' - a'| + |c' - b'| = |ca - a| + |cb - b|
but with
a = u' + ra*u
b = u' + rb*u
c' = ca + cb + ci
a' = (cb + ci) + a
b' = (ca + ci) + b
we have
|a' - b'| = |((cb + ci) + a) - ((ca + ci) + b)| = |(cb - ca) + (a - b)| = |(cb - b) - (ca - a)|

this just yields the trivial inequality
|(ca - cb) - (a - b)| = |(ca - a) - (cb - b)| <= |ca - a| + |cb - b|
we were hoping to do more directly analysis with a' and b' involving u' and u without ca and cb.
we know |ca - cb| >= n - k + 1
and |a - b| = |(u' + ra*u) - (u' + rb*u)| = |(ra - rb)*u| = |u|
by reverse triangle inequality
|(cb - ca) + (a - b)| >= abs(|cb - ca| - |u|)
in total yielding
|ca - a| + |cb - b| >= abs(|ca - cb| - |u|)
maybe we can use all these pairwise bounds obtain a probagbility distribution on single distances.

we would probably condition on |u|.
suppose |u| <= n - k + 1
then since |cb - ca| >= n - k + 1 we have
|cb - ca| - |u| >= 0 for all a,b

...
all this is not with regard to random codes, so its just distance analysis again which we've already solved satisfiably.

for a random code suppose we split it n instances of size n.
then I think we can apply hardness of random decoding
the benefit is fewer openings.
but now each opening requires like n*log(n) field elements.
this is only worth it if less than the n*k original elements to look at.
that is, if the #openings < k/log(n).
this might we worth it for big batch amortization.
now regarding hardness, the idea is we could map to a random vector.
but we can only do so if the rows of the instances are independent.


start with vectors for which you know the closest codewords.
you can mutate these vectors, changing the errors but maintaining the distances.
now you know the closest codewords to more vectors than you began with.
while this seems trivial, make an inductive argument from it.
we want to use the following ability to do random decoding: if you know the closest codewords for some vectors, you know the closest codewords for a noticeable fraction of the space they span.
we iterate from the beginning, starting with the commits.
we take random combinations, and have a certain number of successes for the adversary can return closest codewords.
then the key part is we mutate these vectors, but keeping them within distance.
this way we now have a linearly independent set of vectors from which we can span further.
the expansion grows exponentially with the steps, but at each step, however, there is only a noticeable probability of success.
thus we can only iterate a constant number of times to maintain the noticable probability.
we also have the challenge that our goal is not just to grow the span size, but to target a particular instance, ie a worst case instance the extractor is given to solve. 
if we can't find a way to mutate strategically to reach the target, maybe we just focus on span growth, maximizing the probability it embeds a target instance (or one of many we could be trying to solve for).

an important point here is that in the main statement the probability is taken over the random combination, not including the original commits, because they can be strategically chosen by the adversary.
but we are assuming the adversary also has success for the random combinations we give it, which we choose for it.
if no other way to compensate, maybe we can change the protocol somehow such that the beginning commits are pseuorandom.
then we can take probability both over starting vectors and combinations. 

suppose we take the special case of the closest codeword and convert it to any codeword, by adding the relvant distance.
then it still remains to handle the adversariy chosen error indices and their differences from the closest word.
if this was RS we could partially randomize error positions by rotation, but this is not for RS.
we could handle the number of errors by maintaining it throughout, and then doing a case by case analysis.
but positions and values still remain.
maybe we could argue that positions don't matter, because the code is random. not sure how to state this.
well consider again the computational problem. given a random vector, find a closest codeword.
if you know a set of error positions, I think its trivial to recover the codeword. eliminate erroneous words and compute matrix inversion, even if not unique.
but the adversary task we're considering is, given vectors with certain distance around known, fixed codewords, find closest word of a random combination.
we'd like to contend that if the adversay can do it for any vectors it can do it for all.
the challenge is that the adversary knows the errors with respect to these codewords and may take advantage of their particular pattern. 
well consider starting vectors u' and u, scalar r.
suppose we offset u' + r*u appropriately to return to each of the starting words.
now suppose we take a random combination of any of the 4 pairs of vectors.
this again results in a random combination of u' and u, thus the adversary can solve with noticable probability.
we can continue like this, hoping that each time the adversary returns a vector such that its offset results in one linearly independent from the rest.
if we get enough that are linearly independent we can solve a random decoding problem with noticeable probability.
this protocol differs from the one before in that we don't mutate, only offset.
but what is to guarantee the linear independence?
towards a contradiction, suppose the adversary returns one with linear dependence with respect to those existing.
eg suppose c1 and c2 are starting words, and cr recovered word.
then our new vectors are (u' + r*u) + (c1 - cr) and (u' + r*u) + (c2 - cr).
suppose the first is linearly dependent with coefs k1, k2.
k1*u' + k2*u = (u' + r*u) + (c1 - cr)
(k1 - 1)*u' + (k2 - r)*u = c1 - cr
this means for k1,k2 != 0 this combination of u' and u is a codeword, which we can show to be unlikely.
thus in the first case the returned words won't be linearly dependent.
i hope we can extend this to further iterations.
for futher interations, suppose v' and v are any existing vectors or their combinations, ie in the existing span.
let cv be recovered word.
what happens if (v' + r*v) + (ci - cv) is in the same span, ie not independent.
consider u1,...,ul and k1,...,kl such that
k1*u1 + ... + kl*ul = (v' + r*v) + (ci - cv)
then absorbing v' and v into the combination, we've found a combination that equals a codeword.
so soundness holds as long as we can show it unlikely to find a combination with distance 0.
lets first return to the first case on u' and u.
we may have to take advantage of how cr is partially randomized.
for that matter, c1 - cr is partially randomized in that it can any of a bounded number negligible with respect to the total number of codewords.
We know that if their combination equals a word, no combination equals any other word, so while dependence is possible it has negligible probability for this starting case.
now returning to the general case on v' and v.
k1*u1 + ... + kl*ul = (v' + r*v) + (ci - cv)
again I think we try to say that since cv is partially randomized, so is ci - cv to a certain (less) degree.
further, linear combinations of u1,...,ul can only equal a small number of codewords, hopefully shown to be negligible with respect to the number of possible ci - cv, though we must union bound over i.


back to multivariate

we can show that when offseting a problem by any constants b,
f(x + b)
the system remains random and is just as hard to solve.
eg this is used to show that finding a collision
f(x + b, y + c) - f(x, y) = 0
is hard because its a random quadratic system regardless of b,c.

note how multivariate polys can just be considered as the only single way to implement low degree hashes under arithmetic operations native to the field. any such hash, ie low degree and only involving arithmetic operations, can be represented as a multivariate system.
we should frame the problem for generic 'low degree native operation' hashes, and then comment that we will always represent via multivariates.

note how all our proofs of knowledge are about reducing claim on polys to claims on their linear combinations.
the interesting part here is we are always taking random linear combination, because we have a homomorphism on claims
f(x) + r*g(x) = (f + r*g)(x)
so I'm wondering if we can use any other homomorphims beyond the linearity of polynomials, ie how they form a vector space over scalars.
the reason this special case is natural is because claims are naturally single field values (evaluations of functions). 
thus to randomize them it is natural to take a random linear combination.
any other randomization method seems like it will reduce to this.
maybe this is reason enough to stick with the particular vector space homomorphism we already use. 

assuming higher degree multivariate problems are hard, we should try first with high degree d.
suppose 3 inputs are not on a line, but the outputs are all on the degree d commit.
interpolating the 3 inputs and plugging in we get a degree 2d output that agrees with the degree d commit at 3 points.
we want to draw a contradiction for this.
with high degree d we won't be tempted to using low degree properties, forcing us in the right direction.
to still make this problem intuitively hard, we need the number of 'uncorreclted' inputs to exceed number of possible 'uncorrelated' outputs, which is d+1.
so the problem is to find d+2 inputs that lie on a degree d+1 poly, and hash to outputs that lie on a degree d poly.
in the quadratic case, for verification, this means we have 4 inputs that one on a cubic, and hash to outputs that line on a quadratic.

note that if you have an output of degree k and it agrees with the commit at >= k+1 points, it equals the commit.
this yields a bound in the direction we're looking.
consider interpolation p points into a degree l input.
this yields a degree <= l*d output.
the output and the commit agree on all p points, and maybe more.
the output and commit are equal if p >= l*d + 1.
thus for the output to not equal the commit we have p <= l*d.
so we can interpret this to say that if p inputs hash to a degree d commit, they must have interpolation degree l >= p/d.
for us this means if adversary knows a large fraction of the domain that hashes to the commit, the corresponding inputs will have large interpolation degree.
this basically says you can't barely cheat, either you don't or you really do.
if you know uncorrelated inputs that hash to correlated outputs, your inputs must be very uncorrelated, not just slightly.

maybe can we say there must be a way to take the adversary inputs and expand them in some way other than plain interolation such that composing the expansion with the multivariate results in the commits or for that matter any low degree output?

we are underestimating the hardness in our task definition.
we are not including how the verifier samples randomly.
the prover passes with probability equal to the fraction of the domain for which it knows inputs.
suppose there are p such points.
by above, they must have interpolation degree l >= p/d.
with p being a noticeable portion of the domain, this means an "l to d" collision with very large l and very small d.
a tricky bit is that the prover may not actually have knowledge of all these inputs at any point in time.
rather, the prover has the ability to generate one with a certain probability on a random query point.
still, the extractor running in poly time can extract a noticeable number of successes p' resulting in a still-large l' = p'/d to d collision.
thus we can set p to p' and just assume the prover has static knowledge.
so how hard is it to have knowledge of an l = p/d to d collision where p is noticeable over field size?

try transforming the problem to obtain more outputs
one way is taking combination of equations, for which the same input continues to work.
another is to transform the problems to one still random by offsetting inputs.
for a fixed target chosen independently of the original problem, the new problem is hard to solve for any offset.

each input for the previous problem can be modified (subtracting the offset) to an input for the new problem with the same output.
show that this modified set of input has the same interpolation degree.
since the new inputs are just a shift of the old ones, their interpolation is also just a shift.
but a shift to an interpolation is just an additive constant, thus interpolation degree remains the same.

suppose we plug in the original set of inputs for the new problem.
what outputs do we get?
if the inputs are on a line, then absorbing the offsets of the new problem into the line, we end up with the original problem evaluated on a new line.
in this case the outputs will have low degree like the original outputs.
but we're concerned instead with the case the original inputs have large interpolation degree.

starting with a cubic system f, consider using offset c to reduce to the quadratic system for a target t.
f(x + c) - f(x) = t
c can be chosen adaptively after t, though f is chosen randomly wrt t.
this is only useful when degree f is >= 3.

keep in mind quadratics may still work, because collision resistance may be too strong.
instead we might be able to settle for just second preimage resistance.
with quadratics, commit degree is d=2.
from above this yields interpolation degree >= p/2 for p points.
we're interested in the hardness for p >= 4
but we can take advantage of large p, because prover succeeds for large p.


use notation 'vector commitment' instead of 'polynomial commitment' because its irrelevant how the coefs are interpreted for a polynomial, eg multivariate, univariate. 
instead we are just concerned with interpreting the vector as an element in a vector space. 


what if we can show that due to protocol prover has negligible success of cheating even if cheating on multivariates.
suppose we can assume/show that prover knows at most one input for random commit point t.
this random input will correspond to the (random) t combination of polys and their evaluation claims.
so what is the probability this prover-known input will evaluate the same as the combination?

protocol: prover makes evaluation claims c1,c2 of two virtual polys f1,f2 at point x.
verifier takes random combination these claims with scalar t, ie c = c1 + t*c2
prover returns poly f (denoted 'input') that must evaluate on x to c, and must hash to some fixed (prover chosen) function of t via some fixed (standard) hash.
---
interaction sequence:
prover chooses t function.
verifier chooses x.
prover chooses c1,c2.
verifier chooses t.
prover chooses f.
---
condition 1: the prover knows at most one pre-image for each range value of the t function.
since the t function is a function, this translates to one pre-image for each value of t.
thus we can forget the t function, and the hashing, and simply assume that prover chooses fixed lookup table with one input for each t value.
so the t function instead can be thought of as some blackbox commit to this lookup table.
---
condition 2: at most k of the prover's known inputs can lie on a single line.
---
completeness analysis:
suppose k inputs are on a line.
represent this line with offset and slope.
set c1 to evaluation of the offset (on x), and c2 to evaluation of the slope.
submit c1 and c2 as claims.
if verifier chooses a t value for this collection of inputs, then c1 + t*c2 will be the evaluation of the corresponding t input.
thus the test will pass with probability k/|F|, ie when such a t value is chosen.
---
soundness analysis:
we'd like to state the converse of the above.
that is, if at most k inputs are on a line, then whp the test only passes for the respective t values (ie with probability k/|F|).
we can forget t for a moment and solve the problem in terms of just x.
knowing at most k inputs are on a line, we want to upper bound the number of evaluations that lie on a line.
Of course the evaluations can be permuted such that most lie on a line, but we consider without permutations.
For any subset of inputs that are on a line, so will be their evaluations.

For any subset of inputs that are on a input line, and an outlier input not on that line, we can consider the input line evaluated at the point of the outlier, and compare this interpolated input with the outlier. They are different inputs, so whp over x, the evaluation of the outlier will not land on the line.

Suppose we partition inputs into lines.
Only those sets of size at least three are significant. 
We can call any input that doesn't belong to a set of size at least three, a 'loner'.
Suppose after choosing x, we also partition the evaluations into lines, again with only groups of three significant.
We want to know the probability over x that a set grows in size and by how much.
We know that all sets will maintain their size, because input lines will reduce to evaluation lines.
Thus a set can't grow by taking elements from other sets.
So the only way a set can grow is by partial or full unionizing with another set.
We can consider separately the probability two significant sets unionize and the probability a loner unionizes with another set. 
Consider unionization of significant sets. 

Consider loners. 




well suppose the test passed for more than these k inputs.
consider an additional input for which it passes.
the evaluation of this input is on a line together with the evaluations of the other k inputs, but the input itself is not on the same line as the other k inputs.
we should make use of the fact that the inputs were chosen prior to x.

first c1,c2 is chosen, so we consider evaluations with respect to fixed c1 + t*c2.
for any k+1 inputs, their evaluations interpolation degree must be at least 2, thus the interpolation poly (in t) must be different than c1 + t*c2.

but the interpolation degree is at most k.
thus the interpolation poly for any k+1 points can agree with c1 + t*c2 on at most k points.
suppose we interpolate all |F| evaluations.






try cascading multivariate commits.
they would be opened in reverse order than commitment.
if commit 1 works, where upon a t1 value prover only has one choose for input, that input effectively determines t2.
verifier doesn't even know t2, and doesn't seem to need to know.
the input for the first commit can function as the second commit already evaluated at t2.
---
interaction sequence:
prover chooses t1 commit.
verifier chooses t1 value.
prover chooses t1 input.
prover chooses t2 input.
...
---
this already saves entropy because the commit is smaller.
but prover must still provide all inputs.
maybe to save more entropy, prover need only submit final input.
then verifier plugs it in iteratively to obtain final t1 commit evaluation.


consider splitting commitments into parts.
have a small multivariate, only enough variables to make the problem hard, and have good compression.
then us this commitment many times over (using the same multivariate coefs over and over).
then for each equation you'll have many output commits.

suppose we repeat enough times such that for each equation we can select a combination of corresponding commits to map to 0. 
dividing this combination by the sum of its coefs we end up with zero again equal to the ...







standard annihilating poly is chosen at random.
prover commits to inputs by plugging them into ann-poly and returning output.
verifier chooses random output point.
prover must return input points for pre-image.

or opposite.
standard input polys are chosen at random.
prover commits to an ann-poly by plugging in the inputs to it and returning output.
verifier chooses 

suppose you take the combination mapping the commits to 0. 
such a combination should exist.
then you have a single multivariate with all p points being roots.

consider splitting a quadratic around a variable x.
for every value of the other variable y we get a different poly for x.
for each x we can consider the roots for its poly and their (cyclic) galois group.
I think this means that if (x,y) is a root, so is (x^p,y) and (x,y^p)

suppose we take the characteristic power of a multivariate.
it distributes to each monomial.
it then distributes to the coefficient and each variable.
thus upon changing the multivariate problem by taking the characteristic power of each coef, and by then plugging in inputs modified the same way, we get the same answer 

take the p-th power maps of the multivariat

modify problem with offsets to obain more problems with linear independence,
then take combinations of all problems. 





argue for limited need for security parameter in applications where a proof can be shown to have started after some public beacon, and consumed at some time likewise, thus leaving limited time for prover to break a proof.
the public beacons are assume to be sufficient secure to be an anchor point from which to start fresh again. 










