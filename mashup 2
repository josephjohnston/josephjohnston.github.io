
The fiat hash chain may be more problematic than I thought. Theoretically inputs for the hash should have not just the new input but also a tuple of all previous prover and verifier messages too. Then we have binary relation for all failures. This assumes the prover can continually choose the tuple inputs, which is true, but doing so is much more costly than choosing the new message because a valid new tuple means going through all previous parts of the proof again. This is where we might have an advantage and assume the prover only samples a small polynomial of left tuples, which in our case means the prover can only sample a small polynomial number of hash keys. Then we consider the probability the prover breaks the intractability relation against the new input with respect to any of these hash keys. That means our soudness error is multiplied respectively. How could we formally bound the number of keys sampled?
The hash chain we plan to use means the prover need not repeat the whole protocol to sample a new key. Since we do it sequentially the prover only needs to resample the last hash. We assume sampling the last hash its output will not break the intractability, but that output might be a key that helps the prover break intractability for the next hash. It would be nice to have something to make it expensive for the prover to sample new keys. Interactivity with a verifier is one very effective way to do this, and only allows sampling once. But we are trying to do non-interactive.
I wish we could make 'interaction' with the verifying circuit. But this requires the verifying circuit to respond in some unpredictable way, or for it to interact with the next verifying circuit, etc, a system for which we don't know proofs. 
What if proofs in a chain had counters of their position and each one is expected to use a distinct source of randomness in its fiat hashes, generated by the verfier (or the network) particular for each fiat transform. Suppose the prover followed along honestly, never rewinding. Then it would be just like interaction. But of course the prover could wait for several 'blocks', see the randomness again, and continually try and then rewind. But maybe we could make such rewinding very expensive, by having the prover compute and submit many parts at once independently for a single hash transform. Then the transform would have to accept many inputs at once, but rewinding would mean re-generating all those inputs. But I don't know how to structure a proof like this. 

Maybe we say prover can only sample a negligible number of keys. If prover samples a key and succeeds, then from our reduction of relation intractability to SIS, we know prover has sampled a key for which it can solve SIS. So maybe we can relate this to the complexity of solving an ISIS problem when the target comes from sampling another SIS (or the same SIS) instance. This itself may be an interesting problem. 


So what about fiat-shamir for the lattice challenge randomizations? but the randomization is for two purposes, knowledge and poly eval soundness. For poly eval, each challenge can be thought of as sampling a random point on an l-variate linear poly. Now unlike our linear univariates that have just one root to avoid, these polys have many roots, far too many to compute. But we are still above to hash to the difference poly with affine intractable hashes. However, computing we can't just compute the roots by dividing the difference poly coefs like for linear univariate. Instead we must compute the kernel of the diff poly. Consider for 3 variables diff poly a,b,c, finding x,y,z such that
ax + by + cz = 0
Suppose we iterate the hash, where we hash for x and y, and then apply the univariate linear intractable relation hash for the last variable z. I think this could work to the same extent as our regular poly eval hash works, only leaving the concern about selecting hash keys. 
But we are still left with intractability for knowledge soudness. Does the knowledge argument rely on complete randomization? It relys on challenges being sufficiently uniform that at least one lands outside the set for which the adversary can succeed....


We are sampling hashes by just passing part of the input to SIS as the key. Assuming the key is random, is this hash secure? This can be reduced to asking whether ISIS is secure upon changing the random targets. 

SIS can't be a random oracle due to linearity. Even if we decomposed the inputs, it retains linearity as long as there is no overflow. Also, of course, 0 -> 0. 

Breaking the relation without the key is breaking SIS for a related problem. We would like to reduce breaking the relation with the key to breaking the related SIS with the key, which is equivalent to just breaking another SIS. 
In fact, suppose we include key in the regular input for the regular affine-intractable hash. In the affine function we just cancel it with 0's. Now the augmented relation includes the key as input, and in some non-trivial way the key then determines the polys, so this augmented relationship is not an affine one.  
We need to think more generally because we not all relation will be affine anyway, eg when reducing two points to one across two polys cuz we will need to choose the same point for two univariates. 
I was thinking before we could handle quadratics and other relations by hashing all but the last input then doing an efficiently computable function with an intractable hash. But I realize for many cases the resulting function is complex rather than the simple linear one. For example, with a quadratic poly suppose we hash two of the coefs. Then two of the coefs in the quadratic formula become constants, and we are left to compute the resulting function, which will involve a square root. But theoretically we could use sumcheck to do this reduction and still only need linear polys.
As far as multivariate polys, after we set all variables but the last we are left with an affine function so it might work in this case.
But in univariate constraint check we use multilinear polys but the poly coefs are not determined until the end, which is after the random point is chosen. Instead, the random point must be chosen by the commits of the polys in the integrand. This could be thought of as the problem of committing to polys via SIS such that the outputs are taken as the random points directly (with rehashing), and doing so such that the random points form a root for the sumcheck circuit. This is highly non-trivial to compute, so it seem difficult to make provably relation intractable.

Consider the simplified case that all relations will be the linear ones, and we are only left to determine whether key sampling is secure. The key sampling technique we'd like to use is decomposing the previous verifier message and feeding it as partial input to the hash. Interesting is that the moment the previous verifier message is output, both the relation and the key are immediately determined. This is in contrast to the regular notion of relation intractability where the relation is determined first then the key. The point of this is that the key be independent of the relation. When the relation and the key are determined simultaneously by the same input, of course they are theoretically not indepennt. The goal is to show that the way the key is chosen yields a hash which seems independent of the relation from a computational perspective.

I am very interested in giving up on provable relation intractable hashes for fiat-shamir. I'd like to just model with interactive proofs and use some sufficiently complex hash as a black-box for non-interactivity which is necessary for recursion. I would instead probably formally model with interactive oracle proofs. But I'd still like to show the linear intractable hash I've created, but I don't know how to frame it such that it shows any progress. Even if the key sampling was secure I could only use it with some sumcheck, not constraint checking-type sumcheck, not lattice randomization, and not poly eval of multiple polys the simple way. I suppose I leave this as an open problem now, that is how to make use of the linear intractable hash I have so it can contribute to the paper. 
I could have a section on efforts toward proving intractability, maybe mentioning my hash if I can frame it as progress, because at the moment I've reduced the probably to key-sampling which seems just as hard as the original problem. In this section I could also suggest the possibility of trusting a third party to act as an oracle, accepting inputs, choosing random responces, and signing them with a publically verifiable signature that has friendly verification for circuits. This would capture interaction at the cost of a semi-trusted third party. If the party cheats the most it could do is reduce the oracle to using the signing algo as a hash function, which itself may be hard to break. 
Without the need for the linear hash I had, our prime q can correspond to a subgroup of a multiplicative extension field, letting b be other than 2, and if we want to do randomzation in the main circuit we must have primes b and q such that (b^n-1)/2 = q where q would be isomorphic to the multiplicative subgroup of squares in the extension field. 


hmm, for the basis change technique for DLP, though changing the basis requires much auxiliary info, that info could be committed under the new basis, and could be folded into the next amortization round. Like lattice, could perform evaluation on previous part, not just basis change. When we do this it is clear we are reducing enitrely to new data poly evaluation, so we are not relying on circular security.


hmm, with the above in mind I think we could also generalize to having a means of entropy-dropping. that is an efficient way to compute representations of commits such that they can be combined probabilistically to drop entropy. 

to get a better understanding it would be great to find another example, which are already tried but didn't achieve. matrix multiplication would be a candidate and we also identified others but the problem was they were all based on algebra and even most of them on finite fields. 

I'm wondering whether I can get better motivation for writing up the method, maybe even improving it, by first working on the math want to continue with anyway after I finish. I already tried motivating myself with applications, but despite long effort (in another paper) I didn't achieve anything. In fact the negative results for applications made me a whole lot less excited, even disappointed, in the power of verifiable proofs. In fact, even if we achieved what I thought would be an ultimate goal, that is computing on shared secret data (homomorphic encryption), that still falls short. For a primary reason, big data on shared data requires so much computation (even if the data was not private) and lots of bandwidth and high liveness. This would not just be problem if we used everyday devices, but even personal servers would suffer. We need big server clusters, which means reverting to that externally-closed centralized autonomous systems solution. So party of my lack of motiation is a feeling of loss for my ambition I've long thought I had.
personal snippet follows. I thought an adventure would motivate me because thinking about going somewhere cool gets me going. so I came here to xiamen and I'm already exhausted after 24 hours and I've seen I think the coolest neighborhood I ever have. Thought that give me hint of excitment at this moment: travelling more (but I know I'm lying to myself), getting fluent in chinese, a chinese gf who's not obsessed with her phone, creating visual presentation/web platform for the math I plan to continue with, hearing back from weizmann (actually i just feel this dragging-on and they should have replied by now), getting excited (yes even the thought of getting excited gets me excited).  


hmm try to amortize SIS hash, and of course this requires hashing, but maybe in this case the relation is well defined and is in fact an affine relation, therefore we can use SIS again as an affine intractable hash. 

proof size would be big given all the commitments, of which we take random combinations multiple times. Maybe we can reduce proof size by commiting to them and then doing a proof on them. Doing this I think could also save a constant amount of commits. can we roughly estimate the optimal size?
it will be at least \lambda*q. 


maybe could argue that entropy dropping is information theoretically necessary to combine a large number of proofs and keep the size constant. and to keep soundness this entropy dropping must be randomized. 


think about the possibility of doing proof generation in the browser using wasm. 


try to improve the lattice randomization to only as many challenges as necessary for poly randomization. currently we need 2 extras. maybe do this by improving the heavy argument. 


what about fourier instead of polys. in polys we have a function over a domain, we ask for its value at a point in that domain. with fourier we could have a distribution over the domain of frequencies, and we ask for its amplitude at a frequency. I suppose this is just a dual of polys and is no more useful unless used together with polys. 

another idea is with expander graphs, a way to drop entropy would be taking two commits as walks with directions that encode the data. The commitments are the landing positions. For each commit we want to verify knowledge of data that forms a path there, as well as a property of that data. We want to reduce two such verifications to one verification. We need some kind of randomization. Maybe we could randomly choose an element then offset the path by that amount and derive a corresponding claim. But we need a randomized way of combining. Maybe like lattices we need a large number of commits first. Since expanders are bidirectional, we could randomize a new path out of the collection. Maybe we do one path of all commits, where we randomize the order and direction of each commit. Or maybe we do several randomized paths from subsets of the commits. Then we are left to reduce from a longer path to a shorter path. Maybe we can pass the group through a homomorphism to reduce the size.
suppose we use a cayley graph thats undirected and the problem is path-collision resistant. 
we could consider the transform of the new generating set being the square of the existing generating set. but this doesn't decrease the information needed for the path, as it decreases the number of steps at the cost of increasing the choices between steps. well actually there are nk/2 edges in first graph with n vertices. Then in second with k^2 generators we have (k^2)n/2 edges. This means the vertex number is the same, and the edge number is multiplied by k, and path length has decreased by half. 
suppose we pass the group through a homomorphism, say squaring all elements. Suppose this approx halves the number of elements. the vertices would now be n/2, but k may stay the same. 
a,b,c
aa,ab,ac,ba,bb,bc,ca,cb,cc
aaaa,abab,acac,baba,bbbb,bcbc,caca,cbcb,cccc
so we have randomization earlier above, but we also need entropy-dropping. well technically concatenation is entropy dropping, its just we don't know how to ask the prover to deal with a concatenated path. 


prove zk for simple poly eval, not for special case of sumcheck.


multiply using cyclic isomorphism



try lattices for an algebraic structure. Looking for analogues of the schqartz zippel lemma, which may hold here for multilinear polys.  
eg take lcm and gcd for a lattice, and consider the function
f(x) = lcm(
    gcd(a), gcd(b,x)
)
I also tried min and max.
disjuctions and conjunctions also.
or(
    and(
        or(a)
        or(b,x)
    )
    and(
        or(c,x)
        or(c,x,y)
    )
)
but in all cases it seems that absorptions, not idempotence, is the problem. I tried multilinear because of idempotence. How can we get around absorption? Well maybe its not absoptions thats problematic. I guess a necessary condition that isn't satisfied is irreducibility. Regarding OR and AND, suppose b is 1. Then suddenly maybe the whole expression is determined. But hmm, this doesn't happen for min/max or gcd/lcm. 
I don't think lattices will work.

what about non-abelian groups? what something as simple as x^0ax^1bx^2cx^3d
x^0ax^1bx^2cx^3d = y^0ay^1by^2cy^3d
=> x^0ax^1bx^2cx^3d(y^0ay^1by^2cy^3d)^{-1} = 1
=> x^0ax^1bx^2cx^3dd^{-1}y^{-3}c^{-1}y^{-2}b^{-1}y^{-1}a^{-1}y^{-0} = 1
=> x^0ax^1bx^2cx^3y^{-3}c^{-1}y^{-2}b^{-1}y^{-1}a^{-1}y^{-0} = 1
=> x^0ax^1bx^2c(xy^{-1})^3c^{-1}y^{-2}b^{-1}y^{-1}a^{-1}y^{-0} = 1
=> x^0ax^1bx^2c(xy^{-1})^3c^{-1}y^{-2}b^{-1}y^{-1}a^{-1}y^{-0} = 1
now suppose xy^{-1} is in a normal subgroup N. Then (xy^{-1})^3 \in N. Then c(xy^{-1})^3c^{-1} \in N. 
now lets write the following where we replace the single element of N with the whole subgroup.
x^0ax^1bx^2Ny^{-2}b^{-1}y^{-1}a^{-1}y^{-0} = 1
=> x^0ax^1bNx^2y^{-2}b^{-1}y^{-1}a^{-1}y^{-0} = 1
=> x^0ax^1bN(xy^{-1})^2b^{-1}y^{-1}a^{-1}y^{-0} = 1
this continues, we replacing the element with the set wasn't justified
try a mutilinear version
now use ' for inverse
ax1by1cx1y1 = a'x2'b'y2'c'x2'y2'
=> ax1by1cx1y1y2'x2'c'y2'b'x2'a' = 1
=> a(x1(b(y1(c(x1(y1y2')x2')c')y2')b')x2')a' = 1
maybe try working from the outside in instead of the inside out as above.
consider the simplified problem. for any element a, for what z does
aza' = 1 => az = a => z = 1
this then reduces the equation to 
x1(b(y1(c(x1(y1y2')x2')c')y2')b')x2' = 1
so consider for what z
x1zx2' = 1 => z = x1'x2
then it reduces to
b(y1(c(x1(y1y2')x2')c')y2')b' = x1'x2
now we must think about probability of an equation like this holding

the simplest question we can answer is the probability that for arbitrary target t, random x and y are chosen such that
xy = t
suppose this probability is negligible. what can do with this?
I don't know how easy encoding will be. Its easy with a field because 0 cancels entire terms. We need something similar, like mapping to the kernel. any kernel N is a normal subgroup that corresponds to a homomorphism f:G -> H for some subgroup H. f(g1g2) = f(g1)f(g2). encoding means sufficient cancellation, and using kernels that seems to mean passing through a homomorphism, with the cancellation proportional to |G|/|H|, but the larger that value the smaller the use of randomness, ie many different inputs could result in the same output. 
I think interpolation reduces to solving equations which often may have no solution. eg choosing x and y to find c means solving the equation
axbycxyd = c
=> xbycxy = a'cd'


try matrices. constraints would be in form probably of matrix equations. I think we can use the homomorphic property to reduce many multiplications to one. 
I've looked at matrices before and came up with that system that didn't allow for succinctness. Let me try again from a different approach.
Disregarding how we encode the computation, just consider the task of multiplying matrices at random points, and we want to reduce from two to one across both matrices and points. Consider the same point for two matrices. we could multiply one matrix by a random value. but this won't work for lattices. note that each matrix claim is a series of linear poly claims. we can randomize these claims across both matrices, even across just a single matrix, obtaining new claims for a single matrix of maybe different dimension. 
consider a poly as a row of n coefs and take p points of evaluation expanded into monomials as columns. the result should be a row of p evaluations. suppose we randomize by multiplying both sides of the equation by a random column of height p. this corresponds to taking a random linear combination of the claimed evaluations. we already know about this method. but before we'd reduce from two points to one with the intermediate poly. here we reduce with just multiplication, but the cost is that the resulting final point is a column that doesn't correspond to an expanded input point and is rather the linear combination of previously exapnded points. 


maybe we can avoid elliptic curve cycles by simply taking a single field and a single curve with prime subgroup q not equal to p the field size. we would need to reduce a claim about a poly mod p to a claim about the poly mod q. this may not be an entropy problem and can be computed as an 'isomorphism'. but hmm, I don't know how to do this.


maybe the field {-1,0,1} would be better for computation than the binary field {0,1}. 


try to prove that its impossible to probabilistically reduce two eval points to one on univariate.

think about fourier. is linear just like coefficients. shifting poly evaluation by a constant corresponds to multiplying fourier evaluation by an exponential. 
generalized fourier exists, with w an nth primitive root of unity. let t be time domain and s be spectrum, or frequency domain. 
s_i = \sum_{j=0}^{n-1} t_jw^{ij}
t_i = (1/n)\sum_{j=0}^{n-1} s_jw^{-ij}
putting together we see these are correct as
t_i = (1/n)\sum_{j=0}^{n-1} (\sum_{k=0}^{n-1} t_kw^{jk})w^{-ij}
= \sum_{j=0}^{n-1} \sum_{k=0}^{n-1} t_kw^{jk}w^{-ij}/n
= \sum_{j=0}^{n-1} \sum_{k=0}^{n-1} t_kw^{j(k-i)}/n
= \sum_{k=0}^{n-1} \sum_{j=0}^{n-1} t_kw^{j(k-i)}/n
= \sum_{k=0}^{n-1} t_k\sum_{j=0}^{n-1} w^{j(k-i)}/n
now if k-i != 0 then \sum_{j=0}^{n-1} w^{j(k-i)}/n = 0
while if k-i = 0 then \sum_{j=0}^{n-1} w^{j(k-i)}/n = 1
thus we have in total \sum_{k=0}^{i-1} 0 + t_i + \sum_{k=i+1}^{n-1} 0 = t_i

now how do we use this transform? 
we have to be clear whether we are transforming the coefficients or the function point-wise (ie points). We'd like to do the points, but then since the function is defined over the whole field, then so must the spectrum. this would be an enormous fourier to compute or invert in the first place. 
maybe we can take advantage of our function being a poly, in particular assume a univariate poly of degree m, call it f(x) = f_0 + f_1x + f2_x^2 + ... f_{m-1}x^{m-1}
s_i = \sum_{j=0}^{n-1} f(j)w^{ij}
= \sum_{j=0}^{n-1} w^{ij}\sum_{k=0}^{m-1} f_kj^k
= \sum_{k=0}^{m-1} f_k\sum_{j=0}^{n-1} w^{ij}j^k
so we must consider the term \sum_{j=0}^{n-1} w^{ij}j^k for fixed i and k
when 0 <= i <= n-1 and 0 <= k <= m-1.
suppose we have some efficient way to compute these 'monomials' for any i and k.
then we end up with a spectrum of size n, each frequency a linear combination of these 'monomials' with the poly coefficients. 
now consider the shifting property. consider we wish to evaluate the spectrum for two i values, and we want to probabilistically reduce to one (later consider the dual which is what we actually want). For u and v we have
s_u = \sum_{k=0}^{m-1} f_k\sum_{j=0}^{n-1} w^{uj}j^k
s_v = \sum_{k=0}^{m-1} f_k\sum_{j=0}^{n-1} w^{vj}j^k
suppose we pass in the line connecting u and v as a function of z. (1-z)u + zv
s_{(1-z)u + zv} = \sum_{k=0}^{m-1} f_k\sum_{j=0}^{n-1} w^{((1-z)u + zv)j}j^k
= \sum_{k=0}^{m-1} f_k\sum_{j=0}^{n-1} w^{(1-z)uj + zvj}j^k
= \sum_{k=0}^{m-1} f_k\sum_{j=0}^{n-1} w^{(1-z)uj} w^{zvj}j^k
idk what I'm dong. I instead want to consider the shift property.
Suppose we want an offset on time domain by constant j'. what is the frequency function for j-j' (a function of j). and call the new spectrum s' (a function of i). according to the theorem we have
s'_i = w^{j'i}s_i
To evaluate the original function t at time i we have
t_i = (1/n)\sum_{j=0}^{n-1} s_jw^{-ij}
Now to evaluate the shifted by j' function t' at i we have
t'_i = (1/n)\sum_{j=0}^{n-1} s'_jw^{-ij}
= (1/n)\sum_{j=0}^{n-1} w^{j'j}s_jw^{-ij}
= (1/n)\sum_{j=0}^{n-1} s_jw^{j(j'-i)}
so we'd like to probabilistically reduce these two evaluations to one
suppose we take a random linear combination of these claims.
t_i + rt'_i = (1/n)\sum_{j=0}^{n-1} s_jw^{-ij} + r(1/n)\sum_{j=0}^{n-1} s_jw^{j(j'-i)}
= (1/n)\sum_{j=0}^{n-1} s_j(w^{-ij} + rw^{-j(i-j')})
we'd need some way to iterate over all n frequencies to convert back to time, and above we'd need to do it with a transformed monomial. no idea how we do either, and due to what I'll explore below I think there are better solutions that fft.


see matter labs for implementation examples in rust, in particular the zksync repo.


I've realized the beauty of relying on PCPs instead of crypto because crypto assumptions become so much simpler. like the use of expanders for crypto is hard, but for PCPs its just some algebraic props needed. 

reading about second PCP construction. remember randomness is log and query complexity is constant. it uses a natural strategy of starting with constant randomness and query complexity but with high error probability. then we iterate through log rounds improving the system at each step, maintaining constant query complexity, but adding constant randomness, thus ending with desired complexities.


I'm going to start reading FRI and STARK and IOP related stuff, and I need to make sure I know why. I want to understand ways in which to encode computations for FRI commitments. I want to better understand FRI, and I want to understand whether all these schemes rely on the same FRI. 

note the fudanmental diff between PCPs and crypto directly is while both are forms of commitment, PCPs rely on redundancy for the benefit of no 'special effects', while crypto relies on the special property, in our case, of being homomorphic. is there are further way to characterize this difference? are homomorphic properties and redundancy related? 
f(x + y) = f(x) + f(y)
PCPs allow us to look into parts of it, while crypto does't and looking inside means opening the whole commitment.

PCPs connect naturally to locally testable codes and thus coding theory and notions like codewords and distance.

use univariate sumcheck for reducing multipoint eval to univariate eval. or actually use multivariate sumcheck in order to access constants, but formulate input to coefs poly as univariate input. but this may not be possible. the domain of the multivariates is much larger than the domain of the univariate. indeed, this won't work. maybe take the dual approach where we expand the constants into a poly, then we do something like univariate sumcheck, then we evaluate coef poly with uni commit as usual, but to evaluate the constants poly we take advantage of its expanded redundancy and reduce it later with something like multivariate sumcheck to manual computation involving no additional commits. maybe we can improve this by not expanding all at once, but expanding like a square root number of times, thinking about the matrix formulation of multivariate evaluation. alternative to univariate sumcheck we could use poly div. 

alternative to hamming distance, or rather hamming closeness, is the inner product which may offer better analysis. normalize it and still end up with fraction. they call it epsilon-orthogonal rather than epsilon-far. 0 means orthogonal, 1 means identitcal. so this corresponds to 1-distance.
notice that a q-ary lattice is a linear code defined by othgonality to the constants and its this orthogonality (inner product of 0) that allows for the homomorphism. but note that the input elements are small compared to the constants so we are taking the inner product of two vectors from different distributions. 
an interesting question related to both is inner product and orthogonalities of polys in a finite field. just as its hard to find a lattice codeword, maybe is hard to find a poly that is orthogonal to some fixed function (maybe from a different distribution, ie not a poly). 

affine. 
f(x) = ax+b.
f(x) + f(y) = (ax + b) + (ay + b) 
f(x + y) + f(0) = a(x + y) + b + b
has good test but each element encoding requires an independent variable, so huge domain. 

try using fourier to analyze distance, where an arbitrary function can be represented in fourier. 

I think a good lemma, maybe even optimal, is that if a test accepts with probability delta then it agrees with a proper function on at least a delta portion of the domain. clearly there is a tradeoff to be optimized in terms of sampling size.

I'm thinking the core of FRI is the ability to effectively sample a bivariate poly by sampling a univariate poly at multiple places. 

note exapnders can be permuted. this enables randomization via choosing an isomorphism by applying a random element. I still don't know how to do entropy reduction though. maybe like FRI its not a bad idea to use squares and cut the domain size in half, because like poly degrees halving, so would expander path lengths. but one advantage is expanders may support direct commits. 

maybe use expander as domain for commit to reduce both randomness in sampling but more importantly hash cost for verifier. forming a commit here is more challenging, enforcing consistency, but could be done using a proof tree. proof trees would mean many small proofs, so crypto amortization rather than succintness would be a great advantage here. maybe another way would be using PCPs where entropies to be evaluated are gathered and put into a single tree. we could probably do more with proof trees for commits than enable commiting to an expander domain. eg the tree could testify to the low degreeness itself, at each layer doing something like interpolation, joining child polys to a new one. an alternative is just to construct a commit as usual and have another proof testify to its low degree. both of these seem to spit out more entropy than they consume so are probably not worth it. 

PCPs of proximity put the input in an oracle too. previously verifier always was given input. 

Robust PCPs are where the queries are far in hamming distance from being accepted by the verifier, rather than just not being accepted.
apparently these only make sense for non-adaptive verifiers. this is because the query responses are viewed as inputs to a verifier predicate that outputs a decision, and adaptiness would mean these inputs are dependent on one another, so the signature isn't well defined. 

Supposedly composition is great for an outer robust PCP and an inner PCP of proximity. this is because the output for the outer PCP is far from being accepted, and this is the input to the inner PCP which is being a proximity proof reject inputs far from valid. We don't need to worry about the how many queries the outer PCP makes because the inner PCP can handle those. We also don't need to worry about the encoding the inner PCP uses. we just need to make sure the distance param of the output PCP is at least as big as the distance param of the inner PCP, and decision complexity (always upper-bounding the query complexity) of outer is no more than input complexity of inner. 
thus robust PCPs of proximity can be recursively composed.

worth to note in thesis writing how many related definitions there are to things like PCPPs, IOPs, IPs, property testing, hologaphic proofs, PCP spot-checkers, locally decodable codes etc. and decide whats important to us in our definition, which may be yet another definition. 


first draft of FRI-stuff, called Simple PCPs with Poly-log Rate and Query Complexityâˆ—
proof length npolylog(n), query complexity polylog(n)
Give reduction of satisfaction to verifying closeness to RS code. Then give PCP for RS code closeness, I think this is an LTC, which only relies on the bi-variate low-degree test of Polischuk and Spielman. Thus we get ultimate PCP from an LTC, whereas previous works go from PCPs to LTCs. 
Motivation is using PCPs for positive constructions, thus focusing on shorter and simpler PCPs for explicit use.
PCP(r(n),q(n)) is class where fundamental variable, n, is size of instance.

1. INTRODUCTION

Theorem 1: (Efficient PCPs)
SAT has PCP verifier with r(n) = log(n*poly(log(n))), q(n) = poly(log(n)), proof length n*poly(log(n)), and running time n*poly(log(n)), ie SAT \in PCP[log(n*poly log n), poly log n]

so query size is larger than recent results but proof size is smaller. 
PCPs show existence of witness. PCPPs show closness of oracle to being a witness. 

Theorem 2: (PCPPs for SAT)
For every \delta \in (0,1) there's verifier ppto V_SAT that on input circuit \phi of size n, tosses log(n polylog(n)) coins, and makes polylog(n) queries to assignment oracle \alpha and proof oracle \pi. \alpha is of some length n' and \pi is of length n*polylog(n). Completeness is 1 and when input encoded in assignment oracle is \delta far from satisfying \phi then regardless the proof, verifier rejects with some constant probability. 

the above is confusing because \phi must be at least as big as \alpha in order to accept \alpha as input, so then what's the point of making \alpha an oracle? 
PCPs are similar to LTCs and go hand-in-hand. The rate is important, that is ratio of message over codeword size, a parameter contained in (0,1]. Another param is \delta, the proximity parameter, and query complexity. I would think randomness is also relevant. 

Theorem 3: (Efficient LTCs)

previous constructions start based on multivariate polys. core ingredients they use are
1. low-degree test
2. self corrector
3. zero-testers
4. reduction from satisfiability to zero-testing
we do the same but without need for a self-corrector. 
we describe our constructs in reverse order

reducing SAT to univariate zero-testing
given a formula \phi we give two polys P1 and P2 and corresponding domains H1 and H2 and constraint C such that \phi is satisfiable iff there P1 and P2 exist and vanish on H1 and H2 respectively and C(P1,P2) holds. 

univariate zero testing
reduces to two univariate low-degree testing problems with a consistency test.
query complexity is a constant independent of degrees. 
reduce directly from zero-testing to low-degree testing without self-correction needed.
both this reduction and the reduction from SAT to zero-testing only have constant query complexity. 

Reed-Solomon codes and proofs of proximity
central problem is given finite field F and degree-bound d and oracle f:F->F, test if f is close to a univariate poly of degree at most d. 
If f is \delta-far that means it must be changed on a \delta fraction of its domain. 
direct testing would require d+2 queires or more, so we instead turn to auxiliary proof \pi. This proof is the PCPP (or the assignment-tester). 
Oracle pair is (f,\pi). 
previous work shows this is possible but with large constants and design complexity.
Give an O(n*polylog(n)) size PCPP for RS-codes over field F of characteristic 2 and cardinality n. 

(
hmm, can we try to reduce poly evaluation between fields. for efficiency this would require a relationship between the fields, both the characteristic and the extension degree. well apparently homomorphisms between fields of different characteristic don't exist. this incompatibility is worth mentioning.
Forgetting extensions for a moment, suppose we are supposed to evaluate over prime field p but we want to reduce to q. We have a claim for p. Then suppose we evaluate over Z, not Z_p. We get an answer, confirm that modulo p it is as claimed. Then we take modulo q for the new claim. Since the poly is finite and inputs are finite, in [0,p-1], evaluation should be finite and within a certain range that should be handleable but it will still be large because remember each multiplication doubles the bit-length and the number of multiplications we'll have is log(poly_size). Note we are doing two evaluations, one over Z then one over Z_q but this is because the final evaluation for succiness is best done over Z_q, not Z. We want to say that if the claim to p is false, then the claim to q is false. Well if the claim over Z_p is false then evaluation over Z will not end in correct remainder and therefore be false. Then we want to reduce eval over Z to eval of Z_q. That is, if the claim over Z is wrong, then the claim of Z_q will be wrong. But hmm, this could go wrong by giving a wrong answer that agrees with the right answer modulo q. So idk.
How can we extend this to extensions? Maybe only option is extending to extensions over Z.

also, I'm thinking it may be worth mentioning in the paper how constructions go from complex to simple and my contribution is further simplification.
)

Organization
Section 2 is definitions and overview of PCPP for RS
Section 3 is zero-tester
Section 4 is reduction from SAT to algebraic problem
Section 5 put together other parts for final PCP
Section 6 generalize zero-testing to multivariate case

2. PROOFS OF PROXIMITY FOR REED-SOLOMON CODES

Distance between x,y\in\Sigma^n is noramlized hamming distance \Delta(x,y) = |{i : x_i != y_i}|/n = Pr_i (x_i != y_i).
C, the code is a subset of \Sigma^n. \Sigma is a field, and C a linear error-correcting code (since its a poly).
Distance for x to C is \Delta(x,C) = \min_{y\in C} \delta(x,y). If C is empty then \Delta(x,C) = 1 because it will evaluate differently to another element with probability 1.
Say "x is \delta-far from C" if \Delta(x,C) > \delta, and "x is \delta-close to C" if \Detla(x,C) <= \delta. Thus for fixed \delta, every element is exclusively either \delta close or \delta far from C. 

Proofs of Proximity
Make few queries to both instance and proof.
Soundness is a function of proximity parameter \delta.

Definition 1: (PCPP)
C\subset \Sigma^n has a PCPP over \Sigma of length l(n) with query complexity q(n) and randomness r(n) and soundness s(.,n), if there exists verifier V that tosses r(n) coins, makes q(n) queries to (x,\pi)\in\Sigma^{n+l(n)}, and satisfies the following.
completeness: x\in C => \exists\pi\in\Sigma^l(n) st. V accepts (x,\pi) with probability 1.
soundness: \Delta(x,C) >= \delta => \forall\pi\in\Sigma^l(n) verifier rejects (x,\pi) with probability >= s(\delta,n)

Consider C\subset\Sigma^n to be a property. PCPPs are defined for any property, but we only care about one in particular.

Definition 2: (RS Codes)
For polynomial P(z) over F and S\subset F, define its evaluation table over S to be <P(z)>_{z<-S} := <P(z):z\in S>. The RS Code of degree d over F, evaluated at S
RS(F,S,d) := {P(z)_{z<-S} : P(z) = \sum_{i=0}^{d-1} a_iz^i, a_i\in F}
The fractional degree of the code is d/|S|.

Field over p^l can be thought of as a vector space of dimension l over p. 
S\subset F is 'linear' if its a subspace of the vector space. 

Theorem 4: (Binary RS PCPP)
There exists universal constant c>=1 st. \forall F of characteristic 2, every linear S\subset F with |S| = n and every d<=n, the RS code RS(F,S,d) has a PCPP over alphabet F st.
1. l(n) <= n*log^c(n)
2. r(n) <= log(n) + c*log(log(n))
3. q(n) = O(1)
4. s(\delta,n) >= \delta/log^c(n)
and a verifier for RS(F,S,d) can be generated in time polynomial in |F| given (F,S,d). 

Notation has <w> = {w^0,...,w^{n-1}} where n is the order of w in the cyclic group F^*. 

Theorem 5: (Multiplicative RS PCPP)
There exists universal constant c>=1 st. for w\in F^* of order n a power of 2, S=<w>, and d<n, RS(F,S,d) has a PCPP over alphabet F st.
1. l(n) <= n*log^c(n)
2. r(n) <= log(n) + c*log(log(n))
3. q(n) = O(1)
4. s(\delta,n) >= \delta/log^c(n)
and a verifier for RS(F,S,d) can be generated in time polynomial in |S|+log(|F|) given (F,S,d).

n suffices to be poly(log(n))-smooth, all prime factors of n are at most poly(log(n)). And if we have characteristic <= poly(log(n)) then we use the additive group instead. Above is the case for multiplicative case of 2-smooth n. I don't understand why smoothness matters. 
We can boost soundness to an arbitrary constant < 1 by more sampling and averaging. This means trading query complexity for soundness.

Corollary 6: (Arbitrary Constant Soundness)
There exists universal constant c>=1 st. \forall \delta\in(0,1) we have a PCPP for RS(F,S,d) over F as in either Theorem 4 or 5 st.
1. l(n) <= n*log^c(n)
2. r(n) <= log(n) + c*log(log(n))
3. q(n) <= log^c(n)/\delta
4. s(\delta,n) >= \delta/2
and a verifier for RS(F,S,d) and \delta\in(0,1) as above can be generated in time polynomial in |S| + log(|F|) + k when given (F,S,d,\delta) with k being the bit-length of \delta. 

Sketch of Proof for Theorem 4
Consider P(z) with degree < n/8 evaluated on subspace L of dimension l and cardinality n. 
Create Q and q such that P(z) = Q(z,q(z)) with both Q and q of degree about \sqrt{n}. 
Let L_0' and L_1' be subspaces with same dimension l/2 and direct product L.
Let q(z) = \prod_{\alpha\in L_0'} (z - \alpha)
Apparently q is linear over L.
The kernel of q is clearly L_0'.
Image of L under q is subspace L_1 with dim(L_1) = dim(L_1').
Let T = {(z,q(z)) : z\in L}.

idk the rest.




FRI
Block-length N, prover complexity 6N, verifier 21*log(N), query 2*log(N), soundness min(\delta(1-o(1),\delta_0). 



1 INTRODUCTION

Algebraic coding theory.
|S|=N. S called 'evaluation set'. 
\rho\in(0,1] is called 'rate'.
RS(F,S,\rho) is set of functions f:S->F such that f is evaluation of poly over S of degree d < \rho N.
RS Proximity Problem gives verifier oracle access to f:S->F and verifier must determine whether f is \delta far from the code, doing so with large confidence and few queries.

RS proximity testing: 
When no additional data is provided, d+1 queries are enough, and we say prover gives zero effort to a make a proof of length 0 involving 0 interactions. 

RS proximity verification - PCPP model:
RS Proximity Testing but with access also to oracle proof \pi. 
Proof length, |\pi|, is measure over alphabet (F) length. 
PCP Theorem shows this can be proved in poly N time with constant query complexity and log randomness. 

RS proximity verification - IOPP model:
Generalization of IPs and PCPs. Interactive proof where verifier only queries prover messages at a few random places. 
Prover messages are \pi_1,...,\pi_r for r rounds. 
Query complexity to total number of alphabet elements read from f and \pi_1,...,\pi_r.
Prover is given f (no need to produce it itself) and prover complexity is time to generate all prover messages. 
Proof length is |\pi_1|+...+|\pi_r|.
IOPPs can be used to reduce proof length of PCPPs with no compromise to query complexity or soundness. 


1.1 Main results

New IOPP for RS codes, called Fast RS IOPP (FRI).
Analysis relies on quasi-linear RS-PCPP of BS08. 
Strictly linear prover complexity, strictly linear verifier complexity, constant soundness.

IOP
An IOP system S consists of algorithms (P,V). Input is x, with |x|=N, rounds r(N), and proof length l(N), query complexity q(N). Note this not a proximity proof so x is given to verifier and query complexity to x is not counted. 
Usually q(N) << l(N).
Every round starts with the prover sending a message in oracle form, and verifier responds with message. I think first message by prover is not f, but \pi_1. I think final message by verifier is decision.
For FRI, q(N) = O(log(l(N))).
<P<->V>(x) is output of V, either accept or reject. 
IOP is 'transparent' if all verifier messages x are publicly random and all queries come from these public coins (like AM family). 

IOPP
Generalization of PCPP to IOP model. Limit definition to codes for this paper.
Again, (P,V). Input is a code specification C = {f:S->\Sigma} for finite set S and alphabet \Sigma. Also give prover f^(0):S->\Sigma and verifier oracle access to it, where this function may not be in C. 

Definition 1.1 (IOPP)
An r-round IOPP S=(P,V) is a (r+1)-round IOP. Say S is r-round IOPP for C with soundness s^-:(0,1]->[0,1] wrt distance measure \Delta if the following holds.
1. First message format: first prover message is f^(0) is claimed codeword. 
2. Completeness: Pr(<P<->V> = accept | \Delta(f^(0),C) = 0) = 1.
3. Soundness: \forall P^*, Pr(<P^*<->V> = reject | \Delta(f^(0),C) = \delta) >= s^-(\delta)
Proof length is sum of all prover messages except f^(0). Prover complexity also disregards f^(0). Query complexity, however, accounts for f^(0). (Remark 1.2): Verifier decision complexity is, say, arithmetic complexity which accepts problem input and queries and query responses. 

Main Theorem
S is additive coset of F_q if coset of additive group, ie additive shift of additive subgroup. 
Additive RS code family is codes of form RS[F,S,\rho] for additive coset S. Called binary if F is characteristic 2. 

Theorem 1.3 (Main - FRI properties)
Binary additive RS code family with rate \rho = 2^{-R}, R>=2, R\in\N with \rho*N > 16 has an IOPP called FRI with following properties where N=|S| is block-length. 
1. Prover: Arithmetic complexity less than 6N. Proof length less than N/3 field elements. Round complexity at most log(N)/2.
2. Verifier: Query complexity 2*log(N). Decision arithmetic complexity at most 21*log(N). 
3. Soundness: Exists \delta_0 >= (1-3\rho)/4 - 1/\sqrt{N} st. \forall f \delta-far from C is rejected with probability >= min{\delta, \delta_0} - 3N/|F|. 
4. Parallelization: Each prover message can be computed in time O(1) on PRAM with CREW, with each arithmetic operation taking unit time. 

Remark 1.4
A multiplicative group H\in F_q is 'smooth' if |H| is power of 2. Family of smooth RS codes is when S = H. Theorem 1.3 with smooth RS codes has smaller constants.
Theorem 1.3 for generalized rate has larger constants.
Also can be generalize to groups of order c^k for constants c and k. 

Soundness for Theorem 1.3 is nearly tight for \delta <= \delta_0. 

Conjecture 1.5
Soundness limit \delta_0 of Theorem 1.3 approaches 1-\rho. Forall \delta <= 1-\rho rejection probability for all f that is \delta-far with rate \rho is at least \delta - 2*log(N)/\sqrt{|F|}. 


1.2 Applications to transparent zero knowledge implementations



2 Overview of the FRI IOPP and its soundness

Overview of IOPP for smooth RS code.
Section 2.1 about completeness.
Section 2.2 about soundness.


2.1 FRI overview and similarity to the Fast Fourier Transform

Let w^(0) generate a smooth group of order N = 2^n denoted L^(0) in field F. 
Prover claims f^(0):L^(0)->F \in RS[F,L^(0),\rho]. That is, f^(0) is evaluation of poly P^(0)(X)\in F[X] with deg(P^(0)) < \rho*2^n, eg assume \rho = 2^{-R} for R\in\N. 
From IFFT we know f^(0) = P^(0) implies there exists polys P_0^(1),P_1^(1)\in F[Y] with degrees < \rho*2^n/2 st
\forall x\in L^(0), f^(0)(x) = P^(0)(x) = P_0^(1)(x^2) + x*P_1^(1)(x^2)
Let Q^(1)(X,Y) := P_0^(1)(Y) + X*P_1^(1)(Y), and q^(0)(X) := X^2.
Then P^(0)(X) = Q^(1)(X,q^(0)(X)) = Q^(1)(X,Y) mod (Y - q^(0)(X)) ?
deg_X(Q^(1)) < 2 and deg_Y(Q^(1)) < \rho*2^n/2.
x -> q^(0)(x) is 2-to-1 on L^(0). Output is multiplicative group of order 2^{n-1} denoted L^(1). 
For every x^(0)\in F and y\in L^(1), one can compute Q^(0)(x^(0),y) from two entries of f^(0) since deg_X(Q^(0)) < 2. These two entries are the roots of y - q^(0)(X). 
Verifier samples x^(0)\in F and requests f^(1):L^(1)->F st. that is oracle of Q^(0)(x^(0),Y) on L^(1).
There is a 3-query test for consistency of f^(0) and f^(1) called the 'round consistency test'.
1. Sample y \in L^(1) and compute roots of y^2 s_0,s_1\in L^(0). 
2. Query f^(0)(s_0), f^(0)(s_1), and f^(1)(y), denoting answers \alpha_0, \alpha_1, and \beta. 
3. Interpolate line p(x) passing through (s_0,\alpha_0) and (s_1,\alpha_1).
4. Check that p(x^(0)) = \beta.
(Would make sense to commit to element of f^(0) in pairs like (s_0,s_1) since they will be queried together). 

In first round verifier sends x^(0) and prover responds with f(1):L^(1)->F. Testing consistency of f^(0) and f^(1) requires 3 queries. So we reduced testing f^(0) on L^(0) to testing f^(1) on L^(1) and the rate \rho did not change. We repeat r = log(\rho*2^n) = n - R times. f^(r) is a constant, or a code evaluated on domain of size 2^R, so prover just sends constants.
This protocol has completeness 1. 

Differences between informal and actual protocol
F is finite and binary.
The evaluation domains are additive cosets. Think of these as affine shifts over vector space over F_2. 
q^(0)(X) = X^2 is the Frobenius automorphism so not 2-to-1. Use another poly q^(0) that is many-to-one on L^(0) such that L^(1) = {q^(0)(x) | x\in L^(0)} is also an additive coset with |L^(1)| << |L^(0)|. q^(0) will be an 'affine subspace polynomial' from the class of 'linearized polynomials'. 
q^(0) is of degree 4, not 2, so round count is halved and query complexity stays the same. 
All queries are performed after prover sends all messages f^(1),...,f^(r). The COMMIT phase takes r rounds. In the i'th round, verifier chooses x^(i) and prover sends f^(i). Then in the QUERY phase verifier does consistency test for all r rounds. Query on L^(i) tests both consistency with f^(i-1) and f^(i+1) to save queries and apparently increase soundness. I think this means there is only one choise to make in the QUERY phase and all other queries deterministically follow by squaring. Well I suppose thats the case when using X^2 but for higher degree more randomness may be introduced at each round.


2.2 Soundness analysis - overview

Every round, ie instance of proof composition, incurs two costs for verifier in addition to the randomness. One is query complexity for consistency checking. The other is reduction is distance, ie soundness. 
Assume f^(0) is delta^(0)-far from all codewords. Then we need to prove f^(1) is \delta^(1)(\delta^(0))-far from all codewords. Larger \delta^(1) is better.
With high probability in FRI, \delta^(1) >= (1 - o(1))\delta^(0), ie distance reduction is negligible. Previous constructions have a multiplicative loss, limiting the number of composition rounds to log(N) thus requiring a high degree poly like square-root degree instead of constant degree like X^2. 
Suppose \delta^(0) < (1-\rho)/2, which is the unique decoding radius (for some reason). Then whp sum of round consistency error and \delta^(1) is at least as large at \delta^(0), meaning \delta^(1) is \delta^(0) minus a small loss (the consistency error).  



3 FRI - detailed description and main properties







What if we require the univariate poly in a certain form, eg many factors, and then use this special form. 
Take advantage of how rational polys are maximally far from a poly. 
Eg for a poly commited in root form \prod_i (x - a_i), randomly split it into two parts, and ask for the eval of the original poly divided by one part, and compare the result to the other part. 

Encoding the poly in an arbitrary way is our unique benefit so try to take advantage of that.
So consider using another form of univariate functions, like Laurent polys. 


In an extension field of characteristic p we have (x + y)^p = x^p + y^p.
Of course we also have (xy)^p = x^py^p. 
p(x) = \sum_{i=0}^{n} x^{pi}
p(x + y) = \sum_{i=0}^{n} (x + y)^{pi} = \sum_{i=0}^{n} (x^p + y^p)^i != p(x) + p(y)
so this won't generalize to large univariate polys.
So we can only have linearized polys with as many terms as the extension degree. 
But suppose we try multivariate polys.
p(x,y)^2 = (a + bx + cy + dxy)^2 = a^2 + b^2x^2 + c^2y^2 + d^2x^2y^2 = p'(x^2,y^2)
p(x)^2 = (a + bx + cx^2 + dx^3)^2 = a^2 + b^2x^2 + c^2x^4 + d^2x^6 = p'(x^2)
where p' is p with coefficients squared.
Let p(x,y) = a + bx^2 + cy^2 + dx^2y^2
p(x+x',y+y') = p(x,y) + p(x',y')
This means we can obtain a claim at a sum of points by taking the sums of claims. 
This is the same as what I was thinking about where we do a small univariate poly but replace coefs with claims of other small polys, etc, branching out. 
So I'm thinking this might be an alternative to reducing two points to one on the same poly. But we just take random linear combinations because multiplication in extensions is not repeated additions. Thus we can only make randomized sums. 

But also apparently a linearized poly can be constructed by the zero poly on a subspace. But these polys are not user defined, so seemingly unhelpful. 

What if we did analsis with respect to high degree polys. Then for the consistency test we use two domains F and L. Commit happens on L, so consistency soundness is |L|/|F| if we sample on F, but we didn't commit on F. The solution I'm hoping for is interpolating on values on F by checking multiple values on L. I don't think we can do this. 


For univariate sumcheck to reduce multi to uni eval, use domain (which is subspace anyway I think) that forms a tree of roots so we can plug into the points poly to select monomials without the need to expand that poly then re-contract it. This will mean as high degree as the data poly but multiplying only doubles degree, so they can be split into two if we'd like to maintain degree. 
Actually I think we can do multivariate sumcheck, but for prover univariate sumcheck may be quicker. 

An alternative to the lattice scheme I have is to do a hash tree using homomorhic hashes, eg lattices, then operming the homomorphic operations upon opening. This effectively enables opening many trees by opening a single one. Maybe another source of randomness could be reversing a trees evaluations. In fact, maybe many more manipulations are possible. All are possible because we still end up walking down a tree, but by homomorphism each step that would be necessary on many trees can be reduced to one tree. 
Maybe we could use this method to instead efficiently open a single tree in many places at once. Really you'd end up opening a random sum of leaves.
We'd still store multiple values in each leaf.

Note that hash trees only involve the entropy at the base and all else is derived on top. And note that a hash tree is only of log depth. Thus it can be calculated in a proof with only the entopy of the leaves. 

note that every pair in tree i+1 will be mapped to from two pairs in tree i. therefore it comes at no extra query cost to check both of these sources. this can be generalized. so it makes sense to structure tree i appropriately. but note that of course this deterministically-dependent query has not randomness. 

we will try to simply and be consistent with notation by using only 'polys' instead of 'polys', 'codes', and 'extensions', as well as all their side-notions like 'interpolents'. 

think about decomposing a poly with model where coefs are linear polys. 

maybe we can get more power by making the new poly requested be a function of the queries we see. currently each new poly is decided with just a random element irrelvant of the semantics of the last prover message.


i'm wondering now if the homo hash openings will be possible with only doing one opening. when we open we want to make sure that if prover has opening for the sum, then prover has openings for all parts of the sum. if we use an extractor, then given a prover that knows openings for the whole with non-negligible probability, we must also have it find openings for each part with non-negligible probability. I'm afraid that like the heavy argument this require many tests. 
note this is homo approach is only real contribution to univariate stuff and the only motivation to understand it. 

if the homo won't work then the only thing we can do with univariate stuff is give a reduction from multivariate and delegate the rest to other research and saying it may be used when wanted. we will only know which is better between lattice and univariates in terms of prover time and proof size by implementing.
if homo requires many tests then that destroys the benefit for the most part unless we can reduce the number of those tests enough which is already a problem we'd like to solve for the lattice setting. 
if it turns out homo does not require many tests then we still have the question of whether proof size will be sufficiently shortened. the worry is that the verifier still has to hold a large number of trees before the random selection can begin. but whereas with a regular hash tree each path must be traversed separately, with homo all selected paths can be traversed at once. 

the problem for homo is a bit different than for lattices. for lattice we want to ensure preimages for all targets so we randomly select subset sums to test. for homo we want to ensure preimages only for a randomly selected subset. 
For lattices we also need a certain number of tests to satisfy zippel. we are considering a linear poly, ie of degree 1. So for a fixed subset, the soundness error with coefs in {-1,0,1} is 1/3. But this randomization does not count the randomization of selecting the subset in the first place. 

I think there may be a problem with homo in that you may only be able to traverse the same route on all trees. this is ok if we are amortizing across independent trees, but subtrees of a single tree representing a single function are far from independent. therefore we would need to do this by splitting the poly into many and for each a different tree, then performing all queries that a tree of that size usually needs but doing so simultaneously across all trees. now we need to perform many queries as usual but they will all be of smaller size. this may make the need for multiple tests no longer a problem. each test could correspond to a query, and that query would be performed on a random selection of the trees. Would this querying satisfy the tests soundness? suppose there are as many queries as tests. think of each set of vertices across trees for each vertex position. each set would require many tests but the further down the set in tree depth, the less likely we are to touch it so the less tests it will get. this seems a problem.

I realized we can in fact traverse a single tree with many different routes simultaneously, but at some extra cost. Also, the question of multiple tests still remains. Actually this scheme I'm thinking of would save no space and require just as much data traversing each path separately, because randomizing the direction of descent involves accepting all children, then partitioning them by direction. 

try the heavy argument using a witness multiple times.


I think we can state and prover all necessary proof of knowledge only with respect to 'entropy dropping' mechanisms. this gives indication that this concept is special and deserves separate attention and analysis. our two entropy dropping mechanisms for now are homomorphic commits and hash trees for univariates. we need to prove that all proof of knowledge can indeed be reduced to proof of knowledge for entropy dropping. 
It would be great to also isolate zk proofs to a particular module. Maybe we could also reduce it to the entropy dropping phase (maybe I'll still call it 'molting'). 
Apart from entropy dropping, I think we'd call the other part 'arithmetization'. 
It'd be great to create a modular structure where all modules are proved and thus putting them together in any way yields a proof of the whole thing. 


can we make a relationship between ways of 0 testing. multivariate vs univariate. multivariate has log rounds but linear prover. univariate has one round by nlog(n) prover. maybe the relationship could be that multiplying the number of rounds by the prover complexity yields nlog(n) optimally. note that this distinction is important because we may often want to use univariate because extra prover time of univariate can be cheaper than the fiat shamirs for multivariate in many cases. 
we'd like to prove this relationship. also, we'd like to obtain a way to place ourselves anywhere on this spectrum. I think the latter can be done by some variation of multivariate sumcheck and univariate sumcheck. and I'd like to explore if theres any other way in the finite field setting to do this without sumchecks. 
actually we need to distinguish between vanishing vs summing. multivariate sumcheck is find for both. univariate sumcheck is for summing and I think only takes linear time (but for summing to 0). poly div is for univariate vanishing and takes quasilinear time. 
remember we can reduce vanishing to summing. We can also reduce from summing to vanishing by having constraints forming an addition tree. 

try composing univariate sumcheck, like with multiple rounds like the multivariate sumcheck.

for code-base could use 'indexing' like fractal where we use univariate polys for the code and thus these polys can be pre-evaluated into a tree, so verifier can obtain evaluation without need to tail-recurse. this may only be helpful sometimes. but hmm, this means committing over all of F, which is too much. DEEP-FRI allows out of domain sampling but this isn't determined during pre-processing. so we may need to evaluate only over the pre-commited subset of the field many times for enough soundness. 

still to figure out fully, but they can arithmetize a matrix, or really any data, so that verifier can figure out the function at a random point. 


use alternative basis for univariate commit. 
take advantage of how function with small range, eg predicate, can still encode a lot of information. also take advantage of how upon composition a small range is a small domain. 

I think we could outsource finite field multiplications, because they can be probabilistically checked by Zippel. But this is unlightly worth the bandwidth and the verification cost. 

maybe it would be worth trying other representations of fields. for example, I woudn't be surprised if other forms of polys, like multivariate polys of fixed degree could represent a field. 

maybe could say entropy dropping requires some homomorphism, eg for polys its f(x) + g(x) = (f+g)(x) which as a homo is
eval(f) + eval(g) = eval(f + g)

try using machine language as the native language for a snark. translate the crypto problem into that language. the constraint is we need a language with randomization and holographic properties. we tried the language of lattices and couldn't find holographic properties. 

consider alternative homomorphic commits, because anything homomorphic drops entropy.
On is something like H(S) = \prod_s h(s) or something similar with sum or any other commutative and associative operation. h is any hash. security then is not just reliant on h. suppose h is collisian resistant. must also be resilient to relations, such that one cannot find two subsets such that their hashes accumulate to the same value. suppose h was a truly random oracle. 

note that instead of vector commits I think we could also use homomorphic hash trees. but we may still have trouble with randomization. the benefit, however is that q maybe smaller and enable smaller proofs. maybe we just do the same testing before but with vector commits replaced by tree commits. so given a set of tree roots a single test entails taking a random sum (or linear combination with small coefs) of them to derive a new root. This could be it, just like with vectors, and opening would require opening all branches of each tree. But maybe we could take advantage of the tree structure. For each new root of each test, prover could submit children. I think all these child trees would have to be re-normalized. But then with now two children for every root, random combinations could be taken again. Repeat a lot number of times to finish. So it seems this could be an alternative to basis-changing, each with own benefits and drawbacks. 


secret cryptography seems the dual of hash based, like SIS is dual of LWE. with secret crypto one starts with a small secret and computes a larger output (maybe composed of multiple parts) from it that are public. with hash crypto, one starts with a large input and computes a small output. with secret crypto the task is to compute the small input part given access to the large output part. with hash crypto, the task is to compute a (there are many possibilities due to the assymetry) large input part given access to the small output part.
like for hash inversion, for secret crypto there are many ways to map a small input to a larger one. this is where the randomization of encryption comes in. 
a mapping (or rather a relation) cannot serve both purposes at the same time, because each requires efficiency in one direction and hardness in the opposite.
note how we could compose such systems. one direction gives a mapping from large to large first by contraction then by expansion. the other direction gives a mapping from small to small first by expansion then by contraction. its interesting to consider the hardness of these composed relations. 
I suppose we should restrict to binary relations that are partitions, ie equivalence relations among the larger set according to what they map to in the smaller set. this is because we have functions both ways. Hashing as we consider it is deterministic so a large will only map to a single small. Secrecy as we consider it is like encryption where a large maps to only one small. Note these two constraints are identical.
suppose we consider category of sets with such binary relations. for morphism composition to hold, a set must be partitioned the same as a source and target.

if we'd only like to rely on ad hoc crypto functions with no structure, I think we need to consider more structures with homomorphic and also holographic properties. Both of these seem hard to establish, especially simultaneously. 

note how differentiating a function reduces degree and sheds one coeficient, ie drops entropy. integrating requires specifying a new coefficient, increasing entropy.

Other than the question of entropy-dropping, I think its interesting to consider reducing sequencial computation to parallel computation. testing properties and entropy-dropping usually happens in parallel, whereas the computation we want to reduce from is sequential. it should be interesting to compare trading entropy for parallel computation. the simplest formula that comes to mind is increasing entropy by a factor of n enables cutting sequential length by a factor of n. 
this goes along with the question of benefits of reversible computation. a simple way to use reversible computation is take the trace, fold it in half, then proceed from both ends ending at the middle where that is the entropy at the top. I don't know how to extend this to more than halve the trace length. 


what about lambda calculus where each bounded variable must be used. maybe say each must be used exactly once. then recursion is not possible I think. the goal is introducing enough structure like a group where things don't just disappear, ie no absorption. I'm sure this limits the power of the language. Further, exactly one use of each variable bounds the size of a term with a certain number of variables so maybe we could think of the space of terms with no more than n variables. 
\x.x
\x.(x \y.y)
\x.(\y.(x y))
\x.(\y.(y x))
but composing two, for examle the latter with itself, increases from 3 to 4 variables. 
\x.(\y.(y x)) \x.(\y.(y x)) = \y.(y \x.(\z.(z x)))
can we expand it further? I don't see how.
In fact, it seems the above terms all together are closed under composition. But its not a group because there is no identity. This means it may be interesting. \x.x is a left identity but only a right identity for the first two. Here's the full table. Entry (i,j) is (i j) that is j applied to i.
__1_2_3_4_5_
1|1|2|3|4|5|
2|1|1|1|2|4|
3|1|2|3|4|5|
4|2|*|*|5|*|
5|4|2|4|5|5|
------------
1 (lambda x.x)
2 (lambda x.(x (lambda y.y)))
3 (lambda x.(lambda y.(x y)))
4 (lambda x.(lambda y.(y x)))
5 (lambda x.(x (lambda y.(lambda z.(z y)))))
Actually I was wrong, they are not closed, and the entries with the star are outside this set. But it seems in all cases its invariant that each variable is used once. And the number of variables of terms with n and m variables is at most n+m-1. Notice is only 4 as the function that expands, just waiting for an input to serve as the next function.
(4 2) is \x.(x \y.(y \z.z))
(4 3) is \x.(\y.(\z.(y z)))
(4 5) is \w.(w \x.(x \y.(\z.(z y))))
Notice that the latter is the first with 4 variables and arises in the same way that 5, the first one with 3 variables, did. 
Of course these operations are not commutative. And the example below, it is also not associative. 
((2 3) 4) = (1 4) = 4
(2 (3 4)) = (2 4) = 2
Counting reptitions in the table we get:
Count of 1's is 5.
Count of 2's is 5.
Count of 3's is 2.
Count of 4's is 5.
Count of 5's is 5.
So maybe there is enough variance for randomization.
We'd like to reduce evaluation at multiple points to evaluation at one point. The standard method involves plugging in an interpolation function of the same variable for all variables, thus breaking the property of only having one instance of each variable. Hmm, but this may not be a problem because we know feeding one of our terms to itself results in another term. Given the below it appears that even when we plug in all for one variable we may not be able to reduce until we know that variable. Thus the standard technique wouldn't work.
((x a) b)
((x c) d)
((x e) f)
\y.(y \x.(\z.(z x)))
= ((x a) b) \x.(\z.(z x)))
= ...
But notice that we only need one variable to keep an arbitrary amount of info, in the form ((((x a) b) c) ...). What if functions were always (supposed) to be of this form. But still, how do we go about reducing evaluation. We still have problem of irreducibility. Like polys being highly parallel, maybe we can choose a special format for our encoding also parallel allowing reducibility. For example, a tree form like (((x a) (x b)) ((x c) (x d))). Forget about interpolation and how prover chooses these constants, focusing on reducibility. When we plug in an interpolation poly for x we still won't be able to reduce until we know the value of x. So this won't work either. Maybe we should treat the variable as the argument, allowing reducibility, disregarding the further complication of prover interpolation, thus (((a x) (b x)) ((c x) (d x))) or (a (b (c x))) though the latter may not retain all the info because its partly reducible before knowing x. Well actually it seems out disassociativity and uncommutativity leave us a contradiction in our goals. We want the function with unknowns to be irreducible, but then we plugging in an interpolation poly that still contains unknowns we want it to become reducible. If we can't use the techniques of low-degree polys, how about those for high-degree univariate polys. Like a poly, we can break evaluation down into evaluating two parts. But can we then do the the equivalent of a random linear combination to reduce those two back to one? What means to we have to randomization anyway? Randomization requires changing part of the structure while keep other parts invariant, which seems hard in a system with little invariance properties. 

Groups in general may be easier becuase we have associativity. Or maybe action on a group. To try standard reduction version, we need a function that upon plugging in an interpolation function can be reduced. One property we probably want that's probably not hard to achieve is upon plugging in for one variable and the reducing, we get smaller version of the same function pattern. Something else we want, probably worth thinking about now because its the problem that stopped us above, is randomly combining two into one. randomization seems possible simply by randomly selecting an element and hitting on the left or right. Now we'd like a function form where two can be combined to one of the same size. Actually I think we might have tried this before and failed. Indeed, this is even the problem we had with expander graphs. Lets look for patterns for which two can be reduced to one.
xax' is one as (xax')(xbx') = x(ab)x'
But thats about it for what I can think of.

Another option may be a lie algebra. This would be similar to a field except for multiplication. We'd probably have the equivalent of multilinear polys. We can subtract sides of an equation and from commutativity of addition and distributivity I think we can arrive at asking the probability a single poly evaluates to 0. I don't think there's a universal answer to this like for fields, and instead depends on the lie algebra. Oh, and since we need multivariates to encode function we can't use hash trees for commits but will need direct crypto. Maybe expander graphs, but even that is risky. 

gosh, I don't know, maybe I should just stick with finite fields and stop looking for alternatives. the goal for alternatives to get more examples so that standardization of abstract ideas has more than one example as evidence. Necessary properties we've been looking for so far are being holomorphic and homomorphic, that is respectively that two different instances examines at random places will be different with high probability, and given two instances they can be reduced to one. Many examples have one property or the other but not both. I think lambda calculus is holomorphic, groups are holomorphic, lattices are homomorphic, and lie algebras homomorphic. The homomorphic property is used to reduce the examination of two to one. Along with the homomorphic property I think we need sufficient randomization. I think both these concepts can be formalized in an abstract framework. 


note assuming one way permutation, provers chosen values can be transformed to random values at no cost to entropy. 

multivariate crypto for signatures looks interesting because it compresses input, and although not linear, it operates over the whole field and thus doesn't need normalization like lattices. it may be a good candidate for tree-based constructions.
since we operate in the whole field a source of randomization can be just multiplying each claim by a constant, corresponding multiplying all coefs by the constant.

question shift from prover having coefs and point chosen by verifier, to prover having point and coefs chosen by verifier. 

verifier chooses random point of evaluation of multivariate. prover passes values through hash to get coefs and computes eval and submits claim. Verifier can choose random line passing through evaluation point and ask for small univariate poly representing that eval. verifier checks uni as usual, chooses random point on line, computes new claim, then prover is left with new point to prove. this means prover must find a poly that evaluates at this new point to the new claim. this task requires the prover to solve a relation whence input coefs after transformations yield a poly with a fixed evaluation at this point. this is a relation-intractable problem which may be hard to prove. I'm wondering if we could reduce the relation intractability to that of the permutation function which could be arbitrary. Suppose yes. Then for multiple commits we can reduce their point to a common point. But we can't take a random linear combination and preserve knowledge, unless we use a homomorphic hash which is our original goal. 

I'm realizing the advantage of multivariate crypto over just a hash is that the function can inverted given secret info. Thus unless we can operate in a secret setting this doesn't seem of use. but this doesn't throw out the idea above which is independent of multivariate crypto, and doesn't even assume its hard to find input given output. 

Assuming the function is hard to invert, we have a hash with the structure of a multivariate poly. can we use this structure? we know how to use it to probabilistically with ineraction reduce evaluation from two points to one, but this is just as costly as evaluating the two points separately.

Return to where the prover has the point of eval. Using implicit change of coefs, verifier can randomly change point of evaluation which should result in the same claim. This can be combined with multiplying the claim by a random constant. The point is we can achieve full randomization with one commit unlike lattices requiring many commits. 
But alas, we still can't find a way to reduce two to one. One attempt would be by means of first achieving a common point but we don't know how to do that. 




Maybe I should try other math objects only for the final test of oracle property testing. must reduce poly evaluation to testing another object. of course we need whatever the object is to be representable efficiently in hash trees, which basically just means I think that there is one query variable. So maybe its most helpful to map directly from univariate polys. I realize this is really the question of codes by way of focus on redundantly encoded data, but I don't understand what properties of codes we want, like is decodability good or bad for our purposes?

linear codes can be resenting by matrices, mapping small space to larger space. Can think of input vector as data and output vector as oracle to query. So for input u and output v and matrix H we want to verify that given v, there exists u such that v = Hu. Note that H is left-invertible, that is there is H' (not unique) such that H'H = I (but probably HH' != I). If there is satisfying u then we have
v = Hu
=> HH'v = HH'Hu
=> HH'v = Hu = v
Now if for all u we have
v != Hu
=> v != H(H'v)
Thus testing whether there exists u such that v = Hu, is equivalent to testing whether HH'v = v. Hmm, it doesn't seem like this is dependent on H', like H' need not be random with respect to v. Actually we can have a 'check' matrix M = (HH' - I) such that v is valid iff Mv = 0.

Note how the general idea of mapping from a small space to a large space is like the encryption kind of crypto, the dual of the commitment kind. 

it may be easier to work with groups than finite fields. for most simplicity we work with abelian groups. suppose the group is known. suppose prover data is a number of group elements. 

I think the framework in terms of the recursive linear codes we're interested in could be simplified in terms of homomorphisms, reflecting the linearity. Suppose we have a space G, and a homomorphism to a smaller space H. An element in G is valid  if it maps to the kernel of the homomorphism. Our goal is to reduce testing an element in G to testing an element in H. Suppose we are testing g in G. Suppose there are two element in H, h and h' such that g = h + x*h'. For now we are assuming these sets have two operations, but maybe later we restrict to one and have something like g = h + h'^x. For random r, prover commits now to h + r*h'. With homomorphism f, note f(h + r*h') = f(h) + r*f(h'). Now suppose there is a lemma that if either of two elements of H are far from the kernel then with high probability, a random combination of them is about the same distance. so if g is far from the kernel, then so is the supposed new commit. Now to test that the commit is as supposed, verifier does a consistency check between the oracle on G and the oracle on H. Suppose that by querying g in multiple places, verifier is able to query h and h' individually, thus computing the supposed h + r*h'. Verifier does it multiple times, then passing the result through the homomorphism, and checking that with the commit on H. Suppose that if two elements in H are different then they are different almost everywhere. So in total it appears we have 3 conditions for this to work. The first is the distance, the second is homomorphic property, and the third is the holomorphic property. But note that like in FRI, the holomorphic property may be conditioned on distance.

In FRI, G is the space of functions on a space S, a subset of a finite field. H is the space of such functions but on S^2, and |S^2| = |S|/2. Suppose M is the check matrix for the linear code. Then the homomorphism is multiplication by M. Decomposition is g(x) = h(x^2) + x*h'(x^2). For DEEP-FRI, the spaces are not deterministically related, and the poly goes through a small transformation before the homomorphism.

If we operate over a finite field, then we know that with probability 1/|F| over r, if Mv != 0 or Mv' != 0 then M(v + r*v') != 0. This is an absolute statement, however, and not in terms of hamming distance. Its the hamming distance that is hard to prove. Is there some way to commit and query such that we don't need hamming distance? This means a new approach to consistency testing. 
One possibility that still uses the tree format but tests absolutely rather than querying randomly is one that is probably inefficient in practice. We will use the 'isomorphism theorem'. The new commit is with respect to a poly of half the degree of the original, ie h(x) + r*h'(x). To test for absolute consistency we'd like to query the poly on every element in S^2 and from that compute g at that point and test for equality. Can we do this? Note g(x) = h(x^2) + x*h'(x^2). Querying the new poly we can get h(x^2) + r*h'(x^2), but we need h(x^2) and h'(x^2) separately. So alas, this only work when querying the new poly at r^2 thus computing g at r. 

Suppose we try for an abelian group. 




On possibility for performance, though it requires bandwidth, is a pool of provers using an iso to randomize their data for zk, not losing entropy, then sending the vectors (high bandwidth) to a super prover who joins them together and recursively reduces them, then outputs a proof that somehow can be combined with a proof of each verifier for a proof for that single verifier. 


I was thinking a way to drop entrop using the concatenation-homomorphic hash is by the many to one mapping of how the ordered (non-commutative) output could be split into an ordered list of ordered input strings in many different ways. Eg output abc could be made by either (ab)c or a(bc). I think this an associativity based analogue of the commutativity based scheme where {a,b,c} can come from either {a,b} + {c} or {a} + {b,c}. So we are effectively dropping the entropy of choices on where to cut the string. So the information is not encoded in the final string itself, or any of the characters of the input, but purely the cutting of the input. Note that the ordering of the input is not even relevant because that ordering is determined by the output. And note that characterizing in terms of commutativity and associativity is only partly true, because in neither case are we dropping the full entropy of commutativity or associativity, in particular only the entropy of how the inputs are combined, not any entropy within the inputs themselves. So in both cases the entropy used is the organization of data, but I'm not sure how to extract this entropy for use. This is because the entropy data is not explicitly given. So maybe this is unusable because we are operating in a scenario of verification where non-determinism is needed. If the data was given in whole to the verifier rather than via commit, then the verifier combined them as output, the input to the verifier would clearly be of higher entropy than output and verifier would have an easy way to extract the entropy. But in our case the verifier is not given the data explicity but rather given commits to the data, and it would require non-deterministc extra data by the prover to specify how to open the commits, exactly cancelling any benefit. So to take advantage of this entropy we'd need a verifying strategy where the data can be presented in any form (for associativity in particular the input can be presented in any sliced form) without requiring non-deterministic help from the prover to interpret it. Note that the data itself need not contain entropy, and could even be a fixed block of data. 
Let us focus on the associative case. In this case its the length of each slice that matters, not the contents of the slice. Somehow the verifier must infer the length of each slice. Variable slice count gives more entropy, but for simplicity we could first focus on a flixed slice count. Actually in the case of expander graphs I'm thinking this won't work at all because a commit size is log size the number of vertices in the graph, but what we are commiting to is a natural number of the length of a walk, which assuming no loops is upper bounded by the number of vertices, thus the commited value size is upper bounded by the commit size. Actually it couldn't work for any hash function for the same reason. The commit size must be at least the security parameter, and for the committed data to be at least as large as the commit size, and using natural numbers, the range of numbers must go up to exponential in the security parameter, which of course is too much to commit to. The commutativity case would have for potential but we don't know any post quantum assumptions for that.

It seems the above won't work for associativity with expander graphs. It could work with other hardness assumptions like commutativity for discrete log but I want established post-quantum assumptions, so for expander graphs I need to try another way to use its structure. 
Maybe we could use expanders for oracle commits, where the thing committed to is the random walk taken, and it must be checked for certain redundancy. But actually this is an instance of using crypto properties, not just algebraic properties.


One kind of code that comes to mind is the coefs of the poly taken by exponentiating the source poly by n. With N coefs, the rate is N/(n*N) = 1/n. There could be many variations of this. So the code is the coefs of a poly. Reducing this recursively would seem most naturally done by testing that a poly is small product of a source poly, then testing the source poly etc. Thus the problem reduces to small products. Consider quadratics. 
(a + bx + cx^2)(a + bx + cx^2) = (aa) + (ab + ba)x + (ac + bb + ca)x^2 + (bc + cb)x^3 + (cc)x^4. 
Suppose you are given full access to the source poly. Then you could quickly calculate any coef of the target poly and check it again the commited target poly. Note we're not really using polys, just binomial expansion.  


For oracle proofs I think we should stick with objects that are functions over a reasonable size domain, such that a single query corresponds to checking a single evaluation. This is because I don't know any more efficient way to commit for queries other than hash trees. Eg if we used graphs, a query would probably mean traversing at least one edge, and there's no clear way to commit to a graph such that each query can be efficiently opened. Well some representations, like adjacency matrix, but that only supports one kind of efficient query.


The lemma about hamming distance is easily solved when considering hamming distance in 'norm', ie distance from zero. Since linear codes allow us to work in a normed, rather than just a metric space. Note this isn't an official norm since we don't have notion of signs and order. Use |v| for norm. Suppose |v| = d and |v'| = d'. What is expected norm of f(x) = v+x*v' for scalar x. Well if a pair of entries of v and v' are both zero then they remain zero. If a pair has one non-zero and the other zero, then as long as x is non-zero, the result will be non-zero. If a pair has both non-zero then for random x the probably the result is zero is 1/|F|. Thus with high probability the size of the union of indices with non-zero entries. 
Now instead of considering the hamming distance of a vector from 0, we'd like to consider its distance from the code. Given a vector v, let c be (one of) its closest words. Suppose we have a homomorphism H with the code as the kernel. We would like to reason that if v is delta far from closest word c and v' is delta' far from closest word v', then H(v + r*v') = H(v) + r*H(v') will be far from 0 because H(v) will be at least delta far from 0 and H(v') will be at least delta' far from zero. To use this reasoning, we need the property that if v is delta far from the code, then H(v) is at least delta far from 0. 
So what linear maps H have this property? Let us express the property mathematically. Let c be the closest codeword to v.
We want: |H(v)| >= |v - c|
For dot product '.', what can we say about |v.u| in terms of v and u? Well first, what can we say about the norm of the point-wise product of v and u? An entry is non-zero iff both u and v have non-zero entries. The rest gets more complicated depending on the relationship of u and v. 
Can we characterize c in terms of v and H? Consider the generator matrix G that given a source s maps to codeword v, ie Gs = v. G is left-invertible, that is there is G' such that G'G = I. How does G relate to H? I think H = (GG'-I) such that Hv = 0 iff (GG' - I)v = 0 <=> GG'v = v. Remember there are many possibilities for G', so H is not unique. Suppose we have a G' such that G'v = G'c. Then GG'v = GG'c = c and thus we characterize c, the closest codeword to v. In this case our desired inequality would become |GG'v - v| >= |v - GG'v| which is actually an equality. So can we have a G' such that for all v and corresponding c, G'v = G'c? If not, can there be some probabilistic solution?
The equation G'v = G'c is an algebraic relationship between v and c which have a lexicographic (hamming distance) rather than algebraic relationship, because slightly changing lexicographic property may radically change algebraic property. Well actually since v - c is of small hamming distance we need a G' that can map every small weight vector to 0. This is not possible because suppose we have two vectors with hamming distance 1 that have the non-zero entries differing by one in the same index. Then it must map these two vectors to different values for that index, so both can't map to 0. One reason this doesn't work is because small weight is not closed under addition. 
Suppose for fixed v and thus c, there exists a G' of right dimension such that G'G = I and G'v = G'c. Then GG'v = GG'c = c. Thus we have |Hv| = |GG'v - v| = |c - v|. But note that H is dependent on v. 

Lets start again and be clear about our local goal. If v or u is far (from a and b respectively) then is v + r*u also far with high probability over r? In order to consider norm instead of distance, consider a homomorphism H with the code as kernel. There are many ways to pick H. Above, under an existence assumption, we can show that for an H depending on v, |Hv| = |c - v|. Then for closest codeword c to v + r*u, and H corresponding to v+r*u, we have 
|c - (v + r*u)| = |H(v + r*u)| by theorem 2
|H(v + r*u)| = |Hv + r*Hu| by homomorphism
|Hv + r*Hu| = func(|Hv|,|Hu|) by theorem 1
But we can't say much about |Hv| or |Hu| because H is in terms of v+r*u.
GG'(v + r*u) = c
Hv = GG'v - v = c - r*GG'u - v
Hu = GG'u - u = (c - GG'v)/r - u
idk how to use this.

Trying the other way with H's for v and u first, we have
GG1'v = a
GG2'u = b
GG'(v + r*u) = c
H1v + r*H2u = (GG1'v - v) + r*(GG2'u - u) = (GG1'v + r*GG2'u) - (v + r*u) = (a + r*b) - (v + r*u)
which gives us
func(|a - v|,|b - u|) = func(|H1v|,|H2u|) by theorem 2
func(|H1v|,|H2u|) = |H1v + r*H2u| by theorem 1
|H1v + r*H2u| = |(a + r*b) - (v + r*u)| by above
so we have related |a - v| and |b - u| to |(a - v) + r(b - u)| (probabilistically). 
We could get our desired final expression |c - (v + r*u)| if we can establish that c = a + r*b. It could be that a+r*b is much farther away from v+r*u than c is, in which case it wouldn't be surprising that |(a + r*b) - (v + r*u)| is large.
Suppose that a,b are unique.
What is closest codeword to v + r*u? I think it might not be c. Suppose on half the domain v differs from a and on the other half u differs from b. Then v+r*u could be different than a+r*b on the entire domain. Now this may be unrealistic, and a more realistic case has a smaller distance than 1/2. Suppose the minimum distance is d. Suppose v differs from a on d/2 of the domain and likewise for u and b but a different part of the domain. Then their combination I think would differ on at most d of the domain, which is more than d/2 so it could well be closer to another codeword than a+r*b.

Maybe we have enough freedom to construct an H that works for multiple points at the same time.
Suppose we got an H0 that works for both v and u. Then we could get farther then the second try above. We'd have
H0(v + r*u) = GG0'(v + r*u) - (v + r*u) = GG0'v + r*GG0'u - (v + r*u) = a + r*b - (v + r*u)
which is actually exactly as far as we got before, not farther.
So we'd need a G' such that G'G = I, G'v = G'a, G'u = G'b, and G'(v + r*u) = G'c.
Note that this means c = GG'(v + r*u) = GG'v + r*GG'u = a + r*b as we hoped for above.
So can we find a G' for 3 arbitrary points, and if not, can we take advantage of how c relates to v and u?
The freedom of G' depends on its dimension n and m where G is m by n, which forms the rate as n/m. The lower the rate the more freedom. Potentially this means we could do a longer linear combination, matching more than 3 points.
How do we match a point? Note that G has full column rank m. My naive assumption would be that for each of the n rows of G', n elements must be reserved for the inverse such that G'G = I. This leaves m-n elements free in each row. Suppose we want to match the point G'u = G'b => G'(u - b) = 0. Hmm, my naive assumption would say this only requires one more element for each row. In fact, since in all cases we multiply to the right of G', we could just consider the problem of solving for a G' such that for any target n by k matrix T we can solve for m by k matrix S such that G'S = T. In our case S will be of the form [G,v-a,u-b,v+r*u-c] which is of dimensions m by n+3 but we can consider for m by n+l for arbitrary l. My native assumption is that we could go up to n+l = m. Seems my assumptions are correct as long as the system is not inconsistent. We can view the equation G'S = T more generally as S^*G'^* = T^* where ^* is for transpose. To determine inconsistency we must compare the rank of S^* with that of [S^*|T^*]. The first n rows will be devoted to the inverse and shouldn't be a problem. The last l rows have unpredictable entries in S^*, but they will all have 0 entries in T^*, and thus should not be a problem. The only danger is the combination of the first n rows with the last l rows. The first n rows of T^* will all be 0 except for one which will be 1. So it is only this single line that could conflict with another. It is problematic if and only if this S^* row can be expressed as a linear combination of the other S^* rows. This can then be framed as another equation on whether Ax=b can be solved where b is the transpose of the special row of S^*, and the columns of A are the other rows of S^*. A is m by n+l-1 and b is m by 1. The system will be overdetermined with m > n+l-1 but a solution may still exist and this may be a problem. We may be able to solve this probatilistically because r is chosen randomly after v and u are committed. ... to be finished.
Note that finding G' does not enable efficient decoding, because it relies on knowing the closest codewords. 

Suppose we can finish the proof above and its valid. The theorem would then say something like for generators G of dimensions m by n with rank n, distance between elements is basically preserved under random combinations, modulo some details. How could we use this? We'd like to have a way to split a function (in the range) into (say two) parts, with the ability to query each part from an oracle to the original function. One way to approach this is to view the source as defining a function, and each row of G as an unfolded query. Maybe instead of formulating some explicit function for decomposition we could just think of matrices and matrix decomposition. I think we'd need the matrix to be structured, like a univariate poly matrix is. 
Alternatively, we don't need a structured matrix, and instead we start with multiple oracles of the decomposed parts, and we ask for a random linear combination, then do a consistency test. Actually all the oracles could be combined into a single tree, each leaf filled with multiple corresponding values. It is harder to argue that this reduces entropy. Clearly it can if theres enough decomposed parts to start and verifier fully opens the combined commit. Or rather the resulting poly is submitted to the verifier as plain data. But of course the verifier need not literally receive the poly, and could instead use a virtual poly and things can return to as normal with entropy dropped. So in this case we are not recursing directly, where the result is returned as an oracle which must then be decomposed somehow, eg by structure matrix queries. Instead, its returned as a virtual poly that can be decomposed by any proof means. What if it was returned as an oracle, but destructured in the same way as the original poly? Maybe this could be an alternative direct recursion technique than used by FRI. Direct recursion may have lower soundness than when using virtual polys, but that's yet to be analyzed.
Note that we don't need to use polys in the indirect recursion scheme and can instead use a random linear code. I think we could still reduce poly evaluation to general linear code evaluation, but I'll leave that for now. Can we take advantage of the computational infeasibility of decoding random linear codes? Such tasks are best suited for the secrecy setting. Well maybe it can contribute to soundness in that while prover won't be able to decode a random linear combination of noisy words, even knowing how to decode each of those words. it would be an interesting reduction to prove this. if this works it means the prover can't submit the closest poly because the prover can't find it, thus increasing the probability that whatever poly the prover submits will fail in the consistency test. LWE may be relevant. 

Maybe expander hashes, even with no homomorphic property, would still be useful for their algebraic computation over the field and also their flexibility in the length of input and consistency in length of output, and also by the simplicity of the structure of computation, probably just matrix multiplication over the field, such that verifying hashes in bulk could well be suited for delegation to a specialized proof. Note how the input is probably in large field elements whereas the lattice version has input in small field elements, and each is beneficial depending on the scenario of native circuits and what is implemented where. 

Searching, it appears multivariate crypto is best for signatures, maybe encryption, while isogeny crypto is best for key exchange, while code based crypto is best for signatures and encryption. But we are not concerned with secrecy. Thus none of these are best for our purposes of collision resistant hashing. Only expanders may be useful, which may involve isogenies, for friendly hash functions. 


Suppose given a witness we have a set of vectors to which it should be othogonal, forming a non-uniform program. Actually suppose we view this as taking the dot product of the witness scalars and a vector of polys. The resulting poly should be 0, and this can be tested by choosing a random point. Maybe the best poly is linear, ie a random linear combiation. Then we evaluate this poly at a bunch of random places to form a random matrix for the random linear code PCP protocol. 
Use LWE to reduce need for randomness. Oh, forgot LWE needs errors, so idk about using it.


For randdom codes, split each row into 2 (more generally k) parts, and think of each vector as the coefs with monomials of the same random multilinear poly (or small degree multivariate) but evaluated at 2 random points. suppose for now that concatenating these vectors will appear like a random vector. the coefs and the two eval points are kept secret. after commitment to a word, reveal secret points and go through interactive protocol to reduce those two points to the same for the two different polys (which have coefs the element wise product of the input and the secret coefs). Now upon having the same eval point, the monomials are the same, so we can think about adding the coefs. Actually we should think about absorbing the secret coefs into the input by element wise product, such that upon revealing the secret coefs of the two polys the prover can form a new input. now we add the two parts of the input together, or take a random linear combination, to get a new input.
After each round send a random scalar to prover and prover is to take this combination of the parts and now commit to that. 

what if instead of committing an input to a code matrix we just split the input into two and commit each separately, noting the total output size is the same if we keep the same rate. upon two commits, we then ask for a random linear combination as input, and do consistency checks by opening random entries of the two commits, computing the expected linear combination. this is so simple, would it work? the reasoning is that if either of the commits is not a codeword, then with high probability a random linear combination of them is also not a codeword. then we are just left to compute the probability a non-codeword agrees with a codeword, ie the error probability. 
To pass, the submitted codeword must agree with all initial testing, including those with errors which occur with probability distance. We fail if we sample at least one error, and all consistency tests pass (including agreement with the error). Note that if we sample an error, the codeword cannot be the combination of the commits without errors, or else the initial testing wouldn't pass. It must be another codeword, which may be more than min distance far from the combination of the commits. Now we are left to figure out how much the codword can agree with the combination, both on more error indices and non-error indices. 
Maybe we can reason that the prover knows the nearest codeword to the combination, ie the combination without errors. But from above this codeword won't agree whp with error samples in initial testing. therefore the prover must submit another codeword. if the prover submits a closest codeword to the known codeword, then prover has knowledge of min distance, which has hardness results. But to use this hardness we need a random matrix, and we still lack an arithmetization to reduce to it. 


for efficiently, optimize where prover may only do part of the merkel tree. after constructing, say half, prover goes through and completes proof and hopes that opening will occur on that side of the tree. if not, then it must construct the other half and begin again (though not constuct the first half again). but this may not be worth it because we perform multple checks so probability of starting over increases exponentially. 




I'm now wondering more about the idea of doing an IOP where the oracle is encoded as two instead of one. 
We go through the same interaction type as FRI, where prover sends oracle, verifier replies with randomness, and only at the end does the verifier look at the oracles, performing consistency tests. 
For any linear code, eg univariate poly, encode multiple of them in an oracle such that each leaf is its opening of all them at the same particular location. Then verifier sends random coefficients for linear combination. Prover sends new oracle that should be random linear combination of previous oracle parts, but splits this new oracle into parts as well. The process then continues. For a consistency check verifier chooses random location, opens first oracle, takes parts and computes previously chosen random linear combination. Verifier then opens second oracle parts at that same location and combines them to compute what the whole oracle would render at that location. The the process continues. 
For this to work for a code, we need a random linear combination of codewords to be a code, so presumably we use a linear code. Second, we need the ability to split a codeword or its source into multiple parts such that upon evaluating the parts the verifier can compute the whole. This works for univariates via the even-odd or offset technique. Third, we need the codes to be the same to the extent that upon opening the parts of the second on the domain defined the verifier can compute the evaluation of the whole along any point on the support of the previous oracle, making use of the linear combination coefficients chosen prior. This works for univariates because of the power structure and how cyclic subgroups work.
Now to what extent could a random code meet these conditions? A random linear code is of course linear, but it doesn't satisfy the second or third condition for the same reason that we don't know a reduction to such a code anyway, that is no structure to the matrix. 

Now does this relate to the FRI-style direct crypto? They seem to be opposites to an extent and I hope mine is more flexible. 






Any attempt to structure a random looking matrix would require a setup.
One attempt is using LWE for each entry with the same hidden vector s. To compensate for the errors, I think the underlying vector, and thus the errors would have to be revealed at the end. But this may be possible because all oracles can be submitted before any verification needs to happen. With the hidden vector revealed, the errors can be corrected. Or rather just reveal errors and subtract them. Disregarding errors now, what could we do with this structure?
Dot product of a row with the commit vector x would look like
x1(a1*s1 + ... + an*sn) + x2(b1*s1 + ... + bn*sn) + ...
= s1(a1*x1 + b1*x2 + ... ) + ... + sn(an*x1 + bn*x2 + ...) + ...
but this still has too much randomness for structure it looks like.

Instead of univariate commits we could use multivariate restricting each variable to a few options. 




what about multivariate crypto again, but for oracles not homomorphic commits. the source is an input points, the code is the polys, the output is the evaluations. 

actually a hard on average problem is given the polys finding an evaluation point to evaluate to zero. Thus this presents a hash function with good structure (though not homomorphic) but with limited compression. actually not necessarily because we don't have linearity, so finding a collision doesn't translate to finding 0. but nevertheless, such hashes exist and we could make use of them. 
I thought of this before but forgot why it doesn't work. try connecting two eval points with a line. there are two evaluation points the verifier doesn't know. prover returns poly with supposed evaluation of those points connected by a line. its a low degree poly. verifier then selects random element, plugs into univariate, and obtains new claim. however, the input for that claim is the unknown line evaluated at that randomly chosen element. Since the input points are the data we assume verifier has virtual access to them. Note that by a line we mean a linear poly of the form ax+b for each of the variables. In the last step the verifier must check that all these lines evaluated at the random t result in the new data. I don't think this is doable given the bijective translation reduction (BTR) theorem. What we've reduced to would easily suffice for a solution on its own. We're asking that given two virtual vectors [a] and [b] we choose random t, receive claimed result [c] and want to verify its correct. But doing so clearly violates BTR, ie is not a BT reduction.

how could we use this problem for a code? one possibility is as above, commitment via just evaluation, but unlike a hash in this case its expanding the source. Now this code is not linear since p(x) + p(y) != p(x + y) for non-linear polys. But suppose we try to connect x and y with a line l. the problem is the dimension of this line is the code size. 
we should instead think from the view of what verifier manipulation we can exploit. i see little opportunity except the lines method but the lines are too big for the verifier to touch. 
Surprised if it works because it throws away a round of interaction, but what if the verifier chooses a point on the implicit line and asks for a codeword at that point? Oh, but for constistency checks verifier must open previous oracle and compute expected eval at the point on the line, and this isn't doable due to nonlinearity. 
Asymptotically sending back the univariate is only as large as the number of polys. So suppose the prover commits to this instead of the original oracle. That is, instead of committing to two separate evaluations, prover commits to evaluation on a line such that at specified points, eg 0 and 1, the line yields data evaluation points. Then verifier chooses a random t, and requests the new evaluation (maybe folded in with another), or for now we can just assume prover provides new evaluation point in plain. For consistency check, verifier chooses random entries to open. For each, the opening is a univariate poly, and verifier plugs in the randomly chosen t, and compares the result with the evaluation at the prover provided evaluation point. Another random option is instead of commiting to each univariate, we join them together to form a single univariate and prover commits to that. then for consistency verifier must compute evaluate all polys (not just some for some openings) and from that compute the joined univariate. 
For security, we hope that if the initial commits is not an evaluation, ie not a valid codeword (actually family cuz of the t variable), then it will be sufficiently far from any valid codeword.
Note upon choosing t, the commitment really reduces to a single claimed codeword. So first we'd like to prove that if the commit is not a valid family of codewords, then upon a random t the collapsed commit won't be one whp. 
Second, we'd like to prove that its hard for the prover to find a source that evaluates to a word thats close in hamming distance to the collapsed word. Consider a single entry that agrees. Note that upon choosing t we pick a semi-random evaluation point. So for an entry to agree means that semi evaluation point and the prover's new eval point agree on the corresponding poly. 

Suppose for polys p_i you find two points, x and y that evaluate the same. Upon subtracting the polys, for a quandratic coef ci you have
ci(x1*x2 - y1*y2)
suppose you subtract and add points as so below. For the same coef ci you'd have
ci(x1 - y1)(x2 + y2) = ci(x1*x2 + (x1*y2 - y1*x2) - y1*y2) = ci(x1*y2 - y1*x2) + ci(x1*x2 - y1*y2)
So upon combining and plugging as above, all linear terms would vanish, and all quadratic terms would become like
ci(x1*y2 - y1*x2)
and they would sum to zero.
to really understand we just do an example with more than 2 variables.
b(x1*x2 - y1*y2)+ c(x1*x3 - y1*y3) + d(x2*x3 - y2*y3) + e(x1 - y1) + f(x2 - y2) + g(x3 - y3) + h
now suppose we use first variable (x1 + y1), second (x2 + y2), and third (x3 + y3). Then we have
b(x1 + y1)(x2 + y2) + c(x1 + y1)(x3 + y3) + d(x2 + y2)(x3 + y3) + e(x1 + y1) + f(x2 + y2) + g(x3 + y3) + h
= (b(x1*x2 + y1*y2) + c(x1*x3 + y1*y3) + d(x2*x2 + y2*y3) + e(x1 + y1) + f(x2 + y2) + g(x3 + y3) + h) 
+ (b(x1*y2 + y1*x2) + c(x1*y3 + y1*x3) + d(x2*y3 + y2*x3))
but the left over can't be split such that its the sume of two evaluations.


Suppose instead we just start with the assumption that one cannot find a source for a fixed target with respect to the random polys. however this reasonable assumption doesn't transfer to commits so easily because of nonlinearity. 
In our scheme above, the target is only semi-random with respect to the polys. 

the basic task for the prover is the following over a number of poly simultaneously. 
given a poly, output a univariate such that for a randomly chosen eval point, you can find an input to the poly that evaluates to the same as the univariate. For a single poly this seems simple. for any target (including some random evaluation of a univariate), it shouldn't be hard to find a proper input. A formula for completeness is the prover choosing some line and evaluates the poly and submits that as a univariate. then for any chosen t, the evaluation of the univariate target will match the evaluation of the poly on the line at that point. 
so of couse we generalize the above problem to multiple polys at once. the task is to submit univariates below a certain degree, eg the total degree of the poly. then for a random evaluation, one must then find an input that matches on all polys. the same formula applies for completeness.
Now I'd like to connect the hardness of this problem with that of matching a pre-chosen fixed target, eg 0. But our model is closer to that of a random target being picked for each poly, which is reasonably hard. The difference is we move towards less randomization. In fact, for n polys we move from choosen n random values (targets themselves) to choosing 1 random value to plug into the univariates. In order to connect these two models, maybe we consider submitting multivariates for increased randomness. This corresponds to a changed formula for completeness where parts of the line are with respect to different variables. note that which parts have which variables is not enforceable and prover is free to choose. all the verifier can enforce is the nature of the submitted polys, ie their variacy (degrees of randomness) and their degree. 

But the above task is only partially relevant and doesn't account for codes and errors. In this case prover must only find an input that matches on majority of polys. 

In terms of the relvance of decoding hardness, its not that the prover is ignorant of the errors and must decode, because the prover is the one who chooses the errors. Instead, the relevance is the hardness of finding a valid codeword sufficiently close to a semi-random non-codeword. 
In all cases, ie this protocol, the above one, and FRI, the protocol is the prover submits a function that should collapse to a codeword for all inputs. verifier chooses an input. prover submits codeword as close as possible to the randomly collapsed word. Now I think this is where analysis must branch. 
In one case the prover basically starts with a valid function but introduces some known errors, and upon randomization the provers intended function (without error) at corresponding evaluation is the closest codeword to the randomized word. Here I think we just analyze the distance and hardness is not relevant. 
In the second case the prover starts the same, but after randomization, the collapsed word is closer to another codeword than to the prover's intended word. Here we consider the hardness of the prover finding the closest codeword. Doing so means prover has knowledge of two closest codewords, ie min distance. 
In the third case the prover submits a function for which he doesn't have an intended family of words. This covers all other cases and is complex. I think here we assume, xor to the first and second scenarios, that the prover has no knowledge of closest codewords to (one or both of?) the commits. Then we prove that without this knowledge the prover can't have knowledge of the random linear combination of them without contradiction. Contrapositive is if prover has knowledge of closest codeword to random linear combination of two strings whp over than randomn combination, then prover should have knowledge of codewords closest to (one or both of?) the original strings. Then, assuming this holds, we reason that wihtout appropriate initial knowledge, prover must submit codeword farther than the closest, but this means distance analysis and not necessarily leads the prover to find min distance. Note how prover for this third case is really decoding, so maybe hardness of that is more useful than just finding min distance. 


I've already considered this but I'll try again. Suppose we have a permutation between prover data and poly coefs. for a commit we choose a random point and ask prover for evaluation on it. we could choose different points for different commits. we could reduce to a single point. when we have a single point for multiple commits we take a random linear combination of the claims. for completeness the prover sends that random linear combination of the polys. 
for security, suppose our randomization is a random oracle. upon submitting data the prover receives a random poly in return. ohhh, I remember the problem. its that the one wayness of the permutations, ie the unstructuredness of the oracle, gets in the way of the homomorphism. 
what if we use a homomorphich hash, eg SIS. then we must use a small random scalar, reducing randomness. 


What about the possibility we use rings instead of fields. The smoothness of the characteristic would affect soundness.
The necessary analysis is schwartz zippel for ring polys. 
In fact, I'm wondering if we can work with rings in general (multiplication maybe noncommutative). 
well I think neither of these will work. the reason is the prover has complete control over the poly begin root-tested because its the difference between the real poly commited and the virtual poly the prover is faking. So the poly can be strategically chosen. 
but maybe we can make a condition that the poly degree is sufficiently less than any zero divisor. 
oh, a limiting problem is that, supposing we do mod the product of two primes. Then many elements along the domain will be multiples of either of those primes, and so will monomial expressions of them, so when strategically multiplied by the other prime the product will go to zero. 
so i'm leaving this idea. 

however, one possibility for avoiding the flaw above, that is the user choosability of the poly, is to put the user's input through a random bijection. using this technique failed directly above because it didn't work with homomorphisms. for it to work in this case we need to end up evaluating the outputs. now note that its one-way, so even though its a bijection we can't take the output and invert to get the input as the real data. 
what if we wait until we get final claims for real data, then we randomly choose permutation, doesn't even need to be one way. then via BTR prover submits permuted data. verifier maps to previous data and evaluates, and is left to evaluated new data. the problem now is validity of mapping, ie of obeying the random perm, is dependent on the evaluation of the new data, but the soundness of the evaluation of the new data is dependent on the validity of the mapping, that is a circular dependence. usually this doesn't happen with BTR because the evaluation of the new data is not dependent on any computation on it. 
also, not to mention intermediate polys also would need the random mapping. 

i think its worth dropping the whole concept of not using fields. even if we chose strategically from a subset of the ring for evaluation points, it doesn't work eg for composition of polys cuz we're unsure of the output, like in the poly reduction algo. 

consider the multivariate in hash form, ie commiting a line to represent committing two polys. upon choosing a random t, you have chosen a random linear combination of the two plolys and that new poly is what the prover sends to open the commit. difference between hash version and oracle verion is hash version doesn't deal with errors and prover must solve satisfy all equations rather than a fraction. 
can this form be nested to allow for more compressed commits?

address the difficulty of not just finding a collision or false response, but one that will evaluate correctly for the random combination of claims. 

try other codes with better params

try randomizing the extension basis, ie the irreducible. maybe it can only be sufficiently randomized up to associates. 


maybe we can relate finding a codeword with small norm in a random code to finding short lattices. 



what about treating codeword polys as field elements, and multiplying them. since the oracle has all evaluations, only need to multiply entries. actually, would need q as well. 
p1(x)q1(x) = q(x)pi(x) + r(x)



instead of using subset of roots of unity for evaluation, use a whole field for evaluation but make the field small enough. 

instead of starting with two polys with their own distance start with one poly with a distance, and split into two, and that distance should hold whp for their random linear combination. 



try randomizing via the frobenius automorphism, ie f(x)^{p^i} = f(x^{p^i}) or the like
(a + bx + cx^2 + dx^3)^p = a^p + b^px^p + c^px^{2p} + d^px^{3p}
in fact, if we restrict the coefs to a subfield and choose i to fix that field, then given a claim on x we can obtain claims on its conjugates x^{p^i}. Note that only a log number of such conjugates exist in the field. 
actually maybe don't restrict coefs and thus poly gets slightly randomized.
also consider using the log \alpha^{p^i} values as a basis (if possible) for any point in F, so you query a subset of them and multiply by basis scalars to query at any point. 

note another source of randomization is adding a constant. in fact, to generalize this we add an entire random poly, the sparser the cheaper, but the less the randomization. In this case we address the question of the distance of
f(x) + r(x)
for a random r assuming a lower bound on the distance of f(x).
The cost of doing this comes from generating the randomness of r after commitment to f, because r can't be fixed. 
actually, quite surprisingly, this doesn't help and is equivalent to just adding r after taking the random linear combination. this means the two polys can still cancel errors and then the addition of r does nothing. 

consider a circular hash tree for shifting. there would be the same number of vertices for each step of the tree. This means the number of vertices/hash computations would go from 2l (for l leaves) to l*log(l). this is better than doing l different trees, which costs 2l^2
note the top would have to be hased like a tree to give something small enough to the verifier.
the benefit is the verifier can now easily choose any root along the cycle. 
. . . . . . . . 
 . . . . . . . .
. . . . . . . . 
 . . . . . . . .
. . . . . . . . 
but i realize this isn't the cyclic property I was looking for because what we're looking for is randomizing one hash with respect to another. the only way I see to do this is to commit to two hash trees separately, and randomly cycle one of them.
omg, actually this doesn't cost any more than combining the leaf pairs first. 
with combining the leaves first we have nl leaves so a total of 2(nl) hashes
with n separate trees there are l leaves in each tree so n(2l) hashes
so the hashing cost is no extra, but the verifier cost is twice as much. this may still be worth it for randomizing cyclic shift of one of them. 
as we considered before, we could also just do one tree and randomize the pairs once they are all opened but the reduces randomization and the math is more complex. 

how else can we randomize besides cycling?
ideally we could do all permutations and preserve the codeword.
one permutation that seems viable is flipping direction, ie suppose we evaluate on the cyclic group of order n generated by g. then we treat a query at index i as a query at index n-i. That is, f(g^i) -> f(g^{n-1-i}). Eg f(1) <-> f(n-1), f(n/2-1) <-> f(n/2).
any other permutations?
when a poly is even, ie f(g^i) = f(g^{n-1-i}) maybe we can flip each side because this keeps the 'continuity'.
f(g^i) -> f(g^{n/2-1-i}) for i from 0 to n/2-1
f(g^i) -> f(g^{n-i}) for i from n/2 to n-1
Note that even really means f is the same on an element and its inverse.
forget this, even if it works, its just another single transformation and can't be extended.


suppose for a moment we could reject distance delta with probability delta. how many tests would be required? 





the distance of a vector v is the same as its distance when scaled by non-zero r. 
|u - b| = |r*(u - b)| = |r*u - r*b|

suppose v and u are in unique decoding radius.
|v - a| < (n - k)/2
|r*u + r*b| = |u - b| < (n - k)/2
then with both we have
|v - a| + |r*u - r*b| < n - k
and by triangle inequality we have
|(v + r*u) - (a + r*b)| = |(v - a) + r*(u - b)| <= |v - a| + |r*u - r*b|
then putting the inequalities together we have
|(v + r*u) - (a + r*b)| < n - k < distance
so while (a + r*b) may not be closest to (v + r*u) in this case, they are within the min distance. 

suppose c != a + r*b
since both are words we have
|(a + r*b) - c| >= n - k + 1
from triangle inquality we can also say
|(v + r*u) - (a + r*b)| + |(a + r*b) - c| >= |(v + r*u) - c|
and putting this together with that from above gives
n - k > |(v + r*u) - (a + r*b)| >= |(v + r*u) - c| - |(a + r*b) - c| >= |(v + r*u) - c| - (n - k + 1)
|(v + r*u) - c| < 2(n - k) + 1
giving us a rather meaningless upper bound on the distance of the random combination.

try to take advantage of n << |F|. i think this comes when considering only x and the probability it cancels two errors. 

consider b' = (c - a)/r and a' = (c - r*b) which are both codewords. maybe consider
|(c - a)/r - b| and |(c - r*b) - a|
if either of these distances are zero, we have
c = a + r*b
and our previous analysis applies.
otherwise both distances are >= n - k + 1
also consider the distances
|(c - a)/r - u| and |(c - r*b) - v|

also consider u' = (c - v)/r and v' = (c - r*u)

in the end we want a lower bound on
|(v + r*u) - c|
= |(v + r*u) - z + z - c| >= |(v + r*u) - z| - |z - c|
so one option is we try to say
|(v + r*u) - c| >= |(a + r*b) - c| - |(a + r*b) - (v + r*u)|
now |(a + r*b) - c| >= n - k + 1
and |(a + r*b) - (v + r*u)| >= ~max(dv,du)
but one of these needs to change direction

what's the value of a',b',v',u'? 
|a' - a| = |c - (a + r*b)| >= n - k + 1
|v' - v| = |c - (v + r*u)|
|a' - v'| = |(c - r*b) - (c - r*u)| = |b - u|
|a - v|
|a' - v| = |(c - r*b) - v|
|a - v'| = |a - (c - r*u)|
|v' - v| = |(v' - a') + (a' - a) + (a - v)|
|(v' - a') + (a' - a) + (a - v)| >= |a' - a| - |(v' - a') + (a - v)|

|(v + r*u) - (a + r*b)| + |(a + r*b) - c| >= |(v + r*u) - c|
|(v + r*u) - c| + |c - (a + r*b)| >= |(v + r*u) - (a + r*b)|
|(a + r*b) - (v + r*u)| + |(v + r*u) - c| >= |(a + r*b) - c|
send |(a + r*b) - c| to opposite |(a + r*b) - (v + r*u)| extremes

basically we're only concerned with the case when c != a + r*b
I was trying to consider a' and b' as the should-be linear factors to result in c and then investigate their distances from v and u. But a' and b' don't function that way, ie a'+r*b' != c

I think the initial distance we need to consider about v and u is the |{i : vi != ai or ui != bi}|, that is the size of the union of indices where either v or u differ. this is of course well defined when both are in unique decoding radius.
when beyond unique decoding radius, each still has its distances from codewords, and for any possibilities for a and b, we can minimize over the expression above. 

consider v with a min distance, and arbitrary u. calculate number of r such that v + r*u decreases distance.
if u is a word, then the number of such r is 0. 
suppose we have such an r, and consider one word c of possibly many words close to resulting v + r*u.
let e = c - (v + r*u)

suppose v and u are such that both are far, but there exists r such that v + r*u = c.
suppose there were two such r.
v + r1*u = c1, v + r2*u = c2 => u = (c1 - c2)/(r1 - r2)
this implies u is a word (unless r1 = r2) which is a contradiction. therefore only one such r exists.
this shows that for resulting distance 0 there is at most 1 r.
now suppose we consider resulting distance 1.
then v + r*u + e1 = c where e has weight 1.
v + r1*u + e1 = c1, v + r2*u + e2 = c2 => (r1 - r2)u + (e1 - e2) = c1 - c2
=> u = ((c1 - c2) - (e1 - e2))/(r1 - r2)
now e1 - e2 has weight at most 2, so combining with codewords c1 and c2 and dividing by r1 and r2 suggests u has error 2. This is a contradiction, so there can still be at most 1 r.
Suppose we continue in this manner as long as the weight of e1 - e2 is strictly less than the distance of u. hmm, note how the distance of v is irrelevant. 
So suppose the distance of u is >= d. then we can give e weight < d/2. To state the proof again, suppose we have distinct r1 and r2 such that for words c1 and c2 we have
v + r1*u + e1 = c1, v + r2*u + e2 = c2
subtracting we get (r1 - r2)u + (e1 - e2) = (c1 - c2)
yielding u = (c1 - c2)/(r1 - r2) - (e1 - e2)/(r1 - r2)
now with r1 and r2 distinct we have that (c1 - c2)/(r1 - r2) is a word and (e1 - e2)/(r1 - r2) is of weight < d/2 + d/2 = d
thus u is < d errors to correct to far from the codeword (c1 - c2)/(r1 - r2) and thus u has distance < d. This is a contradiction, therefore the assumption must be false. That is, for fixed v and u there do not exist to distinct scalars that can take the linear combination of v and u to within said distance of a word. 
So to summarize if u has distance >= d, then there is at most one r such that v + r*u has distance < d/2.
Besides the distance dropping by 1/2, a limitation is that we assume the error is in u, when all error could be in v. 

suppose v and u have total distance >= d. 
v + rk*u + ek = ck
where ek has weight < d.
we'd like to count how many (rk,ek,ck) combinations there are with distict rk.

suppose we create G' to map ek values to 0, thus
GG'(v + rk*u + ek) = GG'ck
GG'(v + rk*u) = ck
so |GG'(v + rk*u) - (v + rk*u)| = |H(v + rk*u)| = |H(v) + rk*H(u)|
so for any set of ek for which a G' can be constructed that sends those ek to 0
this then induces a map on v and u and we have resulting distance the combination of weights H(v) + H(u)
in fact, why not choose G' to map v and u to other source to maximize final weights.
this seems too good, which means we may not be able to map all ek as said, meaning they must be linearly dependent. this means they must share indices.
its confusing how this depends on the distances of v and u as it should.
why not just set G' for a single ek and v and conclude distance?

I think the above confusion can be reduced to this possibility. 
set it on v + r*u, equivalent on e, the use the remaining degrees of freedom to alter G' on v and r*u that maximizes the union of their errors. note that the resulting distance should match the weight of e. may not be necessary to include e though, if we just focus on v + r*u.
we can try to achieve the 1.5 johnson bound. this means assuming the max distance of v and u is above that bound, there are only a few r for which the final distance cannot be sent up past this bound. 
I'm not yet sure how the randomness of r is relevant. 

Choose G' such that G'(v + r*u - c) = 0.
Then we have GG'(v + r*u) = GG'c = c
So |c - (v + r*u)|
= |GG'(v + r*u) - (v + r*u)|
= |(GG'v - v) + (GG'r*u - r*u)|

note how GG'v and GG'u are two words, so they must differ in at least n - k + 1 indices. 

u has distance > d
then so does r*u
then |GG'r*u - r*u| > d
now we have the freedom to map v to any source, but note that any choice will change where the errors occur for r*u.

by the reverse triangle inquality, if v has distance <= d' < d we map v to its closest codeword and thus have
|(GG'v - v) + (GG'r*u - r*u)| >= |(GG'r*u - r*u)| - |(GG'v - v)| > d - d'

the problem has reduced to how to maximize |(GG'v - v) + (GG'r*u - r*u)| given only partial freedom.

consider an index i where (GG'v)_i != v_i. For this error to be cancelled we need
(GG'v)_i - v_i + (GG'r*u)_i - r*u_i = 0
(GG'u)_i = v_i/r + u_i - (GG'v)_i/r
thus it constrains that index of GG'u
in general, given a G' we can find all errors of v and u with respect to GG'v and GG'u respectively. We can then reason what it means for two errors to cancel.

an important thing to note now is that we the equality (not inequality)
|c - (v + r*u)| = |(GG'v - v) + r*(GG'u - u)|
this implies the surprising fact that regardless how we choose G' (apart from its minimal conditions dependent on G and c), the right side will always have the same weight. 
this means instead of seartching for the maximum, we could search for the minimum weight or any weight that satisfies.
so we can phrase the problem as calculating the number of zeros in the vector
(GG'v - v) + r*(GG'u - u)
suppose we choose G' to map u to 0. then we're left with
GG'v - (v - r*u) 
so we're left to count the number of indices i for which
(GG'v)_i = v_i - r*u_i
which is the distance of v + r*u from the word GG'v but in this case GG'v = c so this is just the original question.

go back to the problem of calculating the weight of
(GG'v - v) + r*(GG'u - u)

suppose we set G' on u prior to choosing r, then set G' on v + r*u after choosing r.
this means r is random with respect to the vector (GG'u - u) because GG'u is determined prior r despite G' note being fully determined prior to r. 

i don't think we can go much further without more on the relationship of v and u.
think about the capacity to cancel
what does it mean when we constrain (GG'u)_i?
since GG'u is a word, constraining k indices determines the rest. same goes for GG'v
I think constraining on an index is GG'u is equivalent to constraining on one of G'u in that they restrict the same amount of freedom.
we then would like to move to saying something like correcting an error requires constraining GG'u which means constraining an index of G'u, of which there are only k, so we can only correct k errors.

we seem to forget that the choice of G' (apart from basic conditions) does not matter.
But G' on both v and u is subject to r. 
we'd like to limit the set of possible GG'u and GG'v independent of r. then for a fixed pair we know the errors
(GG'v - v) and (GG'u - u) and we consider the probability over r that errors get cancelled.

maybe we can restict r to a subset, and in exchange we can same more about GG'v and GG'u

maybe for a fixed r consider the freedom of G' for codewords GG'v and GG'u, and note that any choice should result in the same final distance. 
think about how that r must cancel errors for all choices of G'.

note how G' is chosen to map on c arbitrarily. instead of c we could pick any other c' and reason that
|c' - (v + r*u)| = |GG'(v + r*u) - (v + r*u)| = |(GG'v - v) + r*(GG'u - u)|
leaving us to figure out the weight of the resulting vector
thus we cannot reason as hoped without taking account of how G' relates to c and taking advantage of how c relates to v + r*u.
so i'm wondering if the idea of using G' is worth further pursuit.

somehow we must take advantage of c being (non-unique) closest to v + r*u. 

maybe consider the |F| function v + x*u and a corresponding set of closest words. 
there are a total of |F|^k words.
suppose we reason about changing v or u and how that changes the resulting sets.

i think a false idea I've held implicitly is that its quite possible for a vector v to be more than the distance of the code away from a word. but in fact given any v, just choose any k entries for an information set, and set the other n-k accordingly. thus any vector v is within distance at most n-k from a word. note n-k is one less than the code distance. 
thus distance 1 is non-sensical and the max distance a function can be is (n-k)/n = 1 - rho. so we should only consider the distance domain [0,1-rho].

maybe we can make the argument that there always exist a and b close to v and u such that a + r*b = c.
once c is determined find all those indices of v + r*u with no error. since the number of errors is at most n-k (since c is closest) there are at least k such indices. for each such index i we have v_i + r*u_i = c_i. consider these indices on v and u and use them as information sets to recover codewords called a and b. we now have a + r*b = c. Since a and b have at least k indices in common with v and u, they are within distance <= n-k.
now suppose c has distance <= d <= n-k from v + r*u. consider the n-d >= k indices with no errors, ie v_i + r*u_i = c_i. use those n-d >= k indices on v and u to recover words called a and b. then there at most d <= n-k indices that differ between a and v and b and u. thus given c with distance <= d, we can find a and b with distances <= d such that a + r*b = c.

can we translate this to imply that if a and b are unique, then c is unique? actually we have the converse. if c is unique, then can have unique a and b. but note that if a and b are unique, they are independent of r and thus c. but this still doesn't imply that a + r*b = c, because in some cases c may not be unique. 

can we conclude the seeming contrapositive that if v or u has distance > d, then v + r*u has distance > d. the contrapositive of this is if v + r*u has distance <= d then both v and u have distance <= d. to say that v + r*u has distance <= d means there exists c with distance <= d from v + r*u. we proceed as above and conclude there exists a and b with distances <= d from v and u. this means both v and u have distance <= d.
worrying that this doesn't depend on r. suppose we eliminate r, and split c aribtrarily into v and u such that v + u = c. here all indices have no error. ohh, the flaw we made is assuming that if > k indices have no error they must still all belong to a single word. 
but the first proof still holds, but that is trivial.



change notation here to match paper.
u' + x*u is the function. the word is v' + x*v.
domain is D. 
distance is d. 
epsilon is e.
code distance is lambda.

maybe we should try the contrapositive as they do in the paper. that is, if for noticeable x, u' + x*u is close to a word, then u' and u must be close to words.
the parameter we'd like to focus on is 'close', captured in distance <= d.

clearly with d=0 this holds, because if u' + x*u is a word for two x we have
(u' + r1*u) - (u' + r2*u) = (r1 - r2)*u
=> u is a word => u' is a word

try by induction.
by assumption at least a noticeable fraction of instances are <= d in distance.
we partition this noticeable set into those of distance <d and those of distance =d.
consider those for which the distance is exactly d.
suppose there was an index where all of them had an error. remove this index and consider the resulting v' + x*u' which is < d from words.
now all those within original distance <= d are within distance < d.
by assumption this set has noticeable size with respect to n, and thus with respect to n-1.
in this case v' and v are close to words by induction. 
now introduce that error index again, and v and u are now at most one error farther away from words. 

note that eliminating an index is valid because that just means ignoring an evaluation point, just changing the code slightly. for this code change to be appropriate we need the hypothesis to hold with respect to both codes.
this argument seems viable because it takes advantage of the base case, and because it will be comparing the number of instances (q) to the number of indices (n). but i don't know if it will take advantage of the singleton bound.

now the assumption that there is an index will all errors is wrong.
let O be the original set of instances with distance <= d. 
by assumption |O|/q >= b(n).
partition O into S and O\S, where S is those with distance exactly d.
now suppose we find l indices i_1,...,i_l such that removing them from u' and u results in v' and v of length n-l.
consider analogous sets O' and S'.
O' contains instances of v' + x*u' with distance < d.
now partition S' into T' and S'\T' such that T' contains instances with distance = d.
now O'\T' contains instances with distance < d.
suppose |O'\T'| = |O| - |T'|, ie removing indices has not left two instances identitcal.
|O'\T'|/q = |O'|/q - |T'|/q = |O|/q - |T'|/q
suppose |O|/q - |T'|/q >= b(n-l)
by induction, v' and u' have distances < d.
we now re-intoduce the l indices to re-create u' and u. 
since u' and u have distance < d, and we add l indices, v and u now have distance < d + l, <= d + l - 1.

now for the heavy argument
Consider the set O.
Imagine the matrix columns to be the errors of v + x*u wrt a closest codeword forall x in O.
Thus the matrix size is n by |O| >= bq.
Every column has d non-zero entries.
Thus the fraction of non-zero entries is d*|O|/n*|O| = d/n.
A row is heavy if it has a fraction of at least d/2n non-zero entries.
At least half of the entries are in heavy rows.
Thus there exists a row with (d/2n)*|O| non-zero entries.
this is clearly not enough.

if we are to account for not only large distances i think it is unrealistic to remove enough indices to catch an error in each instance.
but intuitively, all errors should occur in a few rows. 

hmm, what if we cycled the entries in columns so that we can line up in at least one row the max errors possible for a single row? that would trivially finish the above problem.
But is such cycling allowed?
For a column u' + r*u, consider rotating c_r the same way you rotate u' + r*u.
This way the distance of u' + r*u is unchanged by rotation.
We couldn't continue with induction because we've changed the structure now.
But could we do anything else?

we should still picture a matrix, each column an instance of (u' + x*u - c_x) where c_x is one of many possible closest words. thus there are many such possible matrices.
for now consider all instances, thus q columns.
number of rows is n.
a row is all zero when all instances agree with their word.
our assumption is that a noticeable fraction of these instances have distance <= d.
This means a noticeable fraction of columns have <= d non-zero entries.

I think the paper approaches by arguing there exists a set of 3 columns that share zeros on a large set of rows.
The size of this set is > n(1 - (n - k + 1)/n) = n - n + k - 1 = k - 1
From these columns v' and v can be obtained that are sufficiently close to u' and u.
They say the three points are collinear, and that the line they all pass through can be v' + x*v
Call the columns x1,x2,x3.
They say (x1, u' + x1*u), (x2, u' + x2*u), and (x3, u' + x3*u) are collinear.
Oh, well of course, they are all on the line (x, u' + x*u).
The suppose we throw out all their erroneous rows restricting to those where they all agree with their words. 
Restricting to these rows, they are idential to v' + x*v.
Since there are at least k such rows, these rows can induce the full v' and v to all other rows. 
Now that v' and v are determined we want to find their distance from u' and u.
Consider where u' + x*u = v' + x*v => (u' - v') + x*(u - v) = 0.
For each row where either u' differs with v' or u with v, we have a linear field equation. Thus in such a row at most one column (x value) satisfies (maybe no column).

the claim is that if >= p(n) of q(n) columns have distance <= d(n), then u' and u have distance <= d(n)
suppose we have q(n) total columns.
suppose we hypothesize that at least p(n) of them are of distance <= d.
we can show, and assume for now, that in the worst case all p(n) of these columns of have distance exactly d(n).
focusing on the matrix of p(n) columns, we have a total of d(n)*p(n) non-zero entries.
what is the min of the max, ie the minimum number of non-zero entries in the row with the most non-zero entries?
suppose the min-max is m.
then each row has no more than m entries.
so total number <= m*n.
but total number is d(n)*p(n), so d(n)*p(n) <= m*n
thus m >= d(n)*p(n)/n
thus we are above to reduce >= d(n)*p(n)/n of the p(n) columns to having d(n)-1 errors by only removing one index.
now with the index removed, we are left with a total of q(n) columns of size n-1.
Of these q(n) columns, >= d(n)*p(n)/n have distance <= d(n)-1.
to induce induction we need at least p(n-1) of q(n-1) instances to have distance <= d(n-1)
we have >= d*p(n)/n instances of distance <= d(n) - 1.
now with d sublinear, we have d(n) - 1 <= d(n-1)
thus we have >= d*p(n)/n instances of distance <= d(n-1)
we include these in our q(n-1) total, and exclude q(n) - q(n-1) of the others, including the rest.
thus this requires d(n)*p(n)/n >= p(n-1)
suppose we make d(n) sublinear, d(n) = n^{1/s}.
then we need p(n) >= n^{1 - 1/s}*p(n-1)
suppose this holds for small enough n, large enough p, and small enough s.
now since we have >= p(n-1) instances of distance <= d(n-1) we induce indution
thus v' and v have distance <= d(n-1).
re-introducing the index, u' and u have distance <= d(n-1) + 1.
this is a problem because d(n) <= d(n-1) + 1.
even if it worked out trying out parameters for p(n) >= n^{1 - 1/s}*p(n-1) shows unrealistic constants. 

i don't think our pigeonhole about non-zero entries is strong enough and we need to reason about the matrix structure and distance more. 
we were maximizing non-zero entries whereas they maximize zero entries. 
maybe the best we can do is understand their proof, state it in our own notation, and consider what it would take to improve it.


consider the version of assuming large distance on u' and having arbitrary u.
if distance of u' + x*u is to be is smaller than distance of u', x*u must correct errors.
show that no u exists that can cancel enough errors on enough x.

focus on the tight case where u' has distance d, and u' + x*u has distance < d.
maybe show that correcting an error for a value of x puts constraints on u. 
instead of contradiction by showing that u' has distance < d, try to show that for a certain number of good x-values u cannot be static.

iteratively pick 'good' x-values such that u'_i + x*u_i = (c_x)_i for >= n-d+1 indices i in I_x.
this way a good x-value leaves < d errors.

suppose we succeed with the method to show that for each x that u removes an error, it constrains u by one element.
then u is completely constrained after n elements.
thus at most n x-values can reduce error.
if we make the x-space large enough, this probability can be trivial.
we could generalize this where each x cancel multiple errors, such that the X space can be smaller, but distance dropped will be larger.

suppose n-d+1 >= k, ie d <= n-k+1
then restricting to the I_x indices we have u'|I_x + x*u|I_x = c_x|I_x
since |I_x| >= k we can induce v' and v such that v' + x*v = c_x
but note that not all >= k indices may be valid with respect to each other
thus there may be multiple distinct ways of inducing v' and v

now for a good x-value a, inducing v' and v, consider how v' + x*v compares to u' + x*u for other x-values
note that v + x*v is always a word
v'_i + x*v_i = u'_i + x*u_i
(v'_i - u'_i) + x*(v_i - u_i) = 0
this equation has at most one solution x
thus in each row, v' + x*v = u' + x*u in at most one column

consider two good x-values x1 and x2
consider the line function connecting
(x1,c_x1) and (x2,c_x2)
the line is 
c_x1 * (x - x2) / (x1 - x2) + c_x2 * (x - x1) / (x2 - x1)
call this line v' + x*v
that is, let
v' = (x2*c_x1 - x1*c_x2)/(x2 - x1)
v = (c_x2 - c_x1)/(x2 - x1)
a third point (x3,c_x3) lands on it if
c_x3 = v' + x3*v
we'd like to consider how may other good x-values lie on this line.
for any good x-value x3 that does
u' + x3*u has distance < d from v' + x3*v

consider additive inverse pairs of x-values (a,-a).
u' + a*u
u' - a*u


maybe the idea of cycling could help us with G'. There are n ways to cycle. We could map up to n-k of them to their closest codeword, assume each cycle of u is linearly independent of the rest and v. we still have the problem that G' is dependent on r and thus G'u is dependent on r.

maybe we could calculate the expected number of crossovers (errors in common) after a random cyclic shift. we do this in terms of param d for the distance of v and u, and the expected distance of the output. this wouldn't solve the fundamental problem, but be more of a preprocessing step to increase total distance.

maybe we can reason about list decoding by noting there are q^k codewords, and each is n-k+1 from another.
one can reach at most (n choose k) other words by this min distance. 

is it the case that if you're not within unique decoding radius you necessarily have at least two words at your closest distance? suppose not, that you can still have a unique closest word. consider this word c, and the > (n-k)/2 indices which must be changed to reach v. we'd like to find c' that is close to v. i don't know about this.
|c - c'| = |(c - v) + (v - c')| <= |c - v| + |v - c'|

note one way to take advantage is to limit evaluation to a subfield, and ask that coefs are in the subfield, and then choose x from a larger field. the only way to check is that when opening the commits the result is in the subfield. once multiplied by the larger x it won't be in the subfield anymore, so an isomorphism is needed. 
the subfield could be of minimum size n, and evaluation is on its n elements (including zero).x


since we are in working in the vector space (F_q)^n why don't we instead think in terms of isomorphic F_{q^n}.
so vector is viewed as the coefs of a field element.
a field element is a word if each coef is the eval of a single degree < k poly f at n points over F_q
we can suppose n = q = p^m
this means the field element has the form
y^{n-1}*f(w_{n-1}) + ... + y*f(w_1) + f(w_0)
= y^{n-1}*((w_{n-1})^{k-1}*f_{k-1} + ... + w_{n-1}*f_1 + f_0)
  + ... +
  y*((w_1)^{k-1}*f_{k-1} + ... + w_1*f_1 + f_0)
  +
  ((w_0)^{k-1}*f_{k-1} + ... + w_0*f_1 + f_0)
= f_{k-1}*(y^{n-1}*(w_{n-1})^{k-1} + ... + y*(w_1)^{k-1} + (w_0)^{k-1})
  + ... +
  f_1*(y^{n-1}*w_{n-1} + ... + y*w_1 + w_0)
  +
  f_0*(y^{n-1} + ... + y + 1)
instead of multiplying by an element of F_q we can multiply by a small degree element of F_{q^n}
does this preserve completeness?
no, unless we only multiply by elements of F_q, whereas we were hoping to multiply by low degree elements of F_{q^n}.
I was hoping to take advantage of doing such multiplication.
This is equivalent to the regular case but multiplying by a small degree poly instead of just a scalar.
There is a standard irreducible poly of degree n. Upon choosing the low degre poly r(x), the verifier asks for the reduced version of u'(x) + r(x)*u(x).
we'd like to say that if u'(x) or u(x) is not low degree, neither is u'(x) + r(x)*u(x).
this is not making sense because we don't know what high degree means here cuz its always restricted to n.

but actually we can reduce the commits to small field elements and thus have a big field size, to increase randomness of x. 

note H(u' + x*u + e_x) = H(c_x) implies
H(u') + x*H(u) + H(e_x) = 0
or do it with GG'
GG'u' + x*GG'u + GG'e_x = GG'c = c
we want to exploit the claim that H(u') has weight at least d, while e_x has weight < d.
how?

for two good x-values we have
u' + x1*u + e1 = c1
u' + x2*u + e2 = c2
u = (c1 - c2)/(x1 - x2) - (e1 - e2)/(x1 - x2)
|ei|, |ej| < d => |ei - ej| < 2d
thus |u - (c1 - c2)/(x1 - x2)| < 2d, that is u has distance < 2d

consider again the idea of correcting errors by putting constraints on u.
for a good x-value we have e with weight < d.
u' + x*u + e = c
we'd like to say that there can be at most n good x-values.
for each such x we have
u'|I_x + x*u|I_x = c|I_x, |I_x| > n - d


maybe consider the seemingly simpler question of the distance of
x*u' + u
the distance of x*u' is the d.
for a given x, and a chosen c_x, there is at least a distance >= d from x*u' to c_x.
u must account for at least 1 of these errors.

we can consider the set codewords C of distance d from u'. we can reason about the size of C with the Johnson bound.
We then say that C_x, the set of codewords of distance d from x*u' equals x*C.
in this case we are disregarding candidates that are of distance >= d from u' which my be a concern
in this model x*u' + u has distance < d from some element in x*C
we want to show this can occure for only a limited number of x given that u' has distance d from every element in C.

the distance between any c and c' is at least n-k+1.
|c - c'| <= |u' - c| + |u' - c'| <= 2d
thus 2d >= n-k+1, d >= (n-k+1)/2
this just means u' is outside unique decoding which is obvious by definition.
suppose |u' - c| and |u' - c'| share s non-zero indices.
then the distance from c to c' is |u' - c| + |u' - c'| - s.
thus n-k+1 <= |c - c'| = 2d - s
d >= (n - k + 1 - s)/2
idk wht to do with this

think about d=1.
then the error index i is unique and fixed.
thus c is unique
x1*u'_i + u_i = c1_i = x1*c_i
x2*u'_i + u_i = c2_i = x2*c_i
x1*(c_i - u'_i) = x2*(c_i - u'_i)
and u'_i != c_i because its the index of error
thus x1 = x2
so at most 1 x-value can correct this error.

i'm worried about the possibility that even when c is unique, one can use a different c'.
its distance from x*u' will be greater than that for c.
so u will have to correct more errors.
but maybe the errors are set such that it can do this without extra constraints.

maybe we can take the contrapositive and say:
we suppose d <= (n - k)/2
now if u' had distance < d then 
there exists c (closest word, which exists by above assumption) such that
x*u' + u + e_x = x*c
for all x, with e_x having weight < d.
The contrapositive is that still assuming d <= (n - k)/2
if there is no constant c such that
x*u' + u + e_x = x*c
for all x, with e_x having weight < d.
(ie there is an x for which e_x must have weight >= d)
then u' has distance >= d.
i don't know how this helps

maybe worth exploring reductions between the various problems we've considered
if u' + x*u has distance d from c_x
then x^{-1}*u' + u has distance d from x^{-1}*c_x
so for a fiex u' and u, can we say that
u' + x*u has the same distance with uniform x as x*u' + u as long as X includes multiplicative inverses
but this account only for max distance of u' and u, not their total distance

we can transform the original problem to
x*u + e_x in C - u'
C - u' is a set of vectors all of weight at least d.
find u such that for many x-values there exists e_x of weight < d to satisfy the equation for some element of C - u'.
suppose we generalize to consider C to be the whole code.
|(c1 - u') - (c2 - u')| = |c1 - c2| >= n-k+1
so all elements of C - u' have mutual distance >= n-k+1, and they all have weight >= d, and distance >= d from C.
those are the three relevant characterizationwe have about the set C - u' we want to match for inclusion.
think of the set C' = C - u' as an additive offset of C.
find a u such that for many x-values, x*u has distance < d from C'.
maybe better to frame the problem in the x*u' + u version.
u + e_x in C - x*u'
but with C the full code (or just closed under scalars in X), C = x*C, so C - x*u' = x*(C - u')
u + e_x in x*(C - u')
find a u such that for many x-values it has distance < d from x*(C - u') = x*C'
since u can be anything we want to view this problem as one of list decoding.
almost like we're viewing x*C' as a code where each x gives new codeword. 
this is the case because we're asking for how many codewords, x*C', have distance < d from u.
that is, how many codewords, x*C', can fit a ball of radius < d.
a codeword x*C' fits in a ball if any of its members fit in the ball.
so we could ask how many xi*(ci - u') for distict xi are accessible in distance < d.

|x1*(c1 - u') - x2*(c2 - u')| <= 2d-2
x1*(c1 - u') - x2*(c2 - u') = (x1*c1 - x2*c2) - (x1 - x2)*u' = (x1 - x2)*((x1*c1 - x2*c2)/(x1 - x2) - u')
thus subtracting two codewords in distance <= d-1 gives another codeword in distance <= 2(d-1), namely that wrt
x = (x1 - x2) and c = (x1*c1 - x2*c2)/(x1 - x2)
similarly, adding codewords within distance <= d-1 gives one within distance <= 2(d-1)
x1*(c1 - u') + x2*(c2 - u') = (x1*c1 + x2*c2) - (x1 + x2)*u' = (x1 + x2)*((x1*c1 + x2*c2)/(x1 + x2) - u')
namely with x = (x1 + x2) and c = (x1*c1 + x2*c2)/(x1 + x2)

suppose m instances fit in the ball.
can we show this implies the same list decoding capacity for C?
offsetting C by u' I think has the same capacity as C.
then multiplying by x!=0 I think still holds the capacity.
suppose diameter of ball is < n-k+1, so that only one instance of each c can be in the ball.


try induction on degree
don't mind base case for now
consider poly decomposition on u' and u
now consider the codes on u' and u.
suppose you take each code and interpolate it into its parts.
we'd like to show that if u' is far then, so are its parts, similarly for u
now we have an interpolated code of each part
by induction, the random combination of parts preserves distance.
suppose we then interpolate back from parts to whole.
we'd like to show that if the parts are far, so is the whole.
then we finally need to show that the resulting string would be the same as if we just computed the linear combination without first decomposing and later recomposing the parts. 
i'm not sure about this last condition.
clearly it holds for polys, but probably not for arbitrary strings. 

consider the squared-decomposition.
suppose we start with u' and decompose into two parts u1' and u2'
u1'(x) = (u'(x) + u'(-x))/2
u2'(x) = (u'(x) - u'(-x))/2x
similarly for u
u1(x) = (u(x) + u(-x))/2
u2(x) = (u(x) - u(-x))/2x
then we do the random linear combination for all sets of parts
u1'(x) + r*u1(x)
u2'(x) + r*u2(x)
with these as the new parts, we now recompose these parts
(u1'(x) + r*u1(x)) + x*(u2'(x) + r*u2(x))
= ((u'(x) + u'(-x))/2 + r*(u(x) + u(-x))/2)
+ x*( (u'(x) - u'(-x))/2x + r*(u(x) - u(-x))/2x)
= ((u'(x) + u'(-x))/2 + x*(u'(x) - u'(-x))/2x)
+ r*( (u(x) + u(-x))/2 + x*(u(x) - u(-x))/2x) )
= u'(x) + r*u(x)
so it seems to work, but is the above really for any function or does it rely on poly structure?
supposing all x are non-zero, the functions u1',u2',u1,u2 are all defined and from there its just plugging and reducing which holds for any function. so yay!

use the notation above to clarify reductions needed.
need to show that if u' is far, then u1' and u2' have total distance also far
similarly for u
then by induction we assume if u1' or u1 is far, so is u1'(x) + r*u1(x)
similarly for u2' and u2
then we need to show that if f(x) = u1'(x) + r*u1(x) or g(x) = u2'(x) + r*u2(x) are far, then so is
f(x) + x*g(x)

the reason we can use induction is that by definition
u1(-x) = (u(-x) + u(x))/2 = u1(x)
u2(-x) = (u(-x) - u(x))/2(-x) = (u(x) - u(-x))/2x = u2(x)
and similarly for u1' and u2'
thus they can be defined on domains of half the size with the same rate
so we only need to compute u1',u2',u1,u2 on half the original domain.
now this extends to f = u1'(x) + r*u1(x) and g = u2'(x) + r*u2(x)
so when we ask about f + x*g note that f and g are defined on a domain of half the size as our composition.

now how about the second reduction
if f or g is far, what does that say about f(x) + x*g(x)?
note how this is a very different question than the distance of f(x) + r*g(x)
its important to note the condition that f(-x) = f(x) and g(-x) = g(x)
suppose we are given a lower bound on the total distance of f and g.
what this means by definition is that we consider the minimum size of the union of indices such that a subset of those indices correct f and g.
suppose f(x) + x*g(x) is close.
suppose we interpolate to recover f and g.
(f(x) + x*g(x))/2 + (f(-x) - x*g(-x))/2 = (f(x) + f(-x))/2 + x*(g(x) - g(-x))/2 = f(x)
(f(x) + x*g(x))/2x - (f(-x) - x*g(-x))/2x = (f(x) - f(-x))/2 + x*(g(x) + g(-x))/2x = g(x)
now we're asking the question of whether f + x*g is close implies f and g are close.
the other reduction tries to show the equivalent that if f + x*g is far, then so are f and g.
thus the two reductions are the same relation but in different directions.
to summarize we to show that the relation f(x) + x*g(x) <-> (f(x),g(x)) preservers distance (from relevant codes) in some way.

we're looking for a one-to-one distance correspondence as stated above.
maybe for every c with distance d from h = f + x*g we can deduce corresponding cf and cg relative to f and g.
then with
f(x) = (h(x) + h(-x))/2
g(x) = (h(x) - h(-x))/2x
suppose we define
cf = (c(x) + c(-x))/2
cg = (c(x) - c(-x))/2x
then
cf(x) - f(x) = (c(x) + c(-x))/2 - (h(x) + h(-x))/2 = (c(x) - h(x))/2 + (c(-x) - h(-x))/2
cg(x) - g(x) = (c(x) - c(-x))/2x - (h(x) - h(-x))/2x = (c(x) - h(x))/2x - (c(-x) - h(-x))/2x
similarly suppose we're given f,cf,g,cg first
then with
h = f + x*g
suppose we define
c = cf + x*cg
then
c - h = (cf + x*cg) - (f + x*g) = (cf - f) + x*(cg - g)

lets look at the second, that is c - h
we want to know the weight of this vector
wherever cf - f XOR cg - g is non-zero, so is c - h
when both are non-zero, there is at most one x value such that c - h is zero.
without further analysis, it seems it could be that for all x for which cf - f and cg - g are non-zero, x is the root.
this leaves us with c - h having errors as the XOR of errors of cf - f and cg - g.
actually to detail further, the errors must have certain form in terms of c and h, namely
cf - f = (c(-x) - h(-x))/2
cg - g = - (c(-x) - h(-x))/2x
so in this case that h is not erroneous at x, though f and g are, we see that c(-x) - h(-x) != 0 thus h is erroneous at -x.
so f and g transfer shared errors to at least one of h(x) or h(-x), maybe both.

now for the first, that is cf - f and cg - g
first for cf - f
when c - h != 0, for error cancellation we need c(x) - h(x) = h(-x) - c(-x)
for cg - g
when c - h != 0, for error cancellation we need c(x) - h(x) = c(-x) - h(-x)
for both to hold means h(-x) - c(-x) = c(-x) - h(-x) => h(-x) = c(-x)
but this implies c(x) = h(x) contrary to assumption
without further analysis, it seems it could be that never do cf - f and cg - g share an error
this leaves us with cf - f and cg - g partitioning the errors of c - h.
to detail further, take the case that cg - g is zero.
then c(x) - h(x) = c(-x) - h(-x)
thus cf - f = c(x) - h(x)
similarly, when cf - f is zero.
then c(-x) - h(-x) = - (c(x) - h(x))
thus cg - g = (c(x) - h(x))/x
so this means that when they don't share an error, we precisely know the error of the erroneous side.

suppose we start with c and h, decompose and recompose.
when we decompose the errors are partitioned.
when we recompose when none are shared, we end up with errors exactly as we started.
suppose we go the other way, recomposing then decomposing.
when we recompose we take the XOR of the errors (not union as hoped).
when we decompose we partition the errors.
thus we end up with the distict errors to begin with but redistributed.
so it seems stronger to decompose then recompose than vice versa

suppose we decompose, then do a random combination, then recompose.
errors of u' are partitioned between u1' and u2', similarly for u.
then upon taking a random combination, we'd like to get that whp the results have large XOR errors.
then these errors transfer back upon composition. 

suppose we don't start with two (u' and u) but just with one (h)
we decompose into f and g, and the errors of h are partitioned.
we randomly combine f and g, and by induction we'd like to say that since their errors are partitioned, whp a random combination preserves the error number.
then we can invoke direct recursion on this new result
this is what FRI does.
... how? I don't see the opportunity for induction here.

compare total distance vs union distance
lets use total to mean counting duplicates
total distance >= union distance
equality holds when no errors are shared
total distance = union distance + shared error count
so if something has total distance d, then it has union distance <= d
if something has union distance d, then it has total distance >= d
thus for a fixed d, it is stronger to say that something has union distance d.

we still should more formally make the argument about distance preservation.
if h has distance d, select any closest c.
splitting h into f and g, it partitions its errors relative to cf and cg (defined by c).
suppose the error count in these respects is df and dg.
we'd like to contend that this is the smallest total error count of f and g.
for sake of contradiction, suppose there are alternatives cf' and cg' such that total error count (couting duplicates).
clearly adding duplicates makes things worse for this possibility because it adds an error for both f and g, then that those errors get XOR removed in composition.
thus for any smaller total error count errors should not be shared.
upon taking the XOR for composition, we end up with the same number of errors as the decomposition, which has reduced errors, thus we have reduced distance for h. this is a contradiction. 
conclusion we'd like to make is that if h has distance d, then f and g have union distance d.

try the other way
suppose f and g are in unique decoding radius, and share all the same errors.
upon XOR for composition, h may have distance 0 with respect to the composed c from cf and cg.
suppose so.
upon decomposition, f and g now have distance 0 from cf and cg. contradiction.
try to generalize.
suppose f and g have smallest distance total error counts df and dg. this may require sharing errors.
upon composition, suppose some of those shared errors vanish.
then upon decomposition, those errors don't re-appear, leaving a contradiction.
thus it must be the case that upon composition, shared errors persist.
then upon decomposition, that error is copied to both f and g.
the conclusion we'd like to make is that if f and g have union distance d, then h has distance d.


try the base case.
this is for polys of degree 0, that is constants.
evaluation such a poly at a set of n points just means the repetition code.
since k = 1, the rate = n.
a vector is a codeword iff all elements are the same. 
a vector has distance d when all but d elements are some constant.
we are concerned with the distance of random combinations.
consider two indices i,j of the result that are the same.
this means u'_i + x*u_i = u'_j + x*u_j
if u'_i != u'_j or u_i != u_j then there is at most one x that satisfies this constraint.
otherwise it is satisfied for all x

consider the special case n=2.
there are only 2 distances, 0 and 1.
if either has distance 1, what is the probability the result has distance 0?
for the distance to be zero, we need 
u'_1 + x*u_1 = u'_2 + x*u_2
and unless u'_1 = u'_2 and u_1 = u_2, this occurs with probability 1/|F|

more generally we need a way to frame the question in order of qualifications.
a seemingly complex way would be to do separate analysis for all pairs of input distances and calculate probability of distance reduction for each.
a seemingly better way would be to first qualify the distance of the target. then we'd need to qualify possible inputs and probabilities
the last straightforward way to is to first qualify the probability, then inputs and target.
one way I'm intersted in is to first focus on what we know.
for example, we know that for two input rows to have the same output, they must either be the same or else this only occurs for one scalar.
now let's try to scale this notion.
for example, suppose we we have 3 outputs the same. what can we conclude about the inputs? they are either all the same, or at least one is pairwise different in which case it occurs for at most one scalar.
more generally suppose m outputs are the same.
then we have m linear expressions.
intuitively they are all lines and there must exist an x at which point they all cross.
note no two lines except the same can be parallel.
maybe we could imagine plotting all the rows as lines across all x values.
at every point we can count the number of crossings. if t lines cross at an x-value that means all those rows output the same at that x-value. if at an x-value there are 0 crossings, then upon plugging in that x-value the target will have maximum distance n-1, ie all entries will be distinct. 
the maximum number of crossings indicates the minimum possible distance.
we are interested in the spectrum of cross counts, and the expected cross count for uniform x. 
note two lines that are not identical can cross at no more than one x value.
now remember, the distance assumption we can make is in regard to union distance. 
maybe we should condition on the number of distinct lines.


(
regarding multivariate hash, think about using any two of the adversaries satisfying input answers to create an input line, then plug in that input line to obtain output polys, and compare those with the submitted polys. 
)
 

we should return to making an inductive argument.
suppose we have f and g with union distance d
we split into f1,f2 and g1,g2
our goal is to say the distance f + r*g is d whp
f + r*g = (f1 + x*f2) + r*(g1 + x*g2) = (f1 + r*g1) + x*(f2 + r*g2)
we want the errors to "keep their place"

we should take advantage of how we can quantify where the errors are and what they are

we consider f relative to cf and g relative to cg
with union distance d, the error indices E has size d
let us partition E into F,G, and S for shared errors.
now let EF = F + S and EG = G + S be the errors of f and g
partition EF into F1,F2, and FS
partition EG into G1,G2, and GS

consider recursively splitting all the way down to k codes of length n/k. 
this is a theoretical interpolation of degree k.
in practice we'll do an interpolation of degree t < k, eg t = 2.
the final poly will be constructed of k/t parts, each composed of the same random combination of t parts.
show that after the random combination, the union distance of the k/t parts will the same as that of the original k parts.
this union distance is then preserved upon reconstruction.

what would this full decomposition look like?
a0 + a1*x + a2*x^2 + a3*x^3 + a4*x^4 + a5*x^6 + a6*x^6 + a7*x^7
= (a0 + a2*x^2 + a4*x^4 + a6*x^6) + x*(a1 + a3*x^2 + a5*x^4 + a7*x^6)
= ((a0 + a4*x^4) + x^2*(a2 + a6*x^4)) + x*((a1 + a5*x^4) + x^2*(a3 + a7*x^4))
= ((a0 + a4*x^4) + x^2*(a2 + a6*x^4)) + x*((a1 + a5*x^4) + x^2*(a3 + a7*x^4))
= ((a0 + a4*z) + y*(a2 + a6*z)) + x*((a1 + a5*z) + y*(a3 + a7*z))
which has the form of a multilinear poly
the above is decomposed into two branches. that's why its multilinear. we could use the larger branch factor 4.
(a0 + a4*x^4) + x*(a1 + a5*x^4) + x^2*(a2 + a6*x^4) + x^3*(a3 + a7*x^4)
the larger branch factor 8 just returns to the original poly form.
if we have branch factor b, we have a poly with b coefs, each coef being a poly with k/b coefs.
each of the coef polys need only be defined on a domain of size n/b.

we'd like to consider the case of full decomposition, with b = k.
for a general vector, our decomposition should yield a poly with k coefs, each coef a sub-vector.
these subvectors need only be defined on a domain of size n/k.
a subvector is valid if its a repetition code.
call these k subvectors f0(x),...,f_{k-1}(x)
then we express the original vector as
f0(x) + x*f1(x) + ... + x^{k-1}f_{k-1}(x)
given a distance on the original vector, we should be able to infer union distance on the subvectors
then we're faced with the question of the union of random combinations.
the number of combinations is a variable we before called t. each combination contains k/t subvectors.
the subvectors in each combination are pre-determined.
first we can infer the union distance of each combination set.
then we infer the distance of each combination for the same random scalars.

in general we can have any factor b we want, as long as it divides q-1.

suppose we decompose with b = 3.
we have f1,f2,f3
suppose we are concerned with the random combination
(f1 + x*f2) + r*f3
well this is equal to
(f1 + r*f3) + x*f2
now since f1,f3 are of smaller sizes presumably already proven for, we can invoke induction
note however this decomposition only decreases code size by a third, not half.

one thing we'll certainly need to investigate regardless is the union distance of
f1,...,fb
when there is an error in h, there must be one in at least one of fi, or else contradiction.
for every subset of fi we have an intersection set, ie a set of shared errors by all fi in that subset.
when an error is exclusive, ie in only one fi, then we know the error exactly.


statement: if h has distance d, then f = (h(x) + h(-x))/2 and g = (h(x) - h(-x))/2x have union distance d
for sake of contradiction going one way, suppose there is cf and cg such that f and g wrt to them have union distance < d.
let c = cf + x*cg
show that h has distance < d wrt c,
c - h = (cf + x*cg) - (f + x*g) = (cf - f) + x*(cg - g)
show that if h has an error at an index, then so does the union.
for sake of contradiction, suppose h has an error but the union does not.
then c - h != 0, but (cf - f) = 0 and (cg - g) = 0, and thus (cf - f) + x*(cg - g) = 0
this yields a contradiction, with non-zero on the left and zero on the right.

statement: if f and g have union distance d, then h = f + x*g has distance d
for sake of contradiction, suppose there is c such that h has distance < d wrt c.
let
cf = (c(x) + c(-x))/2
cg = (c(x) - c(-x))/2x
show that f and g have union distance < d wrt cf and cg
cf - f = (c(x) + c(-x))/2 - (h(x) + h(-x))/2 = (c(x) - h(x))/2 + (c(-x) - h(-x))/2
cg - g = (c(x) - c(-x))/2x - (h(x) - h(-x))/2x = (c(x) - h(x))/2x - (c(-x) - h(-x))/2x
if h does not have an error at x or -x, then f nor g have an error.
the contrapositive is that if f or g has an error, then so does h at x or -x.
thus for every error of the union, there is at least one for h.
(
we can say more
only f has an error when c(x) - h(x) = c(-x) - h(-x) and the error is equal to this quantity
only g has an error when (c(x) - h(x))/(x) = (c(-x) - h(-x))/(-x) and the error is equal to this quantity
there is no error when the above equation(s) hold but with both sides zero
both f and g have errors when neither of the above equations holds, ie when the errors of h at x and -x are not associates.
)


I think we should return to the base question.
here we can more generally investigate the distance of a random combination of vectors given a union distance on them.
to begin, we can return to the simplest case we started before, where we have only two vectors.
I think we want to say that whp the distance will be the union distance.

a basic questions is given t distict lines, what is the probability on uniform input that s of them evaluate the same?
for a fixed 2 of them, the probability they evaluate the same is 1/|F|.
for a fixed 3 of them
    suppose all are linearly independent.
    then they cross at 3 distict points
    the probability of a crossing is 3/|F|
    suppose they are linearly dependent
    suppose there are two crossings
    the probability of a crossing is 2/|F|
    suppose there is one crossing, ie a double crossing (maybe call it second degree)
    the probability of a 2nd degree crossing is 1/|F|
we could calculate the expected number of crossings, but would this be useful?

I think we can create classes of pairs of lines according to crossing point.
for each point there is a class. a pair of lines belongs to the class at which point they cross. 
some theorems
(a,a) belongs to all classes
(a,y*a), y!=1 belongs to no classes
(a,b) and (b,a) belong to the same class
if (a,b) and (b,c) belong to the same class, so does (a,c)
we could partition the lines into independence classes.
futher, each class could be partitioned identity classes.

we need to represent our assumption of union distance in terms of lines
suppose two vectors have union distance < d. what does this mean?
it means you only need to change < d lines, whether offset, slope, or both, to obtain all identical lines.
in other words, to say two vectors have union distance d means you must change at least d lines to obtain all identical lines.
so it seems union distance translates into distance of the lines from a repetition code.
without only a union distance assumption however, and nothing about the error values, we can't assume anything about how these lines relate, eg how many in a given independence class.

for the crudest soundness analysis then, we calculate the max probability the distance decreases even by 1.
suppose the union distance is n-1.
in this case all n lines are distict.
all we need to decrease distance is for any two lines to evaluate the same.
how many is the max number of possible distinct crossing points?
whatever, this number, the probability of hitting one is this number over the field size.
without more analysis, we can assume the max possible of (n choose 2) = n*(n-1)/2
remember in this case n is the rate, so this seems reasonable for now, though this might blow up later.
but we can try further analysis, because crossings can't all be independent.
if (a,b) cross at x, and (b,c) at y, where do (a,c) cross?
a1 + x*a2 = b1 + x*b2
b1 + y*b2 = c1 + y*c2
a1 + z*a2 = c1 + z*c2
z = (c1 - a1)/(c2 - a2)
--
(c1 + y*c2) - (a1 + x*a2) = (b1 + y*b2) - (b1 + x*b2)
c1 - a1 = (y - x)*b2 - (y*c2 - x*a2)
--
a1/x + a2 = b1/x + b2
b1/y + b2 = c1/y + c2
(c1/y + c2) - (a1/x + a2) = (b1/y + b2) - (b1/x + b2)
c2 - a2 = (1/y - 1/x)*b1 - (c1/y - a1/x)
--
z = ((y - x)*b2 - (y*c2 - x*a2)) / ((1/y - 1/x)*b1 - (c1/y - a1/x))
---
however, for small n (rate), which we'll have, I think every pair of lines can cross at a distinct point.

So given vectors with union distance d, the above bounds the probability the result has distance < d.
however, also relevant to our inquiry is where the errors are in the result.
suppose the distance does not decrease (which holds whp). then the errors are precisely in the same place as the union distance. 
keep in mind, however, that the union distance errors are not unique, and we could compare to multiple word pairs for union distance d.
we are saying that in any case, whp whence the result has distance d, it differs from a closest word with errors in the same locations as the union distance. 

but we need to consider the same random variable with respect to multiple combinations.
we know the probability it preserves distance for one combination, but what is the probability it preserves distance of all combinations?
for each combination consider its distortion set, ie the set of all x-values that decreases distance.
with strict equality, the probability any combination breaks is the size of the union of all distortion sets over the field size. 
above we bounded the size of the distortion set for any combination. 
thus we use the inequality that the size of the union is less than the sum of the sizes of distortion sets, which we can bound. this inequality is equality when the distortion sets have trivial intersections.
now suppose we do the full decomposition.
we start with a code of length n.
we can do log(2,k) rounds, assuming n and k are powers of 2.
after i rounds, we have split it into 2^i parts.
so the most we can decompose the n size original code is into k parts, each a repetition code of size n/k.
we have really decomposed the n-entropy original code into a k*n/k = n-entropy alternative representation.
the bound above for the size of a single distortion set is (n/k)(n/k - 1)/2. 
there are k/2 combinations.
Using the inequality we bound the union distortion set size to be k/2 times the bounded distortion set size, that is
k/2 * (n/k)(n/k - 1)/2 = n(n/k - 1)/4
we need to make this number negligible with respect to the field size.
2^{-m} >= n(n/k - 1)/4F
F >= 2^m * n(n/k - 1)/4
log(F) >= m + log(n(n/k - 1)/4) ~ m + log(n)
which is reasonable.

we could also try induction instead of the full decomposition and see if we get the same result.
combination of two of size _: distortion set size.
size 1: (n/k)(n/k - 1)/2
size 2: 2*(n/k)(n/k - 1)/2
size 4: 2*2*(n/k)(n/k - 1)/2
...
size k/2: k/2*(n/k)(n/k - 1)/2
same result!

to intuit and legetimize our base case analysis, it could indeed be the case that each of the k vectors has max distance (n/k - 1). this is because any entry from each vector, of which there are k, can remain fixed, and all the others, (n-k) of them, are switched. this means the max distance is n-k, which matches RS codes, because any k elements in an RS code can be an information set, and all the rest may need to be switched. 

we could try to take account of the probability that the distance decreases by a small amount.
for our base case above, breaking distance 1 means cancellation of two errors, thus the evaluation not 2 but 3 different lines to the same value.
there are at most (n choose 3) ways this can occur. 
...

we could try to generalize to larger combination degrees t. 
for the base case this means we take a random combination of t vectors. 
note in general we can multiply all vectors by a random scalar and then make the argument that whp recomposing the results will not cancel any errors, such that they appear at both x and -x.
now again we have the same definition of union distance on t vectors. is the number of lines that much be changed in some way such that all lines are idential. 
so what is the probability that we choose a random combination such that two evaluate the same?
one way to do this is recursively. what is the probability the first random scalar cancels an error, and then probability the second scalar cancels. now since the scalars are independently random, we can say these are independent events.
then we calculate the probability of breaking as 1 minus the probability of niether step breaking. that is
1 - (1 - (n/k)(n/k - 1)/2F)^{t-1}
for t-1 steps involved for a combination of t vectors.
now this probability of breaking is greater than for t = 2.
the benefit is that we multiply this by k/t for the final probability, which is then
k/t * (1 - (1 - (n/k)(n/k - 1)/2F)^{t-1})
eg for t=3 we have
k/3 * ((n/k)(n/k - 1)/2F + (n/k)(n/k - 1)/2F - ((n/k)(n/k - 1)/2F)^2)
= k/3 * (n/k)(n/k - 1)/F - k/3 * ((n/k)(n/k - 1)/2F)^2
= n(n/k - 1)/3F - n^2(n/k - 1)^2/(12kF^2)
which I think is slightly worse than for t=2. I'd expect the trend to continue towards worse as t grows. 
the benefit of larger t is fewer rounds. 


note
in all entropy dropping cases, the commit handle gives the verifier the ability to randomize a new commit handle from it. in some cases this only involves the handle, like in pedersen commits, and sometimes it involves meta data like merkle paths. the point is the prover has no say, even if expected to provide metadata, and verifier has complete control over randomization. 

in fact, this leads me to realize that we can use code based commits more generally where we don't need to rely on making the core query by opening an entry. This means we can use random codes, with no need to reduce to evaluating it on some entry of that code.
the idea is we treat the commits as commits to knowledge of polys. then we take a random combination and ask for the result. the result will only pass whp if the original two commits are valid in that the prover had knowledge. the result is either raw data or a new commit, but in either case represents the corresponding random combination of the polys. thus this approach needs the codes to be linear, though not structured. 
thus like all our other entropy-dropping methods, its an approach to reduce a commit about two polys to a commit about their random linear combination. to use this, the prover will make claims about poly evaluations (maybe multivariate) prior to the verifier choosing the random combination. then those claims are reduced to a claim on the result. so its only the consistency checks that are with respect to univariate polys, while the claims can treat the poly as a multivariate. 
note, however, direct recursion can't be used for multivariate claims, because we must check the claim by calculating on the result before recursing. in the case of univariate claims, however, the code naturally already calculates the claims. what's important to us for short proofs is that the intermediate step before recursing is small enough for multivariate claims. it should really just be a quick sumcheck, with a small proof size. these extra elements are costly, so maybe only worth it if we can take advantage of random codes to decrease consistency checks otherwise necessary.

i'm thinking this may simplify the problem statement. 
in short, show that if result passes, then it is the correct random combination of fixed polys of which the prover had knowledge represented by the commits.
the contrapositive is that if the commits don't represent knowledge of fixed polys, or they do but the result is not their random combination, then the result won't pass.
design an extractor that given an adversary that can pass the test can exctract source polys which have linear combination equal to the result. 

we'd like to take advantage of the hardness results relevant to linear codes
one direction I'm thinking of is to say either the result has distance closest to the random combination of the sources, or if not the prover doesn't have knowledge of the closest word. The argument would be something like such knowledge would allow for finding two closest words, thus calculating a word of minimal weight (ie minimal distance) which we conjecture hard to do.
maybe consider an extractor that finds nearest words for a noticeable fraction of random combinations. then work with these instances and submitted results, and try to find minimal distance. 
for example, subtracting two instances gives a word a certain distance from the scaled vector. maybe calculating this work for all pairs yields enough codewords within a certain distance that at least two must be closest. 


stop and think for a bit about the multivariate hash.
there are a noticeable number of x-values for which the adversary can find inputs.
for any two of those inputs (corresponding to distinct x-values) we can construct a input lines, then pass through and obtain output polys.
we'd like to prove that the adversary has knowledge of a line that results in the commits, and that all correct inputs of which it has knowledge lie on that line. ie all inputs lie on the same line and that line results in the commits.

one approach is to take all distinct constructed outputs, for which the adversary has complete pre-image knowledge, and argue that these outputs span a noticeable amount (too much) of the target space, such that upon a random target the adversary would have noticeable probability in succeeding. 
for this would could argue that any two distict constructed outputs (suppose quadratic) agree on at most 2 points.
suppose the target dimension is t.
suppose we have p distinct outputs.
maybe we can say the outputs span a space of size p*F - (p choose 2).
then the probability of success for a random target is
(p*F - (p choose 2))/F^t
now p depends on the probability of success breaking the commit.
the larger p, the larger the success of breaking the commit, but then we can also say the larger the sucess for a random target. the threshold on this spectrum is we'd like to say when there is noticeable success of breaking the commit, there is noticeable success for a random target. 
noticeable success for breaking the commit, means that there is a noticeable number s of points for which the adversary can answer which do not lie on a line that results in the commit.
I think this means one can construct roughly p = sF distinct outputs.
then the probability of success sF^2/F^t.

what if we interpolate more than just lines to construct outputs. for example, we take 3 points, which don't lie on the same line, and construct a degree-4 output. we could do this for arbitrary l points that form a degree-(l-1) input to the construct a degree-(2l-2) output.
if we manually plugged in a degree l-1 poly we could expand it to an output in poly time, but quicker would be to just interpolate by evaluating in unexpanded form at 2l-1 points then doing lagrange construction for the output. 

actually the previous two approach don't seem to work because they would work in general and don't take advantage of adversary's ability. in general one could just choose an aribitrary number of lines, larger degree inputs, even multivariate inputs, evaluate them, then hope the results span enough of the target space. the reason this approach doesn't work is one would have to construct too many outputs to span the space costing too much time, and once a target is selected one then faces the challenge of finding the output that can be evaluated to match the target. 

try composing multivariates, where the input is linear in as many variables as the original problem such that it just transforms into a new problem instance. the extractor might transform the problem submitted to the adversary. 
think about composing in general.

note a generalization of this hash is any hash where the input is the line in one variable. this works when the output has small description with respect to this single variable. 

for a collision resistant version, consider two output polys for which the adversary has knowledge of input polys, while this may not necessarily include the original commit poly.
then find two different points for which the two outputs evaluate the same. then plug those points into the input polys and whp they should result in two different inputs, which will evaluate to the same output, thus a collision. 
but finding such a point may be even statistically unlikely.

collision resistance is a needed property, so we can use that as a security assumption.

suppose we propose a new problem, maybe reducing it to another multivariate problem.
i'm thinking of modifying where there is only one poly, not multiple. 
suppose we take the original problem with multiple polys, and we expand into monomials. 
we can think of the input as field elements, but the coefs as vectors of field elements.
I think these vectors form a ring, as they are the direct product of fields. 

if we think of the input elements as all-identitcal vectors then we can think of the equation as a polynomial in this ring.
but the original problem in expanded monomial form need not be thought of as a polynomial and can instead be thought of as a vector space with the coefficients and result as vector elements and the monomials as scalars, which are field elements.
thus I think we can generalize the problem most elegantly to finding field inputs for a multivariate vector space equation over that field.
the parameters are the dimension of the vector space, the number of variables, and their max degree.
in the original case our vector space of dimentions m is the mth direct product of the additive group of the underlying field. 
recall that for all vector spaces over a field there is an isomorphism between vector elements and vectors of the field elements (the linear combination of which equals the vector element wrt a certain basis).
thus our choice of the vector space and the underlying field is standard.

we could think of it as a linear equation Ax
A is m by n, and each column is a vector space element.
x is of size n which is the number possible monomials composed of v variables.
the result is of size m and is a vector space element.

it is worth comparing this problem to generalized SIS.
one version of SIS is a combination with small integers over an abelian group.
more specific is where the abelian group is a direct product of n groups modulo an integer q.
note that if we replace the groups modulo q with extension elements of a field of size q = p^d, this would be isomorphic to a direct product of n*d groups of modulo p.
this leads to the known trade offs between q and n.
on the other hand, in a multivariate problem setting these direct products are not isomorphic because the scalars may be extension elements, not just integers.
in particular, scalar multiplication is not isomorphic to repeated addition.
now suppose we replace the small integers with field elements, maybe of an extension field.
then since SIS is linear, we are basically left to just solve a linear system.
thus the linearity of SIS is compensated for by the small integers.
likewise, the field scalars of a multivariate system are compensated for by non-linearity.
now we could take the hard parts of both systems, whence we have a non-linear combination with small integers (ie limited field elements).

I'm interested in a second generalization of the multivariate problem.
suppose we restricted the field elements, eg to a subfield.
analogously, suppose we increased coef field size and in return reduced the number of equations.
with respect to the previous generalization, this means decreasing the degree of the vector space, but keeping its size the same relative to the input space.

suppose this worked.
in the protocol, the verifier is expeded to commit to input x*t + y where x and y are v-length vectors in a subfield.
the commit value is a univariate in the ambient field.
for opening, the verifier chooses a random t = r within the subfield.
the prover is expected to submit x*r + y which should be located in the subfield.

an analogous setup for SIS is where scalars are not restricted to small integers but rather to a subfield.
There is no subfield of a field modulo p, so we would need to switch to an extension field, allowing scalars to be anything in the field modulo p.
Then the difference between this and the above system is that here we would have linearity at the cost of multiple equations.
now given commits, one could take a random linear combination of them that should result in a new commit.
maybe we could do a ring version of this, but with carefull soundness with respect to the random combination because the ring could have zero divisors.
in this case we may not need extensions because subrings exist in non-prime modulo q.
in appears niether of these versions, field or ring, is analogous to SIS, because in these cases the scalars are closed but in SIS, regardless how we modify the problem, scalars remain unclosed.

regarding the SIS version, the only opportunity I see is to further examine the possiblity and then propose a new computationally hard problem in the version where the scalars are closed, eg a subfield or subring.

now with respect to the generalized multivariate problems, do we have reductions for them?
i'd like to focus on the latter version of a dimension 1 vector space, ie inputs in subfield.
in this case we take advantage of there only being one polynomial. 
note the ambient field will likely be very large. 
like before, we need to assume collision resistance, so we could first reduce to that.
then we must reduce collision resistance to a hard problem. 

suppose the subfield is that modulo p.
then a system of n equations is equivalent to one equation of the degree n extension field.
thus we may safely question the one equation system.
the challenge of multiple equations has now translated to the challenge of random choices of t being restricted to a subfield.

maybe show that having knowledge of enough points on the output to fully determine the output should be enough to find knowledge of a line that results in the output.
this is different that what we were saying before, where we only asked for knowledge of enough points to determine an input, which is fewer points than to determine the output, despite that input determining some (though maybe not the same!) output.
if any subset of the inputs (over)determine the right output we are done. this means knowledge of a line for the right output.
suppose none of the inputs determine the right output. that is, upon every pair of inputs that determine a line, none match the right output.
then at most one input can be on the line which determines the right output, for it two inputs were on the line it would determine the right output.
here we can't go further because gaining knowledge of a line now would lead to a collision.

suppose we're given knowledge of enough points to determine the output like above.
we want to use this knowledge to find a line that matches the output.
a basic question is whether all these known points lie on the same line.
if so this line is the natural answer.
but what if not?
i suppose there's nothing we can do.

f(x + y)
= a(x + y)^2 + b(x + y) + c
= ax^2 + bx + c + ay^2 + by + c - c
= f(x) + f(y) - f(0)
f(x + y)
= a(x + y)^3 + b(x + y)^2 + c(x + y) + d
= (a(x + y)^2 + c)(x + y) + (b(x + y)^2 + d)
= (ax^2 + ay^2 + c)(x + y) + (bx^2 + by^2 + d)
= ax^3 + ay^2x + cx + ax^2y + ay^3 + cy + bx^2 + by^2 + d
= (ax^3 + bx^2 + cx + d) + (ay^3 + by^2 + cy + d) - d + (ay^2x + ax^2y)
= f(x) + f(y) - f(0) iff x = y
so it the pattern doesn't extend via decompositions to higher degrees
but I think we have the general pattern
f(n*x) = n*f(x) - f(0) 

suppose we look at a quadratic system plugging in a line, resulting in a quadratic univariate.
we'd like to apply the rule
f(x + y) + f(0) = f(x) + f(y)
we can plug in any value for t we want, but the rule above says for certain combinations of inputs we can get identical outputs.
I don't know how to use this.


if you don't have knowledge of a line that results in the commit, and won't be able to find one, what is the probability you can find an input for a random point on the output?

if you have knowledge of preimages for an overdetermining fractions of points on the commit, can you find knowledge for all points on the commit?
intuitively, these points for which you have knowledge have a pattern, so it seems your knowledge of inputs should also follow a pattern.
A collection of p points have a pattern if their interpolation has max degree < p-1. 
A simple definition for a collision then is when the inputs don't have this pattern, but the outputs do.
Another is maybe the ratio of the input interpolation poly size over the output interpolation poly size. 

form the problem as having multivariate coefficient matrix A which is n by m (n equations, m monomials).
transform the problem to a new one by an n' by n transformation matrix T (n' equations).
to invert the transformation, T needs an n by n' left inverse.

suppose we choose a transformation matrix T that sends the coefs of the commit polys to 0.
this way any adversary input to the previous problem is an answer to 0 for the new problem.
since this transformation has non-zero kernel its not left-invertible.
but we can transform it again with another transformation S to a different problem.
now any answer for the original problem is answer to 0 for the T problem and remains an answer to zero for the S problem.
Of course the S problem is the ST transformation of the original problem.
but an important point is that T is not known until the commits are known, which is not known until the original problem is defined.


only works with cubic or above
try all pairs of inputs given to make as many lines as possible.
each line will result in a different poly, none equal to commit.
each will agree at least two places with commit.
with high enough degree, and enough lines to try, there may be enough probability one can find a third place they agree and the adversary can answer for. 
this then yields a collision.
before we thought the probability of finding this third point was hopeless.
but suppose we can establish that statistically it exists.
then we are left to efficiently find it.
suppose we take a random linear combination of commits and equations.
then we plug in as many lines as we can into the reduced equation and look for a point at which the output evaluates the same as the reduced commit. this is root testing.
the problem with this method is it doesn't seem to take advantage of our circumstances.
you might as well just plug in random lines and whp you'll still get a collision.

maybe consider that in reasonable time one can only compute a negligible fraction of outputs. thus if one is know inputs for more than a negligible fraction (regardless what pattern the outputs have) one must use some other means to obtain these inputs than from an arbitrary table.

interpolate a bunch of points in the input, enough so the output over determins the commit. 
now the new output and the commit can't agree on any more points.
now choose a new point.
if adversary's input lies on the existing interpolation without modification, then we have a contradiction because its output should differ from the output of the existing interpolation.
thus the input cannot be on the interpolation
the point is that to avoid contradiction the interpolation must have max degree, not just degree above a line.

it looks like this task is randomly reducible.
you are given the task of finding such a 'higher' collision.
you first randomly transform, via an invertible transformation, to a completely independent problem.
the adversary provides commits and a higher collision for them.
invert these commits back to the original problem to form you own commits, and your inputs are identical.
but the flaw here is that the two problems are not completely independent unless the number of equations is the same as number of monomials.
or put another way, these problem are randomly reducible between each other with respect to some basis (the multivariate coefs).
if you find a pre-image for zero, or a collision in one, it applies to all.

lets restate the problem in vector space notation.
the problem is with respect to some basis of size corresponding to degree d and variacy v parameters of the problem.
give an output basis to be treated as a univariate expression of degree d.
for a random evaluation of the output, find field elements for a pre-image.
both basis, the main basis and the output basis, span some space in general, and perhaps a more restricted space via their evaluation forms (multivariate and univariate respectively).



back to making use of random codes.
suppose we decompose a random code similar to how we do for RS.
we start with a random code B of size n/2.
then we combine with itself in some pattern for a larger code A, like for f + x*g version of RS we have
A = [A1,A2]
A1 = B + x*B
A2 = B - x*B
from which you extract two instances of code B and then take a random combination.
this is slightly better than just starting with a single n size random code, because our analysis from before still applies which guarantees a certain union distance.
but this still doesn't take advantage of any hardness of random codes.


suppose we are given a set of successful instances, those with close distance, from the adversary.
we'd like to apply some transformation that maps all the codewords to one, and the corresponding instances will surround this single word.
consider just summing them all.
then each instance undergoes the addition of all other codewords but its own.
for instance a, its codeword c, and the sum of other codewords c', we have
|(a + c') - (c + c')| = |a - c|
thus each instance maintains its distance through this transformation.
now we can compare the distances between the modified instances.
for modified instances a' and b' and central word c'
|a' - b'| = |(a' - c') + (c' - b)| <= |c' - a'| + |c' - b'| = |ca - a| + |cb - b|
but with
a = u' + ra*u
b = u' + rb*u
c' = ca + cb + ci
a' = (cb + ci) + a
b' = (ca + ci) + b
we have
|a' - b'| = |((cb + ci) + a) - ((ca + ci) + b)| = |(cb - ca) + (a - b)| = |(cb - b) - (ca - a)|

this just yields the trivial inequality
|(ca - cb) - (a - b)| = |(ca - a) - (cb - b)| <= |ca - a| + |cb - b|
we were hoping to do more directly analysis with a' and b' involving u' and u without ca and cb.
we know |ca - cb| >= n - k + 1
and |a - b| = |(u' + ra*u) - (u' + rb*u)| = |(ra - rb)*u| = |u|
by reverse triangle inequality
|(cb - ca) + (a - b)| >= abs(|cb - ca| - |u|)
in total yielding
|ca - a| + |cb - b| >= abs(|ca - cb| - |u|)
maybe we can use all these pairwise bounds obtain a probagbility distribution on single distances.

we would probably condition on |u|.
suppose |u| <= n - k + 1
then since |cb - ca| >= n - k + 1 we have
|cb - ca| - |u| >= 0 for all a,b

...
all this is not with regard to random codes, so its just distance analysis again which we've already solved satisfiably.

for a random code suppose we split it n instances of size n.
then I think we can apply hardness of random decoding
the benefit is fewer openings.
but now each opening requires like n*log(n) field elements.
this is only worth it if less than the n*k original elements to look at.
that is, if the #openings < k/log(n).
this might we worth it for big batch amortization.
now regarding hardness, the idea is we could map to a random vector.
but we can only do so if the rows of the instances are independent.


start with vectors for which you know the closest codewords.
you can mutate these vectors, changing the errors but maintaining the distances.
now you know the closest codewords to more vectors than you began with.
while this seems trivial, make an inductive argument from it.
we want to use the following ability to do random decoding: if you know the closest codewords for some vectors, you know the closest codewords for a noticeable fraction of the space they span.
we iterate from the beginning, starting with the commits.
we take random combinations, and have a certain number of successes for the adversary can return closest codewords.
then the key part is we mutate these vectors, but keeping them within distance.
this way we now have a linearly independent set of vectors from which we can span further.
the expansion grows exponentially with the steps, but at each step, however, there is only a noticeable probability of success.
thus we can only iterate a constant number of times to maintain the noticable probability.
we also have the challenge that our goal is not just to grow the span size, but to target a particular instance, ie a worst case instance the extractor is given to solve. 
if we can't find a way to mutate strategically to reach the target, maybe we just focus on span growth, maximizing the probability it embeds a target instance (or one of many we could be trying to solve for).

an important point here is that in the main statement the probability is taken over the random combination, not including the original commits, because they can be strategically chosen by the adversary.
but we are assuming the adversary also has success for the random combinations we give it, which we choose for it.
if no other way to compensate, maybe we can change the protocol somehow such that the beginning commits are pseuorandom.
then we can take probability both over starting vectors and combinations. 

suppose we take the special case of the closest codeword and convert it to any codeword, by adding the relvant distance.
then it still remains to handle the adversariy chosen error indices and their differences from the closest word.
if this was RS we could partially randomize error positions by rotation, but this is not for RS.
we could handle the number of errors by maintaining it throughout, and then doing a case by case analysis.
but positions and values still remain.
maybe we could argue that positions don't matter, because the code is random. not sure how to state this.
well consider again the computational problem. given a random vector, find a closest codeword.
if you know a set of error positions, I think its trivial to recover the codeword. eliminate erroneous words and compute matrix inversion, even if not unique.
but the adversary task we're considering is, given vectors with certain distance around known, fixed codewords, find closest word of a random combination.
we'd like to contend that if the adversay can do it for any vectors it can do it for all.
the challenge is that the adversary knows the errors with respect to these codewords and may take advantage of their particular pattern. 
well consider starting vectors u' and u, scalar r.
suppose we offset u' + r*u appropriately to return to each of the starting words.
now suppose we take a random combination of any of the 4 pairs of vectors.
this again results in a random combination of u' and u, thus the adversary can solve with noticable probability.
we can continue like this, hoping that each time the adversary returns a vector such that its offset results in one linearly independent from the rest.
if we get enough that are linearly independent we can solve a random decoding problem with noticeable probability.
this protocol differs from the one before in that we don't mutate, only offset.
but what is to guarantee the linear independence?
towards a contradiction, suppose the adversary returns one with linear dependence with respect to those existing.
eg suppose c1 and c2 are starting words, and cr recovered word.
then our new vectors are (u' + r*u) + (c1 - cr) and (u' + r*u) + (c2 - cr).
suppose the first is linearly dependent with coefs k1, k2.
k1*u' + k2*u = (u' + r*u) + (c1 - cr)
(k1 - 1)*u' + (k2 - r)*u = c1 - cr
this means for k1,k2 != 0 this combination of u' and u is a codeword, which we can show to be unlikely.
thus in the first case the returned words won't be linearly dependent.
i hope we can extend this to further iterations.
for futher interations, suppose v' and v are any existing vectors or their combinations, ie in the existing span.
let cv be recovered word.
what happens if (v' + r*v) + (ci - cv) is in the same span, ie not independent.
consider u1,...,ul and k1,...,kl such that
k1*u1 + ... + kl*ul = (v' + r*v) + (ci - cv)
then absorbing v' and v into the combination, we've found a combination that equals a codeword.
so soundness holds as long as we can show it unlikely to find a combination with distance 0.
lets first return to the first case on u' and u.
we may have to take advantage of how cr is partially randomized.
for that matter, c1 - cr is partially randomized in that it can any of a bounded number negligible with respect to the total number of codewords.
We know that if their combination equals a word, no combination equals any other word, so while dependence is possible it has negligible probability for this starting case.
now returning to the general case on v' and v.
k1*u1 + ... + kl*ul = (v' + r*v) + (ci - cv)
again I think we try to say that since cv is partially randomized, so is ci - cv to a certain (less) degree.
further, linear combinations of u1,...,ul can only equal a small number of codewords, hopefully shown to be negligible with respect to the number of possible ci - cv, though we must union bound over i.


back to multivariate

we can show that when offseting a problem by any constants b,
f(x + b)
the system remains random and is just as hard to solve.
eg this is used to show that finding a collision
f(x + b, y + c) - f(x, y) = 0
is hard because its a random quadratic system regardless of b,c.

note how multivariate polys can just be considered as the only single way to implement low degree hashes under arithmetic operations native to the field. any such hash, ie low degree and only involving arithmetic operations, can be represented as a multivariate system.
we should frame the problem for generic 'low degree native operation' hashes, and then comment that we will always represent via multivariates.

note how all our proofs of knowledge are about reducing claims on polys to claims on their linear combinations.
the interesting part here is we are always taking random linear combination, because we have a homomorphism on claims
f(x) + r*g(x) = (f + r*g)(x)
so I'm wondering if we can use any other homomorphims beyond the linearity of polynomials, ie how they form a vector space over scalars.
the reason this special case is natural is because claims are naturally single field values (evaluations of functions). 
thus to randomize them it is natural to take a random linear combination.
any other randomization method seems like it will reduce to this.
maybe this is reason enough to stick with the particular vector space homomorphism we already use. 
the homomorphism pattern is characteristic of what makes proofs work. the prover always dos the left side, ie treatment of f and g without knowing r, then verifier chooses r, then prover provides f + r*g and verifier can treat it and should get the same answer. in short, the left hand side belongs to the prover, the right hand side to the verifier, and equality allows for completeness, and randomness of r allows for soundness. but this would allow using any finite vector space. another requirement is when elements are different, the evaluations are different. we should frame the problem for any homomorphism, instead of as functions. For homomorphism h, and x and y as vector space variables, and scalar variable r we have
h(x + r*y) = h(x) + r*h(y)
suppose the goal is to compute this homomorphism for fixed x and y. The prover computes for x and y. The verifier chooses r. The prover provides x + x*r and the verifier computes for this result and checks for consistency given computations on x and y. Completeness follows by equality. For soundness we assume that if h(x) != x' or h(y) !== y', then
h(x) + r*h(y) != x' + r*y', that is, h(x + r*y) != x + r*y.
...
this is tricky we should frame it more simply because we're combining two kinds of soundness. for our case of polys, first x and y are chosen by prover, then h by verifier, then h(x) and h(y) by prover, then r by verifier, then x + r*y by prover. Yet both kinds of soundess are schwartz zippel. So may be could frame it more simply with only one round. For polys, this would be prover chooses x and y, verifier chooses h, and checks that h(x) = h(y), but this involves no homomorphism.
We've always thought of the probability as taken over h, but what if its taken over x and y? For our case this mean choosing from a much larger space, and I think probability still holds. For polys this means for a fixed evaluation point, two polys chosen at random will evaluate differently. We think of h as the computation and x as the input, so its natural to consider h as being chosen randomly by the verifier, and the input to be chosen by the prover. I suppose this is sufficient to justify only considering probability over h, because we could switch it if we desired to x(h). So soundness then says that for two distinct inputs x and y, whp over choice of computation h, outputs differ. Thus h must belong to a class of computations. Note how this goes counter to the convenience of computation. Usually programming we want computations that result in the same value for many inputs. But here it must be that results differ whp.


assuming higher degree multivariate problems are hard, we should try first with high degree d.
suppose 3 inputs are not on a line, but the outputs are all on the degree d commit.
interpolating the 3 inputs and plugging in we get a degree 2d output that agrees with the degree d commit at 3 points.
we want to draw a contradiction for this.
with high degree d we won't be tempted to using low degree properties, forcing us in the right direction.
to still make this problem intuitively hard, we need the number of 'uncorreclted' inputs to exceed number of possible 'uncorrelated' outputs, which is d+1.
so the problem is to find d+2 inputs that lie on a degree d+1 poly, and hash to outputs that lie on a degree d poly.
in the quadratic case, for verification, this means we have 4 inputs that one on a cubic, and hash to outputs that line on a quadratic.

note that if you have an output of degree k and it agrees with the commit at >= k+1 points, it equals the commit.
this yields a bound in the direction we're looking.
consider interpolation p points into a degree l input.
this yields a degree <= l*d output.
the output and the commit agree on all p points, and maybe more.
the output and commit are equal if p >= l*d + 1.
thus for the output to not equal the commit we have p <= l*d.
so we can interpret this to say that if p inputs hash to a degree d commit, they must have interpolation degree l >= p/d.
for us this means if adversary knows a large fraction of the domain that hashes to the commit, the corresponding inputs will have large interpolation degree.
this basically says you can't barely cheat, either you don't or you really do.
if you know uncorrelated inputs that hash to correlated outputs, your inputs must be very uncorrelated, not just slightly.

maybe can we say there must be a way to take the adversary inputs and expand them in some way other than plain interolation such that composing the expansion with the multivariate results in the commits or for that matter any low degree output?

we are underestimating the hardness in our task definition.
we are not including how the verifier samples randomly.
the prover passes with probability equal to the fraction of the domain for which it knows inputs.
suppose there are p such points.
by above, they must have interpolation degree l >= p/d.
with p being a noticeable portion of the domain, this means an "l to d" collision with very large l and very small d.
a tricky bit is that the prover may not actually have knowledge of all these inputs at any point in time.
rather, the prover has the ability to generate one with a certain probability on a random query point.
still, the extractor running in poly time can extract a noticeable number of successes p' resulting in a still-large l' = p'/d to d collision.
thus we can set p to p' and just assume the prover has static knowledge.
so how hard is it to have knowledge of an l = p/d to d collision where p is noticeable over field size?

try transforming the problem to obtain more outputs
one way is taking combination of equations, for which the same input continues to work.
another is to transform the problems to one still random by offsetting inputs.
for a fixed target chosen independently of the original problem, the new problem is hard to solve for any offset.

each input for the previous problem can be modified (subtracting the offset) to an input for the new problem with the same output.
show that this modified set of input has the same interpolation degree.
since the new inputs are just a shift of the old ones, their interpolation is also just a shift.
but a shift to an interpolation is just an additive constant, thus interpolation degree remains the same.

suppose we plug in the original set of inputs for the new problem.
what outputs do we get?
if the inputs are on a line, then absorbing the offsets of the new problem into the line, we end up with the original problem evaluated on a new line.
in this case the outputs will have low degree like the original outputs.
but we're concerned instead with the case the original inputs have large interpolation degree.

starting with a cubic system f, consider using offset c to reduce to the quadratic system for a target t.
f(x + c) - f(x) = t
c can be chosen adaptively after t, though f is chosen randomly wrt t.
this is only useful when degree f is >= 3.

keep in mind quadratics may still work, because collision resistance may be too strong.
instead we might be able to settle for just second preimage resistance.
with quadratics, commit degree is d=2.
from above this yields interpolation degree >= p/2 for p points.
we're interested in the hardness for p >= 4
but we can take advantage of large p, because prover succeeds for large p.


use notation 'vector commitment' instead of 'polynomial commitment' because its irrelevant how the coefs are interpreted for a polynomial, eg multivariate, univariate. 
instead we are just concerned with interpreting the vector as an element in a vector space. 


what if we can show that due to protocol prover has negligible success of cheating even if cheating on multivariates.
suppose we can assume/show that prover knows at most one input for random commit point t.
this random input will correspond to the (random) t combination of polys and their evaluation claims.
so what is the probability this prover-known input will evaluate the same as the combination?

protocol: prover makes evaluation claims c1,c2 of two virtual polys f1,f2 at point x.
verifier takes random combination these claims with scalar t, ie c = c1 + t*c2
prover returns poly f (denoted 'input') that must evaluate on x to c, and must hash to some fixed (prover chosen) function of t via some fixed (standard) hash.
---
interaction sequence:
prover chooses t function.
verifier chooses x.
prover chooses c1,c2.
verifier chooses t.
prover chooses f.
---
condition 1: the prover knows at most one pre-image for each range value of the t function.
since the t function is a function, this translates to one pre-image for each value of t.
thus we can forget the t function, and the hashing, and simply assume that prover chooses fixed lookup table with one input for each t value.
so the t function instead can be thought of as some blackbox commit to this lookup table.
---
condition 2: at most k of the prover's known inputs can lie on a single line.
---
completeness analysis:
suppose k inputs are on a line.
represent this line with offset and slope.
set c1 to evaluation of the offset (on x), and c2 to evaluation of the slope.
submit c1 and c2 as claims.
if verifier chooses a t value for this collection of inputs, then c1 + t*c2 will be the evaluation of the corresponding t input.
thus the test will pass with probability k/|F|, ie when such a t value is chosen.
---
soundness analysis:
we'd like to state the converse of the above.
that is, if at most k inputs are on a line, then whp the test only passes for the respective t values (ie with probability k/|F|).
we can forget t for a moment and solve the problem in terms of just x.
knowing at most k inputs are on a line, we want to upper bound the number of evaluations that lie on a line.
Of course the evaluations can be permuted such that most lie on a line, but we consider without permutations.
For any subset of inputs that are on a line, so will be their evaluations.

For any subset of inputs that are on a input line, and an outlier input not on that line, we can consider the input line evaluated at the point of the outlier, and compare this interpolated input with the outlier. They are different inputs, so whp over x, the evaluation of the outlier will not land on the line.

Suppose we partition inputs into lines.
Only those sets of size at least three are significant. 
We can call any input that doesn't belong to a set of size at least three, a 'loner'.
Suppose after choosing x, we also partition the evaluations into lines, again with only groups of three significant.
We want to know the probability over x that a set grows in size and by how much.
We know that all sets will maintain their size, because input lines will reduce to evaluation lines.
Thus a set can't grow by taking elements from other sets.
So the only way a set can grow is by partial or full unionizing with another set.
We can consider separately the probability two significant sets unionize and the probability a loner unionizes with another set. 
Consider unionization of significant sets. 

Consider loners. 









well suppose the test passed for more than these k inputs.
consider an additional input for which it passes.
the evaluation of this input is on a line together with the evaluations of the other k inputs, but the input itself is not on the same line as the other k inputs.
we should make use of the fact that the inputs were chosen prior to x.

first c1,c2 is chosen, so we consider evaluations with respect to fixed c1 + t*c2.
for any k+1 inputs, their evaluations interpolation degree must be at least 2, thus the interpolation poly (in t) must be different than c1 + t*c2.

but the interpolation degree is at most k.
thus the interpolation poly for any k+1 points can agree with c1 + t*c2 on at most k points.
suppose we interpolate all |F| evaluations.



the idea of incorporating the a poly evaluation challenge is convincing, except for the assumption that the prover only knows one input for each t value, ie one preimage or opening for each commit challenge. This is of course justifiable if we assume collision resistance, but we'd only like to assume second preimage resistance. 
Suppose we randomize the commit. The amount of randomization needed is unclear but the idea is by randomizing the output polys (via taking their random combinations), we can completely randomize the output for the single chosen t value, such that after randomization and a chosen t value the verifier has effectively chosen a uniformly random output. Then we'd like to say that by second pre-image resistance, the prover can't find two inputs for this random verifier-chosen output. The problem is that the randomization has also changed the equations. So the output is not in fact uniform with respect to the equations. In fact this has zero benefit because by completeness any input for the original problem will also be one for the randomized problem.

suppose we use a two-layer quadratic, effectively a quartic, for collision resistance. Outputs would be quartics too. The reason we prefer this idea to using a plain cubic or quartic is key size. So maybe we should explore how to use cubics, or quartics, for collision resistance but reducing key size. A similar idea for cubics is to to pas the input through one quadratic, and then pass that output through a quadratic together with the original input, thus achieving a cubic output rather than quartic. This would mean having a separate linear poly where we plug in the original input and the multiply by the output of the quadratic. 

try proving the hardness of finding a second preimage on a commit. This is different than second pre-image resistance. Suppose that given a commit, with noticable probability over verifier's t choice, prover can find two inputs. Apparently there is a reduction from inverting a multivariate to distinguishing outputs of it from uniform strings. Maybe this is relevant, because we'd like to compare the hardness of choosing a target via a commit vs a uniformly random target. 
finding two preimages for any target means find a simultaneous solution for both linear systems
f(x - c) - f(x) = 0, f(x - b) - f(x) = 0
where c and b are the offsets. we can continue like this with more collisions. Solving u such systems means finding a (u+1)-collision. The question is what is the rank of the matrix made by vertically concatenating these systems. When rank is low, ie many equations are linearly dependent, you get some collisions for free. When rank is high you won't get so many collisions. The other variable relevant here is the number of equations in each system, relative to the number of variables, ie height vs width of linear system, ie compression factor. But the tradeoff seems friendly enough, roughly as compression factor equal to the number of collisions. 
This leave the question of how hard it is to find offsets such that the equations are maximally dependent.

I think the challenge for now is just assuming we have a finite field hash leaving all parameters free. But we make the assumption only a bounded number of collisions at each input can efficiently found. Separately we can comment on possibilities for such a hash. We can talk about the quadratic version, posing the challenge of finding linearly independent differences as described above, and we can talk about cubics and prove how collisions mean solving essentially random quadratics, and the ways we might reduce density for space and time efficiency.
Regarding the extractor, we invoke on one random poly challenge, extract pre-images, then extractor invokes adversary again in a separate independent trial on different poly challenge and extracts pre-images, etc, and continues until enough pre-images have been extracted that contradict the collision bound assumed for the hash. Since the trials are independent, the adversary for one trial has no knowledge of the poly challenges from the other trials. Thus the adversary cannot tailor answers to align with answers from other challenges. The adversary is left to either admit new pre-images each challenge, each tailed towards its poly challenge, or to not tailor at all, ie ignoring the poly challenge, in which case all pre-images could be the same. In this case the adversary is not acting adaptively to the poly challenges so we can assume all pre-images are known before the poly challenge.

To better understand the argument for randomness in https://www.iis.sinica.edu.tw/papers/byyang/6151-F.pdf we should understand its representation which is presented in "Cryptanalysis of the HFE Public Key Cryptosystem". Its argued that if F is random in the lifted form then so is F(x + b) - F(x) in the lifted form. Then it says without proof that if a function in the lifted form is random it is also in non-lifted form. At this point I don't understand either argument, or the lifted form itself.




try cascading multivariate commits.
they would be opened in reverse order than commitment.
if commit 1 works, where upon a t1 value prover only has one choice for input, that input effectively determines t2.
verifier doesn't even know t2, and doesn't seem to need to know.
the input for the first commit can function as the second commit already evaluated at t2.
---
interaction sequence:
prover chooses t1 commit.
verifier chooses t1 value.
prover chooses t1 input.
prover chooses t2 input.
...
---
this already saves entropy because the commit is smaller.
but prover must still provide all inputs.
maybe to save more entropy, prover need only submit final input.
then verifier plugs it in iteratively to obtain final t1 commit evaluation.


consider splitting commitments into parts.
have a small multivariate, only enough variables to make the problem hard, and have good compression.
then use this commitment many times over (using the same multivariate coefs over and over).
then for each equation you'll have many output commits.

suppose we repeat enough times such that for each equation we can select a combination of corresponding commits to map to 0. 
dividing this combination by the sum of its coefs we end up with zero again equal to the ...




use hardness of computing annihilating polynomial. hopefully this holds in the average case.
https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/annihilatingPoly.pdf
if prover has one poly f, and verifier chooses random poly inputs, prover can't find another poly g that outputs the same value, or else subtracting from f will yield the annihilating poly for the verifier chosen inputs.
actually f need not be chosen prior to inputs. inputs can be determined first, even standardized.
but a problem is the function is anti-compressing. but perhaps it can serve similarly to a code then.
The trouble seems finding a way to commit.



argue for limited need for security parameter in applications where a proof can be shown to have started after some public beacon, and consumed at some time likewise, thus leaving limited time for prover to break a proof.
the public beacons are assume to be sufficient secure to be an anchor point from which to start fresh again. 


to reduce burden on light mobile clients, maybe use MPC where they randomly select full nodes and ask them the same questions and compare answers. the questions would best be low-bandwidth, and thus are unlikely to be about proving one's own statement as that means sending the witness. Instead, the best questions I can think of are for verification (which are involved in proving too), where one evaluates parts of the codebase at certain points. For this, all the info that needs to be sent is the part of the codebase to evaluate, and where to evaluate it. 
another potential use is avoiding evaluation of large public keys for codes, multivariates, and lattices.


think about how proofs could be used by private, centralized parties. 
one way is if the party is trusted with privacy but not with soundness. Then the party can prove to others it has correctly computed on their data, though it saw private access to that data. 
Proofs can take the operational burden off parties and onto the network they serve. But the party still needs control. 


I've realized the distinction between sagaproofs and others is indeed the saga. Others focus on making verifier time small, but we are only focused on minimizing prover entropy, not verifier time. This is why all our schemes relying on keys only work with sagas, not with others. Its like the pre-bulletproof argument where verifier time is large. 


Describe how proofs are inherently unidirectional. Incoming proofs are able to account for each other, while out going proofs are not. This structure imposes limitations on application. This is a very fundamental concept. Even theoretically, I think verification can only go one direction, and cannot be cirular. So it is worth discussing this at an abstract level of 'verification' and how it relates to potential application, so non-technical people can participate. 


show how in the case of sharding, asymtoptically there's high probability adversaries will have opportunity to collude.
also show how any incentive model involving punishment or reward seems to be fundamentally rooted in a token, whereas proofs can do without a token. 
also how there will always be a problem of many competing blockchains because theres too many choices to make and they're incompatible, eg governance, token policy. But with proofs as we know the protocol on top can be so simple there's no need to split, and custom logic can be had anyway, not preventing incompatibility. 


if we want utmost security and speed, maybe instead of rust, we opt for a functional language for runtime, and C or even lower for the computations. This is because the functional language is better for formal verification, and C doesn't do runtime checks, like rust does. 
For formal verication regarding algebraic circuits, if the circuit is shallow enough we can verify whp that two programs compute the same function by the zippel lemma. This can help us create circuits by translating between forms, from those that are easy to design to those that are suited for proofs. Maybe we could even use this in characteristic two to reduce from proven algebraic properties of a function to a function operating on bits in a way suited for a computer.
This is natural for us anyway because our computations that are to be done fast are specifically those regarding fields, so there is some correspondence. Really anything involving finite fields we'd like to delegate to high performance, algebraically proven correct, code. Anything else should be about networking and interacting with the user. 


refer to challenge of interacting with real world as a problem of finding trustworthy, efficient, oracles. at the moment we only have such oracles for widely available public data, particularly digital data.


testing equivalence of circuits.
for P1 and P2 with P2 higher degree than P1, is there Q such that (Q + 1)*P1 = P2 and Q vanishes on the degree-0 subfield? If so they P1 and P2 agree on that subfield. I suppose this is not a search problem, as you can just test (P2/P1 - 1) for candidacy. Well it's still not straightforward to test the fanishing of a candidate. 
I saw a theorem that a poly in a field divides another in the field iff it divides in all extensions. So we could probabilistically test factorization over extension.


any finite integral domain is finite field. so the minimum we ask for is the former and are forced to use the latter. ie we don't need to ask for inverses, or identity.
or analogously, a finite division ring with identity is a field, so we don't need to ask for commutativity or inverses.
Actually on wikipedia I see it as "Wedderburn's little theorem" and it puts both together to say a finite domain is a field. A domain is a ring with no zero divisors. Actually this is equavlent to the latter because a domain has identity.


i suppose every form of interaction, probably thought of as every verifier send then prover responce then verifier act process, is backed by a homomorphism-type. We should classify interactions by their homomorphisms. 
For example, in reducing evaluation point, the verifier first sends the line, the prover sends it back composed with the poly, the verifier checks the original claim assuming the composition is correct, then checks the composition by rducing to a new point by choosing a random point on the line, computing the line at that point, and computing the composition at that point, then treating the new claim with input as the line output, and output as the composition output. I think the homomorphism here is just associativity of function composition. In particular, the prover submitted sub = (poly . line), and the verifier computes (line . r) and (sub r), and then expects (poly . (line . r)) = (sub r) ~= ((poly . line) . r).
We also use commutativity for lattices and discrete log. verifier sends randomness coefs, prover replies with witnesses. its clear how commutativity works given the structure of these commits. but actually before the commutativity takes place we first rely on distributivity.
In multivariate, we use associativity of composition again. like before, prover submits sub = (poly . line), verifier sends r, and prover replies with wit = (line . r), and verifier expects that (sub . r) = (poly . wit), ie
(poly . line) . r = poly . (line . r).
In pcp's I think we rely on distributivity and commutativity.
So maybe we could use a general ring with no zero divisors. The purpose of the ring structure is to encode functions. Like maybe a matrix ring, like GL.
Maybe we best state our assumption as a finite ring, perhaps without identity, where then number of zero divisors is small enough that zippel mostly holds. So we don't assume identity, don't assume multiplication commutativity, don't assume all are units, don't assume inverses. 
Here is a proof that a finite ring with no zero divisors is a field. https://math.stackexchange.com/questions/620791/proof-that-a-finite-domain-is-a-body-a-little-question. So this covers the case that all non zero elements are units. 
But consider when there are non zero divisors. Non-zero divisors are closed under multiplication since for non-zero divisors a,b, we have
abx = 0 => bx = 0 => x = 0
xab = 0 => xa = 0 => x = 0
since c being a non-zero divisor means cx = 0 => c = 0, and xc = 0 => x = 0
Suppose we select uniformly from all non-zero divisors. All monomials will be non-zero divisors. Perhaps better for soundness analysis is to think in terms of root form for a univariate. Now it seems maybe it's best to select uniformly from the whole ring. What's the probability an element maps such that a pair of root terms are respective zero divisors. Suppose we can prove probability is low enough. Can we extend this to multivariate polys? It may be very useful to keep in mind that for soundness we will use low degree polys. 
While we may not need fields for our homomorphisms, we may need them for our proofs. 
Worth mentioning how an uncommutative group posses associativity and can encode a function but doesn't have commutativity. 

ideals while suited for programmability and unsuited for soundness, are likewise unsuited for program verification using poly identity testing. thus the ability to do program verification comes at the cost of inconvenience for programming. so we could phrase it as the situation that proofs force us to program inconveniently over finite fields, but we can take advantage of this inconvenience to get program verification at little extra cost. 

I've ralized the programmability of ideals is ported to fields by taking the quotient, which is how fields are formed by ideals. So the ideal goes to zero, which is an ideal in the field.


Maybe we can unify the concepts of normalization and tail recursion to simply proof composition. A sagaproof is one that continues, always accepting new input, and amenable to forking, and is able to be passed between parties at arbitrary points as it goes, though it is not aware when. 

proof of knowledge is not enough. Proof of knowledge is to say that if adversary can open the randomized reduced commit, it could have opened the original commit for a unique value. But we also need the guarantee that if a property holds for the reduced opening it some corresonding property would hold for the original opening. So together we need to know that that the adversary knows data and that the data has a property.
So in general I think the problem is efficient property testing. Property testing is determining whether data satisfies a predicate. A proof is meatadata that attests to a property testing for data and a property. Often the property is that input and output data satisfy a function predicate. We try to make this sublinear by transferring much of the complexity to property testing the predicate instead of the data, where the property to test for is a small piece of data dependent on the original data. In the context of property testing the object to be tested is static, and for sublinearity given in oracle form. All you can do is query it. This is not enough for our purposes. So we make the process interactive, or we could say we compose the protocols, where the verifier is given another object to query for a property. In particular, a query is responded to with an oracle response as usual, or with a new oracle for property testing.
In general we will be concerned with the data being a polynomial and the property being that an input output pair satisfies evaluation. We know how to reduce predicates over finite fields to this problem, and we know how to reduce these problems between each other. So far all our commitments allow commiting to vectors such that they can be opened on random linear combinations of these vectors. 
I think the only thing we need to abstract for theory vs practice is oracles/pseuo-random generators. We assume that the prover runs in ppt. We assume commitments are pseudo-random. We assume hashes are pseudo-random. Because the prover is ppt, pseudo-random is the same as random. I suppose this means we don't need to operate in the random oracle model. Note how IOPs are designed for all powerful provers so IOP's must operate in ROM. For our assumptions I think we only need OWF's.
We repreadely reduce between statements regarding commitments. Property p holds on the committed data whp if and only if property p' holds on it. This is the translation phase. Then we do the reduction phase where we reduce a statement regarding the commitment to a statement regarding an opening of a manipulation of the commitment.
I think we should opt for statistical zk, such that data in the future can't be unveiled, and for soundness we reject proofs make in the past, probably by giving each codebase a version number. But zk comes at a prover cost so we don't want to require it for every composition, making it optional. For zk there are several aspects. One is masking commits. Another is masking commit openings. The last is masking the interactive processes. Zk need only be done on the last proof before it is goes to the next party. 
So what do we mean by a proof composition? Are the layers well defined? One way to define would be in accordance with the statement type. Another way would be per commit instance, including a normalization. In particular, the prover gives the commits. Then intermediate interaction occurs with fiat-shamir, until verifier is to check property on committed data. Then random reduction occurs and commit is reduced. Then prover opens committed values and verifier checks property. So this definition is independent of the statements. I prefer this definition. When we compose the last prover message coincides with the first prover message for the next proof. Perhaps we should not regard the first prover message as part of the proof. Afterall, the verifier doesn't compute with them except to return a random challenge. The prover will send commits and doesn't even need challenges before also sending some meaningful claims about the commits, though of course we still accomodate the special case that the meaningful data is null and only commits are sent, requiring an extra round. Thus we should define a proof with the verifier first sending a challenge, and lastly the prover sending an opening. This definition is suited for the assumption that data is already available (via commit) and a meaningful claim has already been made about it. This definition is well suited for computing the total number of rounds with composed proofs. So for the head we assume there is a separate way to make data available and a claim about it.
We know how to apply zk to interactivity. Applying to commits we know for lattices and codes, but not yet for multivariate. I'm thinking if we do cascades, there could be an extra layer at the bottom where one half is random input, assuming two-vector input. As for opening, I think codes are fine, and for lattices there's the expensive and not-fully-complete masking protocol, but another way is just to add sufficiently many random poly commits that will mix with the others upon reduction but this too is expensive. As for multivariates, with cascades we'd need the top layer, instead of the bottom layer, to be half random, which is a lot more expensive than just masking the commit. Least expensive is probably the code so maybe it's best used for zk. But note how the zk proof need not be the last proof before it goes to another party. As long as no private data is added after the zk, any proof type can be used to further reduce proof size. 


Try poly methods with non-commutative groups. I think it will work, but not sure how to use it for useful computations. The benefit is the native language is now more sophisticated. 
or maybe with linear maps, relating to the decomposition form (T - aI)(T - bI)...


Try to make use of evaluation in poK for not just multivariates but lattices and codes too. One way is relaxing to multi-collision resistance. For SIS maybe this means a collision where three or more outputs sum to 0. 


Try expanding SIS as alternative to codes. Would be relevant to closest vector problem probably. Only real difference from codes is probably use of small coefs and combinations of commits for sufficient randomness. 

also try lattices as codes themselves, ie lattice points as codewords. then approximation hardness becomes relevant using the same proof style as for random codes, and maybe we can even use euclidean distance. given the basis, prover would commit vectors as lattice points using the basis. then verifier would take a random combination(s) of them and expect result to be a lattice point. I think prover could provide the coordinates wrt the basis as a witness. But neither hash trees nor compressive commits seem to work here.
How to make use of Euclidean distance? Well it probably isn't very applicable to finite structures but we can try with natural numbers with no wraparound, as we have with SIS. Note how Euclidean distance can be converted to hamming distance wrt a certain basis. But only prover will know that basis. 


Mention how whatever property of the data you compute, you can't assume that to hold for the data in entropy reduction. At the time of reduction you can't assume anything about the data. Doing so would be circular dependence. 


Suppose a prover had its own private key with certain promised properties. Could zk be easier? If so, we could use standard zk to construct these keys for more efficient zk. 
Actually the entire keys of entropy reduction schemes could be custom per prover, but verification would be reduced to evaluating the code-poly belonging to the prover, which the verifier can't do because that poly contains the key and is private to the prover. This could be resolved with regular zk using the poly as new entropy. 


Maybe there should be some way to use lattices without limiting coefs to small values.  But an important pattern to recognize is that lattice problems seem suited for the case where there's a unique solutions that can't efficiently be computed, which is in contrast to hashes where there are multiple solutions allowing for compressions and collisions. 

what about ways to reduce randomness required for SIS. what about treating random values as evaluation points of poly, and then switching to quadratic poly instead of linear poly. 


mention how in this field evolution of improvements move from complex to simple. same goes for our work in and of itself. just as our project will not review the backwards evolution of prior words, we will also now review the backwards evolution of our own work. 


I think the zippel for non-commutative groups can be reduced to the probability
xbx^{-1} = c
This is because we could plug in all variables but the last and reduce to an equation of the form above.
So if b != c, what is the probability this equation holds?


hmm, regarding the random code reduction in terms of the adaptibility of the prover to the instance via the position within the circle radius, I think we can prove negligible probability of choosing an instance such that the prover succeeds. So whereas before we were thinking to take probability over the success for a fixed instance, we now move the probability outside the instance as well. We can think about two choices, the choice of instance, then the choice of challenge for a fixed commit. Our probability is over both of these at the same time, but we have no idea of the weights between the two. I'm hoping this union bound on them is enough.

since LWE is a decoding problem its similar to codes. if LWE is dual to SIS, maybe our compressive commits can then be duals of the expansion code commits.
maybe we take the special cases of LWE and SIS of lattices and generalize them in a different direction suited for our purposes.
to relate LWE and SIS, maybe we think of the error distribution for LWE as analogous to the norm of SIS. the norm is a function from vectors to non-negative real numbers. the error distribution is a function from a per-entry error domain to the unit interval. We'd like to unify them, and actually our purposes suggest we go beyond unification and swap them in a way. In particular, we want inifity norm for SIS, which really brings us to a function on a per-entry domain. For errors, we want zero or arbitrary values per value, and to limit the total count of errors, which requires moving from a per-entry funciton to a per-vector function like for SIS. We can try doing the latter for errors by showing our distribution on vectors has the same properties as the product of per-entry distributions. In particular, I think its the case that the following two distributions are equal over vectors. For Z_q, suppose q is odd and we center on 0. The first distribution takes a p-norm of each entry, then passes through a distribution function and takes the conjunction of all entry probabilities. The second distribution takes the same p-norm of the whole vector then passes through the same distribution function. So basically we're asking for a kind of homomorphism. Make sure signatures agree.
Note how probabiltiy axioms are similar to norm axioms, eg triangle inquality for norm N(A + B) <= N(A) + N(B) is analogous to P(A or B) <= P(A) + P(B). And normalization holds for both. And N(A) >= 0, is like P(A) >= 0. 
Hmm, in a finite case we can use a p-norm that counts the errors, because p can be chosen such that it maps to 0 or 1. Eg for finite field it would be p = |F|-1. But actually this is relevant to the p=0 norm, so perhaps worth viewing instead as p >= 1 for SIS, p < 1 for LWE, but since p < 1 don't make real norms because they invert the triangle inequality (which may be useful for dualship). 
Actually in both cases the search is for the "b" vector, LWE errors, or SIS inputs. In this sense its not a dual but the same problem. In LWE one can search for the b vector or s vector (secret), but in SIS one only search for the b vector, as the s vector (output) is known. This seems to come from the assymetric relationship of how LWE expands, and SIS contracts. 
Maybe to make SIS more like LWE we let the input consist of any regular vector plus a b vector, such that the output is equal to the output on just the regular vector. By subraction this reduces to the regular SIS problem. Here the regular vector is analogous to the output minus the errors of LWE, and the output of modified SIS (for both inputs) is analogous to the secret vector of LWE. So for both LWE and SIS lets call the vectors s, u (both regular vectors), and b. But note how in SIS both u and u+b output to s, whereas in LWE due to expansion only u outputs to s.
Just as we could go with ring based SIS, we could also go with ring based LWE codes.
I suppose the norms are actually dual, zero for LWE, infinity for SIS. This is because for our case, in SIS we want the addition of vectors to minize the norm, to preserve commitment, whereas for codes we want the norms on addition to blow up, to preserve distance. 
What is the dual of zkps? Maybe homomorphic encryption. In both cases we have re-normalization such that the computation may continue, but it's easier with zkp because prover known secrets, whereas with homoenc only verifier knows secrets. For homoenc, the prover would send back the unnormalized result, the verifier would decrypt, renormalize, renencrypt, then send it back. This algo involves a secret. The solution is circular encryption where this computation is also delegated to the prover. This is analogous to proof composition. In particular, maybe the analogy to boostrapping is full proof composition the type others do, and the analogy to other techniques like modulus switching is tail recursion of the type I do. 

Maybe privacy (or zk) is the dual of soundness. I suppose so in the simple sense that soundness means for a compressing function you can't invert unless you've been given homomorphic ability to, and privacy means that for an expanding function you can't invert unless you've been given homomorphic ability to. 

Homoenc expands then contracts. Proofs contract then expand. In both cases the expansion involes randomness. For proofs this is the random combination of commits together with their opening. 
Maybe homoenc and proofs are just higher versions of public enc and signatures. But hmm, verifying signatures deterministic expansion. But relating to proofs, a good concept seems to be signatures use deterministic contraction, then random expansion for soundness where verifier chooses one of multiple possible expansions. To fully match, the signature is an oracle of all possible verifier queries. For enc, on the other hand, the randomness is used for privacy. Actually it's not problematic if signature generation involes randomness, because its like picking between the possible ways to contract, but may also be picking between multiple outputs and their fibers cuz its not like every pick of randomness leads to the same signature. The randomness is signatures to protect the secret key is akin to zk for proofs. So actually randomness is suitable for both expansion and contraction. 
For enc, randomness in expansion protects privacy, but I don't know of randomness in contraction, but maybe it could exist for verifiability, ie soundness. For signatures, randomness in contraction protects against forgery in the case of multiple signatures, ie soundness, and also protects randomness in expansion. For proofs/signatures, randomness in contraction protects privacy, ie zk, and randomness in expansion protects proof of knowledge, ie soundness. I suppose we an identify signatures with proofs because the forgery can be thought of as a breakage of privacy, not soundness. 
I think we can unify the concepts of privacy and soundness as the inability to learn anything more about the function that already known.
Bootstrapping and proof composition are analogous, both instance of composition/recursion. 
Hmm, maybe we say in enc, the evaluator (akin to prover) wants soundness while the client (akin to verifier) wants privacy, while with proofs its the other way around. In this case the opening transformation (expansion for enc, contraction for proofs) is about privacy, and the closing transformation (contraction for enc, expansion for proofs) is about soundness. Just as with proofs the commit opening is random, with enc, the decryption would be random where the evaluator randomly challenges the client for the correct decryption. This eliminates the ambiguity about proofs having privacy and soundness but enc seeming to lack soundness. This is because with enc, soundness is not wanted by the client (like we thought), but by the evaluator and this can be achieved by the soundness of decryption along with the self-incentive to correctly evaluate the function.
So we can see both schemes as means to achieving the same outcome. A function is to be computed. The input belongs to one party and its in the interest of this party (called the teller) to keep it private. Its in the interest of the other party (called the listener) to have soundness. Proofs do this by the teller computing in its own enviroment, not encryption, and the listener verifying soundness. Enc does this by the teller encrypting the input and giving it to the listener who computes, then challenges the teller for proper decryption. So with proofs the teller does the computation, with enc the listener does the computation. So this constitutes a commutative diagram. In both cases the opening transformation is random for the sake of privacy but does not involve interaction, while the closing transformation is random for the sake of soundness but it seems may require interaction. How to achieve the soundess for enc? One way of course is using a proof, but we want to achieve it via enc functionality. I'm thinking its something like evaluator picks some random values, encrypts them, and combines them with the output encryption say via a function f, then asks the client for the decryption of both the output and the challenge, and from that checks for consistency by checking that f evaluates on the decrypted output to the decrypted challenge. 
Note how in both cases the composition/boostrapping is necessary or else privacy or soundness breaks. For proofs the prover would have to open the commits exposing privacy. For enc the client would need to decrypt the intermediate values. We can think of this as the privacy concerned party rejecting to continue. Likewise we could instead think of this as soundness breaking by the privacy parties sending data but the wrong data to not expose their privacy, and in this case the soundness tests would fail and the soundness concerned parties would reject. To be clear, in proofs by composition we mean when a new commit is created, just like in enc is when the data is re-encrypted. 
But an inconsistency is that with proof soundness is had by both parties, but with enc soundness is only had by the evaluator. Oh, actually this is incorrect hah, because the client can check for soundness by just computing the function in plain decrypted form by itself. With this in mind it's worth thinking about the computational burdens of parties. I think they may be assymetric. With interaction instead of composition/boostrapping, in proofs the verifier has little time and space complexity, just sending back randomness, whereas for enc the client must accept all the intermediate values and re-encrypt them (also involving randomness). Note how here this is basically interactive composition (the KDM and relation intractilbity equivalent assumptions below allow for non-interactivity), where with enc the expansion of encryption is taking place again, and with proofs the messages sent by the prover can be thought of as commit contractions. 

Is fiat-shamir the dual of something? Perhaps circular encryption, as they are both necessary to achieve the non-interactivity. Indeed, papers showing fiat-shamir have assumed circular encryption. Can we show the other way, ie with fiat-shamir one can achieve circular encryption? Circular encryption means one can encrypt the secret with the secret. And indeed, it sounds like random oracles have been used to build KDM, and so can probably be replaced with relation-intractable hashes. So actually what I meant is that a KDM scheme is equivalent to relation intractability, not fiat-shamir.
