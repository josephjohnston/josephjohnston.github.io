
STATE
windows
	a collection of windows
		a window has an id, project id, delta id, message
projects
	a collection of projects
		a project has a folder path, binding status, a collection of tips, 
			a tip is a delta id
deltas
	a collection of delta parts
		a part is one of two types, a head or a snippet
			a head has a parent delta id, a collection of children delta ids, and a list of snippets
		a snippet
requests
	a collection of requests from the database

EVENTS
folder save
	folder_bound = make sure folder is bound.
	proper_format = make sure folder contents are proper document format
	new_delta = compute diff of folder contents and current delta
	already_editing = go through the deltas and see if there is one already at this level that is not committed
	if !folder_bound or !proper_format show error
	if alread_editing change that delta contents to the new_delta
	if !already_editing add this new_delta to the delta list
	change the delta of the current window to this new delta
	add this delta as a new tip for the project, or change the existing tip depending on already_editing
change project
	if no window is already displaying the project, change the request_window project and set status to loading
	if the project info does not exist, request it along with the delta
new project
	change window status to loaded
	add project to projects
	add delta to deltas
	post delta page info for window
publish delta
	create delta hash format.
	add server query. 
	post network request.

windows: {
	nonce: <int>,
	<id>: {
		id: <int>,
		projectId: <int>,
		deltaId: <int>,
		message: {
			type: 'error' | 'info',
			content: <string>
		}
	}
}
new
	if nonce is at limit display in requested window an error message
	else add new window
change project
	if 'project change is allowed' change project of window
	else show error message

projects: {
	<id>: {
		folderPath: <string>,
		bindingStatus: <bool>,
		tips: {

		}
	}
}
new project info
	if 'project info was requested' add this project info.
folder save
	'get the delta'
	take a look at the current vertex, by looking at the delta id in the current window and locating the relevant deta dna looking at its
	children. if the have an own uncommitted delta then change that tip in the project to the new delta. if not, add this as a new tip. 


deltas: {
	
}
folder save
	'get the delta' from the contents
	if 
	
requests: {
	<id>: {
		type: 'project' | 'delta' | 'server'
	}
}
change project
	if 'project change is allowed' create request for project info
new project
	if 'project info was requested' remove the request.
change delta
	if `delta not available` request delta
new delta
	if delta was requested remove the request


SELECTORS

project change allowed (project id, windows)
	go through windows and check if one is already displaying the project

project info was requested (project id, db requests)
	go through db requests and see if an id matches project id

delta not available - delta id, requests

get request for project
check if project is already in another window
if not, change current window to new project

in store are documents
when a project is loaded, interact with relevant servers
keep track of tips of every project. interact with servers listed on those tips. 

there is a global state.
there are selector streams that accept queries and will return relevant parts of state
of these selectors could read from other streams, meaning they must be subscribed to those streams as well as the query stream
what about modifying state?
there could be streams that output changes to the state. these streams should have sinks as output. the runtime will apply their changes to state. but what if the changes cause an out of sync state? 

how about not supporting arrays? after all, arrays in most languages are made by linked lists. the only reason to support array is that you can quickly access its contents. and you can also efficiently enumerate them. but this is only practical if the array length and length of items are static. removing the first element of an array would be the worst. assume we can just use a giant key value map instead. we might be able to have a key map to multiple values, ie mapping to a collection, or probably even an ordered collection which is similar to an array. is state easier to handle then? for flatlang this means the state would only consist of shapes, nothing of variable size, only the size of the map would be variable. for streams, this would be only single shapes get passed around, no collections of them. this way, the api of every stream is has input and output of just one shape, though they could be of different types. restricting state to this single, simple data structure should offer opportunities for analysis due to the constraint. we don't want to ruin this opportunity by trying to simulate other data structures using this. notice that this data structure is easily merged with the leveldb database. 
maybe for streams, rather than a global state, there the sinks simply output new key value pairs representing parts of state and those items are fed back in the sinks of streams from the top of the stream network. this way, conceptually, state inside the network or circulating back around to the top. this way, also, state is immutable, where streams can't mutate a global state. all they can do is output new values, representing state, which are then fed back into the network. 
it would make sense that streams listen for changes to certain keys, and are only invoked on them. this just means a filter on the inputs to the DAG. I think it should be enforced that the network is a DAG, having no recurrences within, and only the top level circulation. i have no idea how to implement streams that use state in this way, but I don't concern myself with it now. rather, focus on maximum programming clarity. 

many streams i think will be on hold while they receive input, and then only go when certain matches have been met. 

two parts: Sigma and Delta (names) for now

slim left bar like treehouse used to have lets switch between apps

UI for Sigma:
there are columns, all identical
number of columns is decidable by user (to a limit) but i'm thinking like four
at top of each column, there is input (maybe artificial) for a query 
below it is a special place for a display derived from the type of query
all queries are a 'collection' that can be filtered or ordered or grouped into another collection
types of queries:
hash of a sigma, which queries for all sigmas linked to that sigma. the display below is some representation of that head sigma.
all sigmas of a particular author, display below is representation of author
all answers in an answer chain which will already be naturally ordered
filters: types, dates, authors, 
groups: types, authors, 
orders: types, dates, 

there is always one columns selected and one sigma in each column selected
to move between columns and sigmas, click on them or use arrow keys. 
ctrl-h to copy hash of selected sigma, likewise ctrl-u for user, ctrl-d for date, maybe more later
ctrl-v is for pasting what has been copied into a query

an important query that's not really a query (and for that reason the inputs should not be called queries by something else)
is a submission or a writing of your own sigma. i think a submission would have a @submit command. nothing prevents this input space from being a place where users can construct their own queries, probably under the command @develop. 
at any time, i'd say the key 'enter' could activate whatever is in the input field. 
if it is a query, it is rendered. if a submission, it is submitted to network. if it is develop, it is saved.
with the develop type input, another kind of input will need to be how to retrieve your developed sigmas, delete them, and submit them.

---- redo after sleeping in buena vista
Sigma:
there are columns and items in each column. 
at top there is special item for query and display for query
then there are stack of sigmas
should query be text input? it would be more efficient for user to type single letters and have them autocompleted. 
but i imagine arrow keys will be needed. well since its not text input it may be ok to use shift-arrow to navigate them,
reserving arrow for navigating columns. the types of inputs will be hash of sigma, public key of user, only and except (filters)
with selection by user, date, sigma type, . as well as orders and groupings.  
the sigma app can hold all state i imagine. the columns and the sigmas will be dummies. the app will be the one to parse inputs. the app will pass the columns all they need to render, which will be the list of sigmas, and what should go in the top. the options for the top are based upon query type. they are:
1. the query of all sigmas that follow a sigma. the display is of the head sigma, and buttons for creating a new sigma. 
2. all sigmas published by a user. 
the column will send all input it receives (eg button clicks) to the app to process. i'm wondering if scrolling should also be handled by the app. does scrolling determine the state of the app? no. should scrolling be preserved across renders of the same sigmas? yes. how about for different sigmas? no. does this imply that scroll state need not be of concern for the app, and is only thought of as a convenience for reading a given list of sigmas? yes. are scroll events therefore not passed to the app? yes. 
does the app show users? is the risk of showing users discrimination of age/experience and gender. could only allowing certain info about users help, like only a certain type of profile pic eg from an avatar generator that may generate randomly with no info your age or gender? yes. can we thus proceed with users? yes. is the point of profile pic just to give other users something to remember you by? yes. 
i think a good way to prevent conflicts when a user submits two things (eg profile pics, answers) that conflict is to disregard them both. if the two submissions were done in good intention, like replacing a profile pic or answer, then the user should link the new thing onto the previous, and then the rendering could.. did i also not like the idea of users because it has the effect of winner take all? YES! so maybe i could restrict where sigmas are allowed to identity the author. i think questions and even (actually especially) answers should be anonymous. then maybe there can be the numeric points type to judge questions and answers, and only that type shows users, enabling you to see who thinks what, and who's opinion you value. 
reasons for anonymous questions and answers: questions can be edited without the editor receiving full credit. two users can have the same answer without one of them getting credit and the other not. 
or the new answer could link same as previous answer but reference old answer. this may make more theoretical sense, and it is easier to render. there are questions and answers and points. i don't think i need the 'i don't agree with your justification for this answer' type, because i think attaching negative points to an answer can make the same point. and point types could even have a labeling for what the points are for. maybe the entire sigma graph could be made of these 3 fundamental types: questions, answers, and feedback. points could actually be done just via questions and answers, but i think the reason i created points initially is for its numeric quality enabling users to calculate what their trusted community thinks of it, which would also be helpful for the Delta app to show the 'correct' path. in sigma is there a correct path? i suppose the goal is a sequence of logic, and so the path is an answer chain. can points be any numerical thing that reflects how you judge and answer or question. could points be on the integer spectrum? yes. can questions or answer be attached to points? no. should answer options just be yes and no? yes. is negative points for yes equivalent to positive points for no or vice versa? no. can answers be redone now that answer have no authors? no. can questions be done now that they have no authors? could the publisher sign it and then under certain limits (eg time limits) publish an erasure message that removes it from the graph? might the reason for removal be that the publisher regrets publishing? yes. is it possible for the community to perform removal without the needed approval of publishers? with anonymity, would publishes have any special control? no. could the community vote, as with points, on whether sigmas are worth keeping? yes. can a user change their points? should this rarely happen because what they give points (or anything it depends upon) will never change? well actually, might what they give points for be a sigma who's points have changed, and therefore the points of the connected sigma should change? i think so. is changing points an easy and necessary ability? i think so. if one answer in an answer chain becomes invalid, all answers below it (think of sigma tree growing UP) should also be invalidated because not only do they answer a question, but linking to an invalid answer their dependency is no longer valid. with anonymous answers i was thinking there would be at most one yes and one no answer, but now i realize there would be one for each answer chain, cuz they would each have different hashes. do the yes and no answers with dependencies still sound good? yes. do i need to figure out points? yes. is the purpose of points for questions to give weight, with positives saying 'this is a good question in the right place' and with negatives saying 'this question is not well asked either by content or location or both and should, along with all its descendants, be removed' (possibly pointing to a replacement question). YES. is the purpose of points for answers to give weight, with positives saying 'this answer and its dependencies are sufficiently suited to answer this question' and with negatives saying 'this answer is not logical'. should bad answers be removed? should there be a distinction of wrong answer and nonsense answers? should wrong answers be removed? could they be commented on and explained why they're wrong? would such commenting require that questions be attached to answers? yes. instead of answer dependency references can an answer with dependencies be expressed with questions and answers without dependencies? is this actually closer to how i think? yes. could the references to other dependency answers and the logic of tying them together as a dependency be expressed in questions? yes. does this then mean answers yes and no are suddenly unique? are there no more answer chains? actually does nothing prevent answer chains from remaining, and do i still tend to think with answer chains? yes. example: before answer A depended on and cited answers X, Y, and Z. Now instead of attaching answer A to the original question, a subquestion is attached. This question (perhaps with subquestions) asks whether X, Y, and Z, imply A. This question is answered with a YES, call it answer B. Then the original question in answered with answer A, citing its nephew answer B. If someone disagrees with answer A, they would first give a NO answer called answer C that counters answer B. They would then somehow need to counter answer A without giving the opposite answer of A because what they mean is !A. should !yes and !no be possible answers? 
two cases, an answer is uncited or cited with a reference pointing to the answer of a peer question in which the question lays out the logic. with an uncited question there is no particular reasoning behind the uncited answer, so if someone would like to object to it, they need not point to that answer, but should just make a peer question along the lines of 'does x, y, z make this question inherently <answer>?' and then answer it with No. This question will sit beside the answer it disagrees with, and if it gets high enough score it will get enough attention that people will not take the answer without evidence that addresses what the question asks. Now for a question that cites an answer, someone who disagrees with that reasoning can go inside the peer question that lays out the reasoning and give a counter answer, perhaps with some subquestions. my thinking is that, like with the uncited question, people will see the peer question and go look inside, and there they will see the countering conclusion, which if valid should prompt them to go to the original cited question and down vote it. people will be drawn to that peer question not just because it is a peer, but because the answer cites an answer inside that peer question. so points can give questions + for 'good question' and - for 'bad question' and points can give answers + for 'i agree' and - for 'i don't agree'
could points simply be an 'agree' or 'don't agree' position for each user? yes. remember, votes on points are not about the answer to them, but about whether they are helpful questions. and i am more comfortable with negative points because questions are now anonymous so feelings are less likely to be hurt. 
if questions and answers are anonymous, then how do users get ranked? before i was thinking users get ranked by the votes on the questions and answers they give. now it would be great to be able to still score the sigmas and let users prove their score anonymously. this would entail that sigmas be submitted with some key, a public key, for which only the publisher has the private key. i think this might be a zero-knowledge situation. users need to prove ownership of their public key without revealing it. 
the trouble comes because zkSNARKs is complicated to setup. I think you need a setup ceremony for each circuit that will be proved. i wondered if maybe there is a way to get around this for now. for example, maybe questions can be anonymous while answers are not, or vice versa. the goal is to meet both needs that peoples contributions are visible, while their contributions are not personalized, which is a bit contradictory. i was thinking for now either questions and answers are not anonymous, or they are and with a small community we trust there will be no sybil attacks. maybe sigma would actually be better when at least questions are not anonymous, as sigma was partly designed by way of questions to prevent offense. questions are where the value is so the purpose of tracking contributions to prevent sybil would be met. As a community we would do our best and use the opportunity of known (opposite of anonymous) authors to practice and become familiar with how to best handle personal interactions in a flat, autonomous community. Should answers remain anonymous? I think so, because they don't show much value as are easy to create, and they are where users make judgments which is where feeling can get hurt. This means to find the path, users must still vote on answers to show their opinion.
how does scoring work? the goal is see a path of weight through the sigmas to understand community opinion. To do that you need a weight assigned to every user. To have that you need to score them based on the value of their questions, and you do this by voting on their questions. This bootstraps a process where your scores can take into account not just your votes but the votes of users who you've scored, weighing their votes by your score of them. what if a user publishes a valuable question, but it must be modified and is republished by another user. The community settles on the new one. Does this mean the community down votes the valuable one and up votes the edited one with the trivial correction? Is distributing weight super hard? is that why anonymity is useful. is the philosophy of the system in general that credit should be shared in fuzzy amounts? yes. so should the votes on questions reflect how much they value the user, or even the question? NO. instead should votes reflect whether the community thinks the question should be focused on? yes. is scoring authors not a public process but completely up to users? yes. should votes be taken as equivalents of scores? no. is this because of the editing example? yes. should questions be voted on at all? yes, i think they should be voted on for priority of attention, and maybe to make this dynamic to match the character of the process, instead of giving a static positive or negative score to each (and maybe changing that score as time passes), users could indicate at all times what questions they are focusing on, and maybe this statistic could be automatically kept track of by the app using sigma selection. This stat would be part of the protocol as, and in its simplest form it would be users sending out a list of their most prioritized questions (which implies the inclusion of the parent links of each of those questions) together with a nonce to indicated order of time, and then signing the package and broadcasting. this would take the place of question voting. i think answers could still be voted on with 'i agree' or 'i don't agree'. i suppose there could be a separate UI for the user to keep track of what other users he likes. 
ok, this is hard. the whole concept of question voting or its alternatives like priority questions broadcasting is just for the purpose of showing where attention is located. Maybe the simplest and sufficiently effective way to achieve this is simply to keep question authors known and to track where questions and answers are being made. This works because it makes the accurate assumption (in line with the sigma philosophy) that questions shouldn't be dwelt upon, but should instead be investigated by asking subquestion and answering them. Therefore, the amount of activity in questioning and answering directly reflects the level of attention on a question, which I am assuming in turn directly reflects the value of that question. In the beginning, perhaps it is unnecessary to keep a weight for each user and calculate path weights, instead assuming every user is equally valuable. Later a UI and mechanism can come that lets a user score every other user. For now, the amount of activity in questioning and answering indicates the 'path'. if the path can be found in terms of activity, without any author scores, can questions be anonymous? 
so there are questions, answers, and points. Question authors are known, answer authors are anonymous, and point comment authors are known. 
at this point i only two types of contents for columns. 1 is a DAG level with all attached nodes as the content. 2 is all questions or answer comments for a particular user. In both cases, the nodes can be ordered by type, or date, or . the order of attachment is question, answers, points.
sigma icons:
on left of icons there should be link (button to copy hash) of its parent sigma. on right of sigma should be indication of what follows from sigma. for questions, this would be how many questions and answers follow it. for answers it would be how many users agree and disagree with it. for points it would be nothing. whether to drop a question is based on whether it is seen as nonsense and the activity around it. subquestions can be asked (and eventually baked into protocol) about whether a question should be dropped, and it can be discussed from there how relevant and valuable the question still is. answers should only be removed if the answer on which they depend has been removed (in which case the uncited answer cannot be removed once added) which implies that in order to remove a cited answer, you must first request that the network remove the conclusive question that that answer depends upon. if an answer is not well made a peer question can ask if it should be removed, and then a subquestion of that peer question ask that that question be removed. in general, when something is removed, everything that references it is also removed. since questions are the base of answers and answers are the base of votes, it is the removal of questions that triggers removal or answers, and removal of answers that trigger removal of votes. if a user want to change their vote, they can issue a new vote of either 'agree' 'null' or 'don't agree', and they should hash it to their previous vote. When an answer is displayed the items of the column should contain all votes attached. votes should never be the base of a display. 
vocab: questions, answers, votes, 
questions:
has content, author, date 
icon may have link to parent question on left, in center profile pic of author next to date above content, on right has linked questions and answers info with maybe info about how many are unseen (maybe in form of green color to info).
answers:
type (yes, no), citation, date
icon may have link to parent question on left, in left has date above type and citation next to type (special symbol for no citation) in center, and on right has votes info (probably just current number of agreers and disagreers and maybe undeciders). 
votes:
type (agree, undecided, disagree) implied default undecided, author, previous vote hash
icon may have link to previous answer on left, in center has user profile pic with date and vote (agree, undecided, disagree), on right has arrow going down to more votes of the same user if the had any previous votes. 
now thinking answers and votes are redundant, especially with bidirectional voting on answers. maybe instead it would be better to only have answers, and have them with known authors, but for the UI, users with the same answer would be bundled together. the only reason to have answers not anonymous or have any votes in the first place, is to show the path. a problem with votes or answers is that, unlike answers, when others have already given an answer a user is less likely to do it again feeling the answer has already been sufficiently expressed. S now i'm thinking about how paths can be revealed without the numeric votes statistic. Maybe it could occur in the form of users having profiles showing what they currently believe, in the form of a list of questions and answers. the benefit of a profile instead of answering questions is that it is easier for users to change their beliefs because broadcasting beliefs does not involve hashing. this is in regard to answers, while questions will not be anonymous. what I do believe is that answers cannot be used as a numerical measure of beliefs -- people are too lazy to list everything they agree with. 

perhaps the nature of this democracy could be unlike most democracies where everyone speaks. in this case those who read what others have but don't contribute anything wouldn't leave any mark. it could be the only way to leave a mark would be to contribute a question or answer. i'm using the idea that questions can really reveal answers, and also the idea that each answer (perspective) need only be said once (instead of multiple people subscribing to same answer). i'm hoping that those questions people really resonate with they will demonstrate not by clicking a yes or no button, but by engaging and extending the topic by taking the time and effort to construct new questions around it, whether subquestions or peer questions. i am relying on the process that to flesh out a topic, it must be fully explored in sigma, and only then should it be brought to delta where it can be layed out in concrete to see if anyone disagrees (by proposing a delta). this way, ideas are in one of 3 state. 1 they can be in sigma where they are developed and all perspectives addressed. 2 they can be transitioning from sigma to delta, where different perspectives of what was fleshed out in sigma now gets translated to delta, which may require more work or reviewing topics in sigma. 3 the topic sits in delta and is no longer very dynamic, but if its important it will be continually reference by currently dynamic topics either in sigma or delta. none of these 3 cases require a numeric vote on the importance or the value of the topic. instead each state is based on engagement -- those who want can engage in the topic. the path is found by seeking engagement. the point i was trying to say with the 3 stages is that there is never a point at which engagement fails and numerical voting must be resorted to as indication of value, because there is never a point at which a valuable topic (or a reference to it) is not at the surface of engagement. i'm trying to say the same thing a lot of different ways. another way is that i was wrong to think there will be static topics that are still valuable (and therefore need a numeric indication, eg votes, of value) to it is a visible part of the path. so its the current dynamic level of engagement that shows the path, not the past static votes on answers. so with an emphasis on engagement, how should answers be handled? a good line to draw divides on the variable of anonymity. non anonymity begs the question of how answers are removed. i suppose they can't be removed, just as questions cannot be removed but only discarded, but can only be discarded if their parent question is discarded. is there any benefit to having answer authors known? would it be nice to know who answered your question? could it prevent answer spamming by only considering those answer from users you trust? yes. would it entail that all authors of the same answer must be shown? yes. does this invite the perspective that the more people give a certain answer the more correct that answer is? yes. since engagement is based upon questions and not answers, could i forgot answers all together? do questions need to often reference the answers of subquestions? are the questions asked before the subquestion answers are available? yes. is this a circular dependency? yes. to resolve the dependency could the question be answer twice, once before the subquestion, and once after referencing the answer of the subquestion? actually could the second time not be asking the question again, but asking a subquestion in the form 'do subquestion a, b, and c imply the answer is yes/no?'? partially. does this imply an answer but not actually provide an answer? and without an answer, can other question reference an answer? no. but can other questions create answers to other questions? yes. does this form of answering break the answer chain? is this good such that the logic for arriving at an answer for a question is independent of of the answer of the question, creating a separation of concerns between within the answer and outside the answer? does this mean question A can answer question B with answer C even if inside B an answer of C is nonexistent? yes. does this provide an intuitive mechanism for assumptions? yes. does absence of answers make the DAG shallower and easier to read? yes. is the DAG then of only one type: questions? yes. so is the general mechanism that instead of an answer to A dependently referencing an answer of peer question B, that a subquestion of A asks 'does YesToB imply Yes?'. are answers in this way implicit which follows the philosophy of path-by-engagement, because an answer A to question B is implied by presence of engagement in more questions that clearly assume A as an answer to B? yes. will the method require an protocol for recognizably encoding answers within questions? yes. can will this encoding be rendered as showing an answer of 'yes' or 'no' and then a link to the question referenced? yes. could the written encoding be <answer>:<link>? yes. should i at least implement and explore usage of this no-questions architecture before including the answers type? yes. 

SEPTEMBER 29, 2017

Sigma UI:
sigma lists, for now, are levels. they can have filtering i suppose, or for now just be ordered by most recent.
maybe later they can also be author lists
to accommodate future additions, i suppose the header can have an input where different commands will be displayed as they are typed
cmd-v will paste a sigma hash there then render that level, while u cmd-v will paste a user hash there then render an author list.
on left of header is 'x' button to remove the column. on right of header is 'oo' button divide the column into two identical columns, like cell division.
for columns:
	header will have (from left to right): x button to remove column, ..., oo button to duplicate column (or maybe this could be right above x button), perhaps '+' button to add 
	commands will include: paste sigma id to render sigma subquestions, paste author id to render author questions, perhaps filter commands
for other authors' sigmas:
	header will have (from left to right): author, date, #subquestions, discard button,  
	commands will include: copy sigma id, copy author id, 
for self as author sigmas:
	header will have (from left to right): you, last date modified, discard button, save button, submit button, 
	commands will include: copy sigma id, copy own author id, 

every sigma can be shown in plain text form (eg tex is text) or in rendered mode. i suppose this can be a toggle command, and there can be defaults for other vs self sigmas.
the rendering components have no state (except for scroll) so they are re-rendered on every visible change. so what displays must it show? selected column, selected sigma, type of sigma list (including filter settings). 

sigma id(s), user id(u)		 
date(d), user(u), responses(n)
i suppose there could be, for now assuming the below, only one operation: order by, 
which would accept an argument of either date, user, or responses
note that multiple orders can be app
assumption: sigmas are just questions, and can only relate to other sigmas through hierarchical sub-questioning and containing links
also, subquestion lists are not very long. that is, short enough that they need to be filtered for to easily find something
i suppose there could be a third type of column in addition to sigma and users: date, which given one date would retrieve
all sigmas received after that date. for now the limited input would be a single date (as opposed to range) and would be hotkey typed in format mo/da/ye, or maybe in the form of an integer (less than a limit to prevent too big a response) for hours. 
for the ui, each of the 3 types of queries (for the 3 types of columns) can be searched for by pressing keys s, u, or d respectively and then either pasting (for s or u or d too) or typing (only available for d).

how to do caching of sigma lists? i suppose the client can have a list of basic queries (a cache list), with one list evicted each time a new list is queried. I suppose the 3 types of queries (sigma subquestions, users creations, date) can each correspond to a type of basic list, and those lists are distinct enough from each other (one is not necessarily a subset of the other) in nature that although they may well contain duplicate sigmas, they can rationally be queried for and stored independently. the server can (through websockets) keep track (in the db i suppose) of which connections are holding which queries in their cache list, and when changes to the contents of any of those queries, those changes are pushed down to the client. occasionally the client can push to the server its configuration (the ordering options of its columns and how many columns) for saving for next visit. there is one exception to basic types and that is that lists of type 'date' can be subsets of each other, so really only one list of this type need exists and in fact it can exist by default containing the max length it could. also interesting is the idea that that list could be the only list that gets pushed to, because containing all new questions created since the page began, it can update the list of all other queries without extra server pushes. 
for now for a 'lean' approach with 'unnecessary' features i will forget pagination and just assume that all lists will be reasonably sized. the list type most likely to break this assumption is the 'user' type in that a user may post too many. with no pagination, this means the sigma lists of each column will point to a single list in the sigmaLists array. with pagination, i imagine the sigma lists in the sigmaLists array will contain info about where they place along the pagination of the list, so that the client can request the remaining parts of the list from the server.


PROTOCOL:
login:client
	{
		userId,
		signature
	}
	first showing login page
	if URL contains a key, interpret as public key
		userId = public key
		signature = signed public key
	else it is a visitor
		userId = 0
		signature = 0
login:server
	{
		queries,
		config,
		err
	}
	if visitor set up new connection
		err = null
		config = {}
		queries = []
	if user already has connection, ignore new connection
		config = null
		queries = null
		err = already connected
	if user exists
		if valid, set up new connection
			config = get config
			queries = get queries
			err = null
		if not, reject connection
			config = null
			queries = null
			err = invalid signature
	if user doesn't exist
		config = null
		queries = null
		err = nonexistent public key

addQuery:client
	{
		listType,
		listId
	}
	
addQuery:server
	{
		err
	}
changes:server
	{
		listType,
		listId,
		changes
	}


priorities: 
look again at text rendering to deal with spaces, errors, spell checking, cursor
once better layout of full app, optimize rendering with shouldComponentUpdate


Delta:

UI for Delta:
on far left is control bar just the same
on right there is rendered document, nothing highlighted
on left is column of deltas, 


using Horizon and RethinkDB


SIDE IDEAS:
how about hosting a currency on this chain? it would be dependent upon reputation of writers, trusting them to keep a leading, valid line. Or, what if every person held their own separate line? Regardless, it would be a subjective community only offering robustness through personal trust.

maybe a single page line can be created and shared among a private set of 2 or more users, imitating and serving the purpose of a group email.

many security of the system can rest on the work done of storing and sharing a page line, and this would be done by the community deciding (on the page line) on random comprehension tests that will require storing the entire history of the document (thus proving contribution to the value of the community), and those that do well on the test can have the document reflect an increase in balance in their account.

:END SIDE IDEAS

what should the ui be?
render the file, create change requests, render file with changes
maybe file could be a json file
i realize now the document is made of an ordered series of modifications, which can be processed to yield a renderable file, and hopefully the react diffing mechanism, i forget how it works, can re-render single elements at a time.
the document consists of a series of elements, and they are managed by a series of changes that insert a new item, drop an existing one, or modify an existing one, specifying what part of it were modified. every item has an ID. request formats are:

<drop> = drop <ID> recursively(?) <dependencies>
<add> = add <position> <item> <dependencies>
<modify> = modify <ID> <index> <modification> <dependencies>
<move> = move <ID> <position> <dependencies>
<position> = after <ID> | before <ID> | inside <ID>
<index> = at <int> (i for from beginning, -i for from end)
<modification> = remove <int> | insert <content> (| replace <int> with <content> , maybe not this)
<dependencies> = given <list of changes> | ''
<list of changes> = <ID> <list of changes> | <ID>
<ID> = _primitive_

f20df239ut af&Fs4ifks5 create after 0 and I

modify 31e4a at 45 replace 34 with "hi there" given 50232 09481 48243 82493
modify 31e4a at 45 remove 34 insert "hi there" given 50232 09481 48243 82493

are items nested? yes i suppose they can be, for example a group of paragraphs, or a group of messages that make up a chat.
i realize the protocol can and should be more sophisticated, which will save space with smaller change requests, make editing easier, and will mean more javascript processing, not more sophisticated UI.
there actually doesn't need to be an official record! this can also be distributed. the idea being


how is editing done? the result of editing should be change requests. so how should these change requests be made?
they could be made by a diffing algorithm comparing the old record with the new record that the user has created by freely modifying the old record. This algorithm could be complicated and slow, and just as for the algorithm, it could be hard for the user to remember what parts he's changed and what parts not.
Instead the user could keep track of particular desired changes. But to make this convenient, the process would probably be to construct a change while looking at the existing document, then pushing the change onto the stack of changes to update the current document.
how can you see changes visually? the effect of the last change can always be visible. most of the time multiple changes can simultaneously show their changes. You cannot show some changes together. Which are these? Two changes the modify the same item in overlapping positions, or the insertion or modification of any item together with its removal. every time a change gets puts on the stack, the it could be recalculated which of the below changes are still visually applicable.
i suppose dependencies could be determined by each change referencing those changes that must precede it. in fact, even if a change modifies an item that was just created so it clearly depends on the change that created that item, even in this case that change that is so evident should be explicitly not implicitly referenced. before i was imagining dependencies would be implicit, where insertions have no dependency, drops depend on what they are dropping, and modifications depend on what they modify. in this scheme dependencies are made on items, whereas in the other scheme dependencies are made on changes. I think I will go with the latter, because it is then straightforward to determine conflicting changes.
but now i realize even this doesn't make conflicts detectable as two changes can not seem to conflict when they do which requires examining their content.  suppose two people decide an item requires an insertion and both issue it, but placing it in different, non-overlapping parts of the item. They appear non-conflicting, but they should not co-exist because they are duplicates serving the same purpose.
maybe we can have a system like blockchain where a change doesn't reference those changes it depends upon, but just references the entire document it references. and people can choose which chain they follow. this makes a document coherent forcing people to choose at the document level, not at the change level. This enables people to always be on the same 'page' (no pun) with each other but deprives people of flexibility in cherry-picking which changes they would like. to keep some flexibility in the document model, the document could be designed to layout multiple view points. in fact, I would argue that a properly implemented change model would really be a document model in disguise. The argument is any change is dependent on so many other things in the document whether they are evident or not, and the changes that account for all those things must be referenced. follow a chain of a few changes and the cumulative number of changes referenced grows exponentially, until all changes have been referenced, so essentially you are tied to the entire document. the intuitive logic is all items in a document are so interdependent (because documents tend to be written with coherency) that people should be looking at and editing the same document. so in the change case, if there are differences those differences should constitute separate documents, in which case you might as well use the document model. the change model was conceived with the vision collaboration can be done with documents that differ slightly in a number of ways, and due to the interdependence I've mentioned I no longer think this a valid assumption. I will go with the document model.

for optimization, snapshots can be taken that record the cumulative effect of a number of changes as a set of insertions. the protocol for this I don't yet know.

the trouble with the document model is flexibility, because how is it decided which changes to keep and which to discard? one model is to leave it to the chaining which changes get included, but this effectively forces changes to be decided in order, and probably decided by the wrong people (those that happen to post the next change, even if completely unrelated). another possibility I'm now thinking about is proposals for changes being made separately from the 'existing document' kinda like pull requests. All of them will write changes with respect to the existing document, not other proposed changes. They'll sit on the side and people will discuss and show their support for them. When one gets 'committed' it will be re-edited to...

Suppose modifications cannot be changed by index, but must be completely replaced. To save space this entails items should be smaller. But then this entails for a conceptual change more items will have to be changed. This way there is no race. Then in the case that two modifications are made to the same element, a new modification can be made that merges them.

Maybe decisions can be made at the chain forking level after all. Changes are chained to the 'currently accepted document' forming multiple forks. Those changes that are uncertain should not be built upon. Other paths (hopefully the 'primary' path) can discuss other paths by which requires referencing notes on other paths (by their hashes) which shouldn't be difficult. When its decided on a path that another path is worth incorporating, it will re-make the note (probably a single one) on the other path so it can be incorporated and it is chained to the path. To get fancy, there could be scripts that examine changes in order to identify/declare the primary path, because it should only include discussion-based changes that reference potential changes currently only visible on other paths, or be incorporated changes from other paths that have been sufficiently discussed and approved for incorporation. So the idea of cloning is no longer necessary - modifications with indexing are ok.
So at the UI level we sill render the page, probably on the right side, and a nav place on the left for receiving, reading, constructing, and sending, change requests.

I'm beginning to think this is more difficult than sigma. In sigma, maybe instead of thinking in terms of a concrete graph with strict relationships between the questions, they could be more fuzzy with no particular relationships of references, and just the notion of 'context'. Maybe context could mean creating particular interpretations of reference that could be concrete. How would the questions be viewed? Their references would still form a DAG. Suppose the UI could be done. How about networking? for the moment forget the 'space' method due to its complexity, and leave that to be developed for a more scalable network as such scale becomes necessary. For now we can continue with the single stream of inputs that everyone stores (like bitcoin). as far as storing them, since they are stored on single servers, they could be store in a giant hash table with the keys being the IDs of the items, which I imagine are their hashes; this way something like neo4j will not be necessary. If you decided not to hold a note, you can discard it from your hash table. If you need a note you don't have, you can query for it simply asking other if they have it. this would be a non-scalable mechanism for starting out. As far as the UI, it would need to be interactive to enable users to navigate the note DAG, looking at those notes they want. ...This is still too complicated. The above paragraph specifies my final idea for the moment.

the functionality should still be a basic interface to the network/db for notes, a collection of your own notes in progress, the ability to select any note of the tree and render its thread on the right, the option to highlight the latest edit within the renderer, the ability to publish a note, the ability to edit a note selecting all its types,


goal: find a way to represent deltas and models such that they can be easily sent, computer manipulated, and human manipulated.

escape character method: view all designated special characters (including the escape character) as special, except when preceded by an escape character, at which point you view it as non-special.
problem is your content must change (escape characters must be inserted) every time your content contains a special markup character.

inverse escape character method: view all special characters as non-special except the escape character. view all characters except the escape character after the escape character as special characters. view the escape character after the escape character as the non-special escape character.
here you insert an escape character after every escape character in your content.

delta schema:
<id hash> <prev id hash> <content>


@(
  add after FmAbBdoC4IXLADXJAfC8ZuZGhos=
  before FmAbBdoC4IXLADXJAfC8ZuZGhos=
  @(
    content goes here, whatever you want, except every @@ must be be escaped by another @@. @@( @@@@
  @)
  only need to use @@\s as a separator if you are separating multiple items that are not lists, because lists can separate themselves, like is done in this list.
@)

two types of hashes, one of deltas, one of items.

@(
  add after 0 @(

  @)
@)

regex: @\(.*?@\)

document structure consists of nested elements. some element type only make sense when nested in certain ways with respect to other element types. element types:
headers of different sizes
paragraphs


elements -- all of the below
pages (root pages are the base pages for a project, formerly what i called documents)
sections
dividers
paragraphs
plain / underlined / italicized / bold text
inline math
display math
inline code
display code
svg diagrams (eg avatars, emojis, many of which i can import, and can offer custom)
image


should headers be separate or not?

what should differentiate between documents? its should be based on the notion of 'independence.'

IDs can be the hash of the items together with a counter that documents the number of the element added or modified, this way preventing collision of different elements with identical content. eventually more complex deltas can be constructed, such as a version of modification that captures every instance of a word and changes them uniformly, such as capitalizing them. maybe these complex modifications could simply be done with regex being a selection criterion for selecting text inside elements for modification, and selecting elements in the first place that contain matches (maybe including the number of matches) for dropping or positional referencing for adding.

document model:
@( element_type @ nonce @ children @)
children can be primitive content
@(n @ p @
  What are you doing and the way it works corresponds to the diagram on the right labelled "Diagram 1".
  @(n @ img @ file.png @ 10x12 @)
@)

@(drop /abc/ & 313387d0340151a13715
@)
@(add after 313387d0340151a13715 @
  @(
    n @ p
    @ Ok that seems to work.
  @)
@)
@(modify 313387d0340151a13715=
  at 32 remove 13 insert @( and I, @)
  at 55 insert @( hello @)
@)
// with macros in mind now
@(modify 313387d0340151a13715=
  change name to @( Johnny @)
  modify content at 32 remove 13 insert @( and I, @)
@)

i think the basic principle is to only enclose those things in a list or between separators that are of variable length, and let the rest be single items that are parsed for convenience of the reader and writer and to the extra work for the interpreter.

Ok, i think I'm getting the idea. the list of lists language is what I will call the "document model language", because it describes the final form of the document, and so is a subset of the delta language.
but the delta language is a different language.
however, the model language may best be used outside the model, such as for the purpose of packaging deltas and sending them and other info around. Or, maybe not, and the delta language can be extended to provide the semantics necessary for networking.

i arrived at the list of lists model language as an elegant substitute to xml, which i arrived at as a similar document language to html, thinking markup is better than markdown because its explicitivity makes it easier to read and write in a structured, formal way, while markdown may be better for the latter but in an unstructured, informal way.

for now, thinking about the list of lists language as a model language to describe the document, and how to render the document, but for convenience, use javascript style data structure instead of @-style notation.

the style of display must be extensible, because limiting types to a fundamental few put to much work into the user for making simple deltas (eg to add a simple comment to a discussion a user must write out a chunky structure of layout, avatar, name, and comment content). Yet complex types like comment lists are needed for productivity. the apparent solution is custom templates/macros that enable users to specify complex deltas in simple format. Every line must have an established set of macros in place. How are macro's added to a line? Suppose the community of the line (eg an individual or the whole project community) has discussed and decided (using existing macros ie bootstrapping) that the macro should be added to the line. A special delta that expresses the macro would then be sent. The delta could add to the document a doc explaining its use, but it would also need to provide some info that can be understood by the interpreter (whereas the interpreter blindly manipulates most content). so i see it that a special record must be kept for macros, and it seems like a straightforward step without any evident drawbacks to make that record part of the document itself, because they are both persistent records manipulated by deltas. macros could be page specific and be inherited by all lower pages, but pages are free to override a macro by redefining it, enabling the removal of a macro by redefining it to null. maybe to start for simplicity, all macros could apply to all pages. the record of macros could be implicit, but better that it be explicit, and since the macro docs will have to be built anyway, the explicitivity might as well be in writing the docs in a computer readable way, this way the interpreter can simply examine the document structure (held in the program as a javascript structure) it has built up from the line so far. A macro is the definition of a new type based on existing types, with parameters. So it could have the same general form as an element with contained variables that correspond to the parameters that follow the macro name (basically a function definition). So creating these types is convenient with macros, but manipulating them would still be inconvenient if the macro is limited to creating the types. They should be extended to transform all kinds of manipulation. A macro could create a wrapper around its substituted definition, then that wrapped element could be dropped, moved, or modified by an interface it provides. The modification interface would take input commands and transform them to input commands for its constituent elements, or the primitive command of "at <index> insert <content> remove <num>." only the wrapper would need to have an ID.
maybe macros can take care of custom styles and how to handle distinctions between paragraphs and headers. a macro could be called h1 that would style text in such a way to make it a header. maybe macros could also take care of preprocessing such that you can input something like "text $$display math$$ with [link](src.com) ok" and the macro will


{
  type: 'macro',
  name: 'comment',
  params: 'avatar,name,content',
  context: 'only applicable inside element type "chat"'
  def: [
    {
      type: 'divide',
      ratio: '1:3',
      leftElements: [
        {
          type: '${avatar}',
          src: '${name}.svg'
        }
      ],
      rightElements: [
        {
          type: 'h2',
          text: '${name}'
        },{
          type: 'p',
          text: ${content}
        }
      ]
    }
  ],
  mod: {
    changeName: {
      params: 'newName',
      ops: [
        divide.left.src = ${newName}
        divide.right.h2 = ${newName}
      ]
    }
  }
}

the document would be a structure of nested elements, each specifying a type, maybe attributes, and children. a macro element could appear in the document with or without its substitution visible, and probably by default not visible because its not necessary.
i'm now thinking commands shouldn't just be used for modifications, but manipulations as a whole should be re-thought in terms of macros. a question is to what extent should custom elements by editable? make them fully editable could be hard, and let people misuse the element, but making them only partially editable may prevent necessary modifications that are unpredictably needed. For example, consider a chat. Should a user be able to modify a comment after he posts it? An easy answer would be no, but what if the comment contains unacceptable language? Maybe partial modifications could be carefully chosen such that they can handle any scenario. For example, one can modify the comment of another only if it contains bad language. Such a rule would be subjectively enforced, but some rules could be objectively enforced, for example only letting one modify his own comment by ensuring matching signatures and even then only allowing the modification if the comment has not yet been responded to.
...macros and their functionality, especially with macro composition and scripting eg matching signatures can get real complex real fast. that level of functionality should be left to a full fledged blockchain, at least that's my feeling now, and this document chain concept should be limited to simple functionality so it can serve as i initially intended it, a simple platform upon which to bootstrap a full fledged blockchain. so how much functionality do I implement for now?
One possibility is no macros, and just extending the built-in types to include all that I deem necessary for now, such as tables, chats, and all the rest. Since others are free to change the source code, they are also free to create new built-in types, and the source code can be made to make this as easy as possible. With this possibility the idea is the document model would be a sufficiently simple and shallow nested that users can easily create manipulations without macros. This possibility also makes the assumption that different documents will use a common set of elements, and extensibility and customization are not so important. I don't think built in types would need their own custom manipulations, and instead they would all be sufficiently manipulated by the add, drop, and modify by insert and remove operations. A table is easy to imagine as I did before similar to html tables. A comment in a chat would be a list of two (in addition to the type declaration), the first an identifier for the poster, probably public key which would then be rendered to an avatar together with a name, then the second item would be the content of the comment. so posting to a chat would be like "add after <previous comment reference> @(comment @ <public-key> @ Yes I agree @)." And the UI could even have a button that write out a template for you to post a comment quickly with little effort. One worry about chats in the first place is too high submission frequency.
Another possibility is non-composable macros. These macros would be as I initially imagined above, where they can be created, dropped, and when modified their ID is specified first that way the interpreter knows how to handle the modification command. So now what's needed is a mechanism for commands in the context of them manipulating the elements of the macro def which will not have IDs. the mechanism will consist of ways to identify elements and positions within the substitution, and then traditional primitive manipulations can be performed from there. i'm now thinking the previous possibility is better.
the more sophisticated the rendering, the more difficult to identify elements and positions for editing.


proceeding with the built-in types idea:

different documents use different built-in-types, and they can keep a reference to which they support, and hopefully the source code could be like plugins, and a user could keep the code for each of the types used by each of the documents the user is interested in.

an essential idea to this approach is the process of clicking on the document in ways that enable you to identify elements such that you can then with that info go on to write a delta with predictable effect. this means it is very evident looking at the UI what the different elements are so that you clearly know what and where you want to edit.

deltas should be resubmitted with notification of their purpose for resubmission - branch merging, because another delta got accepted first.

since elements are interdependent anyway, maybe it is not necessary to have a display math or code or image includable in a paragraph, but they can be separate. In order to keep the appearance that they are complementary and part of the same thread of thinking, the default spacing between elements can be small (before i thought default would be larger), and there can be elements (section elements) dedicated to grouping and spacing other elements further apart as appropriate.

new hierarchy of elements:
pages
sections
columns
text like markdown with bold, italic, inline math, inline code, small svg pics, links, date, etc, all with their different markers.
display math
display code
image / svg diagram (maybe interactive)
media
lists

to make the element hierarchy most evident, maybe colored lines can go down the side of the page to show the nesting, and section breadcrumbs can appear on the top, inspired by headers in spectre.css docs.

how are transactions done again?

messaging can happen in DAG form and transient form through the network and need not be recorded in the document. instead, when the messaging decides for a branch, a description can be added to the document explaining the overall reasoning behind the delta. in fact, this can happen for deltas on any line, not just the primary line.

it is important to remember that this document editing model is not ideal for discussion, and something more like sigma is necessary. In other words, this tool is only temporary, so thinking about all the fancy extensions and improvements to it are futile.
maybe sections and even columns are still ok, because all you do to them is add them, drop them, and move them, but not modify them. i only hesitate on columns because it kinda relies on a wide document, and it they would need to be slightly modified because of the ratio property. But columns would suddenly provide a lot more opportunity for using the page as an aesthetic presentation page for a website cover or something. As far as modifying the ratio, there could be a hover button over the divide to show the ratio, and then modifying would actually just be replacing it because its so small, and after all changing one part of the ratio really affects the entire thing, unlike a paragraph.
Vertical spacing in general could be handled with empty elements, space elements. Within columns there could be the option to float top (default), center, or bottom. I could go on with options, but I'm worried about too many options and the editing complexity they will introduce. Like if there is the option for more than two columns, the question comes how they split themselves into multiple rows upon window narrowing. in the case of more than two columns i was also thinking about using them for horizontal spacing, but this then invites question of whether the original default spacing is not good enough, and maybe even single element rows should use columns for spacing, which then would make every element more complex. So I'm now thinking either we go with columns in pairs or two, or no columns at all. with two columns there would still be the option of at what point in window downsizing the row should split. the is also still the option of floating the inside elements vertically and also which i didn't mention, the option of which column goes on top after a split. Is two columns worth the effort, or should columns wait until more than 2 columns are possible? I will use the reasoning that if intended for aesthetic reason like a cover page, more than 2 columns will be necessary... wait a minute! an arbitrary number of columns can implemented with nested pairs of columns, and responsiveness is simplified and limited in a convenient way to still only be in terms of at what point columns will split into two rows. whenever a row splits, each of the two columns occupy an entire row. the seems really cool at the conceptual level, and it would be great for nearly any kind of layout, but how tough would it be to edit? For now they could be called 'divides containing two columns', and these divides could be visible in the UI with a box surrounding the two columns, and a dividing line between them (which is vertical when unsplit, and horizontal when split), and it could be that these lines only show when you hover over a button located on the dividing line, and these button in turn might only be displayed on request by a certain setting.
maybe the ratio need not be constant, but could be a function of the width (side note that the function will only apply for widths above the breaking point).
worth noting is that upon split, both columns should occupy minimal vertical space, ie the larger one will stay the same, while the smaller will occupy less space.

{
  type: divide,
  ratio: '1:4'
  splitPoint: 50,
  splitOrder: true,
  leftFloat: bottom,
  rightFloat: center,
  leftElements: [],
  rightElements: []
}
@( divide @ 50 @ true @ 1:4 @ bottom @ center @
  @(
  @) @ @(
  @)
@)
OR
@( divide @ 50Y:1/4:BC @( leftElements @) @( rightElements @) @) this way its easy to read and modify

maybe text can be modified in a way where it get's highlighted for different reasons, eg concern, question.

so suppose divides are easy to spot and with the little (50YBC) popup. Then divides are as easy to manipulate as any other element, as you can read them, get their ID, see where they place in the hierarchy, and easy to write to. As for sections, they are like divides except they don't have the popup and don't accept modification.

tables could be implemented with divides, so for now tables will be untouched. or they could later be made a built-in-type.
would divides be applicable to structures like chats? Yes, given the terse writing of a divide, it would not me much harder to construct and add than other elements like a paragraph -- its just a slim wrapper around its child elements.
now how about lists? divides could easily handle this, where the left column remains empty, and the ratio determines the level of indentation.
@(| :1/4: @(@)@( This item of the list says this. @)@)
but maybe it would be too inconvenient, and the type should be built-in. in fact, it probably should.


its the job of the user to only accept deltas that make sense, and only construct a document that makes sense, because once constructed, the doc can only be manipulated in certain ways (adding, dropping, moving, and modifying elements) so if for example there's an element with a misspelled type, you can't modify it and respell the type, but must drop it and add back a fixed version.

deltas -> document -> render
what about sections replacing pages?

---- after retreat ----
every element will have to be modified in its own way. block code and math will probably best be manipulated line by line, while diagrams, columns, and other elements will have simple commands for replacing their attributes.
i'm thinking the document be made of primitive elements, all with visible IDs, and macros exist but operate solely as macros in that they are shorthand for more complex operations. all non-default types could be made and manipulated with macros, and their constituent elements could always be handled manually because they each have their own IDs. The problem now i see with modification is that it requires knowing the IDs of maybe multiple constituent elements and providing them all would not be considered shorthand. the possible solution i see is providing the parent ID of what you are modifying, and it would then be inferred what primitive operations the shorthand modification entails. the inference would be performed on the model made of primitive elements like any other modification but the primitive operations would be inferred, then the primitive model would be rendered.
i'm now thinking about instead having what i will call built-in-types. still modifications given in language above the primitive level but it is not shorthand. instead of the model being primitive and then commands being translated into primitive modifications, the model is not primitive and the renderer is tasked with rendering this more complex model. what about modifying nested elements, for example a paragraph in a list item? Well I suppose you just point directly to the paragraph, which will have an ID because it is not part of the list item, but nested inside of it. A chat, on the other hand, would probably have the name of the poster as part of it only editable through commands, not a nested element. some example types
table:
add/drop row/column
in 2:4(row:column) add (after/before) @(@)
list: // contained as whole it its own custom container
add/drop item
move <pointer> to <position>

i'm thinking sections instead of pages, and identifying them by having lines run down the left side, partly indenting the sections, maybe colors corresponding to types (section, paragraph, list, etc) in which case the outer lines, probably all being sections would have the same colors. how about columns? when not split, the one on the right would have a line on the left side just like the one on the left. when split, they would both have lines on the left, and to show that they belong together a little marking between them.
every element is responsible for identifying itself without invading the space it has given to its children. this might be done with no problem by allocating space on the left to draw a line to identify itself. and in fact, this space on the left need not always be there, but can disappear when the user doesn't need to see positions, such as in presentation mode.
spacing is done by element. most elements will have no default spacing. sections will provide generic spacing.

i was getting worried about interacting with a rendered paragraph where you click to find position, because this would mean responding to clicks and keeping track of how their position maps to position in the input text, which would be tough for a number of reasons. Now I realize there can just be a button that switches from a rendered paragraph to the raw input text where your click can be easily mapped to position in the input text. so now the task is just to render a paragraph, no interactivity needed.


should be able to navigate entire tree, and use embedded links as shortcuts
should be able to control if/what folder the window is bound to
request that the folder is rewritten to reflect the selected state
request that the current folder is diffed against the selected state, and a new delta created with result
	or only offer this option when a unpublished delta is already selected (always have implicit new delta available)
see all currently unpublished deltas. 
publish a delta
delete a delta together with all children

each view is always bound to the tree. when the tree changes, all views change. one delta is always selected in each view
a view can be bound to 0 or 1 folders. when bound to a folder, the folder is bound to the view

maybe good would be to match two things. one is the number of deltas downloaded per round of scrolling. the other is the number of deltas stored in the db between the document up to their last delta is compiled and cached. i'm thinking like every hundred or so. this way upon every download the cached document is downloaded along with the 100 deltas. actually i suppose the way it would work is it gives you 100 deltas and the cached document that occurs somewhere within those deltas. 

how should the databases be formed?
if client side db mostly just keeps deltas then they should be stored in enumerated form.
what does the server store? it stores deltas that are posted, and it stores separately a list of queries and corresponding answers
so the posted delta list will be in arbitrary order. 
what queries are supported? i think its of the form, 

https://parceljs.org/
https://argyleink.github.io/ragrid/
https://rollupjs.org/guide/en
http://kaisermann.github.io/rolleiflex/

REVIEW APP ARCHITECTURE
server:
	server holds a base vertex. users submit deltas, with merkel proof that it leaves to the base vertex. server can also check that delta is indeed a modification of previous delta. server holds full deltas of top few deltas at head of chain. server ...
	assume server works. servers should have model of fulfilling one delta request for every update check. with this prerequisite, most of the time there will be no requests to fill.
document:
	a chain from the base. composed of deltas. store in db. servers are specified in document on homepage together with folder structure. 
client-server model:
	users identify server. they query with a hash, and the server returns all above hashes it knows of. if user is doing document for first time, user can then query contents for a particular hash. this is a separate query system for the server. a delta and all its children hashes die after a timeout. But the clock is reset when a new delta is posted for all its child hashes. a content request also times out, and a query response also times out. 
db model:
	i suppose the documents must be stored by delta. so keys can simply be hashes, and their values will be the contents that were hashed.
	document is top key. what defines a document? i would say base hash. but user won't have base hash if they don't download first delta. and a fork into two separate documents would still treat them as one because they have the same base. for a moment, suppose no top key, just hashes. db can easily be searched for a delta. searching for a delta also retrieves the full document at that point. so i think key value store need only store hashes and values, no meta data, no top key. the advantage of a top key is reduced thrashing. but given forked projects, it is not easy to partition projects distinctly. this could be a later optimization. what about searching for all current tips? this is harder. this is necessary because user needs to be able to gather stats about everything stored, to help decide what to delete. 
	in order to map out the documents, and perform db sanitation, deleting anything that shouldn't be there, the entries should be ordered in such a way that moving down in a read stream from top to bottom, a program can verify that every entry is valid, deleting any that is not. info on ordering below.
	regarding thrashing, grouping by a projects history of deltas probably isn't even that smart, because users will mostly likely be switching between document more than they switch between deltas, and remember, projects should be many and small to avoid forking.
delta model:
	a document is a tree folder structure. there could be protocol of how to specify it as a list, which can then be encoded to a merkel tree. but like i realized before, this would mean a single change changes many hashes because the position of many other elements changes. so better is following the folder structure, and even encoding all items in a file this way at a single vertex, which wouldn't be too bad because each will be represented with 256 bits, equivalent to about 32 characters of text, significantly less than the average item text length.
	i imagine there should be an archive, such that old deltas likely unaccessed can get stored permanently to reduce thrashing in the database, but they can be still be accessed if you choose to view them, and the system has access to them to answer potential server queries for them. maybe this archive could be part of the db, and it would be the top level split. in this case, its an optimization we could support later.
interface:
	home window shows list of documents in store, and under each a sublist of current tips. if easy to do, both list types are ordered by most recently visited. there is option to see map of current db state, which draws out all the document currently held. using these two overall pictures of the db, users can pick a delta to view. 
	a delta view consists of not viewing the delta itself, but selecting its children. listing them, and rendering one of them. when one of them is clicked on, it will be rendered, and simultaneously be written to the folder. when the folder is saved, a new delta is created, the one selected is updated, with the new info. 
links model:
	link should be a server address. if link is not just to document, but particular version of document, it should include hash of relevant delta. the latter is for temporary links, like those passed around on social media. what about links embedded in projects? they need to be in a more permanent form. ideal would be the hash of a delta, but then we don't know what servers it associated with. how could we do a DNS system? one possibility is to create projects, probably just of a few files, that associate projects with ids. this would be hard.



DB ENTRY ORDERING:
the reasoning is we want a collection of delta tips, and we want 0 or more of their ancestors. but we don't want a gap in their history. so at the top of the db we have the tips. then below we have deltas in order of history. if we come upon one that doesn't have a child above it, we recast it as a tip. actually instead of distinguishing tips from the rest, we'll distinguish each deltas with an index of their history. this would be most doable by increasing the index from parent to child. using indexes means you can't access a delta just by its hash, but must know it index as well. most of time you will access deltas from within the tree, or from the homepage so knowledge of index is not a problem. but you can't just fetch by a hash (eg if a link is a hash).
after we iterate the deltas, we iterate the documents. in this case, rather than making sure every delta has a parent or is a tip, we want to make sure every level of documents has a parent or is the top of the document, so we are going the opposite direction. but the problem is levels don't point to their parents. maybe iteration for documents could then start from the bottom of the database.
other option is not ordering, but then the challenge is how to sanitize the db. 
now keep in mind that, though i've forgotten to mention, that parents will need to put to their children in order for delta navigation.
keeping this in mind, here's an interesting option. just mark the tips. then go through from top to bottom, and ask if a delta has a child above it in the db (one already processed) above it. if so, keep it. if not, traverse down the delta chain (jumping around the db) until you end up at a child or a dead end, and then keep it or throw it respectively. this doesn't work out so well. it has big running time, even though some results are amortized. why do we need sanitation anyway? if we just want a periodic check that the db is ok, we traverse each tip
back to the root, keeping count of how many deltas we have. then compare that to the total number in the db. if they are out of sync for some unknown reason, we just recommend the user download a new version. any other reason for sanitation? i don't think so. all we need to do is be able to traverse delta chains in both directions, and be able to keep track of all tips for convenience. what about sanitizing documents? i suppose we don't need to as long as we keep it in sync. but its harder to make a check that its in sync. we will just have to rely on correct implementation. but i realize document elements must keep track of how may pointers there are to it, this way when that number goes from 1 to 0 we know to delete it.
so a write operation simply adds a delta and the necessary parts of its document. updating a custom delta basically means removing it and then creating a new one. removing a delta means removing it and removing (or reducing count for) all its document elements, and then removing all its children. filling a request means checking the db for that delta hash, and if it exists, passing it up together with the document. i suppose deltas must be marked as custom or not, and if custom whether published or not. all deltas in the db are committed. uncommitted deltas (the kind that can't have children) only exist in state memory. 
keep in separate db a list of current projects and a list of their tips. home page has option to render a project, which mean traversing the tips of the project until they converge, the showing the number of remaining deltas until the base. also keep in this db metadata such as 
servers also support query for links. asker provides hash link, and responder provides list of servers to contact. if we need a verifiable response, the responder will have to provide entire the entire delta, also exposing the item with the link that lists the servers, so its a merkel proof with two paths. in fact, these will probably be more frequently requested than past delta requests. 

delta-hash -> parent-hash . document-hash . child_hash . child_hash . ...

delta-info -> 

use selectors to query the state. each selector outputs its own type. selectors can listen for changes to state (eg changes to db). selectors can output a change to state. i think state could be one giant data structure of db, localstorage, and loca data structures, merged together by the runtime. changes to state would enter the top of the selector network, and new changes would flow out the bottom. i suppose the external world events must also flow as input, eg clicks, but they are not considered state. 
so suppose a selector gets a request for a project switch. it must then test whether another window already has this project. so the solution i had in mind is the selector would query another selector by the name canWindowOpenProject or something. however, this a pull based communication rather than a pushed based one, and it can't be converted from push to pull, because the pulling involves parameters (ie the particular project). so we want selectors to be pullable with parameters, but also asynchronous. this is a more general problem. the whole state in this model is pushable where changes to it get pushed. but we also need it to be pullable so that selector can read from it what they want without having to maintain their own version of the state given the changes they observe. now generators might be a solution because of their design of pulling with 'next'. i suppose cells (generators with runtime) could have a push command to emit something to subscribers. and also a return command to return to a particular sender an answer to that sender's question. note this is not as before where we use addresses and cells keep track of them and fight for their own survival. instead, cells can only emit, and respond to requests, ignorant of the requester. this model would allow for both pushing and pulling, but important to note is that it is no longer a DAG due to the introduction of pulling. maybe pushing and pulling could really be the same model. pulling would be thought of as emitting a request which only goes to the runtime. the runtime then feeds this request like a regular input to the selector being requested from. then that selector emits the result, and then it is the job of the runtime to realize that the next output of that selector is the response and thus must be fed back to the requester. with regular reducers the input can be amortized because arguments are only other selectors, not any custom arguments. selectors add to the state. whereas the minimal state is external and managed by the runtime, the extended state is derived by selectors, and held within selectors. if selectors are asynchronous, what do we do if one is called again during execution. do we just wait for it to finish, then feed it the new input? do we spawn a new instance of it? with traditional reducers, how would we handle a case like searching through a data structure to find something with certain properties particular to the request? this couldn't be done with a selector because selectors don't support arguments. instead a reducer would have to perform the logic, maybe a sub function. similarly, there could be asynchronous sub cells. they would perform a computational task, such as scanning the db for certain properties, and would block the calling process just like a sub function would. a good model is where a cell takes no arguments from its requests, just from its own inputs. then, when it is requested, the runtime can return its response to anyone that called it. however, this is compatible with the push model and doesn't require the pull model because no arguments are allowed. so it seems now to me we must have two types of cells. those that receive pushes and emit themselves, and those that receive requests and emit responses. a think the request, response type would be stateless, and a new one would be created for every request. they hold no data, and can only read data from the state. but i suppose the one other type of input a pull type cell could receive is a cancellation request. 
pull type:
	no internal state
	new instance created for each request
	no subscriptions except from requester (request info and cancellation)
	no output except response (both request and response can occur in multiple phases)
but now hmm, it seems the push type cell could be reasonably handled with the pull type, given that requests and responses can occur continuously. but this now means new instances are created for everything. but realize, this is also the case for functions, where a new instance is created for each call. oh wait, this doesn't work for push types because they are made to absorb events as input from other cells, which it can't do it its interaction with other cells is only .... idk 
push type:
	internal state
	static, only one instance
	inputs from other push types
	outputs are pushed
i suppose pullers are just an asynchronous version of sub functions, so they should not need to interact with push streams. so pushers can call pullers, but pullers can't call pushers. we want to minimize the waist of pullers, we need to make sure their dynamic allocation is compensated by the degree of custom logic they are doing particular to the request, such that it wouldn't make sense to have a single puller handling all requests. 
how about writing to state? it would be pushers that are able to do this. 

switchWindowProject() {
	const req = yield request()
	const otherProjects = yield get(WINDOWS)
		.map(_ => _.project)
		.filter(_ => _ === req.project)
		.length
	if(length===0) yield true;
	yield false;
}

so the network is a state, the only takes input from the external world, and only gives input to the external world. the only mediums with the external world are the interface and the network, because storage is part of state. but we'd like the network cells who emit requests to be those that accept them. then the sink of the network is only the screen, and the source is only user events. but maybe it would even make sense for those cells responsible for rendering components, to accept the events from it. if components are rendered by individual cells then those cell must be created and destroyed with the component. how are inputs fed to and outputs taken from pushers? i imagine it will be the runtime calls next and gets either 'give me next input' or 'heres my next output'. this would mean pushers can't make calls to pullers or other requests from the runtime. this means if it throttles for example, it will need to handle time as an input. so if dom cells handle input from their own dom elements, and network cells handle responses from their own network requests, then there is no top or bottom of this cell network. instead its a regular graph, with recurrencies. testing should be done one cell at a time. if puller cells just had single input and single output, which i imagine they would most of the time, we could use async await functions. in fact, it would be good to use them for this reason, so that pullers are distinguishable from pushers. also, replication for multiple calls would already be taken care of because these are functions, whereas the runtime would have to handle replication if we used generators. alright, so now using async we can easily get sub functions. sub functions that don't accept custom input could amortize their results. they would need to query whether state has changed somehow and since we want purity we don't want to deal with references, so the state (probably the part in memory) should explicitly keep track of whether certain things have changed. pushers remain, and they would be implemented with generators. a pusher gives commands to the runtime. one command is what events to listen for, the other would be what to output. i suppose instead of calling async function directly, it could ask the runtime to call them, that way making testing easier. so basically it would just be listening, calling sub functions, and outputting. i suppose one root generator would be responsible for pushing changes to state. or the runtime could simply merge all state changes from different outputs, of course throttling them.

how about this model: there is one giant async reducer it accepts events from the user interface and outputs changes to state. state is a giant key value store. changes are only of one type, setting keys to values. as far as how the reducer works: an input action triggers the root reducer to dispatch a sub reducer to handle a transformation, a job consisting of reading the state and proposing changes to state. it is known ahead of time what kind of changes a given transformation has access to, that is what range of keys it has right to modify. if the transformation completes, the root publishes it, and the runtime makes the changes. if while the transformation is in process, the root receives another action, it compares its potential effect on state to that of the currently processing action. root is able to do this because potential effects of transformations are known ahead of time. if the two effects are compatible, root dispatches the second in parallel. then depending on the difference in time between their return of changes, root might decide to merge the changes. if the second effect overrides the first, root sends a cancellation signal to the first. if the two are possibly compatible, root lets them process and then compare their proposed changes, and at that point decides if they are compatible, merging the changes if they are. if root decides to process a quick transformation, eg a click, while waiting for another to continue, eg a network request, it must be sure that the change of state will not interrupt it. it might even be enforcible that the range of keys written to by the change are distinct from the range of keys read from the other transformation. now instead of just using async functions we'll use generators because we need to support cancellations, and also for testing reasons. important to note is that transitions will not be deterministic if they involve networking. but they should still be easy to test. maybe there could be a set of consistency rules that could determine how root choses what transformation to merge and cancel. i realize now not only will networking side effect occur within the reducer, but also folder watching, reading, and writing. oh, and also ipc sending and receiving. the downside of side effects is not just testability (which i think won't be a problem), but also that it risks making actions to the external world that don't accurately represent the state. for example, a sub reducer might send on ipc to a window a delta it can show, only for the sub reducer to later be canceled or for it to return and root to decide not to accept its changes. is this also the case for networking? is it ok for a sub reducer to contact servers on its own? the trouble is that given the tit for tat relationship with the servers, the sub reducers must accurately represent the client and this is difficult if they execute in isolation without coordination. to enable coordination, they should propose their plans as changes in state, and then the either next round they can execute what the state reflects, or the runtime can execute for them. the primary purpose of an asynchronous reducer is the ability to regard the db as state. maybe i could use the initial architecture idea of all impure actions performed by drivers, except that the reducer is asynchronous, and instead of outputting a new state, it outputs changes to the state for drivers to look at. i suppose we could do this for networking and ipc, but for folder binding i think sub reducers can be independent. anyway, if we didn't the contents of a saved folder would have to enter by means of an action. its much better if a sub reducer simply reads from the folder. as far as writing, if it isn't performed by a sub reducer, it will have to be written to state, and then performed by a driver. whereas if a sub reducer did it the writing could be streamed rather that fully written to state which could be costly. the other option is leaving the full process of writing to a driver. hmm, i suppose networking eg in blockchain environment downloads from others may be of high volume. i actually suppose handing these to the reducer via an input wouldn't be so bad as long as its a pointer to the data and not the data that's copied. this means reducers don't just look at simple actions, but possible actions containing large amounts of data, deciding how to incorporate that data into the state. 


{
// local
	windows: {
		keys, (1 byte)
		<id>: {
			projectId,
			deltaId,
		}
	},
	requests: {

	},
// meta
	projects: {
		keys, (2 byte)
		<id>: {
			tips,
			folder,
			queries: {
				keys,
			}
		}
	},
// db
	deltas: {
		<hash>,
	}
}

new window
close window
change project
change parent delta
change child delta
	check delta is not already rendered
	check delta is a valid candidate
	get delta document and send with ipc
	OR don't get delta and leave that and ipc to runtime for sake of purity. 
create delta
delete delta
publish delta
homepage
graph project
change bound folder
toggle folder binding


where i left off this weekend: use the above mode. struggling on what to perform with drivers and what to perform with reducers, eg networking, folder reading and writing, ipc communication. 
maybe the state can be partitioned such that a its clear what parts can be independently modified, instead of partitioning state by local, meta, and db. so what parts are independent? projects are independent. we could group by projects. we would also need to keep track of all current requests, across projects.

{
	windows: {
		<id>: {
			projectId,
			deltaId,
		}
	},
	projects: {
		<id>: {
			base,
			folder,
			queries: {

			},
			tips: {

			},
			deltas: {

			},
		},
	},
	requests: 
}

what if we don't try to fit some universal architectural pattern, but make use of the way each project is independent by having a separate folder process and ipc port for each project. the networking though would have to be handled by the runtime for all projects, because the tit for tat server system means asking for updates on one project may involve pushing deltas from other documents. but having separate states would mean less boilerplate, a big benefit. lets try this approach. a new generator would be spawned for each window. this way, each window process could send its own ipc and folder, and maybe even networking. the only reason networking might have to be global is that in order to get updates, the client may have to upload a separate document for a link. well this would only require reading other documents, not modifying them, so i suppose even this could be doable within separate window processes. 
so plan now is to make a hierarchy of cells. the top receives global events like window open and close requests. it handles dispatching window cells which manage events like project change. window cells dispatch project cells which handle events like fetching deltas and communicating with server. idk whether window or project cells should handle ipc and folders. 
another hard part is reading from a purely key value state, because there is no easily manipulation of collections. we need to make a state compatible with the db that is easy to read and write to using runtime commands. perhaps it could be thought of as a regular javascript structure, so the db would have regular object key types, and specified types for the runtime to interpret the value string as a javascript structure. however, a value for the db couldn't be a nested map, but only a small array of hashes for example. now with isolated parts of state each handled by a separate process, i suppose these processes could write directly to state instead of proposing key value changes as we'd do for a more complex app. so the easiest way would now be reading by giving runtime the key path, and writing by giving the key and a new value. now with arrays in the data structure. is it possible to simulate the state using a regular structure, where the db is a giant object? yes, this is possible using getters and setters. the job of the reducers is to accept actions and change the state. the changes will be communicated to drivers (in this case only the window with ipc). actually now i realize using getters and setters and proxies is not helpful because they are synchronous and we need asynchronous control. 
i think what i'll do for now, and not for a more complex app, is have a reducer that, like with redux, processes one event at a time, but it is asynchronous. the reducer changes the state, and drivers can listen for those changes, communicated to them by the runtime. the catch is that one event at a time with an asynchronous reducer can be slow. in complex apps with an asynchronous reducer, we'd want multiple events processed in parallel, and then their results merged. now the reudcer can process parts of the event in parallel, but still just one event at a time. i suppose subreducers could be structured like redux, or by transformation. i think grouping is best done with minimal indentation so switch statements should be avoided. actually i realize that we should do everything asynchronous in the reducer, but all through the runtime for testability. this includes ipc communication. as a result, no drivers must read the changes from state. the state is simply rewritten for the reducer to process the next event. now what about reading and writing to state? i suppose as long as we won't read directly from the objects or write to them, and instead give them to the runtime with instructions, we could use getters and setters and proxies. they would be of the example form 'yield set(windows[0], {})' but we'd also like to enumerate over them. maybe there's a way to enumerate using the runtime where we don't have a separate yield for each read, but one read for all of them. i think this simply translates to more complex reads and writes that can return (or modify) collections of values rather than just one at a time. iterables could be done by runtime setting up a generator. so from the outside, the runtime will just call next on the reducer i suppose until the command is listening for the next event, at which point the runtime would feed it the next event from the event buffer. maybe there could be rules for how to arrange or cancel events in the buffer, but this is extra and advanced. lets try the approach of grouping sub-reducers by state slice, and using a lot of selectors, which will be generators. 


to summarize protocol, deltas are published with their merkel proofs to the server. users make update queries with a parameter of 'parent length' and 'since time' to ask for all updates that have occurred within a recent period of time (bounded by the expiration time) and a certain number of parent detla ids (bounded by the height of the document). 
the protocol also enables posting search queries, which also have an expiration date if they are not answered. if answered, the answer also has an expiration date. the query is in the form of a delta id. the answer is in the form of a parent delta id and a delta content. queries and answers and both be batched. in future server might impose that user must answer a certain number of search queries before performing an update query. 
so when a user wants to load a document, the document will be loaded and the delta menu with show 0 deltas, with the option to 'download more deltas' at which point the user will post a search query. 
the database of the server will be divided in two for updates and searches.
details...

i suppose the first identifier in a key would be for the base document, that is the base hash. the second identifier could be a number for the level of the tree, and next will be the delta id to distinguish between levels with multiple branches. two things remain. one is how to store documents and the other is how to retrieve deltas by the delta id for eg answering search queries. i suppose documents can be stored by having an extra identifier after the base which is a bit to distinguish between documents and deltas. 
the document section of the db will be made up i think of all file elements, so the next identifier is simply the hash of a file element, no indexing like for deltas because they won't be accessed in a predictably consistent way -- changing the delta will mean accessing a different version of the same thing but each delta is likely to have a change of a different element so different versions of the same element are unlikely to be accessed at the same time. actually this model won't be ideal for retrieving document by delta id. perhaps instead just entire documents could be stored for every constant number of deltas, and the offset with the deltas would be determined by which delta the document is first accessed from. as pas deltas are retrieved, after a constant number the document would be rebuilt. also, after a constant number of updates the document would be rebuilt. 
retrieving deltas by id means translating a level into an id. another possibility is not using levels but just using delta hash for identifier and hope the number of deltas doesn't grow so big that thrashing becomes a problem. this could be the way to start. now i realize the levels will help with metadata, so i will probably use them. 

per window:
folder binding
current document
document history
--
the folder binding process should be independent of the delta/doc navigation process.
delta/doc navigation follows the process of the user instructing the window (via a link or pasting a delta id) which document to display. the window should obtain the document and any deltas already had in the db. the user can view the document and any of the existing deltas. the user can ask for more deltas which will be downloaded, or if the query is not answered it will say 'more deltas pending'.

window should have toggle of 'show diff' so that the plain document can be seen too.

main:
window homepage should show a menu for choosing an existing document, or pasting in the a delta hash for a new document (eg one found online). 


now realize only one delta at a time will be displayed. the simplest way to implement is for each delta clicked to download the entire document and sibling deltas and display that level with that document. more complex to save bandwidth is only sending document initially and then sending deltas which get applied to the document by the window. this second approach only works for changing between adjacent deltas. the first would probably better, but i think the queries should be divided as: document, delta, level metadata. this way, metadata can often be cached in the window. in fact, documents and deltas can be too. this simple approach does not make the window stateless. the window must still keep track of child paths for navigation. also, it must be able to read the bound folder, diff it against the current document, and generate a delta. any time a own delta is created or saved i suppose it should immediately be saved in the db. 

in main, the db needs to keep in sync a record of all deltas, and push any changes to the windows, in which case it should have knowledge of which levels each window depends. there could be a simple mechanism for determining this, where each window caches at most k levels, evicting the oldest level visited in place of a new level visited. a simpler mechanism would be for the window not to cache at all, and leave caching to the main process so caches can be shared between windows, though this results in increased ipc. in this case, the window process would be: download level metadata, delta, and document for each delta requested. render the document and write it to the folder. when the folder is edited and saved, generating the diff requires comparing against the previous document, so that must then be downloaded too. when the diff is made, its sent to main as a creation or update. of course deletions of any delta are also sent to main. main knows at any time what level the window is showing, which is the last level it requested. so the window pushes any updates (delta additions, updates, or deletions) to this level if one occurs.

maybe for a better interface, instead of supporting multiple windows, i'll support only one window, but with tabs. actually this might be tough because naively all tabs would be in same process, and also this makes caching less straightforward for the main process. so i won't do this. 

server config will also have to be in sync. it will be tracked by main for each window per document, and if that config changes it will be pushed to each window with that document.
i need to think about how uncommitted deltas will be synced across windows. suppose at most one window can be open per document at a time. then there is nothing to sync with so this is not a concern. i suppose it could also be policy to only have one uncommitted delta per vertex at a time. after its committed, you can start another one. hopefully only one window per document can encourage modular partitioning of documents. and the policy of only one uncommitted delta at a time should encourage smaller deltas. 
so the state for the window should include a cache of (document, delta)s, the current document, current level metadata, current server list,... The drivers it has should render the interface, send request for add server, remove server, level metadata, document, delta, create delta, delete delta, update delta, commit delta, publish delta, and also to bind a folder, unbind a folder, ask main if that folder is already bound. 

db will have keys:
document_root (hash), delta_vs_snippet (bit), 
if delta_vs_snippet==0 (then delta): delta_id (hash). content: (parent_id, delta_content)
if delta_vs_snippet==1 (then snippet): snippet_hash. content: list_of_child snippets whose hash results in the snippet_hash. 

in another db, probably of a different kind, the server info for each document can be held. 

i suppose to save bandwidth but without too much added complexity, main need only send those parts of the document that have changed. ...

hmm, now i'm wondering if folders should be handled in main (hence returning note writing to this file instead of the one in the renderer interface). we know that at vertex in a window, only one delta can be edited at a time. so if you want to edit two deltas at once you must open two windows. we also know that a folder can be bound to at most one project. thus the editing would have to occur within two folders. we can imagine this is inconvenient enough that projects will only be bound to one folder and only one delta edited there at a time. this creates a 1:1 correspondence between projects and folders. multiple windows can display the same project, but they will all be bound to the same folder. so writing has the same effect on all such windows, so the benefit of such windows is to read the project at multiple places. this benefit might better be replaced by tabs. i think tabs are better, in which case multiple windows are not needed and maybe even disallowed. 
to summarize, there is at most one folder bound per project (this is like one repo per git project and delta switches are like checkouts). any window can edit the bound folder (and servers). when a change within the folder occurs, the new contents are made into a branch of the 'currently selected' vertex in the project. this means only one vertex can be selected a time. each window independently views a vertex of the project, so multiple windows would mean multiple vertexes selected at once, thus multiple windows must be disallowed. tabs are ok but complicated, unless the tab number is fixed. i think i will leave tabs for later. so now there is one folder per one project per window. 

hmm, the above assume projects can be easily distinguished. but really all we have is ranges of deltas and the documents they represent. we don't know where its ancestors converge, root (making them separate projects), or later even recently (making them the same project). i suppose we should make minimal use of the term 'project' and only roughly define it as the collection of documents (delta chains) that share the same delta extension from root. (root represents a single empty folder, ie a blank project). i suppose instead of projects, the homepage could display the highest delta for each range of deltas the db has, along with the number of parent it has (the length of the range). bookmarks can store arbitrary deltas. 

now i realize the way servers work they have knowledge of which document belong to which projects. this is because there is one project per server. thus when communicating with a server, it can be clear to the client what project the document belongs to. thus every delta and document can be bound to a project. i guess i already knew this and assumed it by the db key design. i suppose the homepage could display projects, and under each one have a list of tips for that project, this way giving the user a better idea of prooning. next to each tip can be the number of parents before it converges with another tip (a sense of its length).

in order to build the renderer, let me now summarize the window functionality. when a window opens it shows a list of documents (in order of popularity) and bookmarks (also in that order). any of the bookmarks can be removed. there is also input to paste in a new delta id. the homepage can also display current interaction info with servers. when a document is selected from either list, the window requests it to render, and the delta to render the diff, and the vertex meta data to render sibling deltas. the user can select new documents (deltas) by clicking on links, moving backwards or forwards in the delta chain, or returning to homepage and selecting other documents. main is informed of every change in the window, even if the window already cached the document and delta. windows send the following to homepage. 
- switch to 1) homepage or 2) a delta, and if 2 specify whether document and delta are needed as well (or already cached).
- toggle bookmark this delta
- add server
- remove server
- delete delta
- commit delta
- publish delta
- unbind folder
- bind folder
- toggle binding active (when only reading won't be viewing folder)

for each request, main may send back error. if no error the window should stay in pending mode and wait for other data, because either loading is indeed occurring or server is unresponsive. sending success and then other data is redundant. other data sent is always one of the following forms: 
- error for some request
- new page with info (snapshot, not changes)
- new document and delta data (only relevant if on vertex page)
- new server info (snapshot, not changes)
- new folder info (snapshot, not changes)


should the window cache the homepage info. i suppose the complexity of caching would not be worth the saved bandwidth. on the settings page can be a summary of queries in process for that project. on homepage can be a list of current queries i suppose or some summary of them. or maybe just next to each project listing, or even every tip it could have info on the number of current queries, or other stats. 


now how does main function? main will keep track of windows and their state, pushing relevant snapshots to each one. main maintains a record of deltas, and projects. each project record consists of tree tips, server list, current server queries, and folder info. 



how about the model of main not treating the windows as if it directs them or gives them permission to display certain pages, but rather of main serving windows in their need to interact with the user, ignorant of what exactly that interaction is. however, the client must still ask main for permission for those things that are also of concern to main, such creating and deleting deltas and modifying bookmark list, etc.

the api from window to main would be:
READ REQUESTS
get vertex deltas info by delta id
get delta by delta id
get document by delta id
get project list
get bookmark list by project root
get project tips list by project root
get query info by project root
WRITE REQUESTS
delete delta by delta id
update delta by delta id
commit delta by delta id
publish delta by delta id
toggle bookmark by delta id
bind folder by project root
unbind folder by project root
toggle active folder binding by project root

each request would be responded to with error, or success for write requests and data for read requests with error field null.
state would need to keep track of pending requests and match incoming responses to them. does the window trust main? i think it should. so it doesn't need to even check the types of returned data. i suppose requests must have a request id to enable matching. 

hmm, what if servers weren't arbitrarily added and removed, but rather derived from the document itself. so writing the document is how servers are chosen. doing this there is no need to manipulate servers, it just becomes part of the writing process. however, it must be interpretable so main can contact those servers. so for looking for any extensions of a delta, the last servers specified by the document will be contacted. for searches, you will probably post to those servers from whom you received the delta ids you're searching for. so to bootstrap, you need a delta id and a server ip from which to obtain the document that will in turn specify other servers. forget about this now, but this bootstrapping process will have to be handled later, probably on homepage by input for delta id and server ip.
does the server architecture still work for this? ... 

but the window must be updated on changes to vertex or bookmark or project list, etc. so this model entails both queries and either polling or push updates. what about a model where any interaction client side, eg button pressed, is tied to a handler that immediately uses ipc to inform main of the interaction, instead of passing through window state. main would receive these messages and respond with new state properties that should overwrite the existing react state. this could be complex as main would have to keep track of the ui state. 
lets remember what exactly a window must do. it displays the homepage data, which consists of all projects, with tips and bookmarks and current query info. it also displays a vertex page with the list of vertices and the document with the delta. it also displays the settings page with folder info, which i guess no longer has server info, so i suppose folder info could just be editing as a top bar on the vertex page. i think i will now try the latter where the window just has react and UI state is managed by main, because main must already manage the UI folder binding. 


i realize state and reducers in general would avoid a lot of overhead of requests and responses if instead of treating the db like an external resource it is treated as part of state. after all, its not inherently connected to the outside world like the interface or the network. the challenge immediately becomes that accessing the database is an asynchronous process, whereas a reducers were assume to be synchronous transformations. making the db synchronous may be possible but better for performance would be asynchronous. the only evidence way to do this while keeping transformations coherent, that is not using a separate transformation as a callback, is to use yield, which i think might be a good tool for flatlang anyway if this model is implemented in it. so a yield pauses a transformation until a callback has returned. but what if a new transformation has been called while the previous is paused. oh, i forgot what a huge role try and catch play in yields. i'll now review redux saga. i see, so it uses 'takeLatest' and cancels pending transformations if another of the same type is fired while pending. also, these cancellations are done prior to reducers. hmm, i suppose instead of thinking of transformations as happening by functions, they could be thought of as given by a 'yield function' or 'generator' in js language. what is special about this is that it never returns like a function does, but just continues to yield. so it could take a stream of tx inputs and not have to output a transformed state on each input, but rather internalize those tx and only output when a meaningful transformation has occurred. i think this is the answer to asynchronicity. now does the generator (word for now) output changes to state or state itself, and are selectors still used, or do we use the 'everything by selectors' approach described below. idk, but below i will explore generators, async, await, and think about these objects as alternatives to functions.

the general philosophy of this model is that there is state, a data structure, and a driver for each external resource that the app interacts with (eg api server, screen interface). a driver can only communicate to state by proposing transformations of it. all other drivers will see these transformations. any time a transformation is seen, a driver must figure out what parts of the state have changed in order to figure out how they should act. this differentiation is enabled by the immutability of the state. 

hmm, what about the model where everything including state is derived using selectors. so the base selectors would take a state and an action as arguments and output the transformed state. to do so they would probably call other selectors likely passing the same (or modified) action and a slice of state. it could be instead of an action, the selector has the action encoded, and the dispatcher must invoke the correct selector. in overview, drivers publish and broadcast their 'effects' implying how state should be transformed. 

could this be model of blockchain? of course the blockchain is analogous to state, and drivers are analogous to users in that they are the ones who invoke transformations (via transactions) and also care to reach consensus on the resulting state. in regular blockchain, consensus is reached by one (valid) user being selected at random to propose a list of the floating transactions that should be applied and will determine the next state, along with a fingerprint of the previous state, thus proposing a new state. this won't work with asynchronous transformations because each driver computing the new state from the same proposed action list could still end up with different new states. so instead of a list of actions, the resulting new state must also be proposed. unlike blockchain, this is a trusted environment where drivers are honest, but they might still have different answers due to their different perspectives. 

unlike blockchain, suppose there is a unique stream of ordered tx (actions) received by all drivers (whereas in bchain, tx ordering is different for users). thus each driver can invoke selectors to obtain the state of anything at any point. so for example, the interface driver might send an event on a click to be added to the stream, and it would invoke the interface selector on every received stream item (including those it sent itself), which would yield a complete description of the new interface, and then the driver would compare this against the previous interface and apply a patch. so really this is analogous version of the regular blockchain for front end, except there is no need for consensus due to the centralized control of deciding the tx stream. wait, no, the asynchronicity prevents it from being analogous to the bchain. so i think i should put asynchronicity first. back to saga in a paragraph above. 

generators, async, await... what functionality exactly do we want? we want a function* (thing like function) that accepts inputs at any time by callers, and gives outputs at any time it decides.
i now see how saga works. the generator is just a generator, no async await functionality. instead there is a runtime system that calls next on the generator to get instructions, which is executes and then asks for the next instruction. for example, the instruction is to wait 1 second, so it sets a timeout, and when done, calls next again and receive the description of an async operation, performs that and waits for response before calling next again. and what about canceling? hmm, actually it looks like that only partially how saga works. 
oh, it looks like async/await can be made on top of generators by reading the yielded promises by calling next, and waiting for it to resolve or reject before calling next again. so 'await' is like saying read this promise and wait for it to resolve, then move on. this could be done on generators so i will only work with generators, and only later use async await for performance if they are easily applicable.
so with generators the challenge is there is only one way to interact with it, next, but we want two wants to interact, one for dispatching, and one for getting new states. actually the second way is not 'getting' which would be polling. instead we want the generator to push the updates. doing this by yielding the new state means updates would only be known when calling next. i suppose instead the generator should have access to a push function it can call with the new state, which would then be responsible for passing the update to the drivers. actually i now realize it can use next for both purposes if we run multiple generators at the same time. one is for input, and you call next for every action. the other is for output, and you just call it according to its own yielded requests, some of which may be new states. 
i'm having trouble thinking about the db as state now, because its not immutable meaning drivers can't use diffing to decide their actions. well, we can put a constraint on the db to help, that is that it doesn't support modification, only addition and deletion. the db is already a map, where keys must uniquely map to values at any point in time, and this property can be extended across time (that is no modification). does this property help diffing? it means you can figure out the additions or deletions by subtracting one set of keys from the other. this doesn't seem too helpful. another approach is model the db as a tree, and also storing hashes of each node, so basically a merkel tree, allowing drivers to compare the old merkel tree to the new one to find differences. only two trees would be kept at a time, only for diffing the new tree against the previous. 

what if we change the model. the model still starts with a stream of external effects. in old model reducers output a new state and drivers compare it to the old state to determine changes and then act on those changes. in the new model the reducers output a change and drivers act on that change. in both cases, the pattern is a stream of external events determine the 'state' of the application, which determines its interaction with external world.
a simple state shape interesting to explore (and good for leveldb integration and flatlang) is maps, possibly nested (where values can be maps). changes would simply be add(key, value), remove(key), modify(key, new value). this model can be reduced to a single map with no capability loss. with this model, the reducer that computes the change, and the drivers that react to the change, can both have access to the database, the former write access and the latter read access. the drivers can derive the previous state if they wish by subtracting the changes from the new database. so the debate between the models is how the changes are communicated between reducers and drivers. let us forget this detail for a moment, because regardless the method, the reducer must still compute how the state should change, and the drivers should compute how to act given the state change. 

generator directives: delay, async request, wait for event, send something, call one or more other generators in parallel and wait for them all to finish, same but only wait for one to finish and cancel the rest. 

maybe a better saga model would be like the actor model. generators can create new generators but canceling a generator can only be done by the generator itself. so generators can receive messages from anyone, send messages to anyone, create new generators, and cancel themselves. suppose we want to test a branch, where one of two directives are made depending on result of previous directive. the way redux-saga does it is by cloning the generator before the branch (copying generator state), and testing each version on separate branches. i guess there is no better way to do this with the new model. and testing the full saga will still require mocking the directive responses. the model would need a notion of addresses. i suppose this could be simple, eg an address of a generator being the number of generators created before it. upon creating a new generator, the address would be returned. in fact, in this model everything could be a generator i suppose, including the interface and network. so the re-redux model would be an instance with special constraints (only interface and network are impure) of the actor-generator model. i realize since each saga can only do one thing at a time, it can't have one yield waiting for a cancel request and another suspended performing another task. so i think we'll need one yield that describes a task to perform, but also asks for a certain kind of message like a cancellation request, and the return value is whatever comes first, a cancellation request, or the result of the finished task. logic after that yield will have to test for the type of value, whether a cancellation or a result. this could instead be done by simply making a race between one generator that waits for a cancellation request and another that performs a task. i suppose yet another method is a directive that says, if the task finishes first assign the result value by calling next with it, but if a message of cancellation type comes first, call throw with the cancellation request, and i'll handle it in the next catch block. 

so for this app, how will the generator-actor (GA) model be used? in general, we want generator(s) to take a stream(s) of external effects and output a stream(s) of changes to a state(s). imagine the interface driver as a generator that looks for certain changes to state, and makes directives on dom updates. lets first try with a root generator accepting a single stream of effects, and outputting a single stream of state changes. 
i suppose different GAs could handle different slices of state, and all have write access to corresponding slices of the db. the runtime system would process their write requests. so like there could be a GA for any project or i suppose first a window. when a new window is opened, a GA is made. the GA can take requests for page changes. so when a request for the homepage comes it should obtain the info for homepage then ask the renderer to render it. when a vertex page is requested, the page GA must gather from the db the document, delta, and vertex info, then pass to the renderer with a request to render the vertex page. when events occur on the page, like clicks, the events will reach the project GA, which will simply request a new page, or process an operation. the GA will make changes to the db then notify the renderer. suppose we think of the GAs as nested reducers constituting one giant reducer. 
hmm, what if the renderer, knowing which parts of the UI need update, only render those components, and what if each component is represented by a GA. i think this would be better than one single component, the react root, which doesn't know better than to diff the entire ui. all the components can tell the root renderer when they're done, and the root can lift a veil so the page looks to load all at once. so for UI we need a library for changing the dom. i suppose this could just be vanilla js.
i suppose the runtime could also be customized with drivers that interpret directives and give answers. such a driver would be something, maybe even an impure GA, that accepts a directive, and make a callback with the value when it has one. ...

now i'm considering the redux model compared to the re-redux model. before i disregarded redux because i thought since each reducer is acting independently on an independent slice of state, complex transformations wouldn't be possible without multiple rounds of actions, in order to do cross-reducer communication. but now i realize such communication is unnecessary, because any info needed should be based upon previous state, and the action, which both reducers have access to, so they could both compute it but of course that's inefficient so instead it would be delegated to a selector. in other words, selectors is the key. so from now on i will compare the GA model to the redux model. so really i will compare reducers to GAs. A reducer is a stateless pure function of state and action, while a GA is a stateful thing that holds the state and is only a function of the action. i will try the GA model, now called cell model, and see how i like it, how its best used, and how it compares to redux. one simple version of GA in terms of redux, is that each GA is like a reducer in redux. but instead of just processing an action, and having the state passed in as an argument and out as a result, it encapsulates the state never coming in or going out, and if anyone wishes to have info about that state they can do so by sending the GA a message according to the GA's api. in this way, there is no specified state structure, and GAs are free to handle and structure their slices how they wish. rather than consensus on state structure, there needs to be consensus on GA interfaces (apis). GAs can serve for selector of their own data, so there is no distinction of state and derived state. what would this approach look like for this app.

[113]
{
	windows: {
		id: {
			id: id,
			pageType: 'home' | 'vertex',
			projectId: hash,
			deltaId: hash,
		},
		...
	},
	projects: {
		id: {
			deltas: {
				id: {},
				...
			},
			fragments: {
				id: {},
				...
			}
		},
		...
	}
}


in flatlang, as cells are created and destroyed, memory management is an interesting question. i'm thinking of having states be a map or something such that there is minimal need for ordering, such that parts of the state can be moved around independently (rather than eg shifting an entire array). of course this makes reading ranges of values not available. i'm imagining the cells are coherent segments, and expand or contract width with every delete or write respectively, and move right or left with every write, and the runtime can decide which side of the segment to append the write to for best memory management. 

each slice of state has it own GA with its own state holding distinct data from all other GAs. like a project module's state would hold all deltas for that project, meaning a slice of its state is a slice of the db. this implies accessing state may involve yielding. to be consistent, all state accesses then should be yielded. likewise, writing to db will need yields so all writes to state should be by yield. this approach has the cell agnostic to its state source (db or not). in this case, we could view every step of the cell as a function of its current state and the previous result. this formalizes the constraint: a result must only depend on the current state and previous result.
there is a difference in testing. in redux you test a reducer by giving any valid state and any action, and a valid new state should emerge. a valid state is any state that fits state shape and obeys any constraints. can the cell model be tested just as easily? you set up a state, and query it and for each step would expect something given the previously fed result. the problem is most of the results will be for accessing state. which is a bunch of directives the testing has the process but doesn't care about. i think this would slow down the loop a lot. i don't think this is a good idea. this means the db will be explicitly different from state by the cell's perspective. the cell will interpret the db as another cell even though it is not one. another way of testing is to pass in a reference to a state object as an argument (or function that read and write to state), and the cell can manipulate that state how it wishes, and at anytime the runtime can inspect the state because it has a reference to it. an immutable version of this is also doable. so you could test at any step by setting the previous result, and setting the state, and validating both the next directive as well as the state when reaching the next directive. an immutable version of this seems possible, in which case instead of just a reference to an object there would be functions like setState. 

to review editing format. a file has an un-nested list of elements. types are of headers, paragraphs, and block-tex. inline tex can go in headers and paragraphs. code is also an element. one blank line separates each element

the elements we want to be able to render are an index file (particular format undecided), headers (#), code (>), tex ($), order and unordered lists (+,-,/+,/-), and note (*). in line we'll have $tex$, `doesn't`, *emphasis*, :emoji:, and [link label](link source).

i think folder and file errors will prevent rendering, while in line error will be rendered. so first step is rendering a valid data structure, and displaying in-line errors. second step is reading a folder structure and either converting to a valid data structure, or reporting errors. what's the data structure? a folder could be a map of file names to files and folder names to folders, together with an array ordering the file names. a file could be an array of elements. each element would have a type, a payload, and possibly a label. 

{
	payload: 
	"interface": {

	},
	"driver.js": [
		{
			type: '',
			label: '',
			content: '',
		}
	]
	order: ["interface","driver.js"],
}


do i want to implement the whole thing in SVG?
does the state of the svg's need to be represented in a manipulative format?
can this be the hierarchical dom format?
do the events and event handlers need to be modeled as well?
does the javascript backend need to be modeled?
does the backend have a limited responsibility?
does that responsibility include communicating with servers?
does it include handling events of the SVG?
does it include handling events of the window such as resizing and shutting?

would the front end be a single stack of graphics?
could each graphic be a path with:
  fill, stroke, transformations, event handlers
  ?
could there also be text graphics objects?
could capabilities, eg new content from cross origin requests, be handled independently of layout?
in other words, can we think about a single mind designing the whole system, and then think about which mind that is separately?
are we able to do this because only a single mind at a time should control the layout?
is this the case because we don't want 'lots of artists painting on the same canvas at the same time'?
does this mean one artist/mind will have to implement the layout for any data representation?
is this acceptable because we can restrict possible data representations to simple graphic objects?
could we optimize the data flow by letting the artist assume certain data representations?
does this mean we could switch between artists when switching between apps if one artists does not support the implied data representations for both apps?
so, again, can we first think about a single mind generating graphics objects?
in fact, could this framework designed for a single artist itself be an 'app', in that the system just described is the data representation it assumes, and it may choose to give access to what it models as another 'app' by chain loading an entirely new website, even if that website is ignorant of this whole 'app' concept? yes

do we proceed with a single mind, and leave the switching of minds as a back end task that can be handled by this single mind?

does this single mind have knowledge of and power over the entire front and back end? yes
does the artist get to choose any css and js frameworks?
am i an artist that is working to make a framework out of svg?
could it present a virtual dom, that when manipulated will progrogate changes to the real dom?
does the real dom propagate changes to the virtual dom?
are these changes limited to user events such as mouse events and window events?
once this dom is constructed can we make methods for manipulating it?
will these methods all be implemented through javascript?
will there also be methods for doing arbitrary computation and interfacing with things such as web servers?
could the front be modeled as a large, single, dynamic stack?
is this similar to the flatlang model with hierarhical data types except

start with some primitive svg stuff
the path element, defs, use, fill, stroke, transform through matrix, mouse events, key events, scroll events, groups,
later add other stuff like shortcut transforms, short cut shapes


svg notes:
viewport: the rectangular part of infinite canvas that gets rendered.
viewport coordinate system (viewport space):
user coordinate system (user space):
these systems set so one unit is one pixel in viewport.


use window resizing to declaratively change the layout, even if it takes not a high performance re-rendering, because we assume window size changes should not occur too often.
so assume we have a window width and height, and we want to render everything in terms of those.
most shapes will we made for a certain aspect ratio. they can be written on any numerical basis for this.
but some things, mostly boxes, will need to be resized continually and so will not retain a constant aspect ratio.
we will never use percentage units only fixed units such as px or ex, and i still have to choose one.
attributes we should use:
  fill, stroke, fill-rule, transform,
definitions come with an assumption of their space, the dimensions. it is up to the 'use' of the definition that the shape gets the width and height and position it should, by use of the transform method.
maybe every svg element could correspond to one an object, in other words a hierarchical OO structure could reflect the svg heirarchy. for example, a <g> that contains the elements to form a bird could be an instance of a Bird object, and for performance reasons, perhaps not every child of the <g> need have a corresponding javascript object, but the Bird can instead be responsible for all of them.
i think i have the basic idea: we use svg for shapes, and for moving stuff around we have to scenarios: when we move stuff according to the mouse we simply track the mouse, while when we move stuff for an animation we use requestAnimationFrame.
we want to write an analyze the code in the browser with the environment (eg custom) I imagine. this would be tough to do using existing tools like typescript or flow because they are not made to used in the browser, but rather a desktop ui which is not the ui we want. so maybe we could have an easy little ui that lets people write javascript and it would do some type-checking by analyzing the input and output of what get's created. its the development that must occur in the browser, not necessarily the compilation, because the compilation does not require a collaborative environment, because its done by a machine not a human. so it can be part of the server. i'm thinking about just creating an OO hierarchy in javascript, and not use any tools like redux or vue.
every chunk of code has inputs and outputs. some of this i/o happens at the start and end of the block, some happens in the middle such as calls. the job of the analyzer would be to analyze how all inputs are used to tell whether the outputs and total 'effect' of the function is what it claims to be. it would abstract away the naming, by using these unnamed chunks along with metadata that compensates for no-naming.
i would like the environment that only shows those elements that can be objectively answered, no subjectively. for example, the order in which elements are written in an object doesn't matter, yet people tend to have opinions on it. so leave it to them how they want to structure it, all the matters in the end is which ones go in the object, and the compiler can put them in arbitrary order.

should i go to graduate school?
do i plan to work for a business? no
would i learn better in school than outside school? no
would a graduate program cover my living costs? no
would being a professor cover my living costs? yes
is the only benefit to a graduate program that my living costs could eventually be covered in an academic setting? yes

for now i will forget stroke. later i can add back stroke for the purposes of drawing lines, and i will use the stroke-dashoffset property like vivus.js does to offer animation. you do this by setting the stoke-dasharray property to the length of the path, and then at moving the dashoffset progressively down from the length of the path to zero, thus animating the path. this doesn't make complete sense but it works.

so for now its solid shapes (a circle can be done with two solid shapes, and its rare enough i don't think it deserves the new type of 'stroke').
no strokes, no gradients, no filters, but you know, maybe i'll include the blur filter which would enable shadows and hiding things in the background, having like a window effect.

you can make a path, give it position, give it size, give it color, give it opacity, give it blur, (in the future give it window effect), (in the future give it a color of a gradient)
so this could be an framework focused on composing these properties, resulting in a graphic. primitive graphics have one path, more complex graphics come from multiple paths and are created with group.
at any given point in time, the page is fill with graphics, and these graphics may change over time, and mostly likely it is their position that will change, making only a slightly different graphic. so graphics are defined by their definition.
i don't yet know whether i'll use javascript's object oriented framework or just use primitive functions, probably just the primitive type to make it more understandable. the maintainability comes from the overlay language, which I should develop first. it must be a language that can be written with graphics, this way it is self-hosted. another options for implementation is asm.js but it is evident the lack of support and future of wasm makes asm.js not so worth it for performance because some type checking must still occur, and handling a heap is necessary regardless and better performance occurs with the browser than doing it manually in js using a typed array. but maybe to speed stuff up

of course text elements will be necessary too. position, color, size, will be similar. input will be unicode. font will be Google's Noto. Though I don't think multi-language support is particularly necessary because much will have to be done in english anyway regarding technicalities such as computer and math vocabulary, a lot can still be done in written language and it gives everyone an ambitious feel for how the site could bring people around the globe together.

math typesetting: basis is an object that has a graphic and a function that adjusts the graphic and its arguments by position and size. but the arguments can give hints on how they would like to be sized and positioned depending on their context. these arguments can be labelled by anything (eg pm for plus or minus) and searched for.

functional javascript:
https://drboolean.gitbooks.io/mostly-adequate-guide/content/ch8.html#old-mcdonald-had-effects

what just taking the necessary svg api, functions, numbers|0, conditionals, operators, while loops, etc.

there are cells with info and they are read in a context, and reading them can further clarify the context. for programming these cells will probably want to show functions in terms of other functions. so i think we need a way to encode functions. people can create other contexts later to encode functions
ver

language:
global namespace of functions
each function can take any number of arguments in any order


all responses are served with a type and an error

broadcast
	request:
		visitor: parentHash, text, sigmaId
		member: sigmaHash, sigmaId
	response:
		visitor: sigmaId


client -> server:
	getConfig: 
	setConfig: [otherStuff]

	addQuery: listType, listId
	removeQuery: listType, listId

	createSigma: parentId
	updateSigma: sigmaId, text
	submitSigma: sigmaId
	discardSigma: sigmaId


server -> client:
	getConfigResponse: config (queries, [otherStuff])

	addQueryResponse: error
	removeQueryResponse: error
	queryResultChange: listType, listId, oldSigma, newSigma, changeType, error

	createSigmaResponse: error
	updateSigmaResponse: error
	submitSigmaResponse: error
	discardSigmaResponse: error


query id: listType + listId
development sigma ids: parentId + userId
in memory visitor sigmas: id <integer>, parentId, text

DB representations:
sigma
	hash
	parentHash
	userId (=00.. if by visitor)
	date
	text
	inDev=true (if my member and not submitted)
if by visitor userId is '00..'
if by member and in development, an inDev field is added to be true

user
	userId
	name
	isMember=true (if user is registered)
	privateKey


i now remember that the inspiration was really about the power of thinking in terms of 'shapes'. the shapes can be highly customized, and very domain specific! memory is a flat map composed of many units/things/segments which can take one of many possible shapes. shapes can be easily composed. thinking in terms of these flexible shapes was what flatlang was made for. 
the formatting and placement of instructions was a second thought. i like to forget about how instructions are written, and instead just imagine how shapes can be combined and manipulated. the techniques for doing this, such as encoding of instructions, or optimization of registers, are all second thoughts. 

so you have a shape in front of you, and you want to transform it. don't know yet if should think about modifying a shape, or mapping it. for now leave it either way. you draw a new shape that reflects a transformation of the old shape, that is, in terms of the old shape. maybe a very generalized version of this is that you have two version of the entire program shape, and you transform the old one to the new one (i'm picturing both as long horizontal memory maps, the old one on top, the new one on bottom). the parts of the state that get changed are highlighted. dealing with variable length data is tough, because before hand we can't move it because we don't know where it will fit. this means a more complex interface, where we must distinguish between variable and static length shapes. I'm thinking a solution to this would be to have the runtime system host a simple, high performance key value store, like indexDB. the hope would be that you only have to look at static length data. but not only dumb data like text would be of variable length and thus put in the store, but also for example a collection of shapes, each representing a network request, and you don't know ahead of time how many requests you'll have. i think this may still be possible with static length shapes. for example, instead of an array containing some info about itself, and then a variable length of entries it would just contain one pointer to the store. this implies the store must be able to handle more than just key values. another option is the shape would contain a variable length of pointers, but only pointers. this way you wouldn't have reason to dig into anything inside the variable length shape. this subshape, a list of pointers, would be the only variable length shape. but this still leaves the problem memory management in the circumstance of uncertain shape sizes. this should be avoided. the goal is to have in front of you something you have a clear picture of. it should allow you to clearly express transformations. its not like the store will be an external resource. rather, many of the transformations will be on shapes held in the store. it only makes sense to use arrays when the entries are all of the same length, and probably of the same shape (for offsetting). anything else of variable length should go in the store as key value. i think as a consequence, anything in the store other than an array should be dumb, or shapeless is probably a better word. lets see how this works for an example:
you have raw file text in the store. you want to transform it into an item list. the moment it begins taking structure, eg splitting the text around double new lines, it takes a shape. since the value might be big, and we don't want to move it, the most likely possibility is to create a array, with each item describing the type of item and a pointer to its position in the raw text. but suppose each item may have multiple lines and we'd like a pointer to each lines. this means the shapes will need a variable number of pointers. one possibility is to have a max number of lines, and allocate that must space for pointers in each shape in the array. this may be wasteful. otherwise the shape would need to point to a value in the store that would be an array of pointers to lines. 
so to summarize, the store is a key value store, and the store i think is ignorant of the meaning of the values. its up to the querying shapes to know how to interpret a value. don't know if store needs this rule, but a value is either a shape or a list of shapes. i think i realize now that everything should be in the store. there is no reason shapes should be out of the store. for example, there is no reason two shapes should be next to each other in memory if they really are separate shapes. so the store is an orderless collection of shapes. there is no garbage collection. the compiler must enforce that a shape is always pointed to by something. the runtime will keep a pointer to whatever original shapes there are. transformations i think should apply to the whole store. a transformation can include any number of additions, removals, and modifications of shapes. so for the example above, nothing would happen to the plain text shape. but a new shape would appear in the store: an array of item shapes. also, some other shape would need to be modified that points to this new array. also new to the store would be arrays of pointers to lines, one array for each item. this is a lot of variable length transformations, in fact its double, in that the number of items is uncertain, and the number of lines per item is uncertain. what's nice is that variable length should only occur for arrays, and arrays should only have one type of shape. this way, you only need to show one item, and just \dots to show there's a variable number of those items, and same for the pointer array. this way, the the amount of expression required per transformation only grows linearly with the shape nesting depth. 
important is that keys to the store are a special type of shape the user has no control over. this is because keys are reference not by name as in most languages, but as a position within a shape. in a transformation to show what keys point to what other shapes (and to read this at anytime from any state), i don't think a bunch of lines pointing around would be helpful. either its a matter of hovering or clicking on a key and seeing what's referenced emerge, or ordering the shapes in the interface and having a key show the index of the shape it points to. but hmm, keys don't just point to shapes, they can point to parts within shapes, arbitrarily, such as a point in a shapeless text. but i suppose it could be enforced that a key always points to a subshape of some kind. similarly, when deleting, the compiler should enforce that all keys currently pointing to it should be reset to null or something indicating an empty key. for these compiler rules to work, shapes should always be pointed to by a static length shape, and not by just elements in an array, because given the uncertainty of the array length and its contents, there would be uncertainty on what keys it contains. so every shape, static or not, in the store must be pointed to by a static shape. 
this set of policies makes it such that anything in the store at any time can be described with a shape not too wide, because either its static, or its variable length but the contents are static. this implies a good layout might be a stack of the shapes in the store, because the width of this stack would be bounded. you could scroll up and down a stack. a stack is a state of the program. this way, multiple stacks could be placed next to each other, probably always having two on the screen at once. the one on the left representing previous state, and one on right the new state. vertical bars on the left could show red for those shapes removed. green on right would indicate new shapes, and yellow would indicate modified shapes. or, we show empty slots in new state where deleted shapes are, and empty slots in old state where new states are, this way the stacks line up. 
it could be possible to break up state into multiple pieces, and manipulate each one separately. it would have input and output and that way communicate with other parts of state. an example would be a branch of a redux state tree that takes incoming actions and transforms itself. the input would be placed by the runtime in either a shape or the store and then a pointer in a shape. this sets the context for io in general. it means certain shapes are for io and the runtime has control over them. and when the io changes them, it would change the shape and thus set the stage for a transformation if a transformation of shape is registered for that context of the shape. so in this sense we naturally have a generator like setup. how is output directed? it would just be placed where the runtime knows what to do with it. if external input or output the runtime must be hard coded on what to do with it. but if the io is just cross shard communication, its really just a regular transformation of the whole state and it would be programmed as such. for example, you'd have a transformation showing that if a certain type of output is placed in a certain shape, it means other shapes should receive the same thing in certain place, which might trigger their own transformation. this example would be an event emitter. this method leads to a form of concurrency. 
so programming i think should be a matter of taking a shape and considering every possible form of the shape, and assigning a transformation to each one, including the no-op transformation. this is how conditional logic is expressed. the actual transformation would be written in simple code or left to the runtime. so no longer do we have the concept of the program being one giant shape. rather the program is a store with a collection of shapes. as far as viewing shapes, i think every shape should have a color, and you view that color whenever you view the shape as a subshape. when the shape is in focus, its subshapes become colored and emerge. you view the labeling description of a shape at the same time you view its color. due to the static length of shapes, there is unlikely to be deep nesting.

i like the idea of a declarative style, expressed in visual transformations. and only implementing the transformation logic after. i think movement logic, just about what gets copied, or removed, can all be expressed graphically using the interface. of course there is more logic than movement. for example, a reduce function that passes over an array. the logic will be in terms of an accumulator and the item being passed over. but an accumulator has shape so it should be expressed as one. i think functions may come into play when we must combine two shapes and make another one, eg adding to numbers which is the simplest case. these functions would definitely be pure. and maybe they are even worth thinking about as 'operators' instead of function. these operators could be expressed with symbols like greek letters, and they could shown graphically to indicate that the shapes passing through the operator will output a certain shape. i imagine operators can have multiple inputs and outputs. if a reduce operation is simple it could just be an operator instead of a shape with other operators. inside an operator, there should be no need for naming. everything in the shapes it operates on should be directly reference-able, and those references can be passed to suboperators. i'd like to be able have operators expressed more like math notation and less like typing code. the composition of an orthogonal set of basis operators seems a powerful concept. express your operator as clearly as possible, not accounting for performance, and the compiler will optimize it. what are these operators then? arithmetic operators are obvious candidates. operators say nothing about movement. they are just about selecting info, transforming them, and making new info. an operator will probably dereference keys from the store. 
whats needed now is a way to start drawing shapes to explore. i don't think there needs to be any interactive features. i just want to draw. 

---
i'm focused on complex, interactive apps, like for networking. this is first priority. this logic is naturally about complex state management with a lot of concurrency, so it means a lot of cross referencing. in other words, any procedure will be pulling data from a large range of sources. 
i'd like to revisit the idea of composition. this can be done with fixed shaped that holds a key pointing to the inside shape, making it look like that inside shape is actually part of the containing shape. then there would be no need for dereferencing, but passing around a pointer to it would be harder. 
i'd like to make this a concurrent by default language. data ownership might best be done by having smart data, that is shapes that can transform themselves. i like the idea of holding pending state, eg closures, inside every data instead of just on the stack. shapes might best be thought of as functions or something else rather than data to be interpreted. these functions could be passed around to other shapes that would operate on them. closures mean the data is to be interpreted in a certain environment. currying naturally fits into nesting shapes and then passing them around. if like lambda calculus, then the only possible operation is application of a function to a function. 

reading about dependent types, which might go well with flatlang because the inspiration was about very specific shapes with custom operations. mostly discouraged by it being a functional language with pure functions that can't modify data. with large amounts of data, i imagine modifying data will be necessary. but now after looking at xen and and remembering that unikernels are not best for multithreading, i remember they are best thought of as lightweight processes being dispatched. lightweight means they don't carry a lot of data, but instead network with outside sources for data. this makes me rethink the whole idea of incorporating the db as part of state, which is where i've run into trouble. a lot of trouble i've had thinking about how to design a language is how to make it fit for operating over large amounts of data. but now i realize that may be a false assumption, as the large volume data is rather stored persistently elsewhere, whether on a blockchain network, or a persistent db for a full node. in fact, now i'm thinking of the architecture of a full node app where the db by itself could thought of as state, and the unikernels could spawn each other. they would organize among themselves permissions for writing to the db. one pattern would be that one process, like a reducer, writes after receiving change requests from other processes, which are like drivers and decide their behavior by listening for changes to the db, which would probably be broad casted by the reducer process. so this is a variant of the state, reducer, driver pattern. and this pattern could be recursive in that each of the processes could operate with state, reducer, and network drivers. 
So the good news is now we can think about building a platform for lightweight processes, and such a platform should also work for the electron app, where it will interface with ipc, folders, and the db (and no longer include simulate the db as state). yes, this means more boilerplate in organizing the asynchronous communication with these external resources, but we will have the same challenge in the blockchain setting and so that is what we need a language for. 

that flaw in my thinking was a mistaken assumption. so now i will try to state my assumptions before reasoning further. i assume we want a single threaded, asynchronous platform. assume no large amount of data held in memory. assume most logic is about parsing external inputs and deciding behavior in response. assume that means a lot of matching request/response input types to procedures, like with redux switch statements. i will assume unjustifiably that i can proceed thinking in terms of shapes held in a giant key value store, with keys chosen by the runtime. i will focus initially on one of the two patterns of state reducer drivers, and generator network/actor model. 
in the generator/actor model, for simplicity, and for symmetry with unikernel dispatching design, it makes sense to me for generators to be spawned on requests, and to vanish when the task requested has been finished, rather than having a long lived generator handling multiple requests. in this sense, we might have an architecture where an event triggers an expansion of generators, each spawning others, each waiting for its children to finish, in the same way a recursive function expands on the stack. this, however, assumes a parent child relationship which is an important constraint decision. without the parent child relationship the generators would have more freedom to interact with other generators, eg siblings, but this would mean a complex and probably unpredictable network of generators.
i'd also like to think in terms of streams. before when i was thinking about them a big impediment was handling large amounts of data, because streams are based on the flow of individual items, not the fast locating of item via traditional data structures. streams don't fit in the shapes/map architecture. nice is that streams are easy to test, especially if the network is a DAG. 
a good example of a typical task is to manage a group of other processes, eg windows, network peers. this means not only handling the events of the processes, but handling queries by other processes on the state of the task, eg whether a window is open with a certain project inside. well maybe other processes have no right to know this, and instead the managing process makes all related decisions. for data structures, they could be simulated as being observables passed down the stream, when really just a pointer to them would be passed. this would be pure as long as that data structure is not manipulated. this seems plausible, and means every time the data structure is manipulated, a new pointer must be passed down the stream. this means immutable data structures which should be hard to do. if an stream wishes to alter the structure, it will need to create a new one. using trees, immutable structures should be efficient. streams must thus have the logic to navigate the structures. 
reasoning on the case for immutable structures. regardless whether streams or static generators or dynamic generators or a map of shapes, the architecture will be asynchronous and event driven. processes must be able to write to data structures, and the results of the writing must be communicated to other processes, and we can assume more than one other process. thus giving one of the receiving processes the right to alter the structure is unfair for the other receiving processes in that their reading would be tampered with. thus, assuming there are multiple receiving processes, the structure must immutable unless there is a notification when it changes. this is the SRD model (stare reducer drivers) where drivers are given notification when the state changes, and none of the drivers have right to alter the state. giving this reasoning, we can now assume processes can only read other data structures, create their own, and pass them on, but never alter a structure created by another process.
i suppose a powerful model could be stream based, but where the data structures, and their operations, are customized based on shapes. i'm thinking of data structures made out of fixed size shapes, but a variable number of them. the shapes would have a lot of pointers in them to other shapes. would we need a garbage collector? if a process broadcasts a pointer to its newly created structure, it doesn't know (and neither does any other process) how many processes are still holding on to it at a given point in time. thus the runtime must perform deletion when the last pointer to it is dropped, so a garbage collector is indeed needed. navigating and writing new structures would be an imperative programming task, but it would be straightforward do to the immutability of structures. if structures are passed around, the receivers must know how to interpret them. 
i remember a need for both pull and push based processes. the need for push was obvious because of events, whereas if everything is done with pulling this functionality would have to done with probing. the need for pull based was that the puller would like info from another process, but would like to customize the query with certain arguments, so the pusher knows what to push. we'll see if this is remains a problem or whether immutable structures with the fast diffing enabled by them provide reasonable solutions in all cases. 
an important optimization to save space and minimize copying, is to make keys not as raw pointers, but customized keys for the key-value map that only need to be of size logarithmic in the map size. pointers/keys might even be manipulated by the runtime during their existence to maintain this.
can the network be a DAG or must their be recurrence? i'm hoping its a DAG, so the only recurrence is the external world. take the example of a project change request, and needing to query the existing structure of windows to see if one is already showing that project. the window structure must be examined prior to changing the project, which means changing the window structure. this means either both operations must be performed by the same process, or there must be a recurrence because the examination is above the modification, the new window structure must somehow move upwards in order to be examined. this shows that a DAG with simple stream nodes will not be sufficient. 
a generator model is basically the stream model but with a free network (not DAG) and custom generators rather than standard streams. lets begin using the generic term 'cell' for generators, streams, etc.
i didn't realize this above, but all structures are not just a type of cell output, but the only type. so at this point we at least have the model down as far as cells existing that pass immutable structures to each other. a cell might be a query cell, receiving a query and a structure, performing the query, then outputting a new structure perhaps a simple as a boolean. what remains is the topology of cells, their lifetimes, their internal logic, and the dynamics of their topology. regardless of lifetime and topology, i think we'll settle on similar internal logic. of course any internal logic assumes cells are not just compositions of standard cells. so for now assume internal logic is solved, regardless of whether with composition of standard cells or something else. so what about lifetimes and topology? I should think by example.
the delta app's main process is really a matter of querying and writing to the db's and responding to CRUD requests. so in will come requests and out will flow responses, each pair being a tx. unlike most REST request architectures, we will need the option to cancel an request in progress. 
a large amount of runtime commands would be for querying and creating new structures. but all runtime commands will be pure in that they only depend on the inputs, and the timing of the inputs, no querying of an external resource allowed, even through the runtime. in this sense, cells are pure. so if a cell wants to query another cell, it must do so with its own input and output, but maybe the runtime can help abstract this. 
static cells should not handle requests, because their output should be for particular destinations, but for all subscribers. for requests, the requester should create a dynamic cell and be the only subscriber to that cell. 
maybe an approach is to think in terms of states, each represented by structures, and decide how each structure is determined, that is what other states or structures it is dependent upon. Then we would have a flow based computation graph. The problem is likely there will so little independence of tasks that there is primarily just one structure, like state in the SRD model. but wait, we could break up the state's substructures into separate structures, just as redux has slices. this works because these substructures are usually only dependent upon their own previous state, but also sometimes function of other parts of the previous state. this means we won't have a DAG, because it could well be that every state depends, directly or indirectly, on nearly every other state. the output of the whole graph is dependent on certain states, which the runtime reads to perform. likewise, only some states receiving external input from the outside. an an important design consideration for elegance is that these external inputs are represented as structures, just like any other, whereas in redux actions and previous state are distinguished as two different types. so basically we just have a graph of cells that have static edges from which they receive their input structures and then emit their output structures. the computation of the cells is time-aware so they are not just regular functions. maybe time awareness is best done by the runtime passing in different types of structures that represent info about time. simplest example would be a structure of a single bit that is passed to the cell every n units of time. using this approach cell in fact are just reactive functions of changes in their input. i think this summarizes the basic architecture that should work. the whole idea of requests, like the need for pulling due to custom arguments, should no longer be necessary, because whatever logic decides what would be the custom argument should instead by located in a cell that then passes on this info to what would be the client and the server of the request, this way the server can pass the result to the client without the client needing to query to server. 
now what about programming the internal logic of how a cell responds to a change in one of its inputs, which is the only time a cell has a chance to compute. current state i a function of all previous inputs. well its important to remember that the previous state of a cell will always be provided with a change to one of its inputs, in the same way redux reducers receive an action and the previous state. as a result, we don't need to think of state as a function of all previous inputs, but just as a function of previous state and the latest input. remember that cells are now synchronous functions, only able to process one input at a time. so maybe logic could be expressed as a mapping of state_i, input_j -> state_{i+1} for each of the j inputs, that is j maps. redux uses one map, but breaks it up into j parts with a switch statement. so now the task is giving logic for a transformation from two structures to one. 
but oops, now i realize that if different parts of state are dependent on each other as inputs, they will form an infinite loop. one way around this is to only have as inputs those inputs that should trigger a recalculation, and leave the others as indirect inputs. this still leaves the problem that an indirect input might be of the wrong version. so we can't just break state parts up into independent cells after all. we need to make sure there are no dependency loops. the simplest solution is having one giant cell representing state that takes only external events as inputs, along with its previous state. but the output of the system must be in structures that represent output events, like networking requests. this means there is a cell (what is really a driver) who's input is the giant state, and who's output is external events. also, we'd probably have some cells above the main state cell that process raw inputs, transforming them to a format more readable for state. throttling input packets would be a common example. so we would actually have a DAG, but it wouldn't have a lot of simple cells, but rather one giant complex cell and a few simple ones above and below it. now we need think about the internals of the giant state cell. for example, how might we implement selectors to help the logic, and maybe even make these selector available to other cells, while still remaining within the architecture. selectors i think must be able to handle arguments. even if they don't i don't think they can be cells, because driver cells must be dependent on both state, and its selectors, and we only want them triggered once on a state update, not twice. so i think selectors are different than cells.
we'd like to structure a cell, not grouped by transformation, but by state slice. this might be doable if cells can use selectors to query the larger state, have input only for those events they might trigger on. Also, their parent cells must be able to have control over which of the events they receive they pass on to their child cells (sub reducers). remember that selectors hold a lot of the transformation logic because they have access to the entire state. selectors compensate for cross-slice communication, in that any common knowledge they need from the event and previous state should be computed by a selector. selectors are not worth much if they take custom arguments, but without custom arguments are they any different than cells? i guess not, except that their input is state rather than the event. suppose we make them cells. then we must have a recurrent network of cells because the selectors are used to compute the next state, but they take the previous state as input. we'd also need to prevent infinite loops. on state update, the selectors would be updates, and then the reducers, thus updating the state for an infinite loop. this only happens if the selectors are input to reducers, but they don't need to be. in this case we need to distinguish between dependencies that trigger recalculations and those that don't. we also need to make sure that dependencies are of the right version. since all cells are synchronous, the runtime can make sure of this. ok, i think we can treat selectors as cells with their input as state, but with the condition that cells don't simply recalculate on every dependency update, but instead the runtime obeys a policy to only recalculate when certain dependencies update. for example, a reducer won't recompute when the selector updates from the change in state. rather, the reducer will only recompute on a new external event. the runtime must also be sure to recalculate cells in the right order, for example recalculating a selector before recalculating a reducer dependent on that selector. i think this should translate to a simple procedure for the runtime: on every event, recalculate everything in order from top to bottom, and following dependency loops until nothing is left to update, and only then move to the next event. maybe we could add an appropriate constraint that we have a DAG, but semi-loops exist where the second half of the loop going from down to up is of the type that won't trigger a recalculation above. this way the runtime need only recalculate one in order from top to bottom, no loops. this would work for the SRD architecture because reducers would recalculate on events, selectors would be below the reducers and calculate on a state change from the reducers, but the reducers' dependency on selectors would not cause them to recalculate because they are above the reducers. hmm, so it looks like we have an architecture that would at least work for the SRD model, and probably be useful for other things as well. to summarize this architecture, there are cells that depend on the output of other cells. there can be multiple entries for raw events (those from the external world). for every input event, the runtime recalculate the chain of dependencies. but say 'a' and 'b' update on the event and 'c' has both as dependencies. then the runtime would not recalculate 'c' twice, once for the update of 'a' and the other for 'b'. rather, the runtime would realize that both dependencies updated from the same event, and so 'c' only needs to be updated once. if however, 'a' updates from one event, and 'b' from another event, and both events occur at different times, the runtime could choose to update 'c' twice, though it should try its best to only update once, by realizing that 'a' and 'b' will update is small time apart from each other. in this DAG of cells, there are edges that form loops, and they go up, but they are different from the loops going down in that they don't trigger updates for cells above. so really the job of the runtime is to decide when to update a given cell, and the decision process is based around tracking the updates of the cell's dependencies, and the event that caused the update, always only updating a cell once for a given event even if it receives n changes from its n dependencies (which all updated for the same reason). great, so we no longer need the concept of making requests or 'calling' selectors. we simply have a semi-DAG of cells. 
now we need to think of how to program the logic of cell's mapping from their inputs and their previous state (considered one of their inputs) to their next state. this map is considered their logic. and we should explore the format for the editing. and also whether the architecture can be implemented on top of javascript first. the mapping will be based around imperative logic on the queries on the structures, which may be numerous. we need to choose structure that are suitable for immutability. i suppose regardless, it can always begin a bit that distinguishes whether the rest is a reference, or a value like an integer or a boolean. for non-values, maybe all structures can be encoded in trees because they are well suited for immutability when modifying, that is the property that they can be modified without minimal amount of copying. note that we only need this property if the recipients of a structure plan on modifying it, that is if they plan on creating a structure that contains a lot of the same data. when this property is not needed, we could use any data structure desired. do other structure also have this minimal copying property beyond trees? lets think about operations on the tree type. filter could be cheaply implemented by the runtime by instead building an identical tree, but with each vertex instead a bit indicate whether to include the children of that vertex. eg a zero root would mean filtering out everything. mapping wouldn't be so easy, but if the map is just a projection, then we could just use the actual tree, along with an indication of what to project onto. but these would have to be handled by the runtime, and they have a dirty side in that they rely on the original tree, which may well be untracked while filters or projects remain tracked, thus waisting space. 
part of the philosophy of this language is that its not intended for quick prototyping, but for developing well theoretically thought-out, complex apps. as a result, we don't try to provide some basic constructs like simple mapping and filtering functions to the developer as super easy to use, encouraging them to use it everywhere, and then leaving a nightmare of work for the runtime. rather, we encourage the developers to develop their own constructs because in a complex app composition of simple constructs are rarely enough without being more verbose than they need to. this is why flatlang embraces custom shapes. so i think i'd like to leave the transformation process of constructing a new structure from immutable structures to the developer. this gives them the responsibility to obey the immutability policy. i think the job of the runtime would be to answer their queries for them, and to listen to their commands for creating new structures. hopefully querying and writing can be expressed in a way beforehand such that the runtime can optimize registers for it. i suppose immutability could be enforced by the runtime always interpreting writing commands as creating new structures, not modifying existing ones. i think all that remains is a mechanism for programming reads and writes to structures. 
lets use the delta app example. on structure is the windows, which is a collection of windows each listing ids of current delta and project. it receives new window and close window events. there is a structure holding projects, though maybe not all, but a subset of which are those currently displayed. each project in this structure has id, folder info, server info, and tip ids. i don't yet know exactly what it receives as inputs. then i suppose we have a structure of deltas. it receives queries for cached deltas and can return the delta or false if it doesn't have it. it also receives new deltas, at which point it may decide to dispose of another cached delta. i think all deltas it holds are complete, meaning they hold not just the delta trace but all child documents. we must have some structure to accept window requests, and folder changes, and db events, and server events. for window requests, we'll have a structure to accept page change requests, well i suppose the project structure should do this. but some window events, like changing folder path, will need confirmation so will go in multiple phases from a proposed path to a confirmed path or error. we should have a structure that receives folder saves, as well as well as the content, but only the content was changed. another structure must create the diff so it will also depend on the existing content of the project, but that content will already be in the delta store because that delta is displayed. i suppose we only need to encode queries, not individual requests, because queries are the only type of request. the queries are in the project info, so the projects structure will hold these queries. we then need a db structure that will receive db queries, keep a record of those in progress, and spit out the responses. it will perform both reads and writes. so suppose we get a window change project request. it will go to the window, which will approve it, then somehow a request must be made to the db for the relevant delta. the project structure i suppose must now list the new project but admit that it doesn't have the info. so then we'll need an additional structure that reads the projects, and makes sure the deltas are available, and issues the difference of them as db requests. when the db structure gives out its responses, how do they go back up to the structure calculating the db requests? ok, i see the solution. we can't simply use the traditional way of modelling the db with a structure that performs both input and output for it. rather, we'll need two structures, one for input, one for output, and the output will be located above the input. the structure that decides what db requests to make (which may be multiple structures) will be dependent on the db output structure. the db input structure will also be dependent, and adjust its output (which will trigger db queries by the runtime) according to the db responses from above. at this point what's needed i think is a way to graph out the dependency graph specifying not just the edge configuration, but the type of each edge (structure) perhaps even in a language of types dependent on the input types of the structure. it should also be specified what each structure represents. the logic of transformations shouldn't be needed until later. 
i think we should have a shape well suited for multi-stage processes like requests, or folder changes. we'll also typically have collections of objects such as windows and projects. 

one way to assert constraints on shapes could be with bitstrings such that when an bitwise operation is performed on it and the shape, it always returns a certain answer, eg 1 or 0. theoretically this could check for every possible constraint, but it would require an infinite number of checkers. so maybe checkers could be generated runtime. 

how about using all the collected data to trade the markets for the profit of the project? or somehow sell the trade predictions. in fact, it might be a goal of the project to collect relevant data for trading, and incentive those who collect that data with part of the traded money. 

trying to implement in js i realized that there might be a simpler model, the SDR model but without drivers. events from outside come in, state is divided into slices, and the change is routed to all appropriate slices. each slice reads the event together with the previous state and all its derivations, and then gives a new state or mutates itself. there must be a mechanism such that all slices can read the previous state and derivations during an update, while still able to update themselves. if there is no way to do this with slices mutating themselves, then we must use immutable slices. 

why is the cascade model not good enough? i was trying to implement projects. i was confused how to query for a delta when a new project is requested. some structure must describe the query. i didn't think it was the project structure's job. so we then have another structure called the requester. the requester is dependent on the responder (from the db) so that it can update its request as fulfilled. if this structure is dependent on the project structure then when it fires it will not know if its due to a change in projects or change in the responder. this leads to piecewise mapping when i wanted a simple function to map the structures unconditionally. the requester must be directly or indirectly dependent on the project change request in order create a new request during that round. suppose this was the SDR model. then the requester would directly receive the event and perform the same logic (via a selector) as the window in determining if that project can be changed to. if so, then it requests it. translating to the cascade model this would mean the logic of deciding if project change is ok would be located below both the windows and the requester rather than in the windows. but this mean windows would have to call use the selector upon the request and pass the requested project id to the selector. alas, the cascade architecture was not designed for selectors to receive arguments. I had been thinking that if selectors received custom arguments they have no purpose because each time they'll likely be called with different arguments. this was a fatal assumption. because in fact an event will likely trigger many slices of state to call the same selector with the same argument that are a derivative of that event. those arguments are unknown until the event appears so the selectors cannot compute their state during the previous round. this fatal flaw makes the cascade model worthless for my purposes. I will proceed with the state reducer driverless model, where the only driver needed is the runtime, because state can specify arbitrarily detailed outputs, down to the pixel or the network packet that should currently be on screen or sent on network respectively. 
---- end of cascade model ----

---- start of state reducer selector model ----
maybe we can allow mutations by saving the changes of slices and not performing them until all slices have proposed. the idea is slices should be able to propose all their changes without seeing any of them manifested. i will have to confirm that this is doable. changes would be especially easy to specify if the state is just modeled as a giant map, such that each change consists of a key and a shape as a value. this encourages small shapes, because each change must replace the entire shape. 
so is it possible to propose all changes without manifesting any of them, or in order to decide one change must you first implement a previously decided change? i guess this is answerable by thinking in the redux model about how the new state is nearly always constructed by only reading the old state. i can't prove this yet but i am convinced this property can hold. i hope it is not another fatal assumption. 
i suppose if state was a map then a change could not only specify a value but also an optional index for where in the shape to make the replacement. this way, values can represent large arrays by a list of ordered keys. but remember, we don't want arrays because of the large amount of necessary copying. collections i now remember are maintained as linked lists or trees. so i think a map with simple shapes for values will do. no in proposing changes we'll need a way to specify that a key in one shape points to another shape, even when that other shape has not yet been put in the map because the changes have not yet taken hold. i suppose changes will come in the form of insert, update, or remove because for the first the runtime decides the key, but the second two the user must point to a key. 
is there any reason a nested state would be superior to a flat map? i suppose a nested state would be implemented with a map anyway so there is no difference. so the app then consists of a number of reducers, each taking in an event and their previous state as input, and a number of selectors, each taking custom inputs. selector use memoization which relies on their input being deterministic of their output. often a key to the store will be an input. if a selector is called multiple times within a single round the uniqueness holds because the value of the key will not change during the round. but uniqueness will not hold between rounds if state is mutated round to round. do we prefer mutation, or having to recompute selectors every round even if they have the same arguments? the primary use for selectors it seems is cross-slice communication, which is not really communication, but where slices arrive at the same conclusion by consulting a common computation held in the selector. most of the time i think selectors will be called with arguments derived from an event. these selectors are recomputed each round anyway, so mutation is not an issue for them. mutation is only an issue for those selectors that derive from state with arguments independent of events or no arguments at all. i don't think the cost of immutable data structures which is primarily a cost of copying and garbage collection and a cost of forcing a tall tree state design. i don't think this cost is worth it for these special selectors. preferable i think would be having these special selectors be reactive. they would receive all proposed changes to help them reduce the amount of re-computation. theoretically the selectors could recompute every round with no help, but seeing the changes may help them locate necessary changes to their own computation. but for re-computation, these selectors will need their previous state. so maybe these selectors should accept no arguments, otherwise the runtime would need to save multiple version of the previous state, one for each argument made. we'll see as we get started what the need for these special selectors really are. for regular selectors there is a need to balance between how customized a selector is, that is how specific its arguments can be. the more specific the more computation it can take away from reducers. the less specific the more often it will be called with the same arguments. so the design must be manually optimized in design for minimizing total computation. 
now we need a language for the logic of selectors and reducers. reducers make queries, including those to selectors, and specify changes. selectors only do the queries. the language must consider that state is map of shapes. queries will mean a lot of dereferencing keys contained in shapes. fields within shapes will be located by index, or multiplied bit position. so we have shapes and fields within shapes, which i suppose are subshapes. commands include applying a shape to another shape for locating its subshapes, and dereferencing a shape (interpreted as a key). it seems we might best use imperative style queries. but this would likely introduce the need for variables which require an human language. i suppose the variable names could limited to symbols such as the greek alphabet. 
we need a way to specify changes. changes will never be put in the store themselves so they need not be shapes. hmm, interesting, what if we do the programming where the variables are expressed as shapes. this means no symbols required. the runtime would probably not use the actual shapes because the functions and their logic would be optimized. actually we'd just be able to have one shape per scope, because if there were multiple there would need be a way to distinguish them. the goal of flatlang however is to never let the developer think she is doing something she isn't. since the imperative programming won't be carried out in the runtime as specified, it'd be better to use the more generic instructions i had in mind initially for flatlang. 
what if we abstract state more such as having 'collections' of shapes. what else would we need? maybe lists. there would be available queries for these abstract structures, such as 'find if a collection contains something with this property'. the risk is that users would overuse the abstraction. i suppose under the hood the runtime would need to implement these structures using a map with keys contained in the shapes. thus these abstractions could just be abstractions built manually in the language for convenience. so for now i'll forget about these and assume everything is manual. 

an idea of flatlang was hiding what part of flat memory was in registers and what part was not, optimizing what should be in the registers at any given time. this would be in alternative to the stack heap model were the state of computations is held on the stack. the challenge is to specify instructions such that generic enough they can be optimized, but not so rich that they require so much room for computation state that the registers don't offer enough available space. 
perhaps queries of a single shape could be done with bitwise operations on bitstrings with parts of the shape, because in reality i think this is how conditions will be checked anyway. the 'if' could be implicitly specified by whether the operation results in a 1 or a 0. so programming would involve giving a condition and the two implications. we want to be friendly to chained conditions as well as nested conditions. for chained conditions we need to be able to replace the 'else' implication with another condition with its own implications. this means indenting else statements would be bad because then the next condition that replaces it must be indented. one way would be to forge the idea of 'else' and only support conditions with implications, and if an 'else' effect is desired, to have an easy way to communicate to readers and the compiler that a condition is the reverse of the previous condition. maybe it would be readable enough if just the reverse condition is specified entirely.

maybe instead of specifying a subshape by indexing it could be done by assigning each subshape a unique color within the shape. reading would be easy, and writing would be done by selecting with mouse or keyboard the relevant subshape. 

suppose we want to go through a linked list and find a shape who's second subshape has first bit set to 1, and who's third subshape is a key to the next item in the list.

regular would be:
current_shape = initial_shape
loop {
	if(current_shape[1][0] == 1) {
		return current_shape
	} else {
		next_key = current_shape[2]
		if(next_key == null) {
			return null
		} else {
			current_shape = *next_key
		}
	}
}

with shapes would be:
loop {
	[1] & 100
		:>[1]
	[1] !& 100
		[1][2] & 000
			:> 0
		[1][2] !& 000
			[1] = *[1][2]	
}
or
loop {
	[1] & 100
		:> [1]
	!%
		[1][2] & 000
			:> 0
		!%
			[1] = *[1][2]	
}

changes cannot be shapes if shapes are fixed size because we don't want the content field of a change to be fixed in size. 
note that small subfield sizes don't need to align properly, just the larger ones need to align. 

before i make a new language i'd like to try out the architecture with an existing language. i think i'd rather work in C than javascript, because C offers direct control over pointers, which is important for the architecture. C can still be made a cross-platform app with node or electron. the store would be easy, just relying on malloc, and as the runtime would have control over keys, malloc has control over pointers, which represent the keys. we need a way to specify changes, which will be in the form of a type (enum), and possibly a value, so likely changes will also be made using malloc. so the runtime would need to keep track of the root pointer to state. when an event comes, the runtime figures out which structures have subscribed to that event type, and then calls each relevant reducer function with its previous state and the event. the reducer can call selectors, but the runtime must intercept these calls, so i suppose the selectors won't be called directly but instead there can be a 'get' function calling the runtime that specifies a selector. 
well it looks like C is not very versatile and friendly to custom bit sizes, and requires a lot of boilerplate to interpret them correctly. also, a pointer must know what type its pointing to. also, C doesn't support large size types, whereas we'll need a shape large enough for a hash, 256 bits, which is 4 times 64 bits, and 64 bits is the longest type C supports. so basically C isn't friendly to my needs. 

for content in changes, a new shape should contain the entire new shape. for a removal it should contain nothing, and for an update it should contain a collection of subshapes that should change and their new values. you can't partially change a subshape but must specify the entire new subshape, even if its large like a hash. this is because if there was reason to only partially change it then that subshape should be split up into two subshapes. how should changes be submitted to the runtime? the size of the content of a change is variable. i suppose the runtime can just provide a function to submit them.

what about giving reducer function the environment of a flat segment like flatlang was designed for instead of an abstract function environment based on text. a stack would not be needed, but instead all variables are kept in the segment. query and assignment operations would still need to be specified. i suppose i already had this idea when thinking to use shape for function variables, but i thought it was against the design in that such a shape wouldn't actually exist. but now i realize such a shape could exist and would actually be consistent with flatlang's original design goal. so i suppose the idea is on a function call a shape is created with space for all necessary local variables for the function. then computation moves to that shape. instead of a map, i suppose we could indeed just place the shape on the stack. 


the raw implementation of flatlang with no architecture is to have code of the form of statements of accessing certain fields by their index in memory, and assigning and performing operations on things accessed. then the task would be to convert that code to statements involving registers, that is from a flat memory model to a model of flat memory together with the notion of a process with registers. i suppose the flat memory model could include a stack but i don't remember how defined the stack is on micro processors, and what constraints this would put on the stack in the flat memory. the goal means going through every part of the computation and considering what values are most likely to be needed soon for computation, and loading those into registers. so its like handling the registers like a cache where the task is not deciding what to load, but deciding what to drop to be replaced by whatever the code says must be loaded. 

instruction types would be:
slot_i = slot_j
slot_i = slot_j op slot_k op slot_l ...
an instruction consists of a number of operands, and a single operation definition, and even multiple destinations for the result. 

what if i first start out using redux. i'd want to use immutable js. hopefully immutablejs would compensate for using flow or typescript. the problem with javascript in general is 

grouping logic by event and related transformation rather than by state slice removes any need for immutability because the single function can coordinate its writes so it doesn't conflict with its reads. the problem is it's not easy to see the lifecycle of state slices, whereas you can see this easily if you group by slice. i'm wondering if we can use both, in a way re-sorting logic from one grouping to the other. this way its easy to see the life of transformations, and the life of state slices. What conditions must we put on the formatting of logic in order to make it re-sortable like this? lets explore going from event to slice. if we just take the logic of an event and place it in every slice, removing anything not relevant to that slice, we'd end up with a lot of redundant logic. this is what selectors are for. suppose we go the other direction from slice to event. if we just take all the slice logics and place them one on top of another in the event logic, there would be a lot of redundant selector calls so we'd assign a variable to each and remove all calls but the first one. so i suppose in general this means organizing logic into a collection of selectors, and deciding for each event what the argument of each selector will be. then also a collection of procedures, one for each slice, that reasons on the output of the selectors, and decides how to write to its slice. in the event model the selectors are all at the top, and then there is a stack of procedures, one for each slice. in the slice model, for each event there is a collection of a certain number of selectors at the top, and a single procedure below it. should selec

i have a feeling state shape will always be a single collection of maps. each map will map an id to a record, eg window, project, delta, request. even selectors it seems could return one of these maps. for example, if a selector must filter the items in a map that is simple. if it must then order the items it replaces the key with the appropriate index. we we can standardized every structure as a map of records, we could also standardize all possible queries on these maps. a record would either be of fixed size, like a shape, and contain keys to its variable length values within another generic storage map, or the records would be of variable length. but maybe records should be one of a finite number of types, such as a snippet being a root or an intermediate vertex or a leaf. 
a problem with much of the architectures i've discussed that I haven't realized until now is that if keys are chosen by the runtime and not user specified, then shapes representing eg deltas must have as their values not just the hashes but the runtime keys as well. this means upon inserting a new delta, the leaves must be inserted first, and then their returned keys must be inserted as part of the value of their parent vertices, etc up until the root. the problem is if part of a delta are held in different slices of state, those slices cannot perform their writing independently. a solution is to replace the map with an array (linked list) that is sorted by some criteria such that searches for a particular one eg with a certain id can be done in log2 time. this way no keys are needed. the con is that most particular items with be extracted a lot so O(1) time is better. also (without secondary indexes) they could only be sorted by one criteria, such as an id, but i guess this is the case for a map as well. a secondary index would be implemented by re-ordering the array by a new criteria. but keep in mind since we'd be using a linked list, the array would instead be an array of store keys. we want a data structure good for normalizing queries. this sorted array approach may be the best. implementation wise it would be that the items in a collection are all the same shape of all the same size, and they'd be placed in memory next to each other in arbitrary order. copying would be required however, for every batch of insertions or deletions in order to prevent any gaps. so if a shape is removed from within, a shape on the outside must be moved into its place. only requiring copying when there is a gap in the middle that remains unfilled. there is no primary index. instead there are multiple, at most one for each field of the shape. each index consists of a permutation of the indices of the items corresponding to the sorted order. every time the actual shapes change order or length, eg on insertions and deletions, all indices must be updated, which requires copying on average half the whole index. suppose there an N shapes in an array. the size of an indicy in an index would be log(N) and there would be N of them, so an index would be N*log(N) length. my hope is that this, together with the log(N) lookup time is acceptable for these applications since there don't hold a lot of data. i suppose any field that will be used to search for an item must have a corresponding index, probably ordering lexicographically. one benefit of this memory model there is no need for resizing and since different collections can sit right next to each other there is no waisted space. since the indices of an index must remain in order, placing them in memory is more challenging. we may need a map for them. and since shapes are of fixed size we will need a map for their variable length fields. these two maps can be the same store which operates with runtime chosen keys. 
i suppose in addition to indexes we could have filters as bit strings which would be of size N, or other shapes counting certain stats of the shape
so now what about programming? assume the indexes for each collection are chosen ahead of time. a query of a collection collects one or more of a subset of the shape's fields, which includes the whole shape, or no fields at all. the set is chosen by a set of conditions for certain properties. i don't think we'll support 'relative' conditions, such as all shapes sharing the same property. rather, a concrete condition must be given. so every query specifies a collection, a condition, and the number m of shapes to retrieve that match that condition, eg 1 or all, and the subset of shape fields to retrieve for the shapes that meet the condition. a condition can be a combination of other conditions. primitive conditions i think will just be equality and inequality of the result after a series of bitwise operations on the fields. the result of a query would be an array of shapes, one for each of the m shapes, and each of these shapes would consist of the subshapes that were asked for. if none were asked for then the array would simply be a string of 0s or 1s. so if we want to test for existence the query would be for at most 1, and no subshapes requested, and either a single bit would return to indicate existence or the return value would be a string of length 0. actually perhaps a superior query form and response form would be to not return actual shapes which would require copying them. but rather return an array of indices for those shapes that met the condition. the requesting code can then query those shapes for whatever subfields it likes. maybe we could implement cursors but thats another task. 
so the selectors make queries, and so do the write functions. how does a write function write? i think the operations are add, remove, and update. writing will also involve storing variable length data in the store. hmm, maybe we could think of the variable length data as part of the shape, this way there is always one and only one shape holding a key that points to that variable data. thus a delete operation causes the runtime to also delete anything pointed to in the store. i suppose anytime a variable length part is changed, the runtime will choose a new convenient key, so the key may well also change. nothing will be pointing to the shape so this shouldn't be a problem.
i suppose there is an even superior way of doing queries and involved cursors, which i considered before. the reason is in an array of indices returned needs a place to be and since its of variable length it won't fit with a local variable shape. i think its better for a the logic to set up a local variable representing a cursor, and then call 'next' from the cursor like a generator until it doesn't want any more or there are no more shapes that satisfy the condition. the local variable would keep track of where it is in the query process. this should be possible. but a concern is a write might need to occur between reads. this could be prevented if write logic can only inspect the results of selectors and perform no queries itself. this would also allow mutation. to do this we need all querying to be done before any writing starts. this means all querying should be done with selectors, which will run before any write logic starts. also, the arguments to the selectors must be known. we can try this by trying to write transformation with no more reading after any writing. remember that selectors can call other selectors. maybe instead of selectors being called, they should directly receive the event like a reducer, this way we don't need to determine the arguments for the selectors. in fact, maybe only the selectors need access to the event and the reducers don't since anything they write can and should come from the selectors. so the result of selectors is really available to reducers and thats all reducers have to work with, no custom arguments, and no queries. 
we should strive for a design where a single record can be conveniently portrayed as a collection this way we only have the collection type for consistency.
so now we need to decide the implementation of selectors and reducers. there is a collection of selectors for each event, and there is a collection of reducers for each slice. basically selectors are responsible for receiving the event and computing all derived state and logic relating to the event. anything a reducer wants to know a selector could have computed. so assuming the selectors and computed prior and only a static output is available for reducers, reducers can just be logic around the outputs of selectors thus not needing any local variables. i know this model will work, at least theoretically though it might end up not being convenient, because a selector could go to the extreme of calculating exactly the changes a reducer should write. so reducers are easy, just code, no local state. now selectors.
last night i realized selectors can form a DAG. so each can be a function of other selector outputs and the event. assuming selectors are not recursive, we can have a shape for local variables (eg cursors) available for each selector, and the function can be computed in that environment every time, so no need for a stack. the idea that justifies only ever needing a constant amount of space for computing a function, is that query results of arbitrary length can be processed by iteration of a cursor. though the local variables of a computation may be of static length, the output of a selector is variable, so on every selector call we'll need to establish a new item in the map which the selector can manipulate as its output. the key to this output would then be given to any selector or reducer dependent on the selector. 
how would we process file input? a selector would have a key to it, and then iterate though creating a new array of offsets to the start of each line. this already implies selectors will not only need variable length shapes for their output but for intermediate values as well. however, instead of manually allocating space i think the length of the needed memory will always be the result of a map function or some transformation an arbitrary length input. however, the keys holding things of variable length should still be kept in the fixed size local shape. i'm thinking about processing file input. thinking in terms of a lightweight process, lets instead think about streaming the content. maybe we could do this with collections, where an iterator streams over the content, creating a new item for the collection for each new line. the items in the collection would be ordered by their index. it would be great if we could handle file processing without changing the model. we could have a collection of the current characters in the currently processed line. and then a collection of already processed lines. and then a collection of groups of lines. in all three cases an item in the collection would be a pointer to a position in the original text file. in order for this to be compatible with the model, the collections must be static, and not dynamically created. actually i realize the first collection wouldn't be necessary. maybe we could have the property that if a collection specifies no index for ordering, the default order is not the order corresponding to memory, but an order reflecting the order items were added and removed in. given this, we might even be able to model the file input as a collection. in this case a line in one collection will contain a index for the file characters collection. this assumes the file collections will not change, which is one collection making assumptions about another collection, which i guess we already have to some extent with the connection between windows and projects and deltas, so its not a problem if the developer is aware. 
if inputs are modeled as collections, then collections must be dynamic to some extent, because event collection will be unknown at compile time. or wait, maybe there can be a collection for each type of possible input collection. the only variable parameter would be the size of collections. suppose we try dynamic collections. the purpose for collection in the first place was to avoid the need for keys (either user or system chosen) in records of collections to refer to items in another collection. but if collections are dynamic in the sense that there can be an arbitrary number of them, we will need keys, or pointers to those collections. if these keys are system data, they cannot go in collections but then we'd have no place to put them. so maybe we could have a collection of collections, where each collection has an id that the system can use to locate that collection. the only way to create or delete a collection, or change its indexes would be to modify the collection collection. the system would have right to insert new collection corresponding to the payload of events and reducers could remove that collection if they don't want that input. even if we make this collection special where it can contain the addresses of other collections, can't contain their indexes because those of length that depends on the collection. hmm, but i suppose it could contain the addresses of indexes as well. so maybe instead of working with IDs, we can make a single special collection containing all info for collections. each item, representing a collection will have: start address of collection, size of item, number of items, index info. but indexes are variable in number for each collection. maybe the index info could be expressed as a bitstring, one bit for each subshape of the item shape, indicating which subshapes are indexed, and i suppose each index could just sort lexicographically by binary representation. as far as the addresses for the indexes, which would be variable in length, they could be derived via just one address if the indexes are placed next to each other. 
what is the best placement of indexes in memory? could they be intertwined to minimize copying? if an item changes, the indexes must be re-ordered anyway. when a deletion or insertion happens, however, most everything must be copied if indexes are place next to each other. but since each index is of the same length we could place the first element of each index first, then second, etc, intertwining them. this way, when an addition or removal occurs, ... this actually doesn't help. I suppose we'll just place the indexes next to each other, and next to the collection. a collection would then consist of a single segment of memory with no gaps. since this is a lightweight application we'd expect that for most collections, even those with lines of a file, N should be no larger than a couple hundred. log2(256) = 8, so for N=256 we'd have just 8 byes for each index. and those with large N, such as characters of a file, should not usually be manipulated so index copying shouldn't be a problem. i think every compilation will choose a max size for its shapes, the max size of subshapes, and the max number of subshapes, and this will determine the encoding of the subshape sizes along with the size of the bitstring that maps which indicates which subshapes are indexed. i suppose collections will need a static property such as ids in order to be accessible, because their addresses are not static. collection shape: address|N|size|indexed|subshape sizes. 
so now with dynamic collections we can use collections for intermediate results of computations. but now can selectors modify collections? they will need to in order to process events like a file save. 
a collection can be described in typing as:
collection name: this collection is for...
|64|8|8|1| =>< n
64: this field holds...
text continues here
8: this field represents...
8: this field...
1: and this...
a selector function will describe its local variables shape similarly, though these shapes will not be held in collection, as they are all of different shapes. 


it appears we may need to rethink immutability. suppose an event causes one collection to update with a value from anther collection, but that value is supposed to be modified by the other collection. the danger is the modification could occur before the reading. the assumption in the design was that no reading would occur after writing. so either the two must be strategically ordered, or selectors must copy any collection values in the output instead of pointing to them. essentially all the logic is contained in the selectors, so we might as well go have the selectors output changes to collections and forget the reducers. it would require that all changes can be outputted before any writing occurs. but the output changes still must be copied and cannot point to values in the existing collections. i suppose strategic ordering could always be possible. this is because from the perspective of the event transformation we can have a stack of queries and then below a stack of writes, the order for which they must occur. after all, the compilation will occur via this perspective because code is better stored and executed this way. the slice perspective only for convenience, but still important. even if the write sequence writes to collection A then B then A again, meaning the A slice reducer would need to be executed piecewise (which is not possible), is not a problem because the A reducer is not executed at all because as stated above it is only for convenience. As long as the transformation can be expressed by a sequence of queries and then writes, we can look at reducers and be confident that the data they read will not have been overridden. A subtle but important point I now realize is that not all reading can occur before writing without copying as I assumed before. This is because writing almost always assigns to read values. 

a problem may be that selectors do more querying than they need to, in that one query may indicate other queries should be canceled due to an error but we can't stop querying because we may need to do some writing, eg showing the error, and we can't do that writing until all queries are made. the solution involves selectors doing conditional checks to make sure they should perform their query, eg that an upstream error has not occurred. selectors i suppose are allowed to return if no writing is necessary. or maybe writing is always necessary in order to remove the event from a collection, but i'm not sure this must be manually done. 

uh oh, i forgot that delta snippets are not constant in length, unless they are a binary tree. a big question is how we want to store deltas in the db. it we took the merkel delta and converted it to an operation we could store the operations, and always regenerate a delta when necessary. but operations are also variable in length. anything variable in length must be represented by a collection. and we want to put as much as possible in a single collection. so best would be for a collection to represent the current shown file. this means files would have to be read in and read out for every delta change of view. better there would be extra collections, each to cache a previous file. this is not ideal, and my forgetting that deltas not constant in length and thus don't fit easily with the collection model is enough to discourage me from pursuing the collection model further at the moment for this app. i still value the collection model but my focus right now remains on the app so I will pursue a model best suited for its rapid development, and I suppose at this point the best option is to just use redux. 

so assuming i don't pursue my own custom language right now for the app and instead use existing technologies I will leave these notes for now, as they are dedicated to the new language, not the app. 

i'm back. mar 13. we have a problem where one thing, eg a delta, points to a variable number of others, even if those others are all of the same size. this is a one-to-many relationship. we might be able to invert this as a many-to-one relationship, where the pointer is not encoded in the 'one' but instead in the 'many'. that is, if you want to find all children of delta you don't search for that delta, then inspect its 'child' properties and then search for them. rather, you search for all children that have the necessary 'parent' property. In most settings this wouldn't work because search time and space would be too costly, but in lightweight applications with small collections this would be acceptable. 

do we really need the state? where does variable length data go, such as items?
it would be great to have static collections. every possible shape would have its own collection, and so there would never be two collections with the same shapes, instead they would be merged and an identifier would different which group one belongs to. keep in mind that input doesn't need to be considered part of state, but can rather be iterated over and only certain parts pulled into state. what if we try to invert the relationships as we did for the deltas, that is instead of an item with a variable number of characters, we would have a collection of character lists, each of the same size, and each would reference which item it belongs to. this again goes from a one-to-many to a many-to-one. note that many-to-many, that is *-to-many is not doable because it requires specifying a variable number of whatever the 'many' is. for items, we'd have a collection where each shape has a segment of text the size not too much bigger than the typically small item size. it would also have an id referencing which item it belongs to, as well as an index for its position in that item's text. 
wow, this inversion is great. so now not only do we not need a map, but we don't need dynamic collections. as far as reading a file, it is not considered part of state but instead an external resources to read from where info gets streamed from, in the same way as the network. for example, in xen the streaming is done with a circular queue. as the info is streamed, regardless the source, shapes can be made out of it and added to collections, which can then be analyzed. so great, now we only have to deal with static collections, and as a result, no need for a collection of collections or anyway of interacting with it. keep in mind we will want an index to have multiple layers, for example to order item snippets first by their item id, and then by their index, such that they can be easily read sequentially. i think changes should be batched for maximum performance, especially to minimize thrashing of indexes between memory and registers. for batching, changes must be postponed and only performed after all logic completes, and then the runtime goes through the changes for each collection. maybe the changes for each collection can be stored next to the collection, this way the runtime can easily access them, and any new items created can be placed at the end of the list to reduce probability of need for copying them. updating could also be done by creating a new element at the end and declare the existing one to be removed, or the part of the shape to be changed can be stored. choosing between the two is a matter of minimizing copying, and so is done by simply observing whether the altered part of the shape is more or less than half of the total shape size. 

dependent implication branches. branch implication would be encoded, not with two independent implications, but with one implication that encodes instructions for either result. 

might be able to do finite state verification with respect to an event. for each separate event type there would be separate finite verification. 
maybe collections (forgetting their indexes which are just for performance) could be mathematically modeled inducing a mathematical model of programming. suppose the bits of a shape are the variables in a multivariate poly. the poly could represent a collection and output 1 if the shape is a member and 0 if not. better use of poly would be to encode constraints. 

most of the logic should be extracting small bits of info from the collection, whether looking at just one shape, or doing a reduce operation. operations shouldn't be about gathering information, like deltas to send to a page. rather, a shape should be generated that describes what information the runtime should send to a page. so there should be minimal amount of moving data around. so maybe all instructions can be about querying collections for small amounts of data, and writing to collections using that data as well as event data. queries could be of the form where they ask for something but at the same time express assertions, and if they are not met the runtime will automatically throw an error predefined by the query type. computations represented by a fixed length local state will need to write variable length descriptions of outputs. just as input is of variable length i suppose output could be as well. so a computation iterating over a collection could continually append new data to the end of the output. so the local variables will need to be able to implement cursors. cursors will need to hold runtime info, like the current location of the cursor, which could be the offset of the current index, along with the query type. 

process algebra might be helpful to model the constant interaction between client, app, and db. in addition, collections could be modeled as sets. operations on collections should easily translate to operations on sets. a common piece of logic would be testing set membership. relational algebra would also help. we would still like to express the app in terms of transitions. a fundamental need is that of the implication. so the goal would be to have an expression that evaluates and determines the next state. we'd like to prove that the state is always valid. validity could be expressed by a set of constraints, which would be logical propositions using set theory about the collections. so we'd prove it by induction. assuming a valid state before the transformation, use that to prove the valid state is preserved. of course transformations will have a purpose, such as changing collections or writing an output. the purpose can be verified as fulfilled by proposition across the current state as well as input and output. as far as the output, it means the process must be the same regardless of conditional outcomes, where only the parameters change. 
for process algebra channels would be message types. so before any transition, there would be a series of processes, one for each event type, waiting for a message on their respective channels. many transitions are parts of a larger transition, eg getting a delta request from the client, sending a db request, receiving a db answer, sending a client response. to aid this, instead of having the same transition functions ready on every event, different functions could be ready dependent on the outcome of previous events, and yet this could be done without a hard-to-test internal state. lets use the previous example. a process would wait for a client request, then it would send a db request on appropriate channel, and then it would result in another waiting process which would be considered part of the next transition. however, the processes listening would not be independent, and one triggered may need to cancel the listening of another process. 
we need some way to manipulate collections. it could just be specifying the new version of each collection with a set expression. likewise, the output could be specified as a new set, assuming the runtime has the knowledge of how to assemble that set into a message. how about reading the input? the language could assume the external world encodes everything in collections the same, so input and output could be modeled as such. of course the runtime would do translation. so then programming is a matter of reading input and state, and writing state and output, all of which are expressed as sets. 
maybe the easiest model of programming would be doing a series of checks on every event, and having a predefined message error and output action (that doesn't touch state) for every check. these checks could have have names and be called like functions. these checks could, like selectors, have outputs (subsets) if there is no error. then there could be implication-free reducers that would read from the (verified event) together with the selector outputs, and produce new sets for each collection, as well as an output collection. selectors would be in the form of reading state and input, such that if the read is empty, an error is generated. reducers would be in the form of reading state and selectors and input, and basically expressing a new set in terms of them. one reducer per collection. 

example problem: input has a collection of text blocks. need to generate two collections. one is a collection of items. the other is a collection of new text blocks belonging to each item. items are defined as split along double new lines. this means set operations are not enough. there must be means of operating on even subshapes. well one exploitable property could be that the original text blocks are the same size as the item ones, so there will be the same number of them. filter and map are an instance of fold (reduce), so maybe a fold operation could provide the only necessary operation for iteration. folds could be done over any cursor. if we have a static shape as the accumulator, it would need to perform side effects of eg adding to other collections. i realize i should reinforce the idea that the platform is for handling real time events. parsing a file is then not for this platform, but can be done by a separate unikernel process in a different programming model, and the result made into a collection format appropriate for this platform. we will now assume subshapes are atomic and cannot be modified. so to solve this problem, an input event would say the file has been saved. then the system would make a output asking for the file to be parsed, and the result returned as a new input. would we still need folding? hopefully we could just have commands that perform a cumulative operation on a selection of collections, without custom folding operations. 
as far as the other models, i think there only needs to be one, and i think it should be freshly instantiated for each separate algorithm because they are indeed separate. an example would be each instance of parsing a file. this prevents memory management challenges. the variable length data for the problem would be laid out in memory in a way the code could recognize. code will probably be call based. important is that there is no asynchrony. for now i'll probably write the code with an existing language.

COLLECTIONS
windows: |256|
256: delta hash

deltas: |256|256|
256: hash
256: parent hash

delta snippets: |1024|256|8|
1024: content
256: delta hash
8: content index

EVENTS, each one in a separate file
<event name>: <event shape>
<preprocessor name> = <preprocessor selection>
...
<collection name> = <new collection expression>
...

use '.' for projection
use '+' for union
use '-' for set subtraction
use '*' for intersection
use '\' for '\in'
use ':' for 'such that'
use '!=' and '=' for equalities

so on basic type of query pertains only to a single collection. 
one such type asks for existence, returning a boolean
another asks for the number that satisfy a proposition, and return a count
others can ask for a series of items. but a reducer will not be able to examine those individually and instead will probably add, modify, or remove them from the collection. 
we may need a join, eg to merge parent and child deltas into an appropriate output


I will see when I start writing. I should start with a new folder, for this app, a file specifying collections as well as all constraints, then a file for each event type, containing the event info, the relevant preprocessors, and a reducer for each collection. there will have to be an extra reducer to write the output. below the reducers should be the proofs that the reducer produce a state that still satisfies the constraints assuming the previous state did.


w Window : w.0 != e.2


Jul 26, 2018
to make analysis amenable to mathematics we need to represent items accordingly. we need to represent each shape with something mathematical, like a polynomial. we're assuming the constituents of a shape cannot be reduced further but must be analyzed in whole. this means we could have an evaluation point for each part (constituent of the shape) that yields its value. Then operations on shape are operations on polynomials. Addition of shapes would mean adding the parts. 
the operations we need are testing for membership. searching collections. adding, removing, changing. checking equality. set membership is hard with polys. i think constraints could be specified with existence statements, and relationships would be expressed with existence statements that reference multiple existences. code i think would iterate over sets. implications could be done by making the set that forms the condition to be tested, then iterating over that set so that if its empty nothing is done. 


Sep 22
use visual 'functions' with signatures like "function_name : shape_1, shape_2 -> shape_3" and have the programming describe how this map works. i think the map doesn't need to be linear or anything and as such can contain conditionals like comparing sub-shapes. p
maybe there could be a 'local' shape that follows around the single execution and contains intermediate data for any function. this way for example a fold operation can be reduced to functions together with local state. 
still not sure whether we'll group reducers by collection or by event. I think we'll group by event.
what's the engine algorithm? part of the engine listens for certain things getting put in one or more collections that should trigger requests, and the performing those events, and well as listening for events from the outside world and translating them into changes. this is the driver part of the algorithm. there is also a memory manager and reducer executer part of the algorithm.  



analytic function are redundant, in that one component can be derived from the other
how did we use redundancy before, and does this translate? 
its that the info gets bundled up into a redundant package (previously a poly) that only needs to be tested at one point, rather than at as many points as there are info. but this usually means testing takes time proportional to the bundle size which is proportional to the info size, which doesn't save time in itself unless V doesn't need to do the evaluation but can ask P to reveal the evaluation. 

multivalued function / branch points
u can choose one (or more) arbitrary point on the plane and have it so that for two (or more) identical inputs (same z value but differing representations of angle) you get 2 (or more) arbitrary outputs. 
whereas a real expression cannot perform branching logic, a complex expression can, so its more like a computer program. i suppose real and complex expression each belong to a different computational complexity class.
this doesn't offer help in proof methods, but it makes the polynomial being evaluated probably more efficient since branching can more naturally be represented in the poly. Assuming complex expression is a polynomial only allows first certain branching types (eg not log), but I believe they are sufficient). 
...Does branching actually work like i imagine it? maybe its like the output depends on the direction the input approach the branch point. so at one step of computation suppose you have a number, and you perform a deterministic transformation on it, causing it to approach a branch point. the output of the branching will depend on the direction of approach which will depend on the output of the previous step.
i guess i'm basically just using the discontinuity, which i could for real expressions to. 


only evaluating a poly once, you can verify computations in log time. 
what could be interesting is the idea that you don't need to know the data computed on to know the computation was successful. 
when the operation is the same and the input is different, its the input/output poly that you must validate and so you must know the input/output.
but if you 
ah, i remember
the operation is really defined as what the polynomials are, which consist of the wiring logic (which two output of the above poly give the value of the new poly), what the operation is, and also what constants may be contained in the operation. so basically, the operation is in the form of a series of poly, and each poly is a series of terms, and each term is a function of ....


in general, the question to ask is what can we exploit, and so far the only exploitation is the lemma that gives 


what end result do we want that seems sensible? I want to know that an operation was performed on all leaves of a merkel tree and that the outputs result in the leaves of a different merkel tree. this would be a mapping operation.  

sumcheck is useful for enumerating over a poly for all combinations of an input list. the inputs don't carry much info other than an index basically, so the meaningful info must be encoded in the poly and not provided as input. for example, the input could select one of a number of operations, which would result in an expression involving a number of evaluation of another polynomial, which represents the data. you would reduce all those evaluations to one. suppose a combination represent a path down a binary tree of 2-1 merges. the other input to the poly will have to be encoded in the form of a subpoly, which will have to present the actual proof info for that given combination. stepping back, we'd want the sumcheck to show that the parent poly evaluated to zero on all tree paths. so standard sumcheck addition will probably need to be replaced with another operation. now if there is just one sub poly it will need to contain all proof info. 
hmm, suppose there is one subpoly for each layer of the tree. if the layer has n nodes, in analogy to merkel proof the poly would yield the n node values. but in the poly tree instead of a node being a hash value, a node will have other info.
so any way suppose evaluating for a given path results in an expression with sub polys that yield the necessary proof info. the poly must use this info and perform the verification. 
so most immediate now is the question of evaluating the sub polys. regardless whether there is one giant sub poly or they are partitioned by some means, there will be an a small set of evaluation points for every vertex along the path. the following sub poly structures mean:
one giant sub poly:
	all evaluation points belong to same poly, so V must reduce them to one, and then evaluate sub poly. this is infeasible because this poly would have terms proportional the every vertex in the tree.
one subpoly for each layer:
	this is again infeasible for the same reason as the previous, because together all subpoly will have a sum of terms the size of the tree
one subpoly for each path:
	hmm, like for one giant poly this again means you can reduce all the points to one. but luckily this poly is only logarithmic in the size of the tree, cuz its the size of a path.
so the best sub poly structure is one for each path. then comes the task of obtaining this poly, and this could be done by querying down the path of computers that assembled that tree path. for now, assume incentives exist for these computers to give you the right terms. each one will give you one term to the poly. oh, but now there is a problem that the parent must contain all subpolys and when evaluating at a random point all these subpoly must be evaluated, which is infeasible. so it would be great if we have a reasonable number of sub polys all of reasonable size. in other words, you can only work a reasonable amount of data. 

what would a verifying circuit do?
it would perform verification of another circuit, which is possible until we realize it requires either evaluating the poly of the other at a random point, in which case the size of the verifying polys would grow linearly with the number of verifications. that, or it passes the eval of the poly up but this just pushes work up. 
hmm, what if we use muggles approach for bottom layers, and the data is contained in the circuit. the input poly would represent the opera
in muggles approach what's neat is that verification grows linearly with layers. 

how about the 'reduce' operation, rather than 'mapping'. i'm thinking of passing up the accumulator from the leaves. so this assumes the accumulation happens in a tree structure. 

the silver bullet would be if you can verify that a poly outputs 0 without knowing all the input. this is what i worked towards in Sofia. i will assume that evaluating the poly at a random point is unavoidable. the linear pcp approach is that P evaluates for V without knowing the random point. before i approach was that P evaluates along a random line prior, and the proof proceeds such that the random point at the end lies along that line, but this necessity leaked info throughout the proof to P. 
hmm, what about there being two input polys, one for the input that V must know P_k and the other for the input it doesn't care about P_j. Evaluating the P_k would be done as normal, but evaluating P_j would just be asking P for the values and taking them with no question of validity. at first this doesn't seem very helpful because V must still know info proportional to P_j though it doesn't need to process P_j. 
A verifying circuit has input of what defines the candidate circuit, and the claimed input of the candidate circuit (CC vs VC). Given these inputs, VC performs the steps of verification, including evaluating P_k of CC at a random point. hopefully P_k can be as small as possible, only containing a hash of the data to process which is contained in P_j. the problem with this approach turns out to be that you must evaluate P_j at a random point, and its not enough that P_j exists with the random mapping, but that its the same P_j as the beginning meaning that P_j must be committed to. 


review 2-1
implied: poly
given(5): two inputs w_1, w_2, and two outputs v_1, v_2, and a univariate poly 
do:
	compute \gamma with w_1 and w_2
	check that h(t_1)=v_1 and h(t_2)=v_2
	compute t = rand(w_1, w_2) i think
	compute w = \gamma(t)
	compute v = h(w)
output(2): one input w, and out output v

review of muggles
V_0 is output layer, V_d is input layer.
P must prove that V_0(0)=0
P gives w_1, w_2, v_1 and v_2
V checks that v_1+v_2=0
V reduces w_1, w_2, and v_1, v_2, to w and v
P must prove that V_1(w)=v
sumcheck, ending up with w_1, w_2, v_1 and v_2 again
...
P must prove that P_k(w_1)=v_1 and P_j(w_2)=v_2


new approach is that we can tolerate commitment, and don't have to find a way around it like with a crypto system or with the idea in sofia. rather, we take the approach of starks where we evaluate the polynomial at all possible points prior to proof. the context i'm thinking of is the polynomial encodes state, which is represented as an array of values, each one corresponding to a term of the polynomial. i'm thinking of the standard 2-value-multivariate arbitrary value encoding i investigate in sofia. the reason making an merkel tree of evaluations should not be too burdensome is that the same state will be used in many times in that for a given operation on state, most of state will stay the same, meaning only a few terms must be recomputed (for all possible inputs). but uh oh i didn't realize, but the entire merkel tree must be updated for every change. this may still be possible. another problem i realized is that evaluating over all inputs is exponential since there are exponential possible inputs for the multivariate poly. suppose instead it was a univariate poly. evaluating would be more possible, but then all the n input points must be encoded in it, so it must have degree n-1, which can compromise soundness. 


to help linearlity testing, let circuits do the evaluation of input poly terms. when a term changes it will need to be re-evaluated at every point, but this a great application for the sumcheck. oops, again not the case because the same term evaluated over all combinations we want to add to different terms, whereas the sumcheck performs the operation across the same term.


so what about a mapping problem so all base circuits have the same logic, except they differ in only one part in their logic. they also have the same input, the giant input poly. so the way they differ is the index of the input at which they draw their input. These are the CCs. A VC has 

I think we should use the outsourcing of add and mult. so each is its own circuit that only takes three inputs and outputs one element. so verifying involved encoding a degree 2 poly to encode the three inputs. it also requires evaluating each of the the add and mult of that circuit at a random point. 

hmm, what if circuits just output conditional verification on input polys and add and mult polys being evaluated on particular random points. in subsequent rounds these points would be computed by circuits dedicated to these tasks. of course they would also output conditional verification. recursively, the logic would reduce down until any node could easily compute it to fully verify a round from the past. so could assume there are a fixed number of circuits. they each point to another circuit for their add and mult. so at the end they yield a random point for each of its add and mult circuits, and a random point for its input poly. we want a VC to conditionally verify them. so to reiterate, a CC id identified by its add and mult list, and its input poly. thus for a VC to validate a CC it must have knowledge of these IDs. actually CC doesn't explicitly state them, but just provides VC with values, and so its VC that explicitly encodes the IDs. So the VC takes in as input the io claim and the proof info of the CC and then should either output something or output 0 i don't know which yet. 

I think its now worth making the assumption that VCs must verify two CCs, so as to converge the number of verifications. For convenience, first assume each CC has the same logic. Also for simplicity, assume that even the base CCs have the same logic as the VC, that is assume every circuit is a CC with the same logic. Forget about the input polys for now, and even if they are the same. The VC takes the 2n pairs of random points of the add and mult from each of the n layers of each of the 2 C0s. since each pair of layers has the same logic, C1 reduces these 2n pairs down to 2n single points. Now about about C2 with C1s as CCs. C2 must take the 2n points from C1s and reduce them to n. but it must also take the 2n points from the add and mult of C1s and reduce them to n. C2 must then reduce this new pair of 2n points to n points. this in total means reducing 4n points to n points. but now if the base layer has different logic, there will be another set of m points passed up from each circuit, meaning a VC must reduce 2m to m and 4n to n. now about input polys. so this means input to each VC will be 2m + 4n points. since m and n are logarithmic in the number of gates of a circuit, 2m+4n should be small enough that a VC can interpolate 2 polys, each one for m + 2n points, and then evaluate them at the random point the verification process asks for. so oops, i forgot that now that we consider the input polys, we also have that the input of the VC will not include...

ok, lets try rethinking this but not ignoring input polys or base CCs. 

The a birds eye view at the top of the tree, we get m points and n points, 

let me outline concretely exactly what the input is for an agent (circuit or person) validating another circuit. let n be gates/layer
idk specifics of how it starts
v1_1, v1_2
w1_1, w1_2 (2 assertions about V1) (size 2)
h1 (necessary to reduce to one assertion about V1) (size logn)
g1_i(x) for i from 1 to logn (size 2*logn)
back to w2_1, w2_2
the above is for layer 1. this is done for all d-1 computation layers.
then for layer d, the input layer.
let input layer have m terms, rather than n terms as the other layers. 
Vd (in form of interpolation points, size m)
so the total number of proof element in the input are: m + d * (2 + 3 * logn)
so theoretically given all these elements, a computation can verify it. 

ideal architecture would be a giant merkel tree state. every round unlimited computations would be run (their logic also contained in state). 
review state architecture. there is a set of actions, in specified order, given to every reducer, and each reducer has the task 

perfect would be if base computation could be private.
heres an idea. usually you'd evaluate p on a and q on b, but instead you'd rather evaluate them at the same random point. because if you could, by the property below you could instead defer to evaluating their sum poly at that point. and their sum poly would hide the details of p and q. 
p(a)+q(a)=(p+q)(a)
i'll this property holds for polynomials.
if p and q and input polys encoding private info, each to be evaluated on random points r1 and r2 respectively,
V makes a line \gamma between r1 and r2 and asks for h1 and h2 which h1=p(\gamma(x)) and h2=q(\gamma(x))
V checks that hi(ti)=vi, and then asks for proof that (p+q)(\gamma(r))=

privacy is easy. just add a polynomial mask on top, one that you can evaluate. if you don't care too much about privacy and want to reduce evaluation cost, just reduce the degree of the mask poly.  

assume the evaluation of two different polys at two different points can be reduced to the evaluation of one poly at one point (maybe more). 
take the input and proof info from each CC. reduce each of the add mult points to one. reduce the inputs to one poly at one point. 
hmm, it seems the input poly reduction needs to happen outside. otherwise, both CC inputs must necessarily appear in the VC input, thus blowing up input size.
huh, now i realize the inputs actually don't need to be inputs in VC because the reduction process does not involve observing the polys. but this assuming a reduction is possible. 

OMG, i got it. polynomial commitment schemes for univariate polynomials that encode VC ignorant data of limited size. That is, V_{d-1} references two polys P_k and P_j. P_j is not too huge. and the field size is not too huge that P_j can be evaluated at all possible points and merkelized. This way, VC when it needs to evaluate P_j(w)=v it simply takes a merkel path of w and verifies it leads to v. P_j should encode all info that VC doesn't care about. But now what about the randomness of P_j. The rule is, if the root hash changes, then the point of evaluation must change. for example, the root hash itself could be the random value, assume. Then suppose we go through the val process and end up with a random value to eval P_j, r1. suppose the random value determined by the root hash is r2. suppose they are combined into r for the final verification point. is there a problem with the independence of r1 and r2? Suppose P provides r2 and then churns out different values for r1, then getting values for r. so this is not acceptable unless it take significant effort to generate a valid r1 because of all the sumcheck processing time. this may indeed be a good enough reason. haha, actually this doesn't work because P_j must necessarily be evaluated at r1, not some arbitrary combination of r1 and r2. Ok, here's a formal method i think will work. We simply go through the reduction process with r1 and r2, but only check for r1. so basically we evaluate P_j on a line containing r1, but the direction of the line is dependent upon r2. so P could still keep generating r1 or r2 to change r, but hopefully there is too much work in doing either to finally find a convenient value of r. another approach might be that might prevent independent is waiting til the step when previously r1 would have been generated, and instead requiring that r1 be a source in the evaluation hash tree at the leaves, and then the root of the hash becomes r. this way, there is only one stream of randomness, so if u want control over the output you must change the input and go through the whole stream again. whereas if you have multiple streams of randomeness, you can churn outputs by pertubing just one stream, and the closer it is to the output the less work needed. however the security of streams is directly disproportional to the parallelism you can use in computation. P must wait until he is done with r1 or r2 to really begin the other. however, this might not be a problem because machine 1 maybe be able to pass info to machine 2 higher in the tree so that machine 2 can begin its work, while machine 1 finishes its work, and then sends more info. but this might not work because machine 2 probably requires all of machine 1's data from the beginning. this is an interesting problem. parallelism will be highly desirable, so when the streams are merged will need to be optimized. the way i said earlier with no parallelism is actually a case of not merging at all. the opposite, but with a tiny bit of parallelism would be in the first sumcheck using g1 as well as the root hash in generating r1 for g1. i think it may be best to simply use the previous scheme of merging the streams at the very end by using a line that would be best. and if one stream is a lot faster than another stream, like you can churn rs by validating rather than by churning by hashing which takes more effort, then we'd make the validation more expensive by making the operation bigger. In fact, I think the way it should work is depending on the computation we decide where we merge the streams. we need all paths within the stream to be at least a certain slowness, so if the operation is huge and the hashing is huge we can merge them at the end because both streams are already slow enough. whereas if the operation is tiny and hashing is huge, we pour the operation into hashing. and there should never be a case of both being slow. if this arises the computation should not exist and instead it should be merged with other computations. so these streams and their necessary slowness forms the basis for the security of choosing r for P_j. 
i think P_k will just consist of the reduced set of assertions for add and mult. 

lets review, in general for sumcheck, g1 0:rs, 1:x, and m-1:folds. g1 determines r1 at which g1 is later evaluated. continue and then gm has m-1:rs, 1:x, and 0:folds. gm determines rm at which gm is then evaluated and compared with g evaluated on all rs. 

phi(p,q)(b)=v \implies p(a)=v1, q(a)=v2

p(x)=Ax+B
q(x)=Cx+D
p(w)=v1
q(w)=v2

p(q(x))=A(Cx+D)+B=ACx+AD+B
q(p(x))=C(Ax+B)+D=CAx+CB+D
p(p(x))=A(Ax+B)+B=AAx+AB+B
q(q(x))=C(Cx+D)+D=CCx+CD+D

you know A,B,C,D

ad+b-cb-d=d(a-1)+b(1-c)
2x+3>y => (y-3)/2<x

****************************************************************************************************************************************************************
original question solution. you have P and Q as two different v variate polys. 
want to confirm that P(w1)=v1 and Q(w2)=v2
get a valid form of Q that is translated and scaled such that Q(\psi)=P
find w3 such that \psi(w3)=w2
then Q(\psi(w3))=v2 => Q(w2)=v2
does such a phi exist? big moment!
P(X,Y)= aXY + bX + cY + d = (aY + b)X + (cY + d) = ((B-D) + ((A-C)-(B-D))Y)X + ((C-D)Y + D)
Q(x,y)= exy + fx + gy + h = (ey + f)x + (gy + h)
transform Q to P
y=(a/e)Y
=> Y=(e/a)y
y=(a/e)Y + (b-f)/e 
=> Y=(e*y+(f-b))/a
x=(a*Y + b)/(e*y + f)X 
=> X=(e*y + f)x/(a*Y+b)
x=(a*Y + b)/(e*y + f)X + ((c*Y+d)-(g*y+h))/(e*y + f)
=> X=((e*y + f)x+((g*y+h)-(c*Y+d)))/(a*Y + b)
so X and Y are undefined only if the denominator is 0, which means the coefficient of one of the capital variables in P is 0. but in this case that variable disappears from the entire expression of P, meaning P should not have been defined with it in the first place. this puts a constraint on making sure the poly size is bounded above the number of items encoded as tightly as possible, rather than just keeping the poly size constant. in other words, we must be sure that at least half of the encoded points are non zero, because then all the variable must be present. i think this is correct, but there may be other ways for X and Y to be undefined. oops, looks like this type of error also includes the case that a or e is 0, which throws out the term with all variables in it, which is different than the other cases because all the variable are still present. in this case the poly is then of one less degree which means it could have been done with 1 less variables, so this again falls in the previous condition that at least half points are non zero. 
\psi(X,Y)=(x,y)
Q(\psi(X,Y))=P(X,Y)
they are identically the same because after composing \psi in Q, you are able to do reduction (cancellation) that leads to P, because the denominators are never 0.
we have P(t)=v, \psi(t)=w, Q(w)=v
P(t)=v => Q(\psi(t))=v, \psi(t)=w => Q(w)=v
so the goal is to find an (X,Y) such that \psi(X,Y)=w2
so when computing (X,Y) we know we want the values (x,y)=w2. this means we can apply the algorithm iteratively. 
so y = (a*Y+b-f)/e = (t1-f)/e
and x = ((t1)X + (c*Y+d)-(g*y+h))/(e*y + f)
and as we can see some dynamic programming will be useful
... anyway, this algorithm should only take poly log time. 
****************************************************************************************************************************************************************

find t such that \gamma(t)=w2
q(\gamma(t))=w2 => q(w2)=v2
you can evaluate q(\gamma(t)). you just need proof that its correct.

p(1)q(0)-p(0)*q(0)+p(0)=q(0)[p(1)-p(0)]+p(0)
(A+B)D-BD+B
p(1)*q(1)=
(A+B)(C+D)=AC+AD+BC+BD

aX+b
aXY + bX + cY + d = (aY + b)X + (cY + d)
aXYZ + bXY + cXZ + dYZ + eX + fY + gZ + h = (aXY + cX + dY + g)Z + bXY + eX + fY + h = ((aX + d)Y + cX + g)Z + (bX + f)Y + eX + h

2^{v-1} lines.
\psi1: (aX+d, cX+g, bX+f, eX+h)
\psi1(a,b,c,d,e,f,g,h): (aX+b, cX+d, eX+f, gX+h)
\psi2(a,b,c,d): (aY+b, cY+d)
\psi3(a,b): (aZ+b)

\psi(X,Y,Z) = \psi3(\psi2(\psi1(a,b,c,d,e,f,g,h)))

i want some operation on p and q that yields a poly s of same variate and same degree, that is a line.
if i poll s at certain points i should be able to find the values of p or q at certain points.

s = (AC+CA)x+AD+B+CB+D = 2ACx + B(C+1)+D(A+1)

suppose q is one degree less than p. 

two lines cross somewhere, Ax+B=Cx+D => (A-C)x=(D-B) => x=(D-B)/(A-C)

after ********************************************************************************
ok, few, i think its valid. 
what can we do with this? a VC takes as input the two add mult point sets, and the two inputs, and reduces them in half, which is the output. since this is multi output, it should be expressed in input. so for input there are two sets of proof data, three add mult point sets, two inputs. the VC verifies the proofs, reduces addmults, reduces inputs. 
oh, gosh, it looks like input still blows up. so simply put, we must have that input of a VC does not contain full inputs of both CCs. 
what about multi inputs? what's interesting about this case is we know ahead of time at what points V_0 must be evaluated. so the first layer of \gamma lines can be determined ahead of time. but these lines determine where they are evaluated, ...
what about using same input polys, but different addmult polys. so VC would receive the addmult points as usual. but unlike input polys, the definition of these addmult poly is known ahead of time, and therefore their \psi function can be encoded ahead of time as part of the VCs logic. so the VC would need to find the input values for \psi, then reduce that point with the other for a single addmult point. in fact, it seems all addmult polys could have a \psi that converts it to the same poly. in fact, they could all lead to the standard poly with all coefs at 1. there can then be a single output which represent the single random point at which that single poly must be evaluated, or it could output 0 if there is some error. so when the VC encounters the two points of input poly evaluation for the two identical input polys of its CCs, it reduces it to one point. it could output this as a second output in addition to the addmult point. Or i guess better would be that the input poly is reduced to the standard poly, because its definition is known ahead of time. in fact, maybe instead of a standard poly, all the addmult polys can be reduced to the input poly. so there is a single output, the point of evaluation for the input. luckily this method preserves privacy. the problem is the top level logic will not be able to be evaluated, because the VC does not know their definition. 

i guess most visible possibility at this point is base polys with same logic, and VC polys with same logic. VC polys can two proof inputs, and two input poly commitements. they don't even need to know that their CCs are. they just reduce each pair of addmult points to one. they also receive two sets of addmult points ... this wont' work because the first layer of VCs will have different logic to handle the different logic of the CCs, which in turn causes, the second layer of VCs to have different logic, etc. so with all VCs different logic their addmult points can't be reduced. then it seems the only way is for the base CCs to have the same logic as the VCc, which doesn't offer a very versatile computing engine. 

ideally from the top of the tree you'd receive a hash and be confident that it held correctly computed state, so you could read any part of state with a merkel proof. correctly computed state means the top circuit only accepts correct input from two particular child circuits. in other words, the VC can confirm the logic of its CCs but it doesn't care about the input of its child CCs. so the VC just needs anything it needs to compute the proof that the output of the CCs is as claimed. maybe the VC cares about a small part of the input for CCs. I think its unavoidable that circuits will have different logics. Of course the VC cannot execute the logic of the CCs itself because this entails encoding it, which blows up logic sizes. so it will have to verify logic another way. also, the VCs input can't contain the inputs of the CCs which would blow input size up, and the VC doesn't care about the input anyway. so the VC will have to confirm that the inputs are evaluated at random points another way than evaluating them. the VC can include as input the equivalent of  at most one input of its CCs. it doesn't seem to be a problem that a VC can include as inputs multiple outputs of its CCs. this is because these multiple output should reduce to the same number of multiple outputs for the VC. 
it mapping between polys is such that you can arrive at Y just by knowing y, and then at X by knowing y and x, etc. maybe this means P could commit to the univariate function of Y, then V could choose random y and compute Y. then P could commit, given Y, to the univariate function of X, then V could choose random x and compute X. but the problem is P must commit to the entire definition of the mapping before knowing anything about the x and y that V will choose. P can do this but then its hard to prove to V that he is being consistent with his commitment. 

essay wednesday, numerical tuesday and thursday, complex hw monday, complex reading friday, stats next week. 
https://www.colorado.edu/amath/sites/default/files/attached-files/appm4360_hw_2_1.28.18.pdf

use fourier transform. current thinking: P can commit to all the periodic functions, which should be simple expression only be amplitude and displacement. then V chooses a point and opens and evaluates the functions at the random point and adds the results, until it is confirmed that the answer is close enough to the claimed answer that P must be right. this may be better optimized by evaluating at two different points for fewer functions than one point for more functions. if this doesn't work so well in a finite field, it may be an invitation to move into continuous fields.
before exploring this, should confirm its implications. the input to a VC would (actually no merkel needed) include a series of components that define the input poly. these components would help choose where the random point is they should be evaluated at. then they are evaluated and the results added together and then compared with the claimed value. the question is whether enough components can fit in the input to define the function far enough. for now lets assume its a univariate poly. (similar idea for matrices, where the SVD diagonals are like the components, cuz you only need the first few.) degree testing should not be necessary because V is evaluating himself. maybe the reason fourier has not been evaluating the components requires arithmetic with complex numbers which may be hard to represent in a boolean circuit. so what we care about is how quickly the component summing will converge to the correct point. so after a number of components, the difference is less than the sum of the absolute values of all remaining components. this means each component will cut your error in half.
this is getting worrying. plotting the coefficients i don't see the magnitude decrease very fast. this tells me it won't converge very fast.
it would be great if V could find the average within an area. i suppose a simple way to do this would be for V to make multiple queries to the same area and take the mean. the sampling points might be chosen strategically by spacing the by the same period as final component. this is because the ups and downs of the summed function i think will likely have that same period. so if you sample two places that far apart and and you take the mean, you'll end up in the center. the only doubt is that the period of the function can often be larger than that of the final component. but i don't think it can ever be shorter. 
the procedure is: P defines func approximation. V chooses random point. P claims answer. V checks if answer lies in approximation. the problem could be that after V chooses, P can compute the range of approximation and claim any value within that range, because all such values will pass. So we want to know the probability that any of those points can be bad points. what is a bad point? well actually we have to remember that the value is not directly claimed by P, but rather derived by V by picking a random point and plugging it into a univariate poly. so lets assume the value is completely random, unpredictable by P (which is optimistic). Lets suppose its the wrong value. Then soundness will break with the probability that the random value lies within the approximation range. so the probability of error is the hight of the approximation range divided by the height of the defined function. 


xyz
z: xy, x
y: xz, x
x: yz, y

how about for every variable do sumcheck, where each round a certain permutation of variables are present. since adding the derivatives they'd have to be multiplied so their summation results in the top derivative. so the function is a derivative function. 

maybe combine fourier/spectral estimation with the technique of P evaluating at as many points as there are encoded values (so no low degree testing is necessary), and then V opening the evaluation of the two closest points to the random point. i'm thinking there could be a number of methods, such as these that P commits to in defining the function. and V would open each one at the point of evaluation to help approximate the answer. for example, another method could be the integral, eg to get an idea how much (and in what direction) the curve moves between evaluated points. omg, what if P just commits a separate mini poly for each segment between committed points. or i guess it would some function (not necessarily a poly) that best approximates what's in between. this would probably give P opportunity to present the curve differently. so i think it needs to be that every method P uses to specify the curve must only add new information. maybe the integral, function, and derivate would be a good group, and they could each even be evaluated at different offsets. i think this in general is a good method because it take advantage of P being able to evaluate for V, and then V's only job is to guess the function. 
keep in mind, if trig poly, there are infinitely many derivates, so maybe only need one. 
maybe used line integrals to obtain info about a region (eg closed line to get total curl of a region, or flux or circulation). this is vector calculus. 
maybe the actual interpolation of the input doesn't need to be decided until the second to last layer is reached, because only then must P begin evaluating the input for random values. V decides those random points, so maybe V can help decide what the interpolation should be. 
	at layer V_{d-1} there's the sumcheck, throughout which V chooses the random points upon which V_{d-1}'s term will be randomly evaluated, and which V_d will be evaluated. P will be evaluating V_d on these random points as well as single degree polys throughout the sumcheck. this entails the P must know the results of these evaluations and thus know the definition of V_d. In fact, P must know the definition just to compute g1, which happens before any random points are chosen. therefore this technique won't work. 
taylor series work for any analytic function, and they could be committed instead of the evaluated points. 

why not just make the poly piecewise. its still a poly.
maybe can transform problem of evaluating at a point, to problem of finding a certain point (eg a root), maybe of another related poly. 
	prove that g(r)=v. suppose f(v)=0. then f(g(r))=0. that is, if V knows that f(v)=0, then the problem can switch from verifying that g(r)=v to verifying that r is a root of f*g. 
what is the danger? suppose P used a very high degree poly. back to the lemma, P(h(r)=f(r)-g(r)=0)=1/deg(h). That is, P(f(r)=g(r)=v)=1/deg(max(f,g)) when f and g are different and probability 1 if the are the same. so in the case of proofs, V holds what P claims is v, which V calculated from a univariate poly. So then if P can prove that g(r)=v, it is evident that f(r)=g(r) with probability 1/deg(g). so high degree is bad because it means there are more instances of the domain for which f(x)=g(x), that is where the two curves cross, and yet f and g remain different. So, intuitively, the danger is that the more the function jumps around, the higher the probability it agrees with the claimed value without being the correct poly. 
maybe it could be useful to have P make a commitment regarding the x variable in the poly when sending g1, and then evaluating on r1 and the y variable, and sending a commitment to the y variable, etc. at the end V would open each of the variable commitments and somehow from them evaluate the total poly at r. P could use r1 to decide the form the y variable takes in the poly before making a commitment. this would only be helpful if it makes V's evaluation on r easier, and it would only work if V has right to expect that the evaluation should be easier, or else P could claim its hard when its not in order to make V's approximation less accurate. considering this method of single-variable commitments throughout the sumcheck, it would be great to have a function that could be evaluated at a random point by evaluating the separate expression for each variable and then combining them somehow. 
an interesting way of function is the linear combination of e exponentials. their derivatives are easy to take. also, a denominator of them can be raised to the top and multiplied with the numerator, resulting in another linear combination. they could be multi or single variable, and i don't know what degree those variables would have. the most basic for would be a sum with terms of the form a*e^{b*x} for constants a and b. or maybe same thing with logs. 
what can we exploit? that the values can have any set of possible values, as long as there are two or more such possibilities. but the values can be in arbitrary order. we can force there to be a certain number of values. the values can have equal spacing. in general, we need the function to encode enough information. there has to be enough possible variation in the function to encode the info, but little enough to make the shape between points as similar as possible. we want P to give as much info as possible, but without opportunity to make any false claims about the function. 
theres an input function that encodes all the input points and doesn't have a lot of moving up and down. P commits to this function. V must evaluate the function at a random point r. V retrieves from the commitment all that is needed to do so. 
we'd want a proof that two splines based on two transcripts subtracted from each other have minimal roots.
in multivariate setting, P could commit a set of hyperplanes. but i think the one-dimensional case is easier. 
deep learning, radial basis function network. these basis can also approximate functions. also look at Kriging.
have to find function type that is commitable, and openable, but also has the property that if any of the discrete points changes, almost all the points of the function change. this reminds me of the hash. so the spline won't work, because parts of the function can change independently.
what about the idea of meaningful hashes? can't yet imagine how exactly it would help, but idea is to have hash function take two elements and produce a third that is not just suedo random, but represents some meaningful combination of the two inputs. nested interval maps will probably produce some meaningful function, like a polynomial of degree proportional to the nesting depth. 

P is forced to reveal f(r) for his chosen f. g was committed earlier and now V wants to make sure f=g. V does this by evaluating g(r) and comparing with f(r). the condition that this works is that if f!=g, there is high probability f(r)!=g(r).  
we can assume that the commitment protocol that uses a piecewise function, allows P to 'give' V the function, and allows V to evaluate it at any point with exact precision, with P having no control over the outcome of the evaluation. 
suppose there is a class of functions, and f is one. if any point of f changes, almost all other points of f must change as well. does this hold for an overlapping piecewise function? hah, no it doesn't because if suppose one 'piece' covers n points, then only the interval of those n points will change when that piece changes. 

basically using trees the goal would be to have to trees, and to have P make a new tree with leaves composed of merging the leaves of the other two trees. V would have to verify this. so somehow the root of the first and second trees would have to reflect their leaves, and the root of the third tree, compared with the roots of the first two trees, would reflect whether the leaves where appropriately calculated or not. 
suppose a hash function defines input to a function. suppose the function is question is defined by a set of points. suppose you can 'evaluate' the function at that input by creating a merkel tree using that hash function with those inputs as leaves. the hash function would have to have the signature x^2->x for an input of type x. I think f must be one way, or else P could change the inputs (which define the function) and get the same output, essentially getting the same output for the same input on two different functions. V must have certainty that the function was evaluated at the correct input (the correct hash function). I don't see a way to enforce this. 
i think merkel trees are only helpful if V looks at certain paths rather than just the root. rebuilding a path, V could make sure the hash function is used along that path.
what if evaluating a poly was turned into a separate computation, where the circuit defines the function and the input is just the random point of evaluation? then V will have to know the wiring logic, but maybe P could somehow commit to them without the traditional fuss. in fact i should be able to picture the wiring pattern. first it takes one variable and applies, multiplies it by n/2 numbers, and then adds one to each. half of these numbers serve to multiply the next variable, and half server to add to the next variable, etc. but this form of computation requires all the data that defines the function to occur at the top layer, which doesn't help at all. but hmm, the operation at the top level could be done if we were able to multiply the leaves of tree by a constant, and add the leaves of one tree to the leaves of another tree. suppose the expression of sumcheck uses a binary input combination to select exactly one path of a merkel tree. this has the problem that a random combination doesn't correspond to a path. if this could somehow be done it'd be easy to perform arithmetic across merkel trees. so i'll think about operations on merkel trees. the other option is split the data that defines the function over all the layers. how many layers depends. the idea is that this separates the large poly into multiple smaller polys, which we were seeking before. but now i realize before we'd only have to evaluate one of the parts, whereas here we'd have to evaluate all of them. 


multiplying leaves of merkel tree by constant. 
actually sumcheck wouldn't work anyway cuz we don't wanna add the leaves. but what we could use the sumcheck for is doing operations involving the leaves like multiplying each of them by a different number and summing the results. in other words, it would allow for dot product. 
so challenge is two present a tree in a way that has the properties of an expression. i don't even see that the function must be continuous, just that if any of the leaves changes, the whole function should change. To evaluate at a point, you should only have to examine a few paths. in order for every leave to affect every evaluation, every vertex along the path of evaluation must affect the evaluation, so in all cases, at least the root node will affect the evaluation. well actually better than saying it relies on the vertices of the path is saying it relies on all the vertices provided in the merkel proof necessary to construct the path, and the vertices of the path will also be dependent on these vertices. lets find the simplest case and see why it doesn't work. we know that all vertices but the leaves are random, so P cannot construct them strategically to help reflect a certain function. thus the only way to encode points seems to be to put them in the leaves were P does have control. thus there must be a way to evaluate that just yields a leaf, without the noise of intermediate vertices. 
one option is to have the evaluation multivariate with logn variables. each variable will correspond to deciding on one branch. a value of 0 means take the result of evaluating the left branch on the remaining variables. similarly 1 for right branch. this enables evaluation of the leaves. but now what about values other than 0 or 1. i imagine it will mean evaluating somewhere 'between' the left and right branches. i thought it would mean using the left and branch hashes and giving them a certain weight, but this would not be consistent with the leaf evaluations which have no concept of weight. suppose you take the first bit of each variable, select the corresponding branch, and multiply the hash of the other branch by the remaining bits of the variable, and add the result to your final answer. then repeat beginning at the branch selected using the next variable. the multiplication operation could be anything that yields 0 when one of the elements is all 0, such as AND. 
field size will likely be 64 or above bits. the number of variables will likely be 24 or less. the number of bits in a hash will likely be 256=4*64. so the trouble is the hash size is larger than the field size, so how can the evaluation manipulate the hashes such that the evaluation returns something the size of the field. i imagine this should be done by making the field bit size - 1 a divisor of the hash size, and then the operation involves a recursive operation of the v-1 field bits with the first v-1 hash bits, and then combining the output with the next v-1 hash bits, etc, to produce a v-1 bit field element. something else would have to decide the last bit, for example the parity of the path taken. 64 is a divisor of 256 so this works. the first choice for an operation would be XOR because it doesn't degenerate to 1s or 0s like OR or AND would. but uh oh, remember, we need AND. so maybe what we could do is use AND for the multiplication and OR for the addition, but I suppose XOR could also be an option for addition. Another option is NAND for multiplication, and XOR or NOR for addition, in which case the bit value of the leaves i think must be inverted.
does the property hold for this function? if a single leave changes, all evaluation are changed. but if multiple leaves change, its possible an evaluation could remain the same. 
the only reason to restrict operations to boolean bitwise operations is that the univariate expressions in the sumcheck will involve bitwise operations. also, its simple, and also many operations are known in it (all those for microprocessors). best would be if operations can be custom and of multiple types. 

how about computing a hash using sumcheck. idea because its redundantly coded, with simple logic and small input and output. actually the input would have to encode the constants used in the hash function as well. 

how about reducing two points to one. the idea is to evaluate on an expression that can output both points. this probably means a linear type expression A*t+B for the variable t and vectors A and B, then plugging in t1 and t2 for t yields the two points. i think the tree function can be evaluated for this univariate expression and reduced to a

i suppose what's desirable about a finite field is that nested expressions of one variable can be reduced, due to the distributive and other properties. but then we need methods within the field for extracting the first bit of a variable, that is testing if the variable is 0 or 1. 


MARCH 31, 2018 ---------------------------------------------------------------------
continuing from where i am now with both proofs, and treechain-questions. I will leave those files unmodified. i want to re-explore a redux-style blockchain. the allure of selectors is that cells can respond simultaneously and independently to a message by both performing their respective transformations to state in response to the output of a selector that they both read from. what exactly are selectors? idk. maybe they could be computed by means of proofs. even if selectors could be reliably computed, still a challenge is making messages atomic on whether or not they trigger a transformation, this way slices don't get out of sync and thus represent a corrupted state. maybe proofs could coherently encode messages to avoid verifying each one separately. ideal would be that every round the batch of messages could be encoded, and then the state could be transformed with the message encoding and the previous state as an input, and a single proof could yield the new state. GKR style could be used with the output the root hash of the new state. i imagine state would still be a merkle tree, but maybe with a more math friendly hash function. or another possibility is to have the input as a real number vector, and state as a series of vectors as well. all vectors are input to the neural network, and outputs of certain neurons constitute the new state. i would just say the output of the last layer is the new state, but we might well want intermediate layers to have recurrencies. in fact, if intermediate recurrencies were only fed back into the same layer (didn't jump back in the network) then the whole network could be pipelined, layer by layer. well actually then the output would be delayed by the depth of the network. if we could somehow allow recurrencies to jump back while still pipelining then there could be no difference between intermediate outputs and the final output. pipelining here would actually be natural because it just means all layers are processed simultaneously. ok, so now each round would mean processing each layer. we can assume for now the number of layers would be fixed as well as the wiring topology. as a result, it seems appropriate that each layer could be processed by a separate proof, and they don't need to be tied together like cells in a merkle tree or something because they are not dynamic. i'm also assuming the number of layers is not too huge. so each proof is a matter of collecting the correct inputs, using the weights vector for that layer, and computing the outputs. all polys would be the taylor series merkle tree type. every round, the proof of each layer would be broadcasted and verified by everyone else (assume few enough layers every node can verify the proof of every layer for every round). the proof data might in fact be fed back into the network and stored there for anyone that wishes to retrace the history. the challenge then is coordinating the proof making of each layer. I think the proof method will need to be constraint based, not GKR. there would be the input poly, the weight/bias poly, and the output poly. the task would be to construct g out of the constraint, input, and logic (weight/bias) polys. this would involve constructing the intermediate polys for the sumcheck of the dot product. then g would have to be divided by r, but remember r is a product of terms of the form (x-a) for numbers 'a' where g should vanish. in this case we could maybe use partial fractions to spread out the division among multiple nodes. maybe it could be done recursively in a tree structure by always splitting into two partial fractions and then summing the results. i'll need to look at how to best solve the linear system for partial fractions. 


what if the community made a massive preprocessing effort to make a merkle tree of taylor series for every possible input. The merkel roots would be certified. maybe some 'basis' series could instead be committed, and then the prover could give meta data on how to transform them appropriate for the prover's custom input, but leave the verifier reassured that the meta data modifications will not render the commitment invalid. like the meta data could specify the depth of a valley or hill, increasing the range or available inputs. the basis commitments would be every possible combinations of hills or valleys, ie every binary combination, a 1 for a hill, a 0 for a valley. Suppose P could commit a poly such that V is assured, not necessarily that its a poly, but that is a continuous function with a limited number of hills and valleys. the hardest part is having an interpolation technique that lets meta data specify a poly. maybe its possible all polys extended from the same base poly could all have the same roots. remember that any smooth function could be modeled with a polynomial, by applying approximating the whole thing with one long taylor series. 
best possibility i see now is prover looks at base function, sees what modifications are necessary to make it an interpolating function. then for each approximation slice (by a taylor series) of the certified commitment, the prover looks at what modifications to that taylor series are necessary to reach the provers new function. Then the prover commits those changes. The first challenge is finding a base function form that can be modified to yield a custom interpolating function. Whats important is that the modifications to achieve the custom function maintain the coherency of the function, which probably means they should be valid modifications on the base function. a base function of sinusoids would be nice. 
one idea for a base function is, again, a poly that has a certain sequence of hills and valleys of various widths, probably specified by roots. The only operation on this base function that preserves the roots is multiplying by a constant. Instead, the prover will interpolate a poly choosing the base poly such that the hills and valleys correspond with the interpolated points. Then the interpolated poly will not have the same roots, but should closely resemble the base poly in terms of up and downs, and hopefully a similar spacing of roots. The certified commitment would have a taylor poly for each hill or valley. the modifications the prover commits to would be modifications of the taylor polys. important to notice is that the interpolate poly should have the same degree as the base poly, interpolating as many points as there are roots in the base poly. now i realize the base poly can't just be a product of (x-x_i), due to rungs phenomenon, and for the same reason the interpolation nodes should not be equally spaced. with rungs phenomenon numbers would get too huge. what if we use chebyshev nodes? The roots are no longer specified, because the polynomial is in terms of specific nodes. One basic approach for the base function to be a regular chebyshev interpolant with every possible -1 and 1 sequence. Even though the nodes not equally space, hopefully with chebyshev polys the hills and valleys are about equally spaced. The idea is the base poly should take the same hill and valleys as the data poly. The prover can ... i think too many things can go wrong with any type of this technique.


Apr 19 2018
transformation of univariate polys to multivariate polys, but the transformation only goes one way. this means evaluating a univariate poly at multiple points can be reduced to evaluating a multivariate poly at a single point. so it would be great if we could figure out how to reduce evaluating some multi-variate poly (even if just bivariate) to evaluating a univariate poly. something like drawing a line the multivariate, creating a univariate poly, and evaluating along it. the challenge is making sure the univariate correctly represents a line on the multivariate. 
start with univariate encoded with taylor. wish to validate its of proper form. given claimed encoding in bivariate form, again with taylor. want to evaluate at random point. choose random line passing through point. request univariate poly of this line. suppose its given and by induction you know its of proper taylor form so you can evaluate it wherever. now must use this valid line poly to validate the bivariate poly. 


Apr 21
how to use proofs to scale
state is giant merkel tree, each slice has a reducer function that computes a new version of as a function of unverified incoming messages and the previous entire state.
the reducer function can be implemented with recursive proofs, so we just think of it as regular programming. 
the reducer function processes one message at a time, and only accepts consistent sequences of messages. this way any valid sequence will not contradict processing on any other part of the state being processed in that round.
it finally outputs a new hash for its slice.
proofs then merge the slices back together (assume binary tree), producing their own hash, until the root is reached, which merges with the previous block.
each vertex merger proof will only accept one version for its children. its doesn't matter how the network organizes itself to produce the proofs, or how many are produced. at worst, it couldn't verify the absence of its children so it might accept an empty child. 
the network must organize itself to make sure the data from slices is kept available. 
data availability problem? the one solving the proof to produce the next state does not publicly share the new state itself, only the new hash. this way the new state becomes verified but the state itself is inaccessible. solution: have the state register the nodes that hold state. a non-merge proof must follow each processor proof and verify the signatures of enough state holders who acknowledge that the new state has been shared with them. problem might be false reporting by the nodes. 


classes are more important than their complete problems

time complexity classes are closed under multiplication
space complexity classes are closed under addition

efficient generally means polynomial-time

low-level approach is generally about lower bounds

cryptography is an application of complexity theory

asymptotic notation
f(n) = O(g(n)): f <= c*g holds for all n, and some c
f(n) = o(g(n)): f < c*g holds for all c, and sufficient large n

[n] = {1,...,n}
natural numbers start with 1

two task types: searching for solutions, making decisions

models are uniform or non-uniform

methods, models

problem instances have implicit solution, computation makes that solution explicit

computation environment: repeatedly applying simple rule to large environment.

Rice's Theorem: an algorithm cannot look at a program and determine non-trivial properties about it (eg if it halts).
a property is non-trivial if it varies for computable functions

undecidable: an algorithm that always outputs yes or no does not exist for such a problem

alphabet has symbols, word is list of symbols

universal machine: equivalent to Turing machine that takes <M> and x, and gives M(x). necessarily partial function, because total function is uncomputable.

partial function: one that may not halt
total function: one that halts on all inputs

computable: an algorithm exists that can act as the function

a priori: from deduction rather than observation

efficiency (polynomial time) is closed under addition, multiplication, and composition

computability theory about what functions are computable; complexity theory about the complexity of computable functions

non-uniform: computing device depends on input length. devices are not necessarily uniform.

circuit complexity: edge count in terms of input length

uniform circuit family: construct C_n in poly(n) time

CNF: conjunctive normal form
DNF: disjunctive normal form

complexity class: type of problem (decision, search, promise, etc), model of computation (uniform, non-uniform), complexity measure (time, space) and limiting function set (polynomials, etc) it must not exceed. 

NP vs P: find solution vs check solution formulation. but usually formulated as proving vs verifying decision problem. 
P: set of decision problems that can be decided in polynomial time
NP: set of decision problem that have efficient verification proof system. That is have verifier that runs in polynomial time in the length of proof (this requires that witness be of length polynomial in length of instance).


So lets just make the assumption of what we currently have, and see how we could get a currency out of it. Even if other applications are possible, I think we should do a currency, especially because to do an ICO we must have some token. Remember the biggest catch will be the need for a trusted party.

Notice that if we could do the database then we could do a currency and much more. For a currency, a transaction would just mean reading and writing to the database to adjust balances. For any sort of this problem, we care about uniqueness, which is a generalization of the double spending problem. 


What if each proof was a transaction. that is, the proof must encode that a coin is transferred from one owner to another in such a way that the receiver can later reference that proof. 

What if we actually do a tree that constructs a hash tree of balances. regardless the order, at the end you must 

What if there's no notion of accounts. There's just a lump of money and 


DATABASE:
each proof encodes a segment of the db, and on every round it specifies that entire segment in updated form. this is inefficient for an actual database but for a currency db it might work. to read from the db, you simply specify the latest proof to read from. to write to the db, the segment responsible for writing must write. the motivation and proof for writing is db independent. so from here we consider it as a currency db, where each segment is an account. so we'd have to enforce that accounts are not duplicated. 



what if we had only single thread, such that the thread cannot split. maybe this c

what if instead of an account, each thread represented a single coin, and each proof passes the coin from one person to the next. all participants would know of the valid coins at any point, eg by an index, and only receive valid ones. i suppose the coin need not be spent every round, but then when it is spent, the computation must first verify the absence of a spent since the last spend. the biggest problem is you can only transfer 1 coin, not a fraction of a coin, and not multiple coins, at least not without multiple proofs. this is an interesting concept of keeping the coins stationary, rather than keeping the accounts stationary. suppose we keep the coins stationary and have users jump around to every coin then own, but coins can be in the form of a fraction of a coin or multiple coins, call these lumps. and lets add the property that if two lumps next to each other are owned by the same user, the user can bundle them together. something like this could work, but intermediate payments must be calculated and they must all be one at the same time for a transaction to be efficient, and this requires trusting peers. it would have the same challenges as having each account only able to send to a few other accounts, and having a receiver check that the sender didn't send to any of the other accounts. 

what about a butterfly network but you send on both edges different amounts. each round is a level of the network. each round each account checks its two sources. a proof can output a list of recipients and amounts. an intermediary can be forced to check its possible senders every round and process any inputs. the input it receives could force it on what list to output. this way a payment can reach its destination in log time. this is the most attractive solution so far. computation is correct, uniqueness and inability to ignore inputs are both enforced by having each account connected to a few input and outputs in the form on a butterfly network or the like. intermediate accounts can't steel funds because the code won't let them keep it unless it was sent to them. we should construct the routing network to be most friendly to account creation and deletion. but wait, a butterfly network has multiple layers of nodes and we only have a single layer, so either the network must go through all levels for a single round of transactions, or it must process every layer at the same time. I think the latter is better, and maybe routing network design will matter. 

oh, nice, a debrujin network has the same inputs and outputs for each layer. but with it comes the property that a node's connections are far away. 

we should take advantage of the fact that those in close connection (eg location) are likely to make payments to each other, and they are also likely to be closely located on the commitment tree. 

can nodes (accounts, users, whatever) leave the network? in order the receive, 

regardless the routs, what's the process? a node must construct the proof before it finds its position in the commitment tree. so it seems a node's identity or position in the network that determines its connections must be determined by its position in the previous commitment tree, or by an artificial address. if we use the previous tree, the node may have to participate every round. what we'd prefer is that a node only participates when he must send something. when a node sends something, the node must forward properly from its sources. 

we could have it a

what if a payment doesn't get delivered? fortunately it won't be given to anyone, but unfortunately the sender's account will remain deducted. 

the purpose of a network is to make sinks (and thus sources)

a node points to who it would like to pay, and 

or what if we did a routing network, but 

suppose we could enforce that an account only gets one proof each round. every round an account may choose to perform a transaction that transfers a portion of its balance to another account. it can send the proof to the receiver, who must verify that not only is the payment valid, but also that its the only payment of that value. this would be doable if we could enforce that a particular account be located in a particular place in the hash tree. what we want to prevent is two proofs from being published in the commitment tree, and both being viewed as valid. if the intermediate nodes of the hash tree are honest, we could do this. could we force them to be honest with proofs? the job of an intermediate node is to take 

virtual message space. the space must be a hash tree such that proof can verify the existence or non-existence of a message in a particular neighborhood. 

hmm, what if the coin was in relation to the trusted party. 


or think of a killer application for ethereum. actually integrating with ethereum would be hard. think of a simple application. 


what if by means of address, a particular account is assigned to verify the payments of another account, as well as verify its own sources of payment. this account. 

what if actually did tree, using same hierarchical structure as commitment tree which we need anyway. this would be simple. what applications could we use this for? we could use higher vertices not just for verification, but for analyzing merged results. so one application could be map reduce, where the leaves do the mapping, and the rest do the reduction. i actually think this could be integrated with ethereum where a construct would decide the private party, keep track of the commitment heads for each layer, and verify the final result and store the result somewhere useful. one application could be statistics. in fact, we might be able to do zero-knowledge statistics if we can do additive homomorphic encryption at intermediate nodes to hide the results from them. 


how about micropyaments? every round an account both sends and receives a number of micropayments, equivalent to the number of proofs one can verify, and since the act of receiving would be duplicated so many times it's not a bad idea. 

i like the idea of not enforcing everything is up to date, and one can prove something did not occur by actually proving the existence of it in some block far back and proving the absence of it in any block since, both via hash proofs. the reasoning is transactions that reference things farther back will less frequent and therefore can withstand the large proof size. on the extreme, referencing something so far back it requires multiple proofs to prove the absence of everything since, seems reasonable because it should occur so rarely. a good application for this instance is writing to a database. those parts that are written to often are easy to write to. so now the database concept doesn't require copying it every round.

secret keys for signatures could be easy. the secret would be the image of the hash, which is the signature, just like a password. but rather than having to send the secret to someone else for them to hash, making it only usable one-time, the secret would be verified via the hash function within the proof as a valid witness. 


one possibility is chaining the transactions, and hoping to prevent a double spend by letting the verifications converge in about log time, such that a proof will be the descendant of a double spend and able to detect it. This detection may be hard, but suppose it works. Then one branch would have to discarded, or the network would be split, because we'd have the rule that a proof that detects a double spend returns false, so its not possible to have a unified network in the presence of a double spend. Unfortunately, this could be DOS attack by constantly doing double spends such that transactions building on the double spend would all be discarded. one way to maybe prevent this is to enforce a structure to the network such that a node cannot show two different faces to two different sides of the network. instead, the neighbors of the node would be able to communicate with each other and make sure they are seeing the same spends, this way they are not tricked into building on a double spend. in case the neighbors are malicious, it would be helpful for their neighbors to be connected each other as well, etc. To enforce this neighbor rule, we don't want to restrict node communication, but rather enable neighbors to communicate. Actually, probably the best way is to have a virtual off-chain message space. receivers of a particular 'coin' can come together into the network to make sure there are not multiple of them receiving  the same coin. Even if this fails, or if the receivers are malicious, whoever those receivers send to would also come together. using the mechanism, why are proofs even necessary? without proofs, we'd need some form on coins and proof of possession of a coin. maybe it could just be done with public key system. but we'd need a way to verify that a past-holders possession of a coin is no longer valid. one way is a hash tree, where you prove u had possession at one point and show absence of transfer after that, all with merkle proofs. the other way if for the current holder to always stay online and prove he last latest possession. suppose we take the latter, where full nodes can be online the whole time. you go to a particular place on the network corresponding to the coins (which may be a spectrum, not discrete coins, and perhaps the larger the coins the more space you must go to). in this place you should always be able to find who has possession of the coin. it is there that you receive or send the coin. to send, you sign the coin (or some spectrum) and the receiver's public key, and send the result to the receiver, who then can advertise it as proof of possession. anyone can verify it is correct by seeing it's the signature of the previous owner. so suddenly in this simple scheme, if we use the discrete version, we its not possible to harm the rest of the network. it's only possible to harm individuals by giving them a coin you don't own. this way it can scale. but what if we switch to a continuous spectrum to avoid the limitations of discrete coins. problem with continuous space is when there is reason to specify quantity, it gives opportunity for inconsistency, where for example two possession signatures claim an overlap of coin space. 
in general, a problem with stationary coins is a node will have too many of them to keep track of because nearly every payment will result in token in new location. so we need algorithms and mechanism to minimize the number of locations a node must track. take the special case of one continually sending micropayments to another. i don't know exactly what to do. what's a general algorithm. it should take log transfers. in every coin space, the goal is only one node will gain possession of the whole area. in general nodes will be transferring money to each other to unify what they own. how the nodes figure this out can probably be left out of the official protocol and left to nodes, and perhaps multiple methods are used throughout the network. but one thing that will help in this process for sure is conditional payments so intermediaries can't steel. actually i realize this will likely depend on the protocol so both should be thought through together. the best shape for the network will probably be something homogeneous, which probably means round, like a ring or sphere. maybe the algorithm should go for localization, that is to reduce confusion over the entire network to confusion just over a smaller space. for this reason it would be nice to have a topology that is recursive in size, unlike a ring because a subsection of a ring is not another ring. a mathematical, ring, however has this property, so maybe have the topology of an algebraic ring. the algo would do trades around those looking and willing to do trades for their own desegmentation, and work around those who choose not to participate. a given section there will be a number of owners, and they can share with each other where they're primary lump of money is located (assume this exists by reduction or whatever it's called), and one of those lumps will be where the algo is taking place. 
what would be great is if we could move coins. maybe we could move them in a restricted way, like through a butterfly network or benes network. for benes, we could still have a ring of places, and each would have two sources and sinks. so the coin itself can move across the directed edges, and the owner with it. so some places would have multiple coins, and some would be empty. or forget the ring, the points is we have a number of places, and they have directed edges to each other. assume you can easily calculate the route to where you want. ...
it seems easy. everything has a destination, assume power of 2 locations. every round, we focus on a level (think butterfly network level) pairs are formed where each pair consists of one item from each side. Those two things switch locations. Then we should be able to reason that everything is on the right side for that level. Then we move to the next level. In the case of coins, you would find a partner anywhere on the ring who wants his coin on your side, and you want your coin on his side. turns out this works, but it only allows aggregating your discrete coins, all of the same value. it doesn't adapt easily to continuous coins. 
also turns out the original idea of no proofs and just signing isn't good enough because one with previous possession long ago can claim latest possession and unless the observer has access to the entire history of the coin (or a proof of it), he can't tell the difference. so we'd need proofs. 
whenever you have a message space for coin identities to prevent double spending, the coins can't move so the users must be in many different places at once. if coins move, then how can know the correct number of them exist at any given time. we can verify with proof that any coins are valid, but how can we verify they are not duplicated? i think considered this before, but i forget why it didn't work. That is, have a routing network like a butterfly network, where each account can some small set of sources and sinks. So same as before, but now inputs and outputs are limited. Double spending would mean a proof sends the same coins to two or more of its sinks. Whenever a sink spends, we could check that the other sinks have not yet spent the same money. but this requires proof of absence which is not possible without proof of latest versions using a hash tree or something, but a commitment tree as we explore can't reliably serve for this because a proof nodes can't be trusted to correctly construct the tree. its really hard to deal with proof holding value because they can be duplicated which creates false value, unless we enforce proof of absence through a hash tree, but this is not convenient. 
i think a tree model is necessary if we are going to support something where proofs carry value. so it'd be great to think of an application where proofs can be duplicated with no cost. what about map reduce where users can sell their data by doing proofs, assume public. we should take advantage of the fact that this is a tree. so once the tree completes, if it creates a hash tree in the process, every proof in it can prove in another setting how much it is owed. going from tree to tree, each proof can aggregate what is is owed. to prevent duplication of the same user data, or intermediate vertices, we must enforce that intermediate vertices are each assigned a range of candidate inputs, ranging over the previous tree. Since this model is based on trees, and sequences of trees which are better done with pipelining, we should switch to the tree and pipelining setting. 


trees and pipelining:
this seems the main the most promising model, because it aggregates all proofs to support proof of uniqueness and thus proof of absence. a tree is constructed and with it a hash tree of state. at each layer, the proofs reference their dependences on the previous trees. Though the previous tree could always be up to date and thus be enough, we'd like every tree to only need to give the state on parts that have updated. so a proof can reference any part of state from previous trees, and prove that each part is the latest by proving absence of it in all following trees. 

now to speed stuff up we do pipelining, where you can reference previous trees even before they are completed. but we must make sure when a tree is completed, the trees referenced within it are consistent with the previous trees completed. this takes thinking on how to do. suppose proofs are cells, and intermediate proofs are just for verification. if every cell can cross reference any data from the previous trees, it will need to present the most recent hash for cells referenced. Suppose for now only the previous tree is referenced (because it will be referenced more than any other tree). For every cell referenced, a cell in the next tree will specify its hash. But then we come to the verifying proof, and it knows the cross referenced cells are correct, but it does not know how to keep track of the latest trees of its dependencies because the references are arbitrary and don't follow the structure of tree verification construction. we may be able to aid this with localization, the assumption described below of only referencing cells close to you.

suppose no pipelining. verify two child cells by comparing the hash paths of their cross references, which should all point to previous trees in the chain. we would like to optimize this so that the time between cell computations, proportional to the time necessary to verify all cells and construct the tree so the next layer of cells can cross-reference, is not too long. 
one approach is using the fact that the state can be designed, and will take such a structure naturally, where cells only need to cross reference data in close proximity to them (respective to the verification tree structure). This is an oversimplified assumption. But then only a few layers high in the tree, a verification proof will have all its children cross referencing the same cells (in the simplest case stated these cells are each other). Now the cells would not point to the root of the entire previous tree, but just to the root of their previous subtree. So before the cells begin computation they must wait for the previous tree to reach this critical layer. Then the cells point to the verification of the previous tree that reached that critical level, and uses its hash to cross reference the child cells. Then the proof that verifies these new cells says they are valid with respect to the previous subtree. 
something like this will work, but what can we do with it? or what can we do with trees in general, ignoring the localization restriction? in general you have cells. one can send a message to another by outputting the message, then the receiver can either get it himself if the prover has incentive to do so (eg receiving a payment), or by the receiving cell can be expected to check a number of other cells for messages each round. 

what if we structure the cells as a routing network, so cell neighbors are strategically chosen so that references (and thus messages) can be sent across the network in log time, whereas if cell neighbors are just those next to you in a simple verification tree, messages can only be passed from one cell to the next. remember that cells that reference the same other cells need to be verified together. 


simple trees:
a great argument would be that no pipelining at all is necessary, if we make the intermediate verifying proofs short enough. during the gap, provers cannot start to lay out they're proof but they can perform the meaningful computation and seek other nodes with cells they'd like to draw data from. they can connect with the other cells, such that once the tree is finished, hash path proofs can be quickly sent. the verification network would also be highly structured, and with multiple (for fault tolerance) high powered nodes (for efficiency) at each intermediate vertex that might even work together to make the proof. we need to find the right combination of edges being verified by each proof (base of exponentiation), the exponent, how deep we're willing to go, and how long each proof takes which gets multiplied by the exponent to determine the total time to form a tree. I think our target should be on the order of a few billion. going from base 2 to 3 is significant improvement and little extra cost. This model may be possible. Of course we would also need a highly efficient commitment tree protocol. Suppose a single verification round takes a minute and provers can work with base 10, then there would be about 9 to 10 rounds, so total time would about 9 to 10 minutes.

luckily, i think cells don't need to always be present. instead they can go offline and come back when they please. The matter of proving they were handled when they are online instead of ignored as if they were offline when they weren't should be done by incentives of verifiers to get as many children as they can.




structured commitment tree:

lets come from the other direction and start with a routing network, and then use trees to enforce things. take the normal double spending problem where a node might make two proofs sending the same coin to two destinations, such that prevention requires checking all receivers for duplicated coins. But suppose a proof only has two sinks so only those two other proofs can receive the coin. Each sink proof could only accept the money if it verifies the other sink did not receive the same coin. but how? 
Suppose we form the network so that there is enough trust and structure such that the commitment tree can be used to prove uniqueness of proofs. The key point is that we will not trust the network to enforce uniqueness of proofs in the commitment tree. Instead, once the commitment tree exists, we will enforce that a correct verification proof can only access a particular hash path for a previous proof so that only that instance of the proof is available. I've thought similarly before, but the problem was that the network constructing the commitment tree would be unreliable enough that it couldn't be guaranteed that a particular proof get a particular hash path, at least not without a node really trusting those above it in the commitment tree. And we can't give away the property that hash path is well defined without also giving away uniqueness. But the solution I'm thinking of now is to have a network protocol that allow each node to ensure that its proof has the well defined hash path the node desires. The solution is recursive. The idea is that every round the hash paths so far constructed are sent back down the tree and the nodes make sure the path so far is correct. If a level of the tree was done incorrectly, the nodes below can get another group to construct it until its constructed correctly to their satisfaction. At the end there should be a single hash for the whole tree that is to the satisfaction of all nodes.

Before we detail the above solution, let's make sure the concept of cell addresses is doable. 
actually we might not need addresses, and instead assume the address of a proof is the hash path it is located under. in this case, maybe we have a concept of 'right to address' where if you can prove to others in the network via signature of some type (maybe even a proof) that the proof you constructed deserves a particular hash path. now i realize the benefit of addresses within proofs. without them, the network might be malicious and put another proof at your address (eg to receive funds). can this be prevented? maybe we can structure the network (assume binary tree) such that at every level if the node(s) of either of the children do not agree with the next hash they can ... idk. so if addresses are so important we need a way to prevent one without right to an address from giving a proof that will be compatible with that address hash path. to prevent other from doing it, it will have to be based on a type of signature. if proofs are zk, a signature can be hashing the secret and showing it equals the public key. for a moment suppose this question of preventing those without proof of address from getting the address is solved.

maybe we don't need a routing network. maybe given uniqueness we can just proceed as originally thought, where a proof can send funds to a few arbitrary addresses. A proof also can draw funds from arbitrary addresses, but must verify each. So every round the act of a proof is to say, I have these total funds from these addresses (likely your own address which corresponds to your existing balance), and I am transferring this must to these addresses (likely your own address again). We call this basis act a 'transaction' or 'tx'. We really would like cells to be able to go offline. This means a cell does not have to produce a tx every round, and when it's gone nobody else has the ability to claim their address. when they come back they can prove that they should receive everything from their previous account as well as anything from addresses sent to them while they were gone, and they do this by referencing those proofs where the paths include the tree chain, and showing that their own address did not already collect those by showing that nothing valid is located at their address since. 

that last part might be challenging because an adversary might be able to inject something at their address, just not a valid proof, and the real address holder then having to hash this thing, whatever it is, and show its not a valid proof. this indicates a difficulty that would be encountered in other models, that is proving absence of something in a hash tree, because someone can attack by hashing something huge then inserting it where you'd like to prove absence. how can you show its wrong? it seems you'd have to hash the same thing and also show the thing is wrong. so better would be that attackers don't have the ability to insert anything. if proofs were constructing the tree this would be easy, but if they are not, you have to trust the network to put anything at your address while you're gone. maybe you can get around it by forcing the party inserting to first insert a public key. no i guess not. i just realized we might be able to get around it using proofs, because these proofs are not about verifying other proofs. the idea would be the hashes are computed inside a proof that enforces the format. 
lets reconsider the commitment tree. we would like to keep the property that the correct formating of the tree at a given vertex is of concern only to those beneath it. normally when a parent doesn't format the tree correctly for children, the children complain and get a correct one. when a node is gone we'd like to give incentive to its siblings to still demand a correct form, particularly one where nothing is inserted at its address. this may be too hard to solve.

lets rethink how hash paths (what i was calling addresses) and cells and accounts should work together. above i quickly considered the relationship that each cell represents an account and goes with the same address multiple rounds, but I should consider more carefully. 

how would right to address serve in constructing the commitment tree? take level i. assume groups are content with their hash at level i (this is the case at level 1 because they are content with their own hash). we want a way they can be content with a new hash at level i+1. a group goes looking for the correct other group (or multiple if more than binary tree) that it should hash with. when you find a node that claims to be in the other group, you check their own right to address along with their hash path up to level i. do this for a number of nodes, try to get a sense what the most popular hash is, compare results with nodes in your own group, then hash your own group has with the other group hash you chose, and now compare this result with nodes in both groups. If the other group does not respond maybe your group will settle on the null hash for them. eventually we hope the whole network can come to consensus on the root hash. this is a distributed protocol and does not require a hierarchical network.

how do we do right to address? somehow an owner's public key must be registered with the address such that anyone can verify it quickly. it would be great to have this done with the chain itself. this means the relation is stored in cells. simplest may be right to an address is stored at the cell at that address. in this case verifying a right to address could be done by verifying the proof of the cell and making sure the public key is part of input. suppose we only create new accounts and forget old ones, rather than changing address owners. then over time the address space will become sparse, but hash path length shouldn't grow too much because of log growth. or we could keep, say, a fixed amount of accounts, never delete or add, and only transfer ownership. this seems like it could easily be done by the past owner making a new with the new owners public key. the former idea would be difficult if we add addresses sequentially because we don't know how to assign accounts. one idea I have is that we make the public key (a hash) the address, which immediately makes the number of possible accounts a huge number exponential in the size of the hash. hash paths would have in the beginning mostly null neighbors. right to address now is just an independent zk proof (or if want to use LPs should use trusted party so prover doesn't have to compute new LP for every verifier) that hashes your secret into the public key. accounts are obviously easy to create, and forgetting accounts is no problem. oh, but one big implication with this is your address is not located close to the addresses of those you are geographically close to, but is uniformly distributed over geography. 

so you produce proofs and use right of address to put them there in the commitment tree. in your proof you need to prove your right to that address or else anybody could put a proof there and a verifier wouldn't know who put it there. but we still have the problem if you're away, or for some reason your proof does not make it into the tree (very possible even if u try), you either risk others colluding to put something there that prevents you from using the account again, or if accounts must be online all the time you will lose it if doesn't go in the tree. Even if we can prevent collusion, we still have another problem related to going offline. That is, when you come online you rely on peers to provide you with the hash paths to show your proof of absence of proofs since you've been away. Maybe these two problems can be solve at the same time by giving peers an incentive both construct and provide you with correct hash paths. But now I realize even if we did that's not enough because it's very possible that even if all nodes behave well one will be offline when another is not that relies on the other providing him with hash paths. for this reason i think proofs at vertices may not even be enough to solve it. 


PAUSE
what's best option at this point?
south America not looking very great. Definitely won't find crypto experts there. And I would be lonely. best other place in world is either in US at many places, or Israel. Better plan is either getting a job near MIT, or somehow making money with job or scholarship in Israel. 
Using simple tree or variation of it if I find one I could do some kind of ICO, whether or not I participate in building. 
Most important thing is inspiration and this comes from being with people in person. I can ask questions to experts of the internet. 
Third option I just thought of is trying to get backed and living off crypto rich people. First I would have to prove I have something valuable and I would have to promise them returns somehow. These investors don't have expertise so I would have to propose that we together a team. This might go well with the ICO where we'd be working with other technical expertise and the ICO expertise of the investors. We would all work together for an ICO. We could also make money shorting other currencies. They would all be convinced its needed and maybe even I would be convinced the scheme should be done with a token. If I can verify even that just simple trees work I should easily get backing and help finding a team to build it. If I believe in the project I would prefer using not doing the ICO as a business and having an open source team. 
But one thing I know is I need the right state of mind to make progress, and this state of mind is one of peace but inspiration, not one of pressure to do an ICO. 


simple trees again:
suppose we have the model that every cell belongs to a party that has solve permission to change it. but we want cells to be able to go offline and come back online with no problem. the proofs solve one problem above, the one about a well formatted tree to prove absence, but it doesn't solve the other, the one about an incentive or even capability for others to give you the hash paths you need. maybe we could use the reasoning that vertex proofs have a high incentive to get proofs in when they are available. if no child proofs are available its no use constructing the parent so that too should be unavailable. with this incentive, when you see a proof available (by means of the responsible party making one that is valid and broadcasting it) you want to do everything you need to get it into your parent proof. one such thing you need to do is provide it with appropriate hash paths. if the incentive is now possible, what about the capability? this is still a problem.

basic dilemma: if cells can go offline, how do they prove absence when they come back online? if cells cannot go offline, how to ensure every cell gets processed every time? 
maybe we could make it so that each cell must reference its previous version in the chain to be valid. if it skips a cell and references one not immediately previous the chain can tell. I think this can translate into vertex proofs referencing their previous versions. Take the first vertex proof. If the child is present in previous round, and cell does not reference it, but parent it forced to reference previous parent, then parent will see that new reference is not correct. so we'd like to say that the parent proof must also skip a reference, which is recursive, until we reach the top when of course every one knows what the correct previous version was to be referenced. notice we not making use of a state hash tree, just a tree of proofs.
this about the idea again, starting from the top. the top proof should reference the previous proof (forming a chain without a hash). The whole network is the verifier of this and comes to consensus on what the previous proof is to be referenced. Beyond the top, we use the following reasoning. If at layer i, a child does not reference the previous child, the parent cannot reference the previous parent. Or we take the contrapositive to say that along as parents can reference the previous parent (as we established the base case), then child must be referencing its previous child. This ensures no cell versions are skipped so no double spending. 

let do this more formally, that is the statement that if a child in tree i+1 does not reference itself in tree i, then its parent in tree i+1 cannot reference itself in tree i. 
we take advantage of the fact that the parent in i verifies the child in i. so if the parent in i+1 points to the parent in i, and the parent in i points to the child in i, then the parent in i+1 points to the child in i. Also, the parent in i+1 points to the child in i+1 through verification. And the child in i+1 supposedly points to the child in i. For the i+1 parent's proof to work, it must reference the same child i through both parent i and child i+1. For this to happen, either child i is the right child and we have completeness, or the child is wrong such that parent i is also wrong, meaning parent i+1 can't point to the correct parent i. 
how is this pointing done? I think every point instance of one proof pointing to another proof is an instance of verification. of course this is already the case vertically of parents i and i+1 pointing to children i and i+1 respectively. we ask that parent i+1 and child i+1 also point to parent i and child i respectively by verification. When parent i+1 verifies parent i, it looks at the inputs of parent i and makes sure the identification of child i is same as the identification of child i in the input of child i+1, which parent i+1 also verifies. 

a more general and maybe simpler approach is that a proof must always keep track of its last instance of each child. so every round, if a proof has a child, that child becomes the new reference. if the child is not present, the previous reference is copied. if no children are present, the parent proof is not present. actually its should not be a reference, but the output of the proof itself. so a verifier will look, see if you verified new children, and if so that their output got copied to you, and if not that you properly copied the output from your previous proof. so when a cell comes back after being online, it does not reference its actual latest instance, but instead follows down from the top of the tree the latest outputs of all children. I'm afraid this still relies on requesting data from others about those latest outputs while you were gone, and that they may no longer be around to help you when you come back. 

basic problem: either we need a mechanism for data storage, or we only rely on each node storing data in its own interest. 

so its seems we're having a hard time even using proofs to let cells go offline. so what if we return to the commitment tree but only rely on nodes being able to submit proofs under their address when the want, and not being able to leave and prove absence of proof at their address while they were gone. maybe we would model cells as being born and dying, and after dying the cell cannot come back to life and regain its properties, but must start from scratch. another way to put this is a cell can only use info from the previous round, because skipping any rounds might give wrong info because it might have already been 'consumed' or 'modified' in those skipped rounds. what applications can we still implement with this model? be optimistic for a moment and suppose a node has complete control over when to put a proof at his address, in other words he can keep his cell alive as long as he wants with no risk of dying. remember, right to address is stored in the cell itself, so like any other information stored in the cell, if the cell dies, that permission dies and the owner loses it. of course a node must be online the entire time his cell is alive. when an address is lost, who gets new right to address, and how do they prove it? somehow its must be determined what the new public key is for that address, then the one with that public key can come and construct a proof. for a currency, many other cells can send money to that address, then anyone can construct a proof that receives those bids and assigns the public key to the winner. if multiple people construct this reassignment proof the one with the highest total bids should win. maybe this kind of platform would be good for a game. 
what if to send a payment, assume you have a cell, and assuming there is a vast number of addresses, you just send to an address along with the public key of the receiver. The receiver goes there and gets it. If the receiver does not show up, the sender can possibly recover it by showing a null hash at that address. What if the address space is large enough we don't need right to address. instead, start from a base case of you having a proof at an address, and you send to another address under someone the receiver's key. Receiver can show up there and get it, and if not and sender can keep his cell alive then next round he can use his cell and the null hash at the receiver end to recover the money. if a non-receiver manages to insert his own proof, he prevents the receiver from receiving but he won't obtain valid money. He can also prevent the receiver from receiving by inserting just anything random, that is taking control of that address, but unfortunately this even prevents the sender from recovering. if two valid receivers happen to receive at the new same address, the network just picks one of them for the commitment tree, and the other sender should be able to recover by validating the receiving proof and showing it did not consume its money. we should insist a sender can only send to addresses not used in the previous round, or else the receiver might end up at an address where another node is trying to keep a cell alive. So this requires every receiver (maybe sender instead) to prove that the address had null hash in previous round. 

so its all about addresses and who ends up getting one. none of its guaranteed, its just up to nodes to have convincing enough evident to convince others to let it have the address. what is guaranteed is that if value is sent to an address under a secret key, it can only be obtained at that address and only with the correct key, so even if someone not wanted gets the address the funds cannot be unlocked. as far as data availability, all we care about is that 
maybe a concept of 'knowledge of address' could be used. also, you may not need secret key to unlock funds. maybe instead of the sender sending it under your public key, it just gives you the verification data, which he will have to do anyway. only one with the verification data is able to obtain the funds. it would be terrific if no private keys are needed, making adoption a lot easier. this would be a killer feature. so in addition to eliminating use of private keys in obtaining funds, we need to eliminate private keys in proving right to address. this is probably doable depending on how we do addresses. also, i just realized we can make payments anonymous too and we don't even need zk, because all those beyond the sender and receiver can know is the hash of the proof data. 
biggest challenge now is having a way to keep funds over long time, because right now cells die if you go offline. it would be great to figure out a way for the chain to 'remember' something so it can later be presented in a proof and accepted. the challenge here is proving not only that something is valid but that it wasn't already used. one way to approach this is to make redemption something you can only do once so there is not question of whether you've done it before. ideally, you'd make a cell to receive money on a channel and in that same proof you'd send it over somewhere permanent that another proof can later retrieve from.
we're facing the double spending problem again. the first time we were looking at two instances of a receiver receiving the same coin at the same time but different locations. now we're looking at two instances of a receiver receiving the same coin at the same location but different times. we tried solving the first by enforcing that you can only receive at one location, by use of routing network. This was possible but not desirable, so let's avoid an analogous approach where we enforce that you can only receive at one time. I realize these are actually not analogous problems because before there were two senders now there only one sender. An analogous solution is the projective solution where you can receive at one arbitrary time in the future, just like you can receive at one arbitrary address. this projection technique would be acceptable but only if you can guarantee you can receive your funds.
maybe we could make an effort to design a network where hash paths to prove absence will be provided with high probability. maybe the cost of storing your money is compensating other nodes who collect hash paths even not their own. it would be a market of hash paths. there would be no guarantee you could recover the hash paths, it would just be a lot more likely. if you stay online and your peer leaves you can sell him the hash paths while he was gone at no extra cost. expanding on this, it's always the case that a group of peers can purchase from their neighbor group if they were present, and if not then you from a group together and purchase from the higher neighbor. so hash paths are an asset, and they can be verified (a proof can make sure you have a valid null hash path for a certain address range for a certain tree). 
one possible way to prevent incorrect formats is to raise an alarm. its easy to verify an incorrect format. any alarm can be countered with another alarm that challenges it presenting a correct format, which is especially useful in prevent someone from raising a false alarm about a giant string. in the end, the guarantee would be that if the network's consensus functions correctly, the tree will be correctly formatted.
we want to be friendly to light clients, so maybe we have a way they collect only when online and then send results to big storage units that are always online, then both of them are compensated. 
even if the tree is well formated, what if a path is unrecoverable? i guess the account is lost. and it is the fault of the victim to not encourage others enough to hold them. 
how are others compensated for providing paths?
maybe there can be a constant where every n rounds people can drop all paths assuming they will not be used again, and any node who wants to recover its money before it is probably lost after that point, should recover it at least once every n rounds. this way we keep track of what accounts are still alive and don't waste resources saving paths for gone accounts.
address management? if have big enough address space don't need to reassign addresses but can always find new one. what if someone puts their own proof at your address? they can't receive anything that was sent to you, but if something is sent to it, it could receive. so maybe addresses are not owned. you can show up at any address anytime, and fight for its position. 
i realize we'll need a network navigation space to find peers, but it's shape will be that of the leaves of a tree.

going back to simple trees which outputs a hash tree, have each vertex proof enforce not only are its sub hash trees are well formatted, but that they are available in a DHT, such that absence sub cells can come back online and retrieve the necessary null hash paths to recover their state.
notice that when a node submits a path in a DHT, it can be the node that pays for it and nodes online do not deserve to pay for it because none of their data is there, because it only the nullness of hash paths that they need.
what would be necessary properties for a DHT? inserting is no problem, but reading must be proof-friendly. 


using LPs we might be able to have different cells run custom code. Suppose a verifier verifies code by evaluating a multilinear polynomial at a random point. Unlike witnesses, we don't just want commitment to an arbitrary function, but want to make sure it's a particular function. maybe this can be done by first committing the program by evaluating on decrypted random query. the every round evaluating on encrypted query, as well as linear combination of that decrypted random query and the original decrypted random query. this way intermediate proofs could verify child with arbitrary programs. this is arguably equivalent to original LP construction of r and q.


when come back present latest instance not with raw data but by referencing it somewhere else that is trusted, such that you can only reference the place once, because a successful reference erases it from that storage. by the law of non-erasing, this cannot be done on your own just with fancy data structures but must be done with a network with a dynamic database that can erase your data. i realize this presents the same challenge as the original. the reason is the db must keep track of whether something was already read. unlike the stored null hash paths method above, this requires storing data actually on behalf of the absent cells. but the good part is it doesn't require intermediate proofs. I really think this is the same as the original problem, because we need a db that can handle a lot of reads and writes, where a read is basically a read followed by delete. if we could do this then we could just use it for the currency to read and write balances. 

RENTING ADDRESSES:

so the only way we could make use of the commitment tree and avoid intermediate proofs is by keeping nodes online all the time or letting the cells die. But note that to keep a cell online, a node need not construct a whole proof every time. Rather, a node can just use right to address to enforce a null hash at his address and also store the path. Maybe nodes could go offline and delegate this job of enforcing and storing null hashes to a third party that is always online. if the third party fails, the node simply loses its cell at this own loss for not choosing a reliable third party. it should not be the responsibility of the chain to make sure the partnerships work because we cannot guarantee they will, but we can give a protocol to help them. so how to delegate the 'null' job? third party must have right to address. another model would be only full (always-online) nodes having cells and purchasing addresses. Then they can rent out that address to light clients who trust the third party. Third parties might be done by a group of trusting friends setting up an autonomous cloud machine. The more reliable the full node the more reliable it should be you don't lose your balance. So if you have large balance you may like to choose very expensive third party that gives high assurance. So with the renting model, the customer is registered with the third party at a certain address. the customer acts as before except every round the customer submits the proof to the full node who does the networking and exercises proof of address to get it in the commitment tree. Any time the customer may choose not to submit, and if the full node waits long enough and doesn't hear from the customer, it submits a null hash at that address. When the customer comes back online, the full node provides it with all null hashes since it was gone. The customer should submit a hash, and the full node shouldn't need to check for formatting, because wrong formatting will be at the expense of the customer when it returns. When the customer is no longer a customer, the full node can immediately give the address to a new customer and a new proof can take the address the very next round, but of course it wont' be able to get the funds of the lost address. The relationship between customer and full node is the customer trusts the full node but not vice versa. The two can have a contract where the customer must pay the full node a certain deposit proportional to the number of rounds it the address. Regardless whether the customer submits a hash or nothing at all should not matter for the price. Whenever the customer's rent has expired the full node rents it out again. A benefit of this model is that unlike bitcoin full nodes and the like, a more powerful node is not better, but instead all that is necessary for success is staying online. The customer could pay the full node by sending payments to one of the full node's addresses. The customer could send money like this to the full node from another address, or pay it manually in another currency, before the full node gives the customer the address. A full node should have customers in the same region. i think it would make sense that a node (full node) owns a range of addresses. Since the addresses are next to each other, the node only needs to exercise proof of address for the top level. So the segment should be a power of 2 in width, eg 64 = 2^6, (and of course a full node can own multiple segments in a row) so the first 6 layers of the commitment tree are easy and require no networking or right to address. in fact, the tree shape and depth should only be enforced down to this level. below this level, a full node can create as many accounts as wanted and make the subtree as deep as wanted. So given this we no longer need the notion of a range of addresses (and addresses are no longer necessary like an index, but instead some may be longer than others depending on tree depth), but instead return to the unique addresses and a proof of address for each one. now the tree must have a specified depth so full nodes know how to interact and the timing works out so they all move roughly in sync together. notice that customers need not trust the full node for privacy or secrecy (other than when a customer submitted a tx), but only about collecting null hashes and putting its proofs at correct address so that customer does not lose account or miss opportunity to receive respectively.

So how does right to address work? so how are addresses assigned? if the address space is large enough, it could be first come first serve, assuming there are enough addresses for anyone to find a new one. first come first serve means you can have access to it if either you or nobody else had access to it last round. a proof in some (any) subtree of the address should prove the node has access to it during the previous round. actually i think it should just be a public key hashed into the address space, and the full node sends around a signed version of the address space root, saying 'this is what i want', and others can check the signature corresponds to the public key in the previous tree. Proofs do not need to handle these signatures.  

so finally it appears we have a method for a currency.

this might also serve as a database with permissioned reads by a node providing access to its proofs. Eg a company can have an address and put data up there and provide proofs and all subsequent null hashes to permissioned readers. The company can also allow writes by accepting and posting a proof from a permissioned writer.

can we expand this to smart contract platform? this means for a node must be online whenever it's smart contract is run, assuming it doesn't trust a full node to run for it which would be possible but have no privacy. but unlike payments where messages are always in the form of sending value, and thus the receiver always has incentive to receiver, in contracts a receiver may prefer not to receive and just ignore messages. One way around this is using a routing network where cells must check their few inputs for messages. validating the source would require receiving its messages, not just a reference to them, this way if validation succeeds the receiver cannot claim the data necessary for the message was unavailable. if the validation does not succeed, at least the proof knows the message was not sent, but doesn't know whether or not it's the sender's or receiver's fault. with custom code (which will be needed anyway), maybe the routing network could be dynamic. the fact that nodes may choose to not run their contracts seems limiting enough this is not an architecture worth pursuing. 

so now lets think what proof system is necessary for the currency to work, assuming a trusted party (maybe replaced with MPC). the input should be broken up into parts, one for each receiver. for each receiver, the sender should send them an LP of all other parts except their own, and let them evaluate their own manually. if sumcheck methods work we'll use that. if not, how about QAPs? I don't think so, because we want proofs that are flexible in size. ...

we want a proof light clients can outsource. for sumcheck this would be in form of outsourcing evaluation for every round of sumcheck. light clients would assemble function to be evaluated, expand into multilinear poly, and encrypt each coefficient, taking time linear in the size. the points of evaluation are public, so the prover evaluates the poly at all of them and sums the results. Think of this as the prover having a linear poly defined by the points of evaluation. To prove correctness, the client need to also issue a random query and a linear combination of the two queries. But the random query can be chosen by the whole network, and the prover doesn't need to commit the points of evaluation poly to that random query, and nor does the client need to issue it, because both the evaluation points and the random query are known to the network so the network can issue the query and come to consensus on the result. So both client and prover need to handle two queries. well actually the light clients only needs to form the query consisting of the transcript, and the linear combination of that query with the random one chosen by the network. The prover, however, will have to evaluate a points of evaluation poly and the linear combination poly, for each layer of the sumcheck. Actually maybe instead of prover evaluating whole integrand, prover only need evaluate the transcript, and the rest can be done by the light client with the help of the network. All the help by the network should probably be done by the trusted party.
For simplicity and friendliness to deep circuits which will be necessary due to hashes, lets continue assuming the vanishing sumcheck method. To make this zk, we need a zk for sumcheck, and also a zk for LP. in LP, prover sends evaluation of 3 queries. i need to learn how to prove zk. 
regardless how zk is done, i will need to design the circuit and I can do this simultaneously as I learn complexity theory and how to prove stuff.

a little problem i realized is double receiving the same sender. before i assumed somehow the receiver proof would 'remember' if it had already consumed a transaction. but what about long range, where the receiver receives the same proof again much time later? then the proof would have to remember all redemptions since then. one initial thought is to identify money so if you receive it once you can't receive it again. but suppose you receive it and send it out. another solution is requiring the client to create a hash tree of all references to payments every received at that address, and for every payment require proof of insertion. this may be possible, but the primary drawback is the space burden on the client, but maybe the client can outsource this. another solution is only allowing a receiver to receive by a certain number of rounds. the receiver must keep a record of all payments received in the last number of rounds. another way that has more over head because it requires 3 tx instead of just 1, is sender sends conditional payment. receiver sends back acceptance and 
or another way to solve this problem is extending our first solution to double spending. the first solution was for preventing sending the same coin at the same time to different addresses. the solution was to now we want to prevent sending the same coin at different times to the same address. i actually don't see a good analogous solution. what i was going to say is requiring each tx at a new address for a given account, and using the 'knowledge of address' property. maybe this will be useful solution if we can't find a better one.
another way could be to keep a time count. the restriction would be that a client must receive transactions in a particular order. the order would be from past to present trees, and within a tree i suppose order does not matter. this doesn't seem like a huge restriction on the client. but it would ensure that once a payment is received it cannot be received again. actually we'd also need to enforce order (say alphabetical) within the trees such that if too many payments are in a tree, the proof knows where to end accumulation one round and start the next. i think this is the best solution. one problem might be if the sender sends delayed, and the receiver can't include it in time. then the question is whether it was included or not, the receiver claims it wasn't included but the sender doesn't want to risk sending. i suppose the receiver could give proof to the sender that it was not included, the sender could then send again. also, the proof by the receiver could be used to recover the original payment of the sender. this could be used beyond this scenario to recover any payment sent that was not accepted, but this requires help from receiver. this may not be worth the complication for now. for now, clients should make sure they have good internet connection, and sender should make sure receiver receives it, maybe even before submitting proof to full node for commitment tree. 


MPC:

remove trusted party with multiparty computation. make it so that they together obtain the encrypted random values and the decrypted vector. and only when a number of them agree to it, make the random values decrypted. ideally, the whole network could be involved. this means no trusted setup, but rather a trusted majority!  
it would be spectacular if this could be done in the proofs. in one round each proof plays a part in generating values, and they get merged through a tree of proofs until everything comes out at the top. first thought is everyone creates own random value, then going up the tree they get combined and the consistency vector also gets computed. then proof for next round are created. then people release values, pass around their neighborhood, combine them going up the network tree, then make sure they are consistent with encrypted values. one attack is someone not giving the correct decrypted value, preventing decrypted values from begin recovered. To prevent, we should have a redundancy so that if like a majority of those responsible for a given variable cooperate it can be recovered. also a challenge will be computing decrypted vector before variables are decrypted. so you can already obtain the encrypted term for each vector. now you want to decrypt each term, multiply each by a random unknown number, then add them. the problem is we probably need people to release secret variables in order to compute consistency vector. we might need a way for multiparty decryption, where people don't learn others' keys and only can decrypt what the whole party agrees to decrypt. then we could do the above where we encrypt over separate key, compute vector, then decrypt vector. this may be a high enough priority to implement since its probably possible that I should wait to design actual currency until I figure out what this will impose on chain design. Well actually, thinking more, maybe this can be done by a separate protocol and used for all chains, and doesn't have to be done inside the chain itself, making it a separate concern. 


OTHER

maybe for light clients outsourcing evaluations, they can outsource R and F encrypted, and R + b*F decrypted, using the latter for consistency. 
maybe we could build into the model conditional payments based on proofs, where for example a light client sends a full node a conditional payment, the full node completes the job and can prove it ... actually maybe this could be required by the protocol, with every proof requiring payment to an address of a node responsible ...

for safety to prevent malware from getting your proofs and sending to your server pretending to be you, use password or finger print on phone


maybe do ternary sumcheck over -1, 0, 1. 


make games out of zero knowledge
well actually you can already do all of this


maybe compute shared data by using a common string, and both performing analysis with relation to that, proving to the other the data is true, and drawing from that the correct result. this might be alternative to homomorphic encryption. that is, for reference data R, and inputs data I1 and I2, you compute f(I1,R) and f(I2,R) and then find f(I1,I2). But notice the function must be computed twice instead of once. For example, take matrix multiplication. compute I1*R and I2*R, the party 1 computes (I2*R)/(I1*R) * I1^2 = I1*I2, and similar for party 2.


we should compose proofs for a single prover. the prover conditionally verifies sub proof under the same key. this does not pose the same danger as network composed proof because a single prover trusts himself. conditional verification would mean outputting a poly in terms of keys and secret coefficients that must equal 0.




NEW METHODS:
sumcheck, while attractive and probably possible, doesn't work with encrypted values as I was using because it takes too many commitments.
regular sumcheck would take far fewer commitments, and actually they could be chained, because commitments to polys determine random values which determine the next poly. the original problem with sumcheck is the line at the end is particular to the instance. other than making the same random variables for each instance as I tried, maybe another solution is leaving the two points of evaluation that defined the line as variables. the prover would commit to several random encrypted queries and a consistency vector of them. I was thinking the queries issued by the private party would be the two terms of the line (consisting of the random point on the line) that are each multiplied by the two random points at the end of the lines. The first problem is the point of evaluation on the line would be seen by the prover. Another problem is points that define the line would actually need to be part of the real query. 

what if we issue commitment query, and the consistency query, the solve the consistency equation for what the prover should have returned for the real query, without ever issuing the real query. This way the real query can be completely secret, but if it needs to be broken up into two parts randomly, maybe it can be reasoned there is low probability two answers will satisfy the equation, that also add up to the claimed answer of the real query. Or maybe each part of the query would go with a different commitment poly. 

what about the constraint poly div one? It requires evaluating the transcript at a number of location, and each can be done with a separate LP. 
for a circuit we want a poly in terms of the transcript that disappears on a point for every gate. Let W be transcript. Well why not just use the same poly as the integrand for the vanishing sumcheck, except rather than treating it as the coefficients of an identically zero poly of a multilinear poly, treat it as a poly by itself that should vanish on all binary strings. the root poly would be the usual beta poly that evaluates to 1 for every binary input, minus 1. now since this is a multivariate poly it has many roots, but the probability a random point is one of them is 1/|F|. assume a divisor poly is always possible to find. may choose to strategically break up the transcript into multiple parts. 
this scheme might also work for univariate. 
for each instance of the transcript in the integrand, I think two, there would be a query. we could break each into 3 parts. we would also have a query for the divisor poly, but it would be the concatenation of the other random points also broken up. maybe this poly could be broken up into 3 parts and a little more such that it can actually handle the same queries as the transcript plus one additional small query for the random variables not captured by the transcript. the proof data would just consist of the commitments for the transcript and the divisor poly. the only cryptographic job of the verifier is to decrypt the commitments to the encrypted random queries. 


first step should be verifying PCP. 

(g^j, g^{m + s*j})
group G is cyclic of order |G| and g is a generator. 
suppose one does not have knowledge of any of these values, only the description of the cyclic group. 
for a moment, suppose this encryption scheme is quantum secure such that redundant data can be encrypted. 
then this would be a secure way of encrypting the random queries. In fact, we might choose to encrypt every query this way, each query, even the consistency query, with a different g.
Then g and s are released. Verifier computes (g^j)^s, computes the inverse (easily verified), multiplies by the right side, and gets g^m. Prover must give verifier m so verifier computes g^m and finds it is correct. But this means the prover must be able to compute m, which is not possible without knowing the random variable encrypted. So actually the prover needs to be available after the secrets are disclosed to compute m. Or, the prover can provide the vector for the verifier node, but this break zk. or actually the comparison can be done in the exponent. 

how is the encryption scheme different from the additive one (g*j, g*(m + s*j))? Maybe not different at all. g should be additive generator. luckily in this case the verifier can solve for m from g*m on its own with knowledge of g. 
suppose you know m, then can you solve for s or g? divide and you get m/j + s/j = p => m + s = pj => s = p*j - m and since j could be anything, s could be anything.

if you can construct in image, you know secrets. we want contrapositive.
LS = g*j, RS = g*m + LS*s
take the left side, multiply by s, subtract from right side, divide by g, and result is m. To get j, divide LS by g. 
so for a random j and m, it's unfortunately possible to construct a valid ciphertext. 
maybe it could be enforced that m and j have certain relationships, get j is hash of m, so then it becomes infeasible to construct ciphertext. 

zk? assume LP is zk. verifier evaluates left side just with LP on the transcript. verifier also evaluates divisor poly with LP. So I think if LPs are zk, the whole proof is zk.


what about having layers like GKR, but each is a direct relation in terms of the others, which is good for parallel algos, but the expressions can contain more complex logic functions, as long as they don't require somecheck or something. First, suppose you bind to each of these functions with an LP such that you can query any of them. Then you just go through every relation (an equation between layers), and evaluate at a random point. maybe the expressions can be of two variables or other forms for more complex relationships. 
what we really need to do before exploring this further is establish theorems about what is possible with what soundness for LPs.
p(x) + p(x + 2) = q(x)
oh, the problem here is quadratic expressions in p would double the degree of q. a solution is have an h multiply q such that h is the same degree as q, and h interpolates 1 at every index. so actually h would be known by and evaluated by the verifier. 


I will now move to a new file LP to establish a foundation on LPs. I will also continue below with general methods.


BEST METHODS
suppose we have some form of LPs that enables us to evaluate linear functions reliably as long as the point of evaluation is the same for every instance.

the idea of a transcript is tempting, but for quadratic computation we must multiply the transcript. this is only maybe reasonable for univariate polys. but for transcript of size S, it takes Slog(S) time. We would really like to do the whole computation in S time. 

poly equation method
both sides must be equal, and both are polys, so much be same degree with same coefficients. degree is additive. think of univariate. terms of each poly should all start from 0. think of multilinear. in this case different polys will not all start from the same set of points. instead, different polys will each have different variables. so each side of the poly should have the same number v of variables and 2^v terms. with univariates, degree over multiplication is additive and the number of terms is additive. with multilinear, degree over multiplication is multiplicative, so the number of terms is multiplicative. really, we'd like to avoid materializing any multiplication. for example, doing so in poly div is necessary because one must divide it by the root poly. we want to set up equations where both sides are unique. I don't know if this is possible. Suppose we use univariates. Then the reasoning is a d + 1 points of interpolation correctly define a degree d poly. So if both sides are degree d, and they both must interpolate on d + 1 points, they must be equal. So as long as interpolation is done right, there is no need to manifest the whole polys, just the factor polys, which can each be evaluated separately. What about multilinears? I think they are also unique. But actually this is not enough for either type. We also need to think about uniqueness before and after multiplication. Consider univariates again. Suppose left consists of a d degree poly squared. We know the result is degree 2d. We also know it interpolates the square at the same d+1 points. But now we need 2d+1 interpolation points, so we need an additional d interpolation points. suppose we construct the right side so it's of degree d and also interpolates the squares at the d+1 points. We need to multiply the right side by h, where h interpolates to 1 on the d+1 points, but interpolates to the left side ... we would need to divide the poly, which requires manifesting. but even this may not be possible because we don't know if it will divide without a remainder. suppose it has a remainder. the extra task to make sure the interpolated points are correct is making sure the remainder has roots at each of the points of interpolation. well maybe we could do this the regular way, by having the prover divide it by a poly we know has roots at those points, then testing equality usual way. problem would be determining which roots the remainder should have. maybe this is possible by letting the prover specify which h to use, idk. well i realized the whole problem is equivalent to the typical poly div problem. as seen below, the remainder is the root poly times a degree d poly. the difference between this and what I considered is that instead of having the other poly multiply g and interpolate to 1, it adds to g and interpolates to 0 by means of t, and does compensates to fulfill the equality with h.
what about multivariate? on the left we know it encodes 2^d points. squared it encodes 2^{2d} points. Construct the right side output poly to interpolate the squares at the original 2^d points. now we need a h poly that evaluates to 1 on those same points, and must interpolate the rest. it is also a d-variate poly. suppose none of the terms capture any of the original 2^d points. Then they all evaluate to zero on those points, so adding 1 the poly will interpolate 1s as necessary. Doing this it accounts for the initial 2^d points. There are 2^{2d} - 2^d points left to interpolate. But then since it can only interpolate 2^d points it can only account for another 2^d points, leaving 2^{2d} - 2^{d+1} points uninterpolated. 
maybe this whole idea is equivalent to just taking the right side, subtracting for the left side, then testing for roots at each of the interpolated points. actually it is, so I should just focus on that. But at least now i understand we can do any relationship between polys, but the polys on both sides of interpolation equality must be manifested, one subtracted from the other, then they must be divided by the poly with the interpolation points as roots. But I think each of these steps can be done in nlogn time of the size of the polys for quadratic multiplication. Then we use LPs to evaluate any unknown polys, such as h. Can this be done with multivariate? No, because multiplication of polys gives exponential terms, and the prover will have to manifest this to divide by t. 
f(x)f(x) - g(x) = h(x)t(x)
f(x)f(x) = h(x)t(x) + g(x)

remember this method, the poly div, take SlogS time, but GKR takes S time. 
Take two linear functions evaluated at same point. 
we'd like to reduce two multilinears to one. consider the point of evaluation definition of a linear function, so now we have the same linear functions at two different points to one. standard reduction is connecting the two points, to replace each variable with an affine function. this affine function will be large, but maybe with LPs we can make it work. Then choose random point, get random query, define this to be new multivariate function, then evaluate over original points. 
Each of the two points of evaluation (the functions) are committed to as linear functions. The line is a linear combination of them, with coefficients x and (1 - x). Verifier wants to evaluate the the random point poly over this line. I don think this is possible without a consistency query encoding the functions. 

any non-hash commitments to data, maybe with LPs? make data points a linear function, and commit to it with encrypted random query. also commit to the consistency query, maybe a separate one for each query. The queries are generated by the private party and encode the keys necessary to decrypt. Note that most of the committed data will be encrypted values of univariates. Thus to decrypt them all you need is a query with powers of k with alternating signs. the verifier, without the prover, knows the queries from the private party. The prover needs to give the verifier the function, so the verifier can execute the queries, but the proof doesn't need to see it. This is a big saver, and we can return to sumcheck method.
actually maybe a single query is enough. suppose you write down all queries, multiply each by random point, then collect like terms, and that's the query. the answer should be checked against the same random points multiplied by the purported answers. But random points must be revealed after purported answers or else one could solve the almost arbitrary answers. this may be possible but requires prover committing to each answer, via hash or another LP commitment.

actually at the end of sumcheck, all we need is for prover to commit to original random points. we don't need commitment to them in multiplied encrypted form. so lets assume we reencrypt the random points under new key using scheme with g, and assume it leaks no information, so even with individual encryptions it's unbreakable. Then we can just focus on doing a function at a point.

in general, this gives a way to commit to a vector. have consistency queries that are basis. 
and how to store vectors? to make sure it doesn't change, make a commitment to it by evaluating at secret place ... i think the query will have to be part of consistency test.

(
	don't need R for zk sumcheck. can instead use LP commitment. inputs can be committed and put in hash, and sender only sends payloads to appropriate recipients. 
)


suppose we can establish that two different multilinear functions must return different univariates on the same line. 
if function evaluate on line is same as univariate, test passes

consider function types evaluated on line, with v variables. for what types will the result have a univariate form of degree v?
suppose we can prove with high probability the only type of function that has this property is multilinear function. Then if prover's function is not multilinear, then when evaluated on line it will return function different from univariate.


actually suppose line is prespecified. each term becomes univariate of point on line, defined by constants of line. so consider each item of query to be degree v poly, though many will be of degree less than v. you can query the function on this line by querying the function at (v + 1) queries defining the line. The answers should be the coefficients of the univariate poly.
a1(x^v*cv + ... + x*c1 + c0) + a2(x^v*dv + ... + x*d1 + d0) + ...
= x^v(a1*cv + a2*dv + ... ) + ... + x(a1*c1 + a2*d1 + ... ) + (a1*c0 + a2*d0 + ...)

so what if we query the function along the line 

ok i think we can assume function must have v univariate polynomial form when evaluated on random line. Can we then assume it is linear function? 

so assume role of LP is only to bind prover to function of v variables. now argue that to pass the test this function evaluated over the line must have univariate form. Argue if has univariate form on random line then must be multilinear. 

we can assume function is a multivariate polynomial, and we'd just like to test it is low degree d. so if we evaluate on random line, what is probability degree of univariate is d? first consider case where poly is of degree d+1, then d+2, etc until we reach d=|F| which covers all possible mappings.

my initial thinking was the function need not be low degree, because for a given point, probability it equals random point is 1/|F|. But problem is point is not random, because comes from prover maybe not directly but through poly. So if prover gives correct poly it will work, and yet prover could give correct poly without function being correct. so we need to test that if poly is correct, then function is correct with high probability. Thats why now we investigate probability function is incorrect when poly is correct.


suppose function is of degree d, just not multilinear. Then polynomial will be of correct degree, still agree with function, and so test will pass. need other method to test against this.
Otherwise, suppose functions is of degree > d. 

if multilinear, this uni is right.
this is point along line

continually, I can ask for evaluation along random line. Pick random point. Get correct value of function at that point. 


make sure f is linear in each variable. this gives affine function. 

f(x,r2,r3), f(r1,x,r3), f(r1,r2,x)
given any of these, can compute f(r1,r2,r3)
if all are affine, the function is multilinear
suppose function is not multilinear. and degree d > 1 in a variable.
now we look at two univariates, one of degree d, the other of degree 1. what is the probability they agree on a random point? i think d/|F|.
what is probability degree d univariate passes linearity test? 
oh, easy, just query along that line at 3 random places. if they don't line up. 
or why not just test 3 places for each variable?
suppose you sample at enough points you make sure f is close to linear in each variable. 

could just query along a number of random lines to make sure function is multilinear
if P(reject) > p, f is (1-p)-far from multilinear.
if p-far from correct form, then passes with probability (1 - p)
suppose we query q times. then 1-(1-p)^q is the probability at least one query is not on the line. 
we want negligible P(passes & not all queries on line) = P(passes | )
the greater p is, the greater 1-(1-p)^q is, which is probability not all queries are on same line
the greater p is, the smaller (1 - p)^h is, which is probability test passes
so balance these equations such that whatever p is, ... 
at least this works with univariates
notice that for sumcheck, we care about all queries to function, which occur all throughout protocol. but maybe we can make those queries test queries for example by testing at 0,1,r1,r2 for each variable where r1, r2 are two random points for that variable at end of sumcheck.

what we want to make sure of is that prover could not have encoded multiple functions in the same one. That is, every time we query we want with high probability that we recover the same function, such that we can treat this function as the unique function


or take two points of multilinear evaluation, expand into linear queries, make line between them. then query at 0 to get first result, query at 0 to get second result. then perform h extra queries on same line by choosing points other than 0 and 1, to make sure it is linear on that line.

in general, i think we want to use the following reasoning that comes in two parts. first part is that if poly evaluated on a certain set of lines is appropriate degree poly, it is high probability total function is correct degree. second part is arguing that by sampling at enough places on the lines we can get with high probability it is appropriate degree. 
since we're always querying along lines, maybe we can restrict attention to univariates. suppose by querying univariate enough we can get with probability U that univariate is correct. 

we want to assume prover was using a correct formatting. so take into account all places queried, and make sure forms valid poly.
we can establish with lines it corresponds with high probability to some valid form. now we also need to establish this form is unique.

whatever the function, it can be in form of multivariate poly. we evaluate on random line. high probability over choice of line this corresponds to univariate poly of degree same as total function degree. if poly is not of correct form it will be of too high degree. 
so we go in one of two ways. first way is prover gives what claims to be correct uni. verifier queries this against actual uni of much higher degree to see if they are same. second way if verifier just samples actual uni to see if correct degree.
problem is, what if uni is close to correct, but not. this is case when high degree poly interpolates same except for few points. 
maybe take high degree poly and get probability on random line that it mostly resembles low degree poly. on random line we should get random high degree poly. so what is probability majority of points lie on same line? actually uni will not be completely random. 
what would be great to establish is that if the poly is close on some line it must be far on another line, eg orthogonal line that verifier can test. 


what if ask for linear to be bivariate. 
take univariate. destructure as to form g(x,y) = (an*x^n + ... + a1)*y + (bn*x^n + ... + b1) where f(x) = g(x,x^{n+1}).
this would be useful if its helpful to instead evaluate on 2 univariates of half degree.


we want to make sure we bind to some function. assume regular r,q,r+s*q. then q can be anything in the clear. for multilinear query, prover can treat first v variables as only query.


for univariate, what is probability f(u) + f(v) = f(u + v)? by swartz zippel its probability d/|F| where d is the degree of the univariate. it holds with probability 1 if d = 1. but it doesn't hold for high degree polys.

we need a way to fight very high degree polys. these are the kind of polys where they look like low degree but have some displaced points. so there is no way to test for this just by querying. it would mean destructuring the poly, but LP is not suited for this.

it looks like we must use the distance approach. existing approach show how to test it is low degree. suppose the prover committed to a polynomial. we care about the expected point of evaluation. unfortunately the prover has partial control over it. prover returns univariate that will be evaluated to get expected answer. prover could choose this univariate so it agrees with what will be evaluated as much as possible. 
suppose test passes. then you have probability function is of correct format. but we need to show this extracted function is unique with high probability, regardless of queries, otherwise we lose the randomness because prover could construct different polys for different random evaluation points.

take a uni. this type seems easy. query it at a lot of random locations and with high probability you'll extract the same uni each time. suppose the function is distance D. that means its the distance from the closest degree d poly. that means find the degree d poly that it most resembles. then D is the portion of points where the function does not lie on this poly. if its equally far from multiple polys, choose any. for h points, what is probability all of them lie on a degree d poly? or suppose first we sample d + 1 points to recover a degree d poly. suppose for a moment the points used to recover it are known to the prover. then D is not necessarily the distance, but instead portion of points that don't lie on this unique poly. so now we sample uniformly from these other points h times. the probability one of them avoid D is (1-D), so probability it avoid all of them, and pass the test, is (1-D)^h.
P(distance > D & test passes) < (1-D)^h
P(distance <= D & test passes) + P(distance > D & test fails) + P(distance <= D & test fails) 
= P(distance <= D or test fails) > 1 - (1-D)^h
so pick upper bound for D that is necessary, then determine what h must be used.

now take multivariate function, and assume by above method can determine that any line is < D-far from correct degree uni. to start, take bivariate case, both of degree d. then the number of terms is d^2. but FRI has a way to reduce question of degree about biv poly to same degree question of uni. actually it does this where there are only 2 rows. 

if we take a random line, then it is < D-far from uni with high probability. we'd like to relate the distance from uni along the line to the distance from multi of the whole. P(f(z,z) != U(z)) < D, P(f(x,y) != F(x,y))
for now forget about extracting a unique multivariate. instead think of extracting some multivariate. 
do inductively. suppose f(x,y) 

why not just use linearity test. suppose it's D-far from linear, and D < second root, then probability it passes is 1 - 3*D + 6*D^2. If D > 1/4, probability it passes is 7/9. lets take each case separately.
we want repeat the test h times. use the same analysis as before, where for Q queries, we want the probability they are not all on line to be small. 1-(1-p)^h
general problem is distance must be negligible if we don't want a sample to land on it. and reaching that distance, especially if it starts at 7/9, takes way too many queries. one solution is to not care about the distance for querying, even though this model is convenient, where you say the tests (which are independent of the query) will always recover the same function with high probability. the other model is where the queries are dependent on tests, and actually the queries are part of the tests. here want to make sure recover same function. we want to make sure that if the verifier does not reject, then there exists one function that dominates the encoding of the commitment, and can be extracted. Then we want to query this extracted function. 

what if in GKR, prover just sends evaluation on encrypted values. then 
(g*j, g*(m + j*k1))
(h*i, h*(m + i*k2))
m/j + k1
m/i + k2
m/j - m/i
what is simply use reasoning as before, where encrypt under two (or more) keys, and the only way the prover can produce two ciphertexts that will decrypt the same is by applying the same affine function to them. i suppose to start, only the variables are encrypted, not their multiplications. if the prover knows the key, out of |F|, the the prover can produce any answer desired, and encrypt under the two keys. also important is the prover doesn't know the point of evaluation, and so cannot adapt answer.
start abstract. assume same points of evaluation, say m independently random variables, are encrypted under two separate keys. prove that nothing can be learned about the encrypted variables. Then prove by contradiction. Suppose the prover could produce two ciphertexts that decrypt the same but are not an affine function of the encrypted variable sets respectively. the show the prover could find the key. prove this independent of encryption scheme, and then find post quantum encryption scheme. take advantage of fact that variables are random. first suppose prover applies two different affine functions. Dec(a*Enc(r,k1) + b,k1) = Dec(c*Enc(r,k2) + d,k2) => a*r + b = c*r + d => (a - c)*r + (b - d) = 0 and from this prover can solve for r in terms of the constants it choses for the function.
if apply same, non affine function, different
if apply different, affine function, different
show if apply different, non affine function, different
if apply different functions and get same result, even if different form of functions
if apply different polys, but same form, can obtain that form of polys on the raw variables. 
or suppose the random variables with 1 are the coefficients of a function. if you can apply two different affine functions, then you can find an set of assignments to the variables (an assignment) that is non trivial (all 0) such that the poly evaluates to 0. If you do not know the poly, then you choose your points of evaluation random with respect to the poly. and the probability of selecting a satisfying set of points is 1/|F|. So if you can product the set of evaluation points with non trivial probability then you know the random variables with non trivial probability. 
Or, take D(E(m1,k1),k1) = D(E(m1,k2),k2) and show that if they decrypt the same, they must have been constructed from the same ciphertexts, even if just themselves, and if they were constructed from the same ciphertexts, the construction must be one of linear operations of scalar multiplication and ciphertext addition. 
or take construction and insist that any modification to a ciphertext other than affine will result in a decrypted message that will be a function of the key. this way, regardless what the function, if at least one of the ciphertexts is a function of random key probability it will match other is near 0.
(j, m + k1*j)
(i, m + k2*i)
suppose encryption is of Enc(m,k) and E(a*m,k), to prevent oblivious sampling, or prevent obtaining valid ciphertext except by affine transformations. So now to preserve a valid ciphertext, ... show can only do affine operations. 
now before we consider the actual encryption scheme, which we'd like to only have to encrypt ri and a*ri for each ri, prove sumcheck holds. 


a1*r1 + b1*r2 + c1 = a2*r1 + b2*r2 + c2 => (a1 - a2)*r1 + (b1 - b2)*r2 + (c1 - c2) = 0
show that it implies prover can apply same different functions to decrypted random variables 


P(online) = 1 - p
P(all k on line) = (1 - p)^k


a*u^2 + b*u + c + a*v^2 + b*v + c = a*u^2 + a*2*u*v + a*v^2 + b*u + b*v + c => a*2*u*v = 0

if deg(f) > d, probability pass linearity test < FH
if deg(f) < d, probability pass line test < d/|F|
P(pass) = P(pass|deg(f) > d)P(deg(f) > d) + P(pass|deg(f) < d)P(deg(f) < d) < FH + 



START OVER FROM SCRATCH, SYSTEMATICALLY
suppose we have an m-variate function, and we want to make sure it is of total degree at most d.
there are several special cases
	one is where d = 1, such that we have a linear function.
	another is where m = d, and no variables multiply each other, such that it is a multilinear function.
we are interested in testing the function along lines, that is, where each variable is an affine function of a single variable.


this is a new document started on May 29, following taylor.txt

keep in mind the linear commitment function can be any inner product, and remember a matrix can represent any inner product. even the dot product is flexible. eg instead of doing 1,x,x^2,... for evaluating a polynomial, you could do e^{ix} when the field is complex roots of unity which might be interesting, or sin(nx + m) for the evaluation of a fourier series. 

hashes of an element might be able to be batched. this is because the input is only the unique element being hashed and the common input relating to the hash. the evaluation of the common input can be batched. but unfortunately, a witness, particular to the element being hashed, is probably necessary for the input as well. 


QAPs and related
CRS depends on secret point of evaluation s, and secret scalar alpha. 

wait, instead of evaluating just the sum of two polys, it may work to evaluate a random linear combination of those polys. i was just thinking that the prover may be able to commit such a linear combination with two extra variables, but of the same total size as the regular sum. this is in fact not the case because two coefficients cannot be added when they are each multiplied by a separate, unknown, constant.

DOUBLY-EFFICIENT paper:


ENCRYPTION:
use matrix
X(ab)
Xa

nXn nX1 = nX1

A(x + y) = 
Ax + Ay

A(sx) = s(Ax)


let setup determine random t along line, and let line be chosen at random at end.


matrices might work. only problem is alphas. 
b1*Ax1 + b2*Ax2 = A(b1*x1 + b2*x2)
maybe just explore what's possible assuming matrix encryption but not with bijection. 
 

for GKR encrypt t for line, with t coefficients, then choose rest of coefficients to make appropriate line

using discrete logs may be a stepping stone to getting attention and help with quantum proof systems


R -> r
Q -> q
Q' = R + w*Q
Q' -> q'
q' = r + w*q

Suppose its done like this
p = g^k
E(m) = (q^j, g^m * q^{k + j}), a random j is chosen for each separate encryption. As such, no encryption can be inverted.
P has some vector and we want the dot product of that vector with random vectors.
m1 + k*j1 = h1
s + k = h1
ss + h1 - s = h2

E(x)^a * E(y)^b = (q^ajx, g^ax * q^a{k + jx}) * (q^bjy, g^by * q^b{k + jy}) = (q^{ajx + bjy}, g^{ax + by} * q^k{a + b} * q^{ajx + bjy})
should be able to calculate the inverse of any element. Then we obtain g^k{ax + by}

g^k{ar1 + br2} * g^k*w{ax + by} = g^k{a(r1 + wx) + b(r2 + wy)}
let R be the result from the random vector evaluation
let Q be for the query

i think we are now in a place to do what we could with discrete logs. 
encrypt R,Q,w*Q
f(Q)
f(R + w*Q) = f(R) + w*f(Q)
get back r,q,q'
test that q' = r + w*q

(q*j, g*x * q*k*j)
g*x * q*k*j + g*y * q*k*i = g*q*k(xj + yi)

(j, m + j*k)
x + j*k + y + i*k = (x + y) + (j + i)k
(x + j*k)(y + i*k) = 
i j k^2 + i k x + j k y + x y = 
xy + k(ijk + ix + jy)

left side:
(j, m + j*k)
(j*a, x*a + k*j*a) + (i*b, y*b + k*i*b) = (j*a + i*b, x*a + y*b + k(j*a + i*b))

m*j + k

x*a*j + y*b*i + k(a + b)


Ax + t2Ab 

m + j*k
m*j + k
m*k + j

right side:
(j, m*j*k)
(j, x*j*k)(i, y*i*k) = (j*i, x*y * (j*i)k)
a*x*k
b*y*k

(a*x + b*y)*k

remember about 'a' and 'b'
and remember left side can account for a*j + b*i


(g^j, g^m g^k)

(q(j+i), g(x + y) + q(k + j) + q(k + i))

two challenges:
1. quantum proof, can't just use ordinary logs. if use logs, must make base unknown, or multiply each by a random number.
2. hidden evaluation points doesn't work for GKR.

what we need is a map f(g^a,g^b) = q^{ab} for some element q. Then all multiplications can be done via the map instead of via exponentiation. And additions can be done by multiplying the results. 

for bilinear map input, maybe try a F_p where p is a mersemme prime (same order as multiplicative group of char 2 field), and the elements are encoded as binary strings corresponding except for 0, each corresponding to a multiplicative element of the characteristic 2 field. 

(Ax)^t Av = = x^t A^t A v
Ax B = x^t v


P:
	Com(d)
	Com(<a,d>)
V:
	c
P:
	z = c*x + d
	zd = c*re + rd
	zb = c*rt + rb
V;
	Com(x)^c * Com(d) = Com(z)
	Com(y)^c * Com(<a,d>) = Com(<a,z>)


x, ax
d, ad


so how to use this matrix form?

never thought of this before, but what if we try to make interactive proofs between two proving systems. that is the child and verifier proofs are executed concurrently, probably taking hashes from each other etc. but we still can't have have any privacy for the verifying proof. i think my thought for this was it could allow new random private params, eg alpha, to be released to the entire network. But this would only be useful if somehow private params could be decided first then released, but if everything is public, how could they be decided privately? Suppose there was a trusted party, and every round it would broadcast a public key, proofs at every level would be made, then it next round it would broadcast the private key for the verifying computations, along with the public key for proving the verifying computations, etc. somehow, maybe using the concurrency idea, computations proved after the corresponding private key is released should not be accepted. So actually the party would public encrypted q, alpha*q ...
this could have the same risk as trusting an initial party for a setup, but rather than setting up a CRS, it would setup a system for publishing these keys that operates autonomously, eg a satelight orbiting earth. In both cases an initial, trusted, setup must occur (of a CRS or an autonomous machine). My argument is that if such a setup is needed than a machine is worth it because we will need such trusted autonomous machines in other contexts assuming the blockchain can't handle everything (eg real time systems). The second argument is that a setup is indeed necessary (ether CRS or machine) unless we use something like STARKS or the doubly efficient method that uses discrete logs. 


i suppose having complete zero knowledge wouldn't be that difficult. first have all original computations encrypt their inputs. the circuit would still do multiplication and addition and thus compute anything necessary, and output an encrypted result. but the problem is, if ever the private key is released, everything could be uncovered, but if the private key is never released you get no insight. 


GKR with t:
given calculated line, verifier can calculate encrypted x,y,z,etc via encrypted t. 


maybe there could be a system where the same data cross referenced in multiple computations need only be evaluated once in a verification. in fact for this reason maybe inputs verifications could be a specialized task for verifying in the next round, rather than having the verifying proof be responsible for evaluating its two children. 


commitment to vectors: each element has different generator, g1,g2,etc. for vector elements x1,x2,etc. vector commitment is g1^x1 * g2^x2 etc. Two possible operations: raise a commitment to a constant to multiply by the constant. second, multiply two commitments to add the vectors.
For example, g1^x1 * g2^x2 * g1^y1 * g2^y2 = g1^{x1 + y1} * g2^{y1 + y2}
due to quantum computers we might as well use multiplication instead of exponentiation. 

suppose the two points of evaluation are reduced to one via a line. V can calculate the claimed value before P knows the exact point of evaluation. But P will know, however, the line upon which this random point will be. 
suppose there are M possible vectors resulting in the same commitment. We can assume P has committed to one of these for each vector, but we don't know which. 

so suppose the commitments define a set of M polys, and P only committed to one, but we don't know which one. I think this basically means we need to expand from the probability that a single poly takes a certain value at a random place, to the probability that any of M polys do so. 


what's the most efficient way to use vectors?
((c1*x + c2)y + (c3*x + c4))z + ((c5*x + c6)y + (c7*x + c8))
one way is to do a dot product with the full coefficient vector, one for each variable. 
this becomes a problem of reinterpreting the base values appropriately, and as seen below this requires computing all combinations of the random variables.
a1*X1 + a2*X2 + ...
b1 -> a1
b2 -> a2 / r1
b1*X1 + b2*X2 + ...
c1 -> b1 / r2 = a1 / r2
c2 -> b2 / r2 = a2 / (r1 * r2)
at the end the bases will be interpreted as the original bases divided by every combination of random variables.
this may be possible, but then we need a way to collapse the vector. 
g^a * R * x = q^a * R => x = q^a / g^a = (q/g)^a
so it appears its not possible to arrive at a uniform base without knowing the exponents, thus they must be explicitly sent by P and checked by V.


Complexity:
mxn = N
communication: m row commitments, n exponents, total is m + n
computation:
m (calculate L)
n (calculate R)
m (combining row commitments)
n (checking exponents)
n (doing dot product)

minimizing communication:
find minimum of m + n
m*n = N
m = N/n
N/n + n
derivative is 1 - N/n^2 which assumes 0 at n^2 = N
so m = n = N^1/2

minimizing computation:
find minimum of 2m + 3n = 2N/n + 3n
derivative is 3 - 2N/n^2 which assume 0 at n^2 = 2N/3
so n = (2N/3)^1/2, m = N(2N/3)^{-1/2}


growth of inputs if we use square root:
proof size is p
original input size is n
input size for verifying proof 2(n^{1/2} + p)
assume p is everything except the inputs of the 2 child proofs, including its own witness
for what value of p does the input size remain constant
2(n^{1/2} + p) = n => p = n/2 - n^{1/2}
its Ok if p is a little bigger than this as long as growth is reasonable, because higher tree vertices can handle larger proofs we assume.
even with square root complexity, we will likely be in need of a copying method. 


10101010
11001100
11110000


I suppose the prover could just send the elements of T' and the verifier could exponentiate them and make sure it matches the commitment for T'
Then the verifier would use these elements to compute the dot product itself. So total communication would be 2*2^{n/2} = 2^{n/2 + 1}, sending 2^n/2 row commitments and 2^n/2 elements as just described. Communication would be the same, as the paper's method, but what about computation? Here, computation would be needed for checking the sent values, and then computing the dot product. Checking the N sent values requires exponentiating N bases, each to one of the N values, and multiplying the results. Computing the dot product requires N multiplications and additions. Now for the paper's complexity. Just the same it requires exponentiating for each of the values sent and multiplying results. It also requires computing a dot product with the values sent. But it also requires a additional final exponentiation of the dot product result. 

Suppose that we abide by the discrete log assumption and its not possible for P to find a separate set of coefficients that would result in the same commitment, though such coefficient sets still exist. How does soundness hold? I think it has something to do with, upon knowing the random point, P not able to compute the value of the output of an alternative poly. 
We can basically assume V's test correctly evaluates the function but at ...
So P leans the random value and is then claims a value, and then V verifies.
If P knew of alternatives, P could take the random value and then choose from any of the alternative polys, compute the result, claim that result, and V's test would pass. If P does not know of alternatives, then if it claims the wrong value it is low probability any of the alternatives will evaluate to that at the evaluation point (even though that point was not random with respect to P's claim). 

There are a total of N^L possible polys (L is the size of the field, from which the coefficients can be drawn, and L is the length of the poly). For a given commitment, L-1 of the coefficients are free, but 1 is dependent on the rest. Suppose the 1st coefficient is the one that is fixed. Then there are N^{L-1} possibilities in this case. There are L possibilities for the fixed coefficient, so at most (actually less due to redundancy) there are L*N^{L-1} possibilities. L*N^{L-1}/N^L = L / N
So the question is what is the probability that for the random point chosen, any of these alternative polynomials will allow for a false proof. First let us assume that it is the final round and we use the sumcheck, and due to the hashing technique each of the variables is random. Also, dependent on the final variable chosen, the claimed value is random. Now V is left to compute the input at two different points. Suppose we don't reduce to one point for now to preserve randomness. 

Oh shit, the application of the discrete log assumption actually comes more from when P sends V the values that should represent the exponents of T'. P can send any set of exponents, only one of them is fixed and dependent on the rest. All will pass V's check. So what's the probability than with M free elements and 1 fixed, the dot product with another vector will yield will yield a particular value. We can suppose this other vector is random because would enforce that the random points it encodes are not determined until T' is computed, which involves P sending this other vector. In this case P can't strategically choose the vector, so its a small probability the dot product will evaluate to a particular point. But what if the variable vector is known to P? This is equivalent to finding a vector X that solves the following system where all other variables are constants. 
x1*r1 + x2*r2 + ... = h
x1*c1 + x2*c2 + ... = v
This is equivalent to finding vector X = (x1,x2,...) that satisfies AX = H where H = (h,v) and A is
r1 r2 ...
c1 c2 ...

Suppose ri = 1
x1 = h - x2 - ...
h*c1 - x2*c1 - ... + x2*c2 + ... = v
sum_i xi(ci - c1) = v - h*c1

There are L*N^{L-1} possibilities for X to satisfy the top equation (the constraint).
For each such poly, the probability that at the random point C it takes the value v is 1 / N. If we add this probability for all possibilities we get total probability L*N^{L-1} * 1 / N = L * N^{L - 2} which is way over 1. maybe I did it wrong.


we need a homomorphic hash function, like discrete log products, where you can commit a vector into a single value, and then manipulate that value with other values to either multiply the vector by a constant, or add two vectors together. getting homomorphic properties is easy, just do a sum of the vector elements. or getting the hash property is easy, just hash each element then add the results. But getting both properties is more difficult. if we did something like that latter, what would be the constraints on the hash function necessary for homomorphism? Then multiplying or exponentiating the hash function by a constant should multiply the vector the a predictable constant. Adding or multiplying the hash functions together should add the corresponding vectors. Those homomorphic properties imply an simple, linear-like, algebraic form for the hash function that maps many elements, the vector, to a single element, the hash. And I think any such simple form is not quantum proof. For example, a hash function might be a polynomial form, which is algebraic, but it is nonlinear so there is no way to manipulate two instances P(x), P(y) to easily obtain P(x + y) or P(a*x). It may be worth asking experts their opinion, but its looking like this matrix method which doesn't require a setup is only possible with simple form of discrete logs, and no such simple form will be post quantum, and its unlikely to find a post quantum scheme that can commit a vector with a constant size commitment and still posses homomorphic properties. Also, most post quantum schemes seem to have large keys and such for small commitments. For example, LWE methods seem to require on the order of matrices to only encrypt a bit.


((c1*x + c2)y + (c3*x + c4))z + ((c5*x + c6)y + (c7*x + c8))

c1 c5 c9  c13  x*y
c2 c6 c10 c14  y
c3 c7 c11 c15  x
c4 c8 c12 c16  1


once T' is determined, think of its elements as the coefficients of a multilinear poly, evaluated at the random point of the random vector. Then the question is, does there exist a set of coefficients, that is a poly, that satisfy the constraint, but also evaluates to the claim evaluation value? Note that the constraint is dependent on the random point. And note that the claimed evaluation value is also dependent on the random point. 



gkr. get to end of last sumcheck. Suppose P claims wrong values for at least one of the input evaluations. V creates line. If P correctly evaluates input on the line, then one or both of the univariate checks by V will fail because they will output the correct evaluation when P claimed the wrong evaluation. So suppose P incorrectly evaluates the input on the line, and thus the univariate is incorrect. Then V selects random point on line and with high probability the correct and given univariates will disagree, leaving P claiming a false evaluation on the input. 



i liked the idea of finding T' when only half the variables are determined by pausing the sumcheck protocol, then continuing and finding the rest of the variables and computing the final dot product. this exploits that T' is computable, and the elements are checkable when only half the variables are known. oops, but then P can use the resulting T' to strategically claim an output value. the sumcheck would proceed as normal after pausing. Suppose we arrive at the top level where we have the integrand, a function of two applications of the input, resulting in a value. Oh wait, in the sumcheck we'll have two different sets of variables for the two separate evaluations. Suppose we pause when both have half the first half of their variables determined. Then for each the verifier obtains T'. I think a third invocation at the sum of the two random vectors could check the consistency of the two T's. Having obtained the T's, V has made P commit, and P can only perform further manipulation by strategically returning messages in the remainder of the sumcheck. (In fact, in Giraffe style, the first few variables of both input invocations will be the same, so only one T' would need to be computed.) Suppose we don't reduce the two invocations to one. Suppose the second to last variable is determined, and as a result the value of the first invocation is determined. Then the last variable is chosen randomly by V, and the expected output of the whole thing is determined by plugging in that value into the univariate returned by P. Upon choosing this random variable, the value of the second input invocation is determined. V then checks that if plugging it into the integrand yields the same value as the univariate. I suppose the prover could cheat just by returning messages by not evaluating the actual integrand, but instead evaluating the newly committed function with T'. 


Post Quantum Linear PCPs with GKR
we will assume a trusted setup is necessary as we did when starting this page. We want to do one for GKR. 
I think this means the secret must be the point on the line. Using the encrypted point on the line, V will compute all the random variables. But V must then generate the query from all combinations. Another problem is that V must generated alpha times the query, but this may be possible by the public key encrypting di*t and di for di being all nth roots of alpha where n is the number of total variables, this way when multiplying a set of variables they will always multiply to have alpha as a multiplier. I think the only reason V needs to know the query is in order to correctly compute r + alpha*q to pass to P. But what if P could calculate this himself, would it be the same? If P computes it of course P could compute whatever it pleases. If V computes it and sends it to P, P could still choose not to use it and use whatever it wants. I think security comes from the fact that if P uses another other vector than the one correctly computed, the check will not pass. 

What does it take a linear PCP to work?
The purpose of r + w*q is to make sure P evaluates q and not some other query. Thus P cannot know how r + w*q is constructed or else it could reconstruct it with a different q. In other words, P shouldn't be able to obtain r + w*q' knowing q' for q' != q. So its not just r and w that must be private. 

V can calculate a few values of q, chosen at random. Maybe when r and w are released, it can compute r + w*q' and somehow compare those values with those of the vector P used. Perhaps P would need to commit to all the values of the vector and then V would open up the right ones. But this would not be useful if P could cheat by only altering a few values of q, and I suppose it can because it can just change one of them to obtain an arbitrarily desired evaluation value. 
But of course the entries of q are highly redundant in their underlying logic, so perhaps we could use the univariate sumcheck method to iterative through the entries of q and q' and make sure their difference is 0.
But in both the above cases, how do we know what vector P actually used in r + w*q? 

f(r + w*q') = f(r) + w*f(q') = p

As seen below, doing multiple queries is also an option
a1*(cv*t^v + ... + c2*t + c1) + b1*(dv*t^v + ... + d2*t + d1) + ...
t^v*(a1*cv + b1*dv + ...)
...
t*(a1*c2 + b1*d2 + ...)
(a1*c1 + b1*d1 + ...)

How is the check done? V compares the response to r + alpha*q with r' + alpha*q' where prime denotes the response. Since q' depends on the function being evaluated, alpha*q' cannot encrypted in the setup, thus alpha must be released, unless we use a bilinear map. 

I we only need the additive homomorphic property. If there are v variables, then the degree of the univariates representing the random variables will be at most v. Instead of alpha, lets call it w, and the random point on the line t. We encrypt w*t^i from i = 0 to v.

E(m) = (j, m + j*k)
c1*E(t^2) + c2*E(t) + c2*E(1) = (c1*j1, c1*t^2 + k*c1*j1) + (c2*j2, c2*t + k*c2*j2) + (c3*j3, c3 + k*c3*j3)
= (c1*j1 + c2*j2 + c3*j3, c1*t^2 + c2*t + c3 + k(c1*j1 + c2*j2 + c3*j3))


what if we leave q and r + w*q both where each element is a function of the 2v line variables? Unfortunately, each entry of q would have terms all of the same degree, just with different variables, w*q would also be of such form, and thus r + w*q would make r immediately recognizable as the constant terms. 


So I don't know of a way to use linear PCPs with GKR, because the private party must construct both q and r + w*q, but these vectors are different for every proof. 



Linear PCPs with Evaluation Point Independent of Instance

Poly Div Method
in this model we evaluate at a random point. I'm think of systems of the form C(z) = h*b, where b is the vanishing poly common to all computations, z is the transcript, C is the constraint poly which can invoke z in multiple places, and h = C(z)/b. 
Univariate: Look at BS material. 
Multivariate:
Does the same construction apply? Suppose you can still construct b such that it disappears on certain points. If the poly is small enough it should divide C. 

I don't know if you can do this with univariates, but let the transcript consist of two parts, one V cares about (containing output and necessary inputs) and one V doesn't care about. V 


VANISHING TRANSCRIPT METHOD
have a multivariate transcript, or some function of it, that should vanish on a subset of points friendly to the sumcheck. Use sumcheck and multiply by powers of a univariate poly. Evaluate transcript, or function of it, at random point and end, and be convinced it disappears at all correct points. But this is the sumcheck again, so the random point will be particular to the instance. Uh oh.

SUMCHECK WITH LINEAR PCP
Is it possible to invoke sumcheck with encrypted random points?
Forget a moment about the need to choose a random point dependent on P's messages. 
First round V sends E(r1) and asks for sum_bi f(b1,b2,...,x,r1). Can P compute this? P can certainly first sum-out all the bis. Then I suppose P can multiply every term containing E(r1) by its constant coefficients (enabled by our particular encryption system), leaving x as a variable, ending up with a univariate poly as a set of encrypted coefficients. 
It is indeed possible to evaluate the polynomial at any place such as 0 and 1 my simply multiplying it into the encrypted coefficients as a constant. Then the verifier can sum the results. But obtaining the value of the evaluation isn't possible until decryption. 
In the ith round P must evaluate a poly on f(b1,b2,...x,ri,...,r1) which either means having encrypted values of each of the random variables that obey multiplication, or encrypting every combination of these variables. I think the latter is more likely. The setup would encrypt each of them, and in fact I think this could be done all at once, before the proof, at which point all combinations of all n random variables are encrypted, and then in every round the prover uses the appropriate combinations. Every round V is supposed to compare the value of the message at the sum of 0 and 1, to the value of the previous message at the random location. But doing the latter requires encryption multiplication which this approach does not support, so evaluating at the random point will only be possible once the random value is decrypted. all random variables will be decrypted after the proof is complete. Before the proof starts, P commits to the r vector, producing r'. Then after the final round P produces q' and (r + w*q'). So P's proof consists of its encrypted messages for each round, and the evaluations of the three vectors r, q, and r + w*q. Once the proof has been sent, k2/k1 can be released, along with w. The verifier then decrypts all of P's messages, including those V already evaluated at 0 and 1, also decrypts each single random variable, and also decrypts the evaluations of the three vectors (because each side of the comparison equation will have been encrypted with different random values). V evaluates P's messages at the random variables, checks they are consistent doing the round check (actually V will only evaluate at 0 and 1 now, not before, because it's a non-interactive proof. so even though evaluation at 0 and 1 is possible in encrypted form, V will need to decrypt the polys anyway, so it might as well be done in decrypted form). V also checks that (r + w*q)' = r' + w*q'.
The above is only described for the integrand as a multilinear polynomial. If the degree of each variable is above one, the pre-encrypted values will have to again be for every possible combination of variables. so for v variables and degree d in each variable, there will be 2^{vd} combinations to encrypt. In GKR, i think d = 3. 

what about r + w*q, is r necessary? QAPs seems like it doesn't have r, but it uses w for sure. it checks w*P = P' for each query P, but also does a random linear combination of some vectors which is think is for the purpose of verifying they are in the proper span. 

whats the difference when q and r + w*q are also sent encrypted?


what about hash transform?
why is it usually needed? if the proof is non interactive, P must determine the random values. of course P is not free to choose them, but what if P and V agree on a seudorandom function to invoke that is independent of P's messages? Then P will know the random point and can choose any poly that passes the test, and when invoked on that random point yields a desired value. This can be easily done by just interpolating three points, which can be done with a degree 2 poly. So P cannot know of the random point when sending the poly. This simple condition is satisfied because the random point is represented with encrypted combinations of variables, but further analysis and proof are needed to confirm the hash transform is not necessary, which i think would be incompatible with the linear PCP method.


LINEAR PAPER


QAPs METHOD
why not use multilinear? degree would be much smaller. 

idea: a given Wi should only appear a few times, say d, so we use a degree d poly such that its roots are the d points that Wi should appear. The we must form an expression out of the poly such that it evaluates to 1 when the poly evaluates to 0, and evaluates to 0 when the poly evaluates to anything other than 0. 
((a*a + a) + 1)(0 or 1) + (a*a + a)( not 0 or 1 )
0 or 1: ((a*a + a) + 1)
other: (a*a + a)
neglecting the single point p such that p + 1 = 0, we could have
actually i don't think this is possible, at least not in polynomial form, because then we'd have a polynomial of low degree but many roots.

multiple proofs. provers commit W and H all to same encrypted r vector. then verifier chooses a random point, computes all polys at that point (as well as the vanishing poly), and constructs 3 q's for W which can be public, and privately constructs r + w1*q1 + w2*q2 + w3*q3 which can also be public. The verifier also constructs a single q in similar way for H. Provers give their evaluations, then the w's are revealed and the verifier performs comparison. the constants, that is those assignments that the verifier cares about are contained in a single poly that serves as one of the constant terms. Luckily these constants should be the same for all computations at a given level in a given round. But this poly will not only have to be evaluated every round at a new point, but it will also have to be reconstructed, that is re-interpolated every round, whereas the rest remain constant as long as the logic remains constant. But of course we might actually choose to change the logic round to round as well. We would even have the chain itself responsible for computing these polys. The network or the chain could also choose the random point of evaluation, and maybe even construct the queries. The only parts that must be done by the trusted party is generating r, wi, and r + \sum wi*qi. So verifying a computation is done with the correct logic and correct constants is just a matter of using the right polynomials. 


POST QUANTUM ENCRYPTION FOR LINEAR PCPs
Suppose we use E(m) = {k1*j, m + k2*j} such that you can multiply them by constants and add them together, but not uncover k1 or k2 or m. But when you're given k2/k1, you multiply the left side by k2/k1, then subtract that quantity from the right side. example:
E(2) + E(3) = {3*12, 2 + 4(12) = 50} + {3*2, 3 + 4(2) = 11} = {42, 61}, 61 - 42*4/3 = 61 - 56 = 5
so suppose we need to evaluate a linear function at a random point. the elements of q, r, and r + w*q have all been separately encrypted. But all must be random, eg not something known like 1. To compensate for this, I think we either have functions without a constant term, or once k1 and k2 are released that point can be added. Assume otherwise the cryptosystem cannot be broken. The prover gives the evaluation of all three vectors, then k2/k1 is released and the verifier decrypts the provers evaluations. If the prover is not consistent with the left and right sides the decoded evaluation should just be random. then w is released and the verifier checks that (r + w*q)' = r' + w*q'
I think this system should work, its now mostly a matter of finding proofs where the point of evaluation is the same across instances.
Properties
a*E(m) = E(a*m)
E(m1) + E(m2) = E(m1 + m2)
we want to prove that regardless how many are committed and how redundant they are, perhaps even if the same thing is committed twice, as long as k1, k2, and j are random (even if the committed value is known) it's impossible to uncover j, k1, k2, or the committed value if it's not known. we should prove this by showing that these values could equally likely be anything from the information given.
if the committed value is known, i now see its easy to uncover k2/k1. but if its known, we can choose j to be 0 (which is also known) and it will behave as normal. But we will still need to handle redundancy. Its evident k1 could be anything, and the random values could be chosen accordingly. similarly for k2. we may not need k1. Suppose we don't have k1, in which case its only k=k2 and the messages that are secret. I think the way to prove is that if the system could be solved for k or any of the messages, then something impossible could be done. 
If the messages are each chosen independently, then solving for k or the m vector, given the j vector and the M + kJ vector is not possible due to the same proof as IKO07, where M functions as r, k functions as alpha, and j functions as q. This is the case even if one is given M + kJ and M + k'J if for some reason that becomes helpful later. We would like to prove that it is still safe even if the messages are not chosen independently. We want to argue that the messages cannot be uncovered. It is evident that if any message is uncovered, by knowing the j, one could uncover k. knowing k, one can uncover the rest. it doesn't seem intuitive to me, but lets take the contrapositive, that if k cannot be uncovered then some m cannot be uncovered, that is there does not exist an m that can be uncovered, that is non of the messages are uncovered. thus we will prove k cannot be uncovered. we can prove this by showing that k is equally likely to be anything. that is, if k were something else the messages could change to validly encode some other random point (valid meaning encoding the same combination pattern). let n be the new message when the key changes to k + c for arbitrary c
n + (k + c)j = m + kj  =>  n = m + kj - kj - cj = m - cj
what if the inputs are dependent?
X = x - c*j1
Y = y - c*j2
X*Y = x*y - c*j3
(x - c*j1)(y - c*j2) = x*y - c*j2*x - c*j1*y + c^2*j1*j2
j2*x + j1*y - c*j1*j2 = j3
unfortunately it looks more difficult to prove (if its the case) for dependent messages.
but fortunately, due to the tuple multiplication method below, we don't need to encrypt combination of messages, but can encrypt each independently chosen random variable separately and then multiply them in encrypted form. 


E(k1*j, m + k2*j)E(k3*i, n + k4*i) = E(
	E(k1*j, m + k2*j) * k3*i,
	E(k1*j, m + k2*j) * (n + k4*i)
) = E(
	E(k3*i * k1*j, k3*i(m + k2*j)),
	E((n + k4*i)k1*j, (n + k4*i)(m + k2*j))
)
E(k3*i * k1*j, k3*i(m + k2*j)) * k4/k3 = E(k4*i * k1*j, k4*i(m + k2*j))
E((n + k4*i)k1*j, (n + k4*i)(m + k2*j)) - E(k4*i * k1*j, k4*i(m + k2*j)) = E(
	(n + k4*i)k1*j - k4*i * k1*j,
	(n + k4*i)(m + k2*j) - k4*i(m + k2*j)
) = E(
	k1*j((n + k4*i) - k4*i),
	(m + k2*j)((n + k4*i) - k4*i)
) = E(
	k1*j(n),
	(m + k2*j)(n)
) = n * E(k1*j, m + k2*j)


lets start again but with no k1 and k = k2. lets try to multiply them.
(j1, r1 + k*j1)(j2, r2 + k*j2) = (j1*j2, j1*r2 + j2*r1 + 2*k*j2*j1, r1*r2 + r1*k*j2 + r2*k*j1 + k*k*j1*j2)

(j1, r1 + k*j1)(j2, r2 + k*j2)(j3, r3 + k*j3) = (j1*j2, j1*r2 + j2*r1 + 2*k*j2*j1, r1*r2 + r1*k*j2 + r2*k*j1 + k*k*j1*j2)(1,j3, r3 + k*j3)
= (
	j1*j2*j3,
	j2*j3*(r1 + k*j1) + j1*j3*(r2 + k*j2) + j1*j2*(r3 + k*j3),
	j3*(r1 + k*j1)*(r2 + k*j2) + j2*(r1 + k*j1)*(r3 + k*j3) + j1*(r2 + k*j2)*(r3 + k*j3),
	(r1 + k*j1)(r2 + k*j2)(r3 + k*j3)
)


Multiplying is now easy, just pad the smaller tuple with 0's then multiply as above. Luckily 
Using this technique, I think it is sufficient to encrypt only the individual random variables, not combinations of them. 
I think you can expand any tuple into one of higher degree my padding with 0's on the left side. I also think we can treat constants as encrypted by having a 0-tuple with the constant on the right side, that is with 0 as the random number since it is a known constant.
so i suppose all terms can be converted to a single tuple of fixed width. Then its straightforward to see these tuples can be added together. Worth saying, I think there would be an analogous way to do this where instead of m + k*j we have something like m*k*j and then multiplication is index-wise, but addition take the same exponential cross additions and multiplication as we see above. 

Now luckily the work is all for the prover, (maybe provers can collaborate if they choose to compute all the combinations). the trusted party also has little work. the verifier must only perform one decryption, which has work proportional to the number of variables. 

And remember we can treat encryptions with other keys as constants in the form of a tuple. nesting will only go as deep as there are layers of encryption. 

So the formula for decrypting is sum_{n=0}^{rowCount - 1} (-k)^n * nth-row, where rows are counted from the bottom (or right).
For multiplication, we can always assume we're multiplying two tuples of the same length n. Let tuples be 0-indexed. Take the indicies of the highest non-zero items (rows, columns, whatever), and add them to obtain the index of the highest non-zero item in the resulting tuple. I think the way we do it is go through each index of the resulting tuple, and find and add together the product of all pairs of items with indices that add up to the current index in the resulting tuple. The number of such pairs for a given index should follow the binomial theorem. Of course dynamic programming will be useful to the prover most of the time. But I don't know the optimal way to compute all combinations. 


NEURAL NETS
we could use rational numbers in a finite field like Ginger, but let's think about using real numbers (maybe even complex numbers) and finding a scheme that can represent a feed forward network. Suppose we first consider a single layer. The straight forward pattern for a net is a linear combination of inputs multiplied by weights, and then put through an activation function. Almost all methods use the zippel lemma by comparing an expression like this to a polynomial that should have identical form eg the input in GKR or h/r in poly div. But in our case the expression passes through an activation function so it does not have polynomial form, so if we compare it to something it should have the same form. Whatever this form is, eg a quotient of exponential polynomials, the zippel lemma should still hold. 
Suppose Q encodes the 2^v=n values for a given layer, and suppose it has v variables. W is the weight vector the layer and encodes 2^{2v}=n^2 values.
sum_bottom j sum_top i top_i*W_j_i
Q_i(x) = S(sum_i W(x,i) * Q_{i+1}(i))
so clearly we need form of S,W, and Q such that composing W*Q in S yields the form Q. One problem is if we use functions that interpolate points, it probably has finite form, but when plugged into S, it will have infinite form, and thus Q must have infinite form. The only possibility I can think of is that after plugging into the actual S, we truncate the output cutting off the least significant terms, such that the two sides are approximately equal. Note that this doesn't mean truncating S before plugging in, but only truncating the result. Actually I realize truncating S is necessary for numerical methods, but analysis should still be done on how to collect the most significant terms if S was actually used. What I'm thinking of is that the integrand to the sum above is a linear combination of e^{ix^2} for where i is a sequence of numbers I haven't decided yet, but a good choice might be something like -0.2,-0.1,0.0,0.1,0.2. The reason this matters is to keep the terms bounded. So its univariate. That is, both W and Q are of this form, but of course W has square the number of terms as Q. Unfortunately multivariate doesn't work because plugging into exp multiplies the exponents by constants. So the integrand would actually have the same degree as W, but plugging it into S the degree would expand exponentially and then be truncated. What's important is that when plugging such a linear combination into a truncated version of exp, each term gets raised to a positive integer, thus the series of exponents does not change. Theoretically, I suppose the prover could use as long a sequence of the terms as desired, and whatever range of them as desired to best represent a function. That is, we may not need to always trim the output to contains the same number and range of terms. So far this seems reasonable. 

another possibility is we use the n roots of unity on the complex plane, such that raising any to a positive integer results in another roots. Actually if exponentials present problems like for the zippel lemma, regular polynomials also return the same form when passed into exp. When passed to exp(-x^2), that is the gaussian, we still get back a poly. But even with polys, multivariate is not doable. And unfortunately as I just now realized, the sumcheck is only useful for multivariate. 

Suppose we use a RBF network, with gaussian activation, then univariate W and Q regular polynomials, passed through the activation, then truncated, will yield another polynomial. Despite the problem of univariates with the sumcheck, we have found an equivalent representation of the left and right sides. but univariates on the sumcheck may not be a problem using linear PCPs. 

do a linear PCP where the function is the values of a network layer. and the queries are the weights, one query for every child neuron. after getting the results, the next layer is constructed by passing each result through the activation function. either the layer could be committed and queries are public (because weights are public anyway), or there need not be a commitment and instead the queries are all encrypted. 
how to move to the next layer? the outputs, after going through the activation, would form the function for the next layer. Maybe the encrypted outputs could be committed. Then a verifying proof would take the hash and a witness as input, decrypt the values, somehow apply the activation to each one, then output a commitment ...


RELEASING SECRET KEYS
we need a way to prevent false proofs from being generated for a given round after the secret key for that round is released. one simple way is to have all proofs merkleized into a single network-wide hash, then the key is released, then the verifications start requiring the proof data to have a merkle path. another more complicated but desirable way is to have the verification start such that the proof data is committed, then the key is released while the verifier pauses, and then the verifier resumes completing the verification. Note that the prover need to pause for the verifier, only the verifier needs to pause for the prover. As such, each verifier need only pause once. Pausing is not hard, but we still have the problem that the verifier may not resume and may instead let the prover create a new (false) proof, and then start verification from the beginning. 

if we really don't want a trusted setup, we could waist energy like bitcoin and have anyone spit out a problem with a public key and then have a problem that can be cracked in the right amount of time that will yield the secret key. but this is more dangerous for several reasons. 
another possibility is having hardware, or even obfuscated programs, that operate as the trusted party, and there are many of them and they are not hackable. at any time, a given set of them can be registered with the chain, and the chain can pick which one will be responsible for the next round. 


LINEARITY TESTING
The number of times the verification must be performed for negligible soundness error is concerning. 
I'm confused because QAP doesn't seem to do this. maybe its cuz its encrypted

Let d be delta, what does d-close mean? and what does it mean for a test to pass with probability d?
let f be delta far from L, the set of linear functions
that means Dist(f,L) = min_{g in L} P_x(f(x) != g(x)) = delta

we want f to be as close as possible, that is smallest distance from as possible, to a linear function, so that all our queries to it will return the same answers as if we were querying that linear function, whatever it is. 

would it help if the queries are encrypted?
it may help to randomly combine two linear functions into one

it may be useful to split the problem up into two around whether P uses the same non-linear function every time, or uses multiple functions, linear or non-linear. Of course the case of using the same linear function every time is a cooperative prover. 



MOST EFFICIENT METHOD

think of way to efficiently compute a lot of hashes

inefficient way, but let the transcript be the function. encrypt a vector of constants, most of the 0, query to obtain any linear combination of the values. repeat any number of times. decrypt the answers. use these answers and more constants to form another query, this time in effect obtaining quadratic results. continue this process to compute an arbitrary arithmetic circuit. since the queries define the logic, another problem is the prover could cheat if they know that logic. or maybe the encryption even prevents cheating if they know the queries. 

what if we go beyond linear functions, such as quadratic PCPs? so instead of just multiplying every variable by a constant, you can square linear combinations of them too. but what property to test for? I actually don't think you can test for it, and if you could it would look more like low degree testing which is complicated, especially for multivariate functions. speaking of which, maybe low degree testing for multivariate polys, like evaluating over a line, could be done easily with a multiplicative and additive homomorphism, but I remember soundness wasn't so good anyway.


I was thinking if we use the sumcheck without hash transform and instead use encrypted points, maybe we can evaluate top to bottom, or layer by layer in some way that a verification proof can be concurrent, and we could do the circle thing.
Every intermediate prover message would be a univariate poly as large as W. V's check involves evaluating it two points and summing the results. We could use linear PCPs and only submit one query for each check (the sum of the two points to be checked), but the query must be generated by a trusted party. 


evaluate top input.
then evaluate second level poly. the variables used in the sumcheck must be those the top was evaluated on. so you must decide those variables first. then you can decide the random variable for the second level. then you can evaluate second level. after evaluating second level, the variables for the first level will not be used again, so they can then be exposed. at this point the sumcheck for the second level and the evaluation of the first level can be checked. notice that there is no linear PCP method except for the top level. actually i realize for a given level with the sumcheck, the variables of the sum check and the variable for evaluating that level will both be encrypted, and they will be multiplied inside the integrand. but the variables of the sumcheck must be decrypted first in order to move from top to bottom. Looking below, the two different encrypted variables will appear together in beta. It expands to a multilinear polynomial in both variables. So supposing both sets of variables are encrypted with pairs, every term contains only one multiplication of the two sets. oh shit, i just realized the integrand in the sumcheck will not be multilinear in the variables of the sumcheck, but will be like degree 3, this entails encrypting far more combination of those variables. but in any case, if you expand the integrand into terms, each will consist of a constant, an encrypted combination from the evaluation variables, and an encrypted combination from the sumcheck variables. so the questions is, can we have pairs like this and then add them together? looking below, yes we can, that is we can take the variables of evaluation, which are to be decrypted second, and multiply them into the sumcheck variables like constants. then we end up with a sum of terms encrypted sumcheck terms, which can be added together. then the result can be decrypted, yielding an expression in terms of encrypted evaluation terms. in fact, we can multiply two terms encrypted with the same keys, but one becomes nested inside the other, and then decryption requires two phases. actually looking above at work on the encryption scheme, we can multiply terms and obtain a resulting term with a tuple of width that grows linearly with the degree of the multiplied encryptions.

I think this encryption scheme has the unique property of being able to encrypt other encryptions. oh, this crypto system doesn't have the benefit of being public key. 

whats interesting and exciting and a little concerning is that (for the same reason as the security), k can be anything, and upon a given k, the messages all obtain random values. since the values are random to begin with, it seems, values can be encrypted without even deciding a key by randomly choosing a left side and a right side for each encrypted item. Then a key can randomly be chosen, and decrypting the regular way will yield a random value that wasn't known until the key was decided. this suggests maybe a private party is not needed, and instead the chain and randomly encrypt the values, compute the proofs, then randomly select a key and perform verification. and in fact, each verification could choose its own key, or even its own random encryptions. oh now i see the catch, and that's although we it works for a query q, in linear PCPs we need to construct r + w*q. And remember, the prover can't know how r + w*q is constructed, even if through encryption or else the prover could reconstruct it using whatever query it wants. So a private party is still needed to choose w and construct r + w*q.

so the core of the test it compare f(x) and f(y) to f(a*x + b*y). fortunately x and y can be expressed by giving the individual variables. but a*x+b*y cannot be expressed as combinations of constituent variables expect in the actual form of a*x+b*y but this reveals a,x,b, and y. So the trusted party will have to construct this. i think the way it will work is the party will choose x and y variables encrypted, and with them publish a*x+b*y in decrypted form. the prover will commit to all 3 evaluations, then the keys for x and y, and a and b will be released, and verifiers check that the evaluation of a*x+b*y equals the linear combination of a and b with decrypted evaluations of x and y. testing the linearity property means evaluating at least 2 queries and a 3rd expression a linear combination of them.
we can't just have a one-time setup because we must decrypt evaluations in order to compare them, because this encryption scheme requires random numbers, because if it didn't it would be the type of public key that is breakable.

maybe we could have it that a given layer verifies its own sources also uses them to compute more. this would mean a lot of copying. what if we have a single layer and we check for copying by using sumcheck with encrypted random variables, and at the end we must evaluate both the top and bottom at separate random points. You evaluate the bottom via sumcheck which leads to evaluating the top at another random point. it is those two random points for the top that constitute the two encrypted linear PCP queries. 
it seems that in general we could take any number of vectors represented as multilinear functions, make a random linear combination of them multiplied and use the sumcheck to check they have equal values in certain ranges. at the end you'd evaluate each constituent function via a linear PCP. 

using linear PCPs as commitments to vectors for vector computations would be cool. for instance, multiply a vector by a constant. to do this suppose the prover is committed to both X, the original vector, and b*X, the constant multiplied by the vector. the latter is to be verified as a correct commitment. maybe just query both at a random point and check that b*(X') = (b*X)' assuming linearity. similarly for adding two vectors. this could be used to multiply a matrix by a vector in square root time. 

evaluate its own decryption circuit. in our case, rather than reducing noise of a ciphertext, we'd like to reduce width of a tuple. how would we do this if we knew the secret key? we'd take a tuple, decrypt it, then re-encrypt it with a new random value. Now how can we do this homomorphically? For one we need to encrypt the secret key, that is (j, k + k*j), but unfortunately this can reveal k because
k + k*j = k(1+j) = p, but what about k^2, k^2 + k*j = p so it can be solved with the quadratic equation. I don't think we can encrypt any power of k under this scheme because it always results in an algebraic equation of 3 variables only 1 unknown, that is k. I was thinking we should instead encrypt the key under a different key, and it turns out this is what Gentry does. 
(j1, k + K*j1), (j2, m + k*j2)
(0, m + k*j2) - (j1, k + K*j1)*(0, j2) = (0, m + k*j2) - (0,j1*j2, k*j2 + K*j1*j2) = (-j1*j2, m + k*j2 - k*j2 - K*j1*j2) = (-j1*j2, m - K*j1*j2)
(m - K*j1*j2) - K(-j1*j2) = m - K*j1*j2 + K*j1*j2 = m
so given the first key encrypted under a second key, any ciphertext of a given width encrypted under the first key can be encrypted with the same width under the second key, without revealing the first key. unfortunately it doesn't appear possible (despite a lot of text I wrote out and erased) to reduce the width. I think this is because when you decrypt you reduce the width by one, but since you must decrypt under a new encryption, encrypting it increases the width by 1.
encrypt first key under second key, then encrypt first ciphertext under second key. then inside second encryption, decrypt first ciphertext using first key. the result should be the first plain text encrypted under the second key. but the problem is we don't know the ciphertexts that must be encrypted, and the prover can only encrypt them if there is a public key. 
assume the prover's computation is correct via verification. we want to the prover to be able to compute on ciphertext and keep the width down. unfortunately from above it doesn't seem this is possible. 

if we did everything with homomorphic encryption, the trusted party holding the secret key could be responsible for decrypting appropriate data. without this data used in another computation will be public to the one performing that computation. 

decryption can be done as a linear function, where the function is the ciphertext and the query is the powers of the key. the intermediate results of the sumcheck could be decrypted by such a linear PCP, but unfortunately this would take an extra round because the linear combination query evaluation would need to be committed to before the w can be released for linear PCP validation. 

hmm, i just realized the queries q and r + w*q will probably need to be encrypted under separate keys because they contain redundant data. 


note that intensive computations like FFT based on redundancy can be outsourced using GKR or something to powerful provers such that light clients can prove stuff for themselves.



CHAIN DESIGN
I think a good primitive to strive for would be having all proofs generated, and committed to a single merkle tree, then having the key released. then letting anyone verify any of the computations. call the primitive an 'aggregate proof.' this primitive could be used to build a chain, where each aggregate proof is followed by another one that verifies them ... but wait we need all verifications proofs fully verified before anything is committed to the actual chain.

biggest inconvenience right now is committing to proofs before secret key is released. the verifying proof must be able to look at the proof data and confirm it was constructed before the key was released. the verifier must have a sense of relative time of the publishing the proof data and the secret key. if the proof data is committed in a global hash, then the trusted party can claim the secret key was published after that hash, and since that hash was published after the proof data, the verifier can conclude the secret key was published after the proof data. I see no other way to do this, and this will be of the biggest overheads for the network, but likely we can keep the network structured so hopefully it the merkle tree can be computed quickly. the trusted party will have to know when to release the secret key. perhaps the trusted party need not know of the hash, and instead does not interact but just publishes it on time. it is then the job of the network to settle on a hash before the secret key is released. and then those in the network who concur on a certain hash will decide what code they trust (because that code will test for a certain hash). the trusted party should have a simple job, new instances of it should be able to be setup immediately, and the network should be able to switch between them if one is compromised. 

state shape? if we use tree shape, we still have the trouble of waiting for the whole tree to be verified. also the trouble of updating different parts of state separately, and relying on them to represent a unified state.
the alternative of every computation verifying its dependencies may be better. in the case that every thread depends on everything else and we have one giant state, we would end up with the same scheme as a tree, where all computations would form a verification tree, and only then computing the next layer of leaves would start by referencing the final verification of the previous tree. to minimize redundancy of verification, certain computations could be devoted fully at verifying a series of computations, packaging them together a single dependency such that another computation only need verify that package. in this model the chain is a DAG, not a chain of trees. a given line of computation can split and reconnect later. a computation only needs to wait for its dependencies to be verified, not the whole chain. so overall we have a DAG of computations, each one verifying its predecessors and possibly doing additional computation with the outputs of those computations or its own custom input. I'm assuming outputs are in the form of vectors as linear PCPs. I think the network-wide design is a given layer of the DAG, across all threads is computed at a time using the same proving key. All the proofs are committed, the key is released together with the next proving key, then the next layer begins. it may seem a benefit of this design is that is suits long narrow circuits, and this would indeed be the case if a private party wasn't needed. but with this way of PCPs and a private party, the network must move in sync and every round it must construct a giant merkle tree. for this reason, each layer should be large enough to be worth the time for a commitment, that is if computations are too small then proceeding at the pace of the network, even if the network constructs commitments back to back with no gap, would make computation too slow. 
what is the unit of computation? this should be chosen based on the complexity of verifying other computations. since we want each layer to compute a decent amount we should look for a construct suitable for a general computation and we want whatever is most efficient.
a computation may like to use output from a layer prior to the previous. for example, a package manager may only validate its dependencies, but not provide the outputs themselves, but instead specify the dependencies verifying they're valid. it seems the 'chain' may actually be the commitment trees chained together. 

QAPs. there is transcript W. with bounded degree, an arbitrary number of constraints that should evaluate to zero can be specified on these items. I suppose the queries can be simple with no r query. the queries qi for each linear function of W are encrypted, after of course the trusted party first chooses a random point and evaluates all polys. the trusted party also constructs a random linear combination of these queries in decrypted form. prover commits to evaluations on all queries. then the random coefficients and the keys for each query are revealed, and the original random point for evaluating the root poly, then the verification takes place. unfortunately this must take place for every separate circuit. so likely there would be a different trusted party for each query. unfortunately evaluating the polys at the secret point takes S^2 time. in most constructions this is not a problem because its only done once, but in this construction a new point is chosen every round so its not so convenient. assuming we want many different computations interacting one computation will have to verify several of different types which means it will interact with a different trusted party for each one. Handling many different trusted parties which must all interact, and be trusted together by the network, and must also perform S^2 computation each round is not so desirable so QAPs seem an unlikely candidate.

beyond QAPs, Hadamard, and sumcheck are there any other evident options? we want the LP to have the same queries across both circuits and instances. I can't think of any right now.


Sumcheck: an some integrand v variables each with their own sum sets, that is there are v summations. a single value is claimed as output. the prover commits to this value, as well as v univariate polys, one for each summation. The trusted party constructs the encryption of v random variables under the same key. P should have no trouble evaluating the integrand on each combination of these random variables, which result in P's v univariate polys. Each such poly should be delivered as a univariate where the coefficients are tuples of degree equal to the round number, ie first poly has degree 1, while 0th round is P's claim of the output. During verification, V takes each of these univariates starting with the 1st, decrypts each of the coefficients, then evaluates the resulting decrypted poly on the sum corresponding to that round. V then compares the result with the result of the previous decrypted poly evaluated on the current decrypted random variable. the only difference between this and the regular sumcheck is that all variable are given prior to the proof, and they are encrypted. 

the prover can perform the check himself. make poly, evaluate over sum. evaluate previous poly on random point. make sure two evaluations are same. P is cheating by having the current poly not correctly represent the integrand, but have the next poly correctly represent the integrand. P takes the next poly, which is not flexible because it must be correct, and evaluates it over its sum. since the next poly and the current do not represent the same thing, they are not identical in the current variable. if the current variable was random they would probably not agree. but P knows the current variable and the evaluation of the next poly at that variable, and must construct the current poly such that evaluated on that variable it returns the same particular answer. Of course without decrypting P does not know what this answer is, so P must rely on constructing the current poly so that is encrypted form upon evaluating on the current variable has the same form as the next poly evaluated on its sum. P's flexibility to cheat is to choose the current poly. That is P can construct any appropriate degree poly with the coefficients all in proper tuple form. ok so consider the next and current polys both evaluated everywhere but the current variable. they are not identical functions. they will both be evaluated on the same encrypted point, which P knows when constructing the current poly. Can P construct the current poly such that upon evaluating the two polys at the current variable they will yield identical encrypted form? from below, we see that if two different functions evaluate the same on a point, their encrypted evaluations do not evaluate the same on that encrypted point. if we take the contrapositive of this statement, it says that if two encrypted evaluation are the same (not different), then it is not the case that the two functions are different and the point is the same, that is either one of two cases: the functions are the same and the point is the same, or the functions are different and the points are different, or the functions are the same and the points are different. Suppose V checks that the encrypted forms are the same. Then if the functions are the same the prover is honest, and if the functions are different than the points must be different, in which case decrypting will reveal that the points are different. so in the sumcheck, upon every check, suppose V sums the current poly in encrypted form, and evaluates the previous poly in encrypted form, makes sure the two have identical encrypted form, and then decrypt and make sure the decrypted forms are also the same.

p: 3
first: 2*x + 4
second: 3*x + 1
encrypt p: (j, 3 + k*j)
encrypt 1st: (2*j, (2*3 + 4) + 2*k*j)
encrypt 2nd: (3*j, (3*3 + 1) + 3*k*j)


Theorem 1: Suppose we have two different univariate polys. Consider this function as a vector's dot product with the powers of the variable. Now consider a particular value p and its encrypted form p'. Based upon j, p' is random with respect to p and vice versa. Thus the vectors of the powers of p and those of p' are random with respect to each other. Thus if the two functions evaluate the same for one vector then with high probability the two functions are not random with respect to the other vector. In this case, the functions must be random with respect to the other vector. Then with high probability the two functions will not agree on the other vector. 
Actually this must only be proven one direction. Suppose it's proven that if the two functions agree on p, then they do not agree on p'. The contrapositive is, if they agree on p' then they do not agree on p. 

Theorem 2: Consider two univariate polys of degree d (d + 1 coefficients). When evaluated on an encrypted value p', the highest power d of p' will have width d + 1. If the evaluations are to agree on p', their leading coefficients must agree. From this it is straightforward to show that all coefficients must agree. Now we can say it is not possible for two different polys to agree on the same p'.

Theorem 3: Given an encrypted value v', suppose you are able to find another encrypted value w' != v' such that w = v, while not knowing v. Simply subtract one from the other such that you have a valid tuple of the same width that will decrypt to 0, that is the encrypted message is 0. Although k is unknown, you can set up the decryption univariate of degree d, equal to the width minus 1, with k as the unknown. Set the poly equal to the message, 0, then solve the equation for k, that is solve for the roots of the polynomial. There are at most d roots, so at most d possibilities for k. 
how to choose the right one? suppose you can exhaustively search the k's. Is one uniquely right? I think no information given by the encoding of the iid random variables can reveal the correct key. it could be any of them, and the variables would all be correspondingly random. but we can make use of the linear PCP consistency query by using the key to decrypt the queries, then comparing it to the consistency query, and with high probability only one k will correspond to a valid pair of alpha and beta. the logic goes that with high probability that if you can create w', then you can find k. the contrapositive is our theorem statement, that is if you can't find k, then you can't create w'.
... assume for now this is sufficient. this assumes the consistency query uses both alpha and beta. if it used only alpha, then alpha could be tested immediately and k could be found quicker.



Theorem 4: I must redo Theorem 1, because the univariate polys don't have raw coefficients, but encrypted of up to v + 1. We'd like to establish that if two polys agree on p, they will not agree on p', or vice versa, because these two statements are contrapositives of each other. We must consider the coefficients of the polys being of any width. 
Given a univariate poly, when evaluating on p, the result's right item is the dot product of the powers of p with the right items of the coefficients. when evaluated on p', the result's right tuple is the dot product of the powers of the right item of p' with the right items of the coefficients. Keep in mind the right item of of p' is random with respect to p. 
Given two univariates, suppose they agree on p' ( let the function be f.
	expand the evaluation on p' by expanding each power of p'. then f(p') = f(p) + f(powers of p' - powers of p)
)
consider the non-zero difference of two different univariates. if they agree on p, p will be a root of their difference. 
ok, suppose the prover constructs I1 such that I0 - I1 = Z yields a custom poly Z. P chooses this poly such that it has one root at p, and one root at p', this implies that I0(p) = I1(p) and I0(p') = I1(p'). Suppose the prove also wants I1(0) + I1(1) = r.

another sufficient theorem for our purposes would be that there is only one way to construct a coefficient-width-w univariate such that evaluated on a particular width-3 ri' it yields a particular width-(w + 2) vi'. 

or let us consider the decrypted polys. if they are different, 
if the two encrypted polys are different and agree on ri', then the decrypted polys will agree on ri. this is because 

what we can change theorem 2 to say is that it is not possible for two different decrypted polys to agree on the same encrypted point. we can get the value of I1 decrypted on ri', but can we get the value of I0 decrypted on ri'? 
hmm, but the encrypted value of I0 on ri' 

(a,b,c)(x,y,z) + (d,e,f)(0,y,z) = (ax, ay + bx, az + by + cx, bz + cy, cz) + (0, dy, dz + ey, ez + fy, fz)

consider only raw univariate polys as functions
if two different functions agree on p', then they agree on p. (this is never the case)
if two different functions do not agree on p, then they do not agree on p'.

if two different functions agree on p, then they do not agree on p'. 
if two different functions agree on p', then they do not agree on p. (this is never the case)

if two different encrypted functions agree on p', then the decrypted functions agree on p.

we'd like to establish the following.
if two different encrypted functions agree on p', then the decrypted functions agree on p'. 
i don't think this can be established.


maybe we could ask for each univariate to have the newest variable encrypted under a different key, this way upon receiving the poly we can decrypt it under the first key, and thus obtain a valid value of the raw poly at the encrypted point. then this could be compared to the invalid raw poly at the same decrypted point, and tested if they're equal, and if not they are different functions.
it seems this would work as long as encrypting the same variable under two different keys does not reveal anything
(j, m + k*j)
(i, m + k'*i) = (i, (m + i(k' - k)) + k*i)
the last equation shows that encrypting m under a random different key is equivalent to encrypting the random value (m + i(k' - k)) under the original key, and this value is indeed random due to k'. but k' can only be a source of randomness for one new encrypted value.
remember how we can encrypt k under k' and put any m under k'?
(0, m + k*j) - (0,j)*(i, k + k'*i) = (0, m + j*k) - (j*i, j*k + j*k'*i) = (-j*i, m - k'*j*i)
so suppose we use this mechanism to let the prover re-encrypt every variable under the new key. 
does this leak any info? (i, k + k'*i), this can be thought of as (1, k'*i + k*1) where although i is known k'*i is completely random. in other words, this is the encryption of a random number under k but with j = 1. since the random number could be anything, k could be anything.another way to think of it in its initial form is that k could change arbitrarily and k' would change accordingly. so i don't think it leaks.

New Proof:
	This time we will move from the top of the sumcheck to the bottom. Suppose there are d levels. Suppose we can establish that the dth level (the top level) is correct. By correct we mean a given poly fully decrypted correctly represents the integrand. Note that if a poly given was not correctly computed but it still decrypts to the correct poly it is regarded as correct. We'd like to prove that if the ith univariate is correct, and the ith test passes, then the (i-1)th univariate is correct. Prove by contradiction, supposing there exists an i such that the ith univariate is incorrect, the (i+1)th test passes and the (i+1)th univariate is correct. 
	The test is the following: The verifier decrypts the (i+1)th poly under the first key, creates the sum, and obtains a valid vi'' of the correct ith poly at ri''. The verifier then decrypts the incorrect ith poly under both the first and second keys, and evaluates at ri''. If the answer is vi'' the test passes, otherwise it fails.
	By Theorem 2, since the incorrect ith poly is different from the correct ith poly, it cannot evaluate to the same value vi'' on the same input ri''. Thus the test will not pass.
	Since the (i+1)th poly is correct, by decrypting under both keys and summing, the verifier can get a valid evaluation vi of the ith poly at ri. Then suppose the verifier decrypts the (i+1)th poly only under the second key and then sums, and obtains wi''. Supposing wi'' = vi'', (the supposedly V knows that f(ri'') = vi''). Suppose wi'' != vi'' but decrypting wi, V finds that wi = vi. Then to obtain wi'', P must have broken the system.

	(
		let us consider 'correct' to be what the prover should send, that is the encrypted representation. then we can say the verifier can decrypt the correct ith poly under the first key and get a correct representation of f(ri'') = v''. Now we can examine the incorrect ith poly, and we'd like to say that its value decrypted will not be the correct decrypted form. the we can say that evaluated at ri'' it cannot be vi''. 
		so we want to say that P cannot create a different poly that will decrypt to the correct one. well actually to do this, each coefficient would have to decrypt to the correct one. P already knows the 'correct' encrypted coefficients, but chooses not to use them. P must create different encrypted coefficients that decrypt to the same values. we established previously that this is not possible without k. thus if the encrypted coefficients are incorrect, then the decrypted coefficients are incorrect, so evaluated on ri'' it will not return vi''. 
	)

	(1, r1 + k)
	(1, r2 + k)
	(2, r1 + r2 + 2k)
	(1, r1 + r2 + 2k, r1*r2 + k*r1 + k*r2 + k*k)
	(1, -k' + k), subtract from anything encrypted under k to obtain encryption under k'. 
	provably this doesn't leak info because everything including k' is random and presented as a sum with random k, so any of them could be anything.
	so it appears we don't need random j's anymore. 

	just cuz we know the poly 

	suppose correct or incorrect means whether the prover evaluates them correctly on the encrypted point. suppose we can establish this for the top level via the queries. the we can establish the correct value of the ith poly at ri''. Since the provers encrypted ith poly is incorrect, the provers decrypted ith poly is also incorrect

	we want to show that if the correct ith poly on ri is vi, which can be verified, then on ri'' it equals vi''.
	suppose we can establish this with high probability, with something like saying the prover must know vi. 

	P can construct f such that on ri'' is evalutes to another, say wi''. if wi'' != vi'' but wi = vi then P has broken the system. If 
	if f(ri) = vi, what's the probability f(ri'') != vi'' ?
	or if f(ri'') = vi'', what the probability f(ri) != vi
	f(ri'') = vi'' => f(ri) = vi
	
state shape. need a way to prove absence of something, eg another payment, another write to the database. the only way I know to prove something didn't happen is to record everything that did happen and show by showing existence of something in that, one proves absence of something else. 


suppose you take a poly, and evaluate it on encrypted input, and evaluate it on same decrypted input. if the input 



can decryptions be aggregated? a decryption condition can be expressed as a polynomial evalution at the unknown point equal to 0. So I think by choosing a random scalar (not a variable but an actual number) for each poly, and summing them together you can get good proof, and the secrets need only be released at the very end. but of course the nodes must then trust each other.

Note: consider a plain or encrypted poly, and a plain or encrypted variable. any of the 4 combinations of the poly and the variable can be computed, and then decrypted they will all yield the same result. 


Start from the bottom of the sumcheck. the claimed value is incorrect for the integrand. we state 'correct' or 'incorrect' in terms of the integrand that V will evaluate at the end. with high probability, if the ith univariate is correct, and the ith test passes, then the (i + 1)th univariate (such as the answer) is correct. The contrapositive states that with high probability, if the (i+1)th univariate (such as the answer) is incorrect, then the ith univariate is incorrect, the ith test fails, or both. Thus in order to cheat, there must exist an i such that the (i+1)th univariate is incorrect, yet the ith univariate is correct and the ith test passes. So let us consider a correct ith univariate, and an incorrect (i + 1)th univariate, and the case that the ith test passes. With what probability will this occur?

Although in the test the decrypted (i + 1)th poly will be summed over a series of constants and compared with another value, suppose this restriction does not apply. Instead suppose the test consists solely of evaluating the decrypted (i + 1)th poly at a decrypted value ri and comparing it with the evaluation of the decrypted ith poly summed over the constants. The prover knows ri' but not ri. Let vi and vi' correspond to the output of the ith poly on ri and ri' respectively. The prover knows vi' but not vi.

One method for the prover to use is constructing the (i + 1)th poly such that when evaluated on ri', it yields vi'. It can be shown that the only possible construction for this to occur is the correct construction, contradicting our assumption that the (i + 1)th poly is incorrect. There are two ways to prove this.
1: The correct ith poly takes the value vi' when summed over the constants. But the correct ith poly summed over the constants will yield the same value as the correct (i + 1)th poly at ri'. Thus on ri' the incorrect (i + 1)th poly must evaluate to the same value, vi', as the correct (i + 1)th poly. But by Theorem 1, if two different polys evaluate to the same value on ri', then they evaluate to different values on ri. Thus these functions will not pass the test.
2: If the two functions are to evaluate to the same encrypted value, then by theorem 2 they must be the same functions. This is basically a contrapositive of the first proof, and I think its a better proof because Theorem 2 is better than Theorem 1.

On the other hand, if the correct and incorrect (i+1)th poly evaluate to different values vi' and wi' on ri' respectively, then when vi' and wi' are decrypted, either they will be different or the same. If they are different they will not pass the test. If they are the same with high probability then with high probability by Theorem 3, the prover can find k, which is contradictory.

The correct (i + 1)th poly is different than the prover's incorrect (i + 1)th poly. So if the (i + 1)th poly is to take the value vi' on ri' such that it passes the test, it is effectively taking the value of the correct (i + 1)th poly at ri. By Theorem 1, if these two

Beyond the above method, the prover is left to construct the (i + 1)th poly such that on ri' it yields a value wi' != vi' yet wi = vi. Suppose the prover is able to construct the poly such that on ri' it yields any desired wi'. Then the prover is left to choose a wi' that will decrypt to the same value as vi'. The only information that the prover has to pick wi' is vi', so by Theorem 3 above, this is not possible without knowing k. 


MIP2 vs GKR: using the sumcheck method we can have an integrand that should disappear on a series of points. at the end the integrand is evaluated at a random point. the integrand would consist of it input and its witness. I don't see any problem with MIP2, and it may enable more forms of computation than arithmetic circuits.


General Scheme:

instead of the trusted party using a digital signature, the trusted party can hash together the content for the first phase, and separately hash the reveal content for the second phase. Then in the first phase output the hash of the first phase content together with the hash of the second phase. In the second phase output the content for the second phase and one can hash them and see it must have been sent by the same party.

i think the way it should work is at any given round there is a set of 'programs', each defined by logic.
we have to think about what programs operate on, and whether it matters what they operate on, or just that the operation is valid? this is also an opportunity to think about the programming model, and applications. I consider this below.



consider the generic process of verification. what is input, what is witness (input the verifier doesn't care about), what is logic, and what is output? abstractly, what does the verifier care about? the verifier cares about the input and the logic. the verifier also cares that the proof data was committed before the key was released. for this reason, the verifier wants a certain commitment hash, and a certain reveal hash. To know that the logic and input are correct, the verifier wants hash roots for both those trees, along with bistrings indicating the positioning of the answers which corresponds to the particular logic and input polys. 
inputs: 

...
a circuit must validate its dependencies. consider the inputs and the logic necessary for verification of a single dependency. The input is the proof data for that dependency, which is in the form of the data itself with a merkel path to the last commitment root hash. The data itself consists of the each of the prover's messages (in the form of a list of encrypted coefficients) in the sumcheck, as well as the provers answers to each of the (probably 3) queries. Another input is the key and the random coefficients revealed by the trusted party. The verification process first decrypts everything, then performs the checks for each round of the sumcheck, then evaluates the integrand using the query answers and doing the consistency check for them. But of course the verifier cares that the computation used certain inputs and certain code. So suppose the integrand consists of three type of functions, the witness, the input, and the logic (eg io, add, mult). The witness can be evaluated with a linear PCP, but the verifier will evaluate the input and logic manually. This whole process of verification is doable by a verifying circuit, but this circuit will not do the last part of evaluating the input and logic by itself. This is because both input polys and logic polys should be common across a large number of computations, and the whole network should be able to verify them. Actually I suppose the whole network can be doing this using the encrypted variables, and coming to consensus on the answers at the same time as constructing the proofs commitment. So another piece of data for the verifier is the encrypted evaluations for the relevant input and logic polys and merkel paths that prove they were agreed upon by the network and also serve to indicate the ordering of the answers so its confirmed those are the answer for the appropriate poly. So the verifier will decrypt these answers and use these them for evaluating the integrand. The verifier outputs that the computation is correct, and cites the hash roots it used for commitments, and input and logic answers, and also cites the secret values it used. 

now consider a verifier that must verify the previous verification. 


APPLICATIONS:
in general the context is that anyone can compute as usual, but with the new restriction that everyone must compute in sync, and with the new flexibility that you can assume other's computations are valid when you use them as input. 

think of a digital currency. how to prevent double spending? if we have to, we could do the state tree, but those are slow and remember we want to avoid opportunity for state inconsistencies. what if every thread represents an account, and at each round the account can point to any number of other accounts specifying how much it would like to transfer. at every round, the account also uses as input any accounts that are pointing to it, and updates its balance accordingly. data availability is not a problem, because each user is responsible for his own account. if other accounts disappear that doesn't damage others on the network because it only destroyed their own value. every account must be updated every round. if this is too costly, one can send one's money to a 'bank' account that holds it. of course account addresses would be necessary, and accounts must know of their addresses so that only appropriate accounts can receive money. Perhaps there is a special account, the root account, that creates other accounts. so the validity of an account comes from its verification chain to the root account.

a db would enable persistent state so threads could share data and operate like users sharing a server. i like the concept of having a database of arbitrary size, and a chain that can operate on it. of course the database itself cannot be input, but instead we need some way to verify that a value read was the last value stored. suppose in any round, only one thread can write to a given address, and any number of threads can read from it. a read operation could specify and prove that an earlier write operation wrote that value, but it remains to prove no other write operation occurred since. one inefficient possibility is each part of the db can only be written to by a single thread. the thread outputs new write values. any other thread wishing to read from that part of the db, must 
...


how about having multiple sub-trusted parties such that individuals can run computations faster without waiting for the rest of the network. this would be possible but require the trusted party to have signatures. 
what if we returned to the cell concept (together the unikernel cells in the cloud), and modeled threads as cells, such that it is natural for threads to split just like cell division. but the difference is we can trust a cell to only split when it should, be users can split threads whenever they want independent of the thread's logic. 

it would be great if we could unify the cloud and the chain to enable privacy and solve the splitting problem simultaneously. i suspect most computation will involve private data in which case what use is the chain? we could use the cloud for everything but we shouldn't have to rely on it for everything and we'd like to make use of the chain some of the work. We want to rely on the chain as little as possible in the sense that if it fails we can easily recover. One possibility is having the unikernels coordinate passing around data to each other, and then outsourcing computations on it to the public symmetrically encrypted form friendly to homomorphic operations. verification of outsourced operations could also be outsourced operations. when appropriate the unikernels would decrypt computations. in fact, even the heavy task of encrypting new data could be outsourced and performed under homomorphic encryption. the unikernels would be responsible for preventing splits. maybe the task of unikernels could be so light that we don't really need a cloud and trusted IoT devices like personal phones could perform all necessary trusted tasks. a first priority for this is finding a homomorphic encryption scheme that can encrypt any data, that is data that is partially predictable but does not compromise the rest of the data (as the sumcheck random point scheme does). if we can do this then we really only need to program unikernels for real time reasoning, because anything computationally intensive or involving algorithms would be offloaded. we like to handle any kind of operation, including neural nets. 
hmm, what we did the elgamel type but didn't release j, because the only purpose of j is to keep track of the inputs the user has which the encryptor does not know. but since we are doing a verifiable circuit we may be able to keep track of the garbage without exposing j. but even if this works we still have the problem of the width growing too large. we don't want the unikernels to have to decrypt and re-encrypt after every computation. 
ideally i could find a way to use random numbers so i don't have to rely on cryptographic assumptions. 
(r1 + j*k) + (r2 + i*k) => (r1 + r2 + k(j + i))
(r1 + j*k)(r2 + i*k) => (r1 + r2 + k(j + i), r1*r2 + k(r1*i + r2*j))
freshman's dream looks like an opportunity for FHE. this idea is also called the Frobenius endomorphism. in the binary field, the map is x^2. but of course to have privacy we'd do (x*j)^2, but whatever the final results are they would still be disguised with these random numbers. 

ooo, use random numbers,
then have someone else compute with those random numbers under another key, but this time they're random. 
a problem with random numbers is that things cannot be compared because of noise of the random numbers
how about multiparty secret computation. all data would be held unencrypted by whoever should have access to it, which may be nobldy but an autonomous unikernel in the cloud.
the need for privacy might change the verifiable computing method we use. all we need is a way to compute on encrypted values. we could encrypt values and then outsource multiply them by constants and adding them together, we can also outsource multiplications, but separately. a linear function or multiplication must first be decrypted then re-encrypted before the other operation type can be performed on it. is it possible to outsource the decryption and re-encryption of data? i think decryption could be a linear function, and if we can outsource that we can outsource decryption. I think encryption for linear functions would be m + j for message m and random value j. by computing the linear function on j, by outsourcing via j + k for all j, one can decrypt the linear function on m. hmm, maybe we don't need multiplication, if we can construct the coefficients of the linear function via previously computed functions decrypted then re-encrypted. 
maybe we can use somewhat homomorphic computing if our circuits are shallow enough. this is because traditional schemes don't require random numbers so we should be able to compare values, which we cannot do when random numbers are present. but a concern is that the homomorphic scheme would enable users to see which values are the same. hmm, but it looks like the 'noise' of current schemes are random values, which is good news. 
suppose we can get FHE or SHE to work over low depth circuits and with GKR. then i just realized we can use any bootstrapping scheme we please to allow the prover to obtain the witness of the computation, then in a low depth circuit we can verify that witness without any bootstrapping. 


idea of only revealing secrets at top, and 
how will threads express their address, it can't be in code because addresses are too specific. i think it can be in input. and the computation can verify the given address is consistent from verifying the previous instance of the account. what if one splits an account into two, replicating value? 

poly div method seems only one remaining, but consider using poly div in place of sumcheck for disappearing multivariate problem
but my hope for using linear PCPs given a trusted machine should not be lost!


STRUCTURING THREADS
we want to prevent a thread from splitting into two. so we want to assign each thread an address, and then enforce via the commitment tree that there is only one continuation of that thread. this means any continuation of a thread must appear in the same place. the commitment tree could be constructed (assuming trust throughout the network) such that each thread must be located in a certain place. the next round of proofs can verify that the path of each thread corresponds to its address. to prevent double spending it must be that each thread only has one possible path. So the network must cooperate to produce the commitment tree. this is challenging, especially when addresses must change. 

what if we use a virtual message space. of course general problem is making sure messages sent are received and not ignored. we need a mailbox that a receiver is forced to check every time, and that a sender is forced to fill on order to send a message. one possibility is a state hash tree. another possibility is having a graph of delivery paths, where each thread has a set of neighbors and it must check those neighbors every round for a message. this should reduce the problem of messaging between two arbitrary parties to messaging between two deterministic parties. 

Maybe we can use post quantum cryptography to prevent a setup, eg with the vector commitments scheme. But of course for this I have to learn a lot. 


Discouraged about learning anywhere in China, even cryptography at UESTC, because professor is not so willing. Also, faculty in general is not so great so it won't be surprising. I'd like to learn on my own in an inspiring environment, like MIT. If I had enough money I could do this. If I could do an ICO with a proof based currency (or some other application) I could get the money. If I can find a solution to the uniqueness problem I could use my existing solution. I pursue this under ICO. 



oh, interesting idea is that we use encryption on data for privacy, and we enforce correctness separately by making an invalid computation detectable. that is, encode it such a part of the computation should give a certain result that has an expected value when verified, and this value is correct iff the entire computation is correct. 


g(q) is the field element that f is most likely to output, with probability c
assume for contradiction g(q) is not most likely value
probability f outputs g(q) > 1 - c
probability f doesn't output g(q) < c
contradiction: probability f doesn't output g(q) > c
this means probability f output g(q) < c
probability f doesn't output g(q) > c
now partition F into two sets, and probability f outputs one from either set is > c/3
but this means f can output something other than f with probability > c/3
but this means f can output two different values with 

contradiction: probability don't pick g(q) > c or probability pick g(q) < c


first establish boundedness, that there is an extractor function we can assume is queried every time instead.
second use linear test to assert it 

understanding existing versions:
extractor is an algorithm that extracts the function g. 
with high probability if receiver does not reject, then it outputs g(q). in other words, if test passes, we can assume all queries are made to linear function g. 

show bound to nearly linear function g.

Lemma 0:
are bound to some function

Lemma 1:
exists extractor function g

Lemma 2:
probability of test passing and query results not equal to g results in negligible

Lemma 3:


Eb:
	Eb = (2/9)(E1)^3 

El:
	El = 1/N + Es

E1:
	E1 = (Eb * 9/2)^{1/3}
	With prob < E1, g(a)

E2:
	E2 = ((9/2)^{1/3} + 1) * (Eb)^{1/3}

E3:
	E3 = E1 + E2 = (2 * (9/2)^{1/3} + 1) * (Eb)^{1/3}

Ec:
	evaluations are on extracted function g except with probability Ec
	Ec = mu * 2 * E3

Ef:
	Ef = El + 6 * E3


partition space into two sets, those where the are equal and those where they are not. then map via permutation

f(a) + f(b) = f(a + b)
f(1) + f(1) != f(2)
if distance is one more


so assume extractor exists, its the function used, and 


for now think about generic LP but where prover commits over two different encrypted random queries, and a random linear combination of the two queries decrypted. we want to prove soundness first, and then later hopefully zk assuming queries are uniformly random. 
soundness.
show that 

if the test passes, then with high probability there is linear function g such that f(q) = g(q). so what is the probability t
g is closest linear to f. 

assume 

rej(x) >= 3x - 6x^2

x >= 1/4, 

min, 2/9, 3x - 6x^2

if test passes then with probability c prover is bound to linear function. 


how to take a regular LP and make it zk. 

with zk sumcheck below we must evaluate on two polys R and F at the same encrypted random points.


if P can pass the test without being honest, then P can break the system. We'd take the contrapositive and say if P cannot break the system then P cannot pass the test unless honest.

queries are same, answers are different, test passes
queries are different, answers are different, test passes
queries are same, answers are same, test passes
queries are same, answers are different, test fails


Not to be bound means running two tests with same queries (same or different consistency query which is defined by alphas), get different answers, and both tests pass

f(q1) = b1, f(q2) = b2, f(a1*q1 + a2*q2) = b3
f(q1) = b1', f(q2) = b2', f(a1'*q1 + a2'*q2) = b3'

if you don't know q1 or q2, you don't know a1 or a2
contrapositive: if you know a1 and a2, you know q1 and q2


for all q1, q2, suppose probability f(q1) + f(q2) = f(q1 + q2)

(1, r1 + k) * (1, r2 + k) = (1, r1 + r2 + 2k, r1*r2 + k(r1 + r2) + k^2)
(1, r1 + r2 + 2k, r1*r2 + k(r1 + r2) + k^2) * (1, r3 + k) = (1, )

(1, r1 + r2 + 2k, r1*r2 + k(r1 + r2) + k^2)
(1, r1 + r3 + 2k, r1*r3 + k(r1 + r3) + k^2)
(1, r2 + r3 + 2k, r2*r3 + k(r2 + r3) + k^2)

(1, r1 + r2 + 2k, r1*r2 + k(r1 + r2) + k^2)
(0,            1,                   r1 + k)
(0,            1,                   r2 + k)
(0,            0,                        1)

(0/3,        2/2,         (r1 + r2 + 2k)/1)


(1, r1 + r2, r1*r2)
(1, r1 + r2 + r3, r1*r2 + r1*r3 + r2*r3, r1*r2*r3)

let g be the closest linear function to f, such that they agree with probability e
suppose c*g(a) = f(a), d*g(b) = f(b), then what is probability f(a) + f(b) = f(a + b)?
f(a) + f(b) = f(a + b) => c*g(a) + d*g(b) = g(c*a) + g(d*b) = g(c*a + d*b) = f(a + b)
if c = 1, d = 1, the probability is e
if c != 1, d = 1, the probability is 
	c*a + b - (a + b) = a*(c-1) != 0 unless a = 0
	so probability c*a + b = a + b is 1/N, so probability they are same is e/N
	on the other hand, suppose c*a + b != a + b
	g(c*a + b) = g((c-1)*a + a + b) = (c-1)*g(a) + g(a + b) = f(a + b)

if c != 1, d != 1, 

h(a)g(a) = f(a)

suppose f is unknown, g is linear, they differ on d out of N locations where |F| = N
let h be a corrector function such that h(a)g(a) = f(a) for all points d points where f and g differ. Then h can be a linear function of size d.
suppose h is the 
suppose f(a) != g(a) and f(b) != g(b), then
first suppose f and g do not agree on (a + b)
=> f(a) + f(b) = f(a + b) 
=> h(a)g(a) + h(b)g(b) = h(a + b)g(a + b)
	=> h(a)g(a) + h(b)g(b) = h(a)g(a) + h(b)g(b) + h(b)g(a) + h(a)g(b)
	=> 0 = h(b)g(a) + h(a)g(b) = (b/a)h(a)g(a) + (a/b)h(b)g(b) = (b/a)f(a) + (a/b)f(b)
	=> (b/a)f(a) = (-a/b)f(b)
	=> f(a)/f(b) + (a*a/b*b) = 0
=> g(h(a)*a + h(b)*b) = g(h(a + b)*(a + b))
=> g(
		h(a)*a1 + h(b)*b1, ... , h(a)*an + h(b)*bn
	) = g(
		h(a + b)*(a1 + b1), ... , h(a + b)*(an + bn)
	)
=> g(
		h(a1*a + b1*b), ... , h(an*a + bn*b)	
	) = g(
		h(a1*a + b1*b + (a1*b + b1*a)), ... , h(an*a + bn*b + (an*b + bn*a))
	)
=> g(
		h(a1*a + b1*b), ... , h(an*a + bn*b)	
	) = g(
		h(a1*a + b1*b) + h(a1*b + b1*a), ... , h(an*a + bn*b) + h(an*b + bn*a)
	) = g(
		h(a1*a + b1*b), ... , h(an*a + bn*b)
	) + g(
		h(a1*b + b1*a), ... , h(an*b + bn*a)
	)
=> 0 = g(
		h(a1*b + b1*a), ... , h(an*b + bn*a)
	)
now lets examine the query to g in terms of a,b, and the size of h. 
normal possible queries for g is N^n
now how many possible in the form of h(ai*b + bi*a)
there are N^2n possible 


now suppose f and g do agree on (a + b)
=> f(a) + f(b) = f(a + b) 
=> h(a)g(a) + h(b)g(b) = g(a + b)
=> g(h(a)*a + h(b)*b) = g(a + b)
=> g(
		h(a)*a1 + h(b)*b1, ... , h(a)*an + h(b)*bn
	) = g(a + b)
=> g(
		h(a1*a + b1*b), ... , h(an*a + bn*b)	
	) = g(a + b) = g(
		a1 + b1, ... , an + bn
	)


h(g(a)) = f(a)
h(g(a)) + h(g(b)) = h(g(a + b)) = h()


h(a*a + b*b) = h(a*a + b*b + 2ab) => h(2ab) = 0 => h(ab) = 0
so what is h? it is a function that can map arbitrary values to arbitrary values.


let f be an unknown function with signature F^n -> F, with |F| = N. 
There exists a multilinear function g with identical signature and has the same mapping. So we may regard g as f.
For random a,b \in F^n, what is the probability g(a) + g(b) = g(a + b)?
consider all the different forms g could take, and in every case, what is the probability the equation holds
for each case let's examine g(a + b) - (g(a) + g(b)) and determine the probability it is zero.
then for each possible form of f, only one of which is correct, lets add up the probability this remainder is 0.
for simplicity, for now lets assume for each case that f is of degree >= 2 (ie nonlinear), the probability is 1/N.
how many possible forms are there of f?
take the case it has degree d, possible values for d are 2,...,n
now take the case it has t terms, possible values for t are
the number of possible terms are 
each variable can appear 0 times, 1 time, ..., 
there are 2^n possible terms. for each term there are N possible values. 
so just think of all the terms layed out, and then just choosing a coefficient for each one. 
N^{2^n}
now if each possibility passes the test with probability 1/N 

we should reason that it must take a multilinear form. and that if it has degree other than 1, it will not satisfy with high probability. 
there are only n variables, and if each can only either appear or not appear in a term, then there are 2^n possible terms. to have F^n possible mappings it can't be multilinear but instead must be of degree F. This is provable. 
Now think about a degree F multi poly evaluated on (a + b). think about a given term. it has form 
c (\prod_{f \ a1} (f - x1)) (\prod_{f \ a2} (f - x2)) ... (\prod_{f \ an} (f - xn))
or think of every term as x1^d1 * x2^d2 * ... * xn^dn,
(a + b)^d1 * ... * (a + b)^dn
this isn't helpful anymore because g(a + b) - (g(a) + g(b)) would still be of degree F. so the probability it would equal 0 would be < |F|/|F| = 1 which is no help.


another way to use secrets:
what if connect line, return univariate. 
also evaluate on random point on line.
verifier checks evaluation at random point is same univariate at random point. 
problem is random point on line depends on other random points so it can't be fully encrypted
for random point r, variable i is of form r(bi - ai) + ai = r*bi - r*ai + ai = bi*r + ai(1 - r) + k
what if instead private party chooses random point on line and expands evaluation point. this is still solvable.

actually previously there was problem with decrypted consistency query because i think it can be solved.
in any case of using the encryption scheme, all encrypted values must be random. But the only way enforce something is to have redundancy. 

c = r + k + a*q - k
c - (r + k) = r + a*q - (r + k) = a*q - k

maybe a solution is to encrypt 6 completely random queries. issue each of them and issue a random linear combination of them decrypted. this should establish that the same function f is committed to all 6 random queries. they can each be encrypted under a different key.
then take the two queries, split each into 3 parts randomly, then issue each of these queries with one of the random ones. this should repeat the test 6 times. in total there are 6 + 1 + 6 + 6 = 19 queries. the real queries are themselves tuple encrypted. so in their consistency queries since r is decrypted they should also be decrypted.

omg, i just realized all the intermediate values of sumcheck, which will be around 30 width, will each need to be committed to separately. this is a lot of hash commitments. gosh, we either need a different way of commitment to proof data, or a method with far less commitments.

OMG, for this reason I'm leaving this file for now and moving back to the ICO file under NEW METHODS.


given the way we expand it, coefficients follow a pattern. maybe exploit this. 


f(a) + g(b) = f(a + b) => g(b) = f(a + b) - f(a) => g(a + b) - g(a) = f(a + b) - f(a)
f(a) - g(a) = f(a + b) - g(a + b)
suppose g and f are continuous real functions, and let b be small
then limit in b -> 0, this is equivalent g'(a) = f'(a)

suppose g(a) != f(a), g(b) != f(b), then what is the probability f(a) + f(b) = f(a + b)?


suppose f is a function such that the fraction of pairs a,b, such that f(a) + f(b) = f(a + b) is e.
now suppose we draw a,b at random. then with probability e it holds that f(a) + f(b) = f(a + b).


q = a1*b1 + a2*b2
f(b1) + f(b2) = f(b1 + b2)
a*f(q) = f(a*q)


Not to be linear means f(u) + f(v) != f(u + v) for all u,v. 

one problem is the queries are not completely random. 

think about testing real valued functions for linearity

maybe we could split each of the 2 queries into n parts, and then n times perform a 2 query test, and each should be independent, so security level should be (64/6)n, so n=6 would be great. doing this would serve two purposes. it would make each set of queries uniformly independent, and it would also increase the soundness n fold. 

if you split the encrypted values up, evaluate each, add the results, then decrypt
does this work? first lets assume the queries are uniform. and w

i think that once very term is computed, each item can randomly be split into n pieces. oh i get it, and n-1 of them would be uniformly random but the last would not. so actually we'd need to split them up into 7 pieces. 

maybe take advantage of the fact that evaluating the function on tuples is just evaluating the functions on scalars for each row of items. also, rows come in sets, where row i corresponds to all combinations of i variables. 


zero knowledge sumcheck:
https://arxiv.org/pdf/1610.03798.pdf

execute on R + b*F, because verifier doesn't know R or b, F could be anything. But if over the evaluation points, R = 0, and F = 0, then R + b*F = 0. But if F != 0, then R + b*F = 0 + a*F != 0 so soundness still holds. this is even the case with high probability of R != 0.
Oh, i think P chooses R, and only after that V chooses b. Then they execute on R + b*F, and at the end V evaluates R and F at random point. And actually R doesn't have to equal 0 for soundness, that's just for completeness so it's in the provers interest to have R = 0. So P chooses and commits to R, then b is determined by hash transform of R's commitment, and prover commits to F. we still need to give prover a way to choose a random poly R that sums to 0.

this the hash transform is unfortunate because it requires the verifier time R which is equivalent to the whole circuit size. an alternative may be to also encrypt b. 

hmm, at then prover knows integrand decrypted equals 0. but the decryption equation will be identically zero so I don't think you can solve for k.  


take the commutative relation R defined by (u,v) @ R iff f(u) + f(v) != f(u + v)
take relation S defined by (c,u) @ S iff c*f(u) = f(c*u), where c is scalar and u is vector.

Theorem 0:
# f is scalar-linear
	(u,v) @ R => (u+v,-v), (u+v,-u) @ R
Proof:
	f(u) + f(v) != f(u + v) => f(u + v) + f(-v) != f(u), f(u + v) + f(-u) != f(v)

Theorem 1: 
# f is scalar-linear; (u, u+v) !@ R
	(u,v) @ R => (2u,v) @ R
Proof:
	f(2u + v) = f(u + (u + v)) = f(u) + f(u + v) != f(u) + f(u) + f(v) = 2f(u) + f(v) = f(2u) + f(v)

Theorem 2:
	(u,0) !@ R => f(0) = 0
Proof:
	f(u) + f(0) = f(u + 0) = f(u) => f(0) = 0

Theorem 3:
# f is scalar-linear
	(u,u) !@ R
Proof:
	f(u) + f(u) = 2f(u) = f(2u) = f(u + u)

Theorem 4:
# f is scalar-linear; (u,u+v), (u,2u+v) !@ R
	(u,v) @ R => (3u,v) @ R
Proof:
	f(3u + v) = f(u + (2u + v)) = f(u) + f(2u + v) != f(u) + f(2u) + f(v) = f(u) + 2f(u) + f(v) = 3f(u) + f(v) = f(3u) + f(v)


start with (u,v) @ R, c = 1
(u,u+v) @ R => c = 2
(u,u+v) !@ R => (2u,v) @ R, c = 2

(u,u+v)
	(u,2u+v)
		(u,3u+v)
		(2u,2u+v)
	(2u,u+v)
		(2u,3u+v)
		(4u,u+v)
(2u,v)
	(2u,2u+v)
		(2u,4u+v)
		(4u,2u+v)
	(4u,v)
		(4u,4u+v)
		(8u,v)

commitment may do more than binding. encryption that only allows affine operations says if P invokes non affine operations, the result will be random with respect to the plain texts. This means the result is random with respect to the other answers, so the probability the equation holds should be 1/|F|. then from this assume the answer to the encrypted query is an affine function of the plain texts. A non affine function is one involving manipulation of inputs other than scalar multiplication and addition. Prove that non affine function will pass with low probability. Then assume all functions are affine, and prove they must all be same affine function with 0 constant.


prove if all functions are affine, answer will be wrong unless all are same affine function with 0 constant term.
suppose they are all linear but possibly different, and degree 1
f(a*x + b*y) = a*g(x) + b*g(y) = g(a*x + b*y), probability 1/N
f(a*x + b*y) = a*f(x) + b*g(y) => f(a*x + b*y) - f(a*x) = g(b*y) => f(b*y) = g(b*y), probability 1/N
f(a*x + b*y) = a*g(x) + b*h(y) => f(a*x) + f(b*y) = g(a*x) + h(b*y)
	now suppose f != g or f != h, then the two sides are different linear functions, so the probability is 1/N
	if f = g and f = h, this contradicts our assumption that the functions are not all the same
... not done


prove if any function is non affine, answer is likely wrong.

queries:
(j1, r1 + s1*j1)
(j2, r2 + s2*j2)
(j3, a*r1 + b*r2 + s3*j3)
check:
R3 - s3*L3 = a*(R1 - s1*L1) + b*(R2 - s2*L2)

i think proving post quantum security for my own encryption is not so easy, so I should use existing form of quantum encryption. 


ENCRYPTION
suppose there is an encryption scheme such that the encryption of any plain text is uniformly distributed, and random with respect to all other plain text encryptions. 

prove an encryption scheme can only be broken by brute forcing key space, and key space is double security level to protect against Grover's algo, which can search a space is square root time, which mean achieving same security requires double security bits. 
(j, m + s*j), key space is |F| so make |F| = 2^{128}
(g*j, g*(m + s*j)), key space is |F|^2, so |F| = 2^64
still need to prove these can only be broken by brute force, not by period finding algo or something else.


MULTI KEYS
what if we just evaluate a single linear function at the same random point encrypted a number of different times. No linearity testing. We just decrypt all the answers and make sure they are the same. For now just think of two encryptions. 


GENERAL METHOD
prove if prover can provide answers that solve equation when the prover 'cheats', prover can break system.
original LP uses cheating to mean giving different answers on same queries that both pass test. this form of cheating means using different functions, because we get different outputs on the same query.
but the form of cheating we need to prove is using anything but the same linear function. We can split this into two. The first is using different linear functions. The second is using at least one nonlinear function.

IDEAS
suppose we issue 3 random queries, and a random linear combination query for each of the 6 pairs of these 3 queries. Then there are only 9 queries.

what if the point is not random? this doesn't matter for encryption but it does matter for the squartz zippel lemma, so they must be broken into two queries. well actually we may be able to execute on completely random r, and r + q for very non random q, and the result would be random, and then the also execute on r under different key, and then infer answer of q. but this too requires 2 queries. i think 2 queries may require a linearity test. the query not being random is in fact the case for all of them. for sumcheck we'll need to issue multiple non random queries.


Let G be the integrand of the sumcheck. it is a v-variate, total-degree td function, and degree di in each variable.
There are v polys.
For i=1,...,v. P_i is to be G evaluated on D(x,i) = \sum_{bi in {0,1}} (b_1,...,b_{i-1},x,r_{i+1},...,r_v)
P_0 is G evaluated on D(_,0) = (r_1,...,r_v)
P_{v+1} is G evaluated on D(_,v+1) = \sum_{bi in {0,1}} (b_1,...,b_v)
The 0th test is:
	P_0(D(_,0)) = P_1(D(r_1,1))
For i=1,...,v-1, the ith test is:
	\sum_{h in H} P_i(D(h,i)) = P_{i+1}(D(r_{i+1},i+1))
The vth test is:
	\sum_{h in H} P_i(D(h,i)) = P_{i+1}(D(_,i+1))

 
Theorem 1: For i=0,...,v, if P_i is correct, and the ith test passes, then with probability d_{i+1}/|F|, P_{i+1} is correct.
Proof:
	Suppose towards a contradiction that P_i is correct, the ith test passes, but P'_{i+1} != P_{i+1} is incorrect. Then
	\sum_{h in H} P_i(D(h,i)) = P'_{i+1}(D(r_{i+1},i+1))
	but
	\sum_{h in H} P_i(D(h,i)) = P_{i+1}(D(r_{i+1},i+1))
	thus 
	P_{i+1}(D(r_{i+1},i+1)) = P'_{i+1}(D(r_{i+1},i+1))
	That is, two different polynomials will only agree on r_{i+1} with probability d_{i+1}/|F|
	Let P denote P_{i+1}, and P' denote P'_{i+1}, and r denote r_{i+1}

problem is, if prover knows what encrpted variable will be, prover can construct poly so on that point it yields any desired encrypted form. this form would be chosen identical to 

if prover knows r, then can construct P' such that P'(r) = P(r)
P could construct P' such that evaluated on r, it gives same encrypted version as 
So we must have the polys presented 


cannot create different encrypted version under same key that decrypts to same.
so must show they must be different in encrypted form


under same key:
if encryptions are same, decryptions are same
if encryptions are different, decryptions are different

raw uni on encrypted x. if evaluations are same, polys are same. ie if polys are different evaluations are different.

get raw P an E(r). get P'. If P' != P, then P'(E(r)) != P(E(r))
to trick is to get valid form of P(E(r))


if decrypted is correct, then encrypted is correct.

consider correct to be the correct evaluation of the encrypted points. 

so P_i is correctly encrypted, so one can obtain a correct value of P_i encrypted under key 2. 
P_{i+1} is incorrectly computed. so we want to make sure it decrypts to incorrect poly. this is the part that might not be the case. 
so P_{i+1} must have correct decrypted form. we want to say for this to be the case, it must have correct encrypted form. but maybe its possible for prover to construct ano
i think prover could just choose poly sucs

i think job of simulator is to efficiently use a false proof to break a system

oh, maybe you can obtain encryption of C (left side) using affine, and also for T using affine, and also for H using affine. But, given that for C and T, you must find one for H such that when multiplied by the one for H, yields the one for C. But we assume the encryption scheme does not support multiplication. 
or take V*W = H*T
without ability to multiply, suppose prover gets v, w, t, but must find h so the decryption is satisfied. prover does not have ability to multiply ciphertexts. 
for public verifiability, they use bilinear map, so you cannot even have the encryption equation until you produce h and map into the new group.

maybe use exactly same as bilinear group with logs, but don't make g public. but since g is private, maybe go from logs to multiplication. then when verify, trusted party reveals g such that verifier can compute it with input. to but in bilinear map. 


S(x)*S(x) - D(x) = H(x)T(x)
assume prover uses linear functions. we want to prevent prover from manipulating H such that both sides decrypt to same value.
logs are 1-to-1 
suppose we can establish by the linear-only property of encryption, that the prover cannot obtain the encrypted form of (S*S - D)/T. 
if prover cheats, then p(x) = S(x)S(x) - D(x) - H(x)T(x) is non-zero degree 2d poly with s as root. show that prover can compute from this together with E(s^i) something impossible. For example, there are only 2d possibilities for s now. So prover can guess s with probability 1/2d instead of 1/|F| as before. this works as long as the encryption scheme can only be solved by guessing with probability 1/|F|.


do sumcheck with no multiplication allowed. prover can obtain correct value of a P_i by evaluating the previous poly at 0 and 1, but must construct
P'_i != P_i such that it evaluates to that encrypted value. So we're looking for same property. how to state this property?
maybe it's that given an encryption of two values, you cannot obtain an encryption of their product.
if you can't obtain encryption of product, you can't o
Suppose you have P and P'. You know P evaluated on E(r), and you'd like to construct P' != P such that when evaluated on E(r) it yields the same. Your expression of each coefficient is encrypted. If you want a different poly the terms must be different. 

it's hard to prove security of an encryption scheme, because it's only useful if it encrypts redundancy, but this enables one to search for keys and confirm them, and a method by which is search could have a lot of possibilities.

Is there any way to use secrets besides encryption? we want a scheme where the secrets need only be revealed for verification. 
maybe could use original LP, where prover gets E(r), r + c1*q, r + c2*q. From this we might be able to deduce a linear function because we are getting evaluations on q, c1*q, c2*q. as long as r,q,c1,c2, are all random.

think of sumcheck with correct for actual polys. consider correct and incorrect P and P' that pass test. That means P - P' is a degree 3 poly with r as a root. Thus there are only 3 possibilities for r. assume the prover sends the input to the verifier, and at the end, the verifier evaluates it on the actual values of ri. The reasoning is as long as the P_i is correct and test passes, P_{i+1} is correct. We know P_0 is correct because verifier evaluates it. That is base case. Then suppose P_i is correct, test passes and P_{i+1} is not correct. The test passes means P_i(0) + P_i(1) = P_{i+1}(r) = P'_{i+1}(r). But then p(x) = P_{i+1} - P'_{i+1} is a non zero poly of degree d with r as root. So probability of identifying r is 1/d. But we know probability of identifying r is !> 1/|F| so this is contradiction. 
the encryption scheme assume each variable is uniformly and independently random and unknown. independence does not hold for (ri + k), but it does for g^r1, g^r2, g^{r1*r2} by the Diffie Hellman assumption. 
this reasoning assumes the prover is able to obtain P', but the prover presents it in encrypted form, so prover may be able to construct it without knowing it.

for non quantum secure, poly div should work with g,g^s,g^{s^2},..., and g,g^{a*s},... maybe even quantum secure if use elgamel. 
also could use square root witness method without trusted party.

assume encryption scheme only allows for linear function. In poly div case with univariates we know that every evaluation presented if a valid decryption must be a linear combination of the encrypted values, such that it is necessarily an evaluation of some univariate at the encrypted point. The prover must know the univariate having constructing the evaluation. Thus if test passes but two sides are not equal, then prover has a non-zero degree <= d poly (degree of either side of equation) with the secret point of evaluation as a root. Show that prover can efficiently obtain the d possibilities for the secret point. Assumption is encryption scheme does not allow for efficient reduction of key space. Contradiction.

try same for sumcheck. suppose all combinations of variables are encrypted. suppose only linear operations can be performed on them. Every round of sumcheck the integrand is evaluated over a number of points and summed together. This can be done with linear operations. So when a prover returns a value encrypted (or a set of values representing a univariate poly) with a valid decryption, we can assume it was obtained a linear combination of the encrypted values. This means it must be some evaluation of the multilinear function. And whatever coefficients are applied to the encrypted variables to make up the univariates, the prover knows them. But this doesn't mean the prover knows coefficients of the univariates, because those are made up of encrypted variables. prover constructs P and P' subtracts them, and obtains set of terms for univariate, each a linear combinations of any of the encrypted variables, including the any encryptions containing random variables that should not be included. Upon evaluating this poly at the particular random variable, the result is 0. This implies the prover knows a linear combination of the random variables that evaluates to 0. This contradicts the assumption below, that this should not be possible.

without having knowledge of random variables or their relationships, prover should not be able to find multilinear function that evaluates on them to 0. This is because upon choosing any function, the probability it will evaluate to 0 on the random variables (which are random with respect to the chosen function) is d/|F| where d is the number of variables. 

take scenario: given E(r1), E(r2), E(r1*r2). show it is infeasible to find E(A) != E(r2) such that r1*Dec(E(A)) = r1*r2. 
It must be that Dec(E(A)) = r2. So prover must find another encryption other than E(r2) that also decrypts to r2.
if this is the case, what does it imply? it may imply linear only, or vice versa. it implies that even if you know what a resulting encrypted form should look like, you cannot create an encrypted value other than intended t...

we need an encryption scheme for powers of m, which would also probably yield one for multilinear form. We need to show it is inefficient to reduce search space. Or use assumption you can't use encrypted point to encrypt m^q. this would be reasoning for poly div. for sumcheck, luckily all we need is that neither the variables nor their relationships can be found. 

by Diffie Hellman assumption I think we already have the necessary properties for sumcheck. Suppose g is known. Then we use g^{ri*rj*...} for every combination. If it helps, we could choose to not release g. Maybe at end all we need to do is encrypt evaluation points under different generator h, or use line reduction under same generator g. 

at end of sumcheck, no need to encrypt under different key. we just assume evaluation of integrand over all random variables is correct because verifier computes parts except input. and for the two input evaluations we need to know they are same linear combination of right variables. to make sure variables are right, we may need to encrypt g^{b*r..} for a consistency constant 'b' particular to those variables in addition to the 'a' one for all variables. But the matter of evaluating the same function might have to be done by reducing two points to one with a line. 
or maybe use linear consistency test, where private party also computes r + a*r1 + b*r2, where r is new completely random vector, r1 is first set of random points, r2 second set, and 'a' and 'b' are random. prover evaluates on r, r1, r2, and the consistency vector. maybe a simpler consistency vector like r1 + r2 would be possible. this method would also enforce using the right variables. so show that commitment to the same function holds for this scheme. 
i now remember for vanishing sumcheck we must have evaluation at 3 points, not just 2. 


GKR. don't really see benefit over vanishing sumcheck, because we don't care how expensive it is to evaluate 'add' and 'mult'. 

if relying on discrete logs, why not rely on them for hash function.
what about using g^x as a hash where x is what is to be hashed. so a hash of two hashes g^x, g^y could be g^{g^x*g^y} but this would be equivalent to the hash of the hash of x+y. oh, and even adding them instead would not be ok because that would be commutative. So instead we'd use the one H(x,y) = g^x * h^y. 
look at https://crypto.stackexchange.com/questions/32122/discrete-logarithm-hash-function-exercises
and https://pdfs.semanticscholar.org/3532/201c834c12ff8ea530d9b3d2dbc17b5c88cd.pdf
this might make the circuit a lot smaller and easier to code.

it would be great if preprocessing could be done during the tree commitment where every level more is determined and at the top all constants are available. 



doing sumcheck the way I thought would not be feasible because must at least twice (and 3 times with vanishing sumcheck) the number of variables as used for the transcript. So expanding the integrand would mean wayyy too many terms. What might be possible instead is to evaluate each factor of the integrand separately and add them. so every factor of the integrand would have its own set of variables. they could also be encoded in index form so there is no need to even expand the factors. we would build for a circuits size, but the prover could do any proof smaller, and leave the rest at 0 because regardless the logic it would be satisfied because 0*0=0, 0+0=0. But when using products, with so many less variable combinations less to encode, note that the cost is simple univariates will not be returned for each round, but rather a univariate for each factor (if each factor is multilinear this means a degree 1 or 0 uni for each factor) that the verifier will have to decrypt for each, evaluate, then multiply together. but remember, it's not easy to decrypt logs. before we didn't need to cuz we would already in expanded form. 

lol, what if just change point of evaluation at random point to another entirely arbitrary, unrelated point of evaluation via sumcheck. so integrand is multilinear function, and sums are over encrypted random variables. at end must evaluate integrand on encrypted random variables. with this the regular GKR could work, but we need a more efficient way of hashing the intermediate variables into the next random variable. also important, it still requires the prover to expanding the witness in order for the sumcheck to work to change the point of evaluation. this is because the sumcheck changes one variable at a time. 
in regular GKR, what is proof? for regular sumcheck i think it's just the random variables. because they are each the commitments of the univariates. for regular GKR, there is also matter at each layer of evaluating integrand and reducing to one point or doing linear combination. a drawback is 

wow so actually it looks like logs won't work for either sumcheck or polydiv. This is because in both cases we need multiplication of encryptions. Only way around it is having prover available during verification to re-perform the evaluation on the decrypted random variables and show the result is correct. another way is using a bilinear map. this requires single multiplication in poly div, and might work for sumcheck where the map is only used for V*V, others like 'add' and 'mult' can be decoded by network. but actually prover must be able to compute additions for univariate which requires manifesting the integrand so this actually not possible. 

for poly div, might design where for each index you have a gate, all terms in left hand side except one drop to zero, and that term encodes the logic of that gate. but the prefixes would be long, proportional to the number of gates. lets try using multiple equations. consider one for computing an exponential. if must raise to the n, must decompose into powers of 2. after repeatedly squaring to obtain largest needed power, one has obtained all powers of 2 needed. 
f(0) = g
f(1) = f(0)*f(0) = g^2
f(2) = f(1)*f(1) = g^2*g^2 = g^4
...
f(n1) = f(n1-1)*f(n1-1) = g^{2^n1}
this poly of powers of 2 of g or other generators could be computed ahead of time and available for the whole network.
so exponentiating actually only requires log(|F|) multiplications.
suppose h is this predefined function. suppose g is poly containing bit strings for what powers of base to multiply together.
f(0) = h(g(0))*h(g(1))
f(1) = h(g(2))*h(g(3))
f(2) = h(g(4))*h(g(5))
f(x) = h(g(2x))*h(g(2x + 1))
so if h is degree 64 (for field of size 2^64), and g encodes n bitstrings, and so is degree n*64, then h(g())*h(g()) is degree n*8192. we could optimize this by decomposing powers with respect to basis higher than 2. We could also structure differently.
let's assume the interpolation points are always the integers, and different proof sizes correspond to different ranges of integers. thus the root poly for each relation is of the form of prod_{range i} (x - i). Assume the ranges are consistent enough verifiers can get easy access to any of them necessary. This would give verifier knowledge of how many payments the sender sent, but if sender cares to hide, sender can always make large proof size but leave many parts blank. The fact that the prover could use high degree poly but claim smaller proof size so only requiring it vanishes on some places should not be a problem because the results of those evaluation points will be correct, and verifier should ignore all other interpolation points.
we want the number of equations not to vary with the proof size. only the number of interpolation points (size of root poly) to vary. so proof should consist of a constant length vector of points, each corresponding to the evaluation of a created poly at s^i and c*s^i. verifier trusts inputs and knows what they encode. this should be viewed as a satisfaction problem where the input and witness are separated and verifier need not look at witness, only at input which should also contain output values. maybe there needs to be a separate output poly for each receiver. now might be a good time to review inputs, outputs, and code, in order to better understand what relations would be necessary.

show null hashes since last here. compute hash tree for each one.
verify incoming proofs. for each, check that:
	proof data is committed in previous commitment tree. ...
	input specifies correct constants
	proof is correct
use outputs of proofs to update balance
output list of payments consisting of address and amount pairs

one thing that will definitely vary in length is the number of null hash paths to compute. 

CODE:
BINARY VECTOR:
	f(x)*f(x) = f(x)
	proof: x*x = x => x = 0 or x = 1
0 VECTOR:
	2*f(x) = f(x)
	proof: 2*x = x => 2x - x = 0 => x = 0
1 VECTOR:
	probably must be given. 0 vector can be too.
DIVIDE:
	f(x)*f^{-1}(x) = U(x)
	U is all 1s
EXPONENTIAL:
3 base:
	exponent_spec = [0,2,1]
	powers = [g^1,g^3,g^9]
	h(g(0))*h(g(1))
2 base:
	g = binary vector indicating powers of base
	h = given base powers
	f1(x) = g(x)h(x) + (1 - g(x))
	f2(x) = f1(2x)f1(2x + 1)
	f3(x) = f2(2x)f2(2x + 1)
	

	if we want parallel we might need bivariate poly
	f_i(x,y) = f_{i-1}(64x + 2y)f_{i-1}(64x + 2y + 1)
	well actually not
		for mapping from fi to fi+1 we just don't need it
		for other mappings, we can use another poly say p th
		p(0-63) = 1, p(64-127) = 2
		(1 - p(x) + 1)g1(x) + (2 - p(x + 64) + 1)g2(x)
	another method would be first copying h to another poly that has h copied many times
		h'(64x) = h(p(x))
		p(x) = x
	in general I think we can do it by nesting polys, but unfortunately inside one must have same degree as outside one 
		f(g(x))
	maybe verifier can check nested polys by checking g, and then checking f. prover should be able to take evaluation of g, create a valid encryption symbol, then evaluate f on that symbol. unlike usual, the symbol of evaluation of f then is not known ahead of time but determined by g. other possibility is prover committing to f(g()) and treating as it as a another poly and somehow making sure it is right composition. 


	maybe take advantage of cyclic ness, especially considering prime order of group


how to ensure same functions are used when evaluating at multiple points? we should commit each unknown poly via an LP. then we can have multiple queries for it, and I think each could be kept secret without encryption by making a consistency query for it then inferring the evaluation. actually we'd need 2 consistency queries for query. so we'd like a post quantum encryption scheme for a completely random vector. if can't find one, maybe logs will do. if we do logs, r won't be able to be decoded so to infer answers, and this would mean answers would be inferred in exponential form, so a bilinear map would still be needed. 

post quantum encryption forms like lattices might be best. we want some way to encrypt random values R, upon which we can perform additive homomorphism, and yet the prover knows nothing about R. suppose this is the case. we want to prove the rest of the plan would then work. We'd like to prove that we could then submit R + ai*qi for random ai and any qi (no need to be random), and be sure all queries qi are received from the same function as the commitment for R. Then separately we will prove that if one query is s^i, and another c*s^i, and the answers relate as they should, we can be sure the function is linear. 
so our starting assumption is that prover knows nothing about R, and that verifier knows f(R). No assumptions yet about f. 
to be bound to the same function f means it is not possible for prover to submit two different answers to the same query.
since we are inferring, the answer for q is contained in the answer to R + a*q. 
we could show usual way that there is only one possibility for q, and then reason that we can infer this possibility.
unfortunately, this might not hold because the binding relies on prover having access to queries. 
testing for linearity might also require evaluating on encrypted variables s^i and c*s^i or else we're back in oracle setting. 
worst case is we must use E(R), E(qi), and R + \sum ai*qi or E(R + \sum ai*qi), but for this case we'd only have 2 extra queries. Maybe we could get away with just E(qi) and E(\sum ai*qi). 
Either way, which I'll have yet to find out, right now we need homomorphic encryption scheme friendly to decoding. lattices might be best possibility. 


so if trusted party only needs to output E(s^i) and E(c*s^i) how could we do this? maybe it could be done by the full nodes. in a given round they create a set, and it emerges at top of commitment tree. next round they use it. during the next round they not only create new set for following round, but also work again to reveal previous construction so decrypted results should appear at top of commitment tree.


given encryption of s^i, prover could obtain evaluation of (s + c)^i by expanding binomial theorem, getting appropriate powers, multiplying by appropriate constants, then adding together. this may be helpful for a more expressive left side. actually this may not be verifiable. instead it would be necessary for trusted party to a separate encrypted query, perhaps over a different key, for (s + c)^i. keep in mind that c is known.


I realize we could use encryption of bits if our field is prime characteristic. suppose the element to encrypt is s^i, then you obtain the binary break down of s^i as bn*2^n + ... + b1*2 + b0, and encrypt each of these bits. Then we just use binary operations of addition and multiplication on the bits. actually i don't know if this will work, or if the computer need to know the value of the bits to do it (eg for carrying). I actually think it would at least require both multiplying and adding, which means fully homomorphic. But suppose we're in characteristic 2 extension field. Then we can add two elements by adding the bits, because adding two bits modulo 2 corresponds to XOR, so by XORing each bit we add two elements of the field. but we would need a decryption scheme that we handled by operations in this field. but we know we can do general operation under characteristic 2 by unpacking and packing bits. 

bits might work, but first we need better understanding of what we need for soundness. we want to prove that if we encrypt s^i and c*s^i and we get back encryptions S and CS, and c*Dec(S) = Dec(CS), then it must be that S is some linear combination of s^i encryptions. We cannot assume what form E(s^i) will have, all we know is it keeps everything private. I think we want to say that if the prover succeeds, then the prover can find c, which breaks the encryption scheme. and let's suppose s^i and c*s^i are encrypted under different keys.
if both are linear combinations they must be different by swartz zippel because they are both evaluated at c*s^i and prover does not know this point. so only possibility is at least one of them is non-linear function. 

or maybe we could encrypt s and c and allow for multiplications


for any algorithm outputs pair of encryptions with ratio alpha, there is extractor that outputs linear representation
knowledge of exponent means that if prover can output such a pair the prover must have knowledge of the appropriate exponent ie the appropriate linear combination. we prove this by existence of an efficient extractor function that could output such a linear combination, because if the extractor is efficient than we know prover could employ same method and thus has ability to get knowledge. then we prove that prover having such knowledge leads to a contradiction regarding encryption security. 
so we need knowledge of exponent assumption and maybe logs are best. for logs we'll need verifier to use bilinear map. actually we'll need to extend to bilinear groups. but maybe both equation sides could be done like this and with bilinear map we could verify everything so only initial setup is needed. but two challenges that remain is the need to compose functions, and the need to ensure function commitments over evaluations at multiple points. 

i just realized that if we're gonna have a commitment round we might be able to use something other than LP. for example they could commit the sum of two witnesses, and then its decided what the random point of evaluation is. or what about committing going through a commit phase with a commitment tree where the witness of all children are reduced to one, so at the top only one function needs to be evaluated. The regular sumcheck occurs, and at the end a verifier translates the point of evaluation to another point of evaluation on the grand poly. so we need a way of reducing witnesses to say that the witness evaluates to something if and only if the reduction evaluates to something else. this doesn't work. each poly needs to be multiplied by a random point but provers cannot know these points. maybe could encrypt random points then commit E(r1)*f(x) + E(r2)*g(x), which is done by coefficients. But unlike before, maybe we don't need to require that the encrypted coefficients are necessarily linear combinations of E(r1) and E(r2). instead, provers can choose any encrypted element for a coefficient and as long as they don't know anything about its underlying value, that is what it will decrypt to, they don't know the behavior of the resulting polynomial. 

oh wait, if there is some commitment to the integrand of sumcheck, it must be input to the hash transform, or else prover could just perform hash transform on everything else, obtain point of evaluation, then choose to commit to any poly that will evaluate as it wants. so random values must be dependent on hash transform of integrand commitment. and we want all random values dependent on it so in addition to very first poly that prover sends, the first variable should also be dependent on hash commitment. maybe instead prover commits to both R (for zk) and input, and prover uses hash transform on them to get p. ... anyway, assume all random variables on dependent on integrand commitment.

prover commits integrand, verifier decides random variables, prover claims evaluation, verifier chooses random linear combination, verifier evaluates. but to prevent verifier from needing to evaluate all polys in linear combination, the linear combination must be committed to first but without provers knowing values of random variables, not even with respect to each other. 

problem is even though provers don't know scalar, they have their encrypted form, so they can choose answers such that the encrypted random linear combination is equal to that of commitment. 

two contradicting statements: 
given encrypted random variables E(ri), can you construct two different vectors ai, bi, such that E(ri)*ai = E(ri)*bi?
one answer says no, because that would mean one can construct a vector ci = (ai - bi) such that E(ri)*ci = 0. But this is low probability when ri is hidden. actually this assumes the prover has knowledge of ci by knowledge of ai and bi.
another answer says yes, because it seems you could just treat E(ri) as variables and solve that system. 
in both cases we should instead as the problem of constructing E(ri)*ci = 0
suppose prover knows E(0). Then prover needs to find ci such that E(ri)*ci = E(0). But since E is homomorphic, we have E(ri*ci) = 0
so another possibility to proving is with the assumption that given E(ri) is not possible to compute E(0). 
one possibility could be g^ri where g is kept secret. unfortunately, we can't just randomly generate g^ri then later decide g, because finding ri would mean solving discrete log. 

this is confusing. g^{ri*ci} = 1 => ri*ci = 0.
i as thinking the comparison could be done in the exponent, and actually would need to be done there. this would present a case where there is no need to decode. but then g^ri could just be thought of as random variables themselves never encoded or decoded. but actually the difference between this and an unencrypted version is that here instead of ri*ci = 0, we have ri^{ci} = 1, where ri is g^{ri} for the original versions of ri. So is it possible to find ci != 0 such that this holds for random ri. maybe we can reduce this to discrete log assumption. I have a feeling this would indeed reduce. For example, given such a ci, it could random be split into ai and bi with ai != bi such that ri^ai = ri^bi which would break security of vector commitment in doubly efficient paper. For now suppose it does. 
wow this major reliever, though not quantum secure. before I though it always easy for prover to find two solutions for linear combination, but now its clear that if we switch to multiplication, its hard under discrete log assumption. 


FORMAL STATEMENT:
yet to be said

ri*ai = ri*fi(r)
g^{ri*ai} = g^{ri*fi(r)}
(g^ri)^ai = (g^ri)^fi(r)
ri'^ai = ri'^fi(r) = ri'^{v1*ci1 + ...} = r1'^{v1*c11 + ...} * r2'^{v1*c21 + ...} * ... = (r1'^c11 * r2'^c21 * ...)^v1 * (r1'^c12 * r2'^c22 * ...)^v2 * ... . 

so each user has a different ri and commits to its poly by raising ri to each coefficient cji term j. I think each user can choose its own ri. then to reduce two polys to one, the two users multiply component wise they're commitments. do this for any many poly as wanted. then to evaluate the poly for term variables vj, raise each item of vector to vi the multiply together. Take this expression and rearrange with respect to the base. It is visible that with this rearrangement this expression is equivalent to the product over i of ri^{\sum_j cji*vj}. The exponent of this form is the evaluation of the poly committed to by user i on vj. 

since there is a single comparison at the end, i think we'll need hierarchical network structure. for a moment assume binary tree. at each vertex two users multiply their commitment vectors. 

need to reduce two to one. that is, reduce r1^c1 * r2^c2 to r3^c3 where r3 is known.

r1^{c1*c2} * r2^{c1*c2} = (r1*r2)^{c1*c2}
this would be equivalent to saying 

r1*f(r) + r2*g(r) = r1*a + r2*b => f(r) = a, g(r) = b


chaining proofs could work if each party was reliable to provide answer. 


b is length m, v is length m*64
f(x,y) = b(y)*v(y + x) + (1 - v(y + x))
f1(x,y) = f(2x,y)*f(2x+1,y)
f2(x,y) = f1(2x,y)*f1(2x+1,y)
...
f6(x,y) = f5(2x,y)*f5(2x+1,y)
it actually looks like this should be way to compute all exponentials, regardless whether we know b or v. 

if f is degree d and variacy v, it has (d + 1)^v terms. eg univariate degree d has d + 1 terms, and v-multilinear has degree (1 + 1)^v terms.
in our case v is 2, so computing f*f, we need to make sure d is manageable. suppose we keep an upper bound on d for our polys. we then make sure (m + m*64) is in this range, where m corresponds to the square root of d.

given commitment, and given variable exponents, and given bases, and given answers, we can use two instances of this equation. We can't use a single instance because we can't join the user provided data with the network provided data without another variable to distinguish them. 
so we'd compute two instances, then we'd reduce each by f(y) = f(2y)f(2y + 1) to a single variable and we'd make 

g1,g2,g3,g4,g5,g6,g7,g8,
g1*g2,g3*g4,g5*g6,g7*g8
g1*g2*g3*g4,g5*g6*g7*g8
g1*g2*g3*g4*g5*g6*g7*g8
f(y + 2x)*f(y + 2x+1) = B(y,0)f(8 + x) + B(y,1)f(8 + 4 + x) + B(y,2)f(8 + 4 + 2 + x) + B(y,3)f(8 + 4 + 2 + 1 + x)

as a general rule, maybe we could form our left sides as a sum of expressions of the form B(y,n)Q(x,n) where B captures a particular y from range m and thus has degree m-1, while Q is a quadratic expression of functions in x. 


we could even make polys where y corresponds to different mapping operations

with bivariate, maybe we could split commitments between the variables.


how to compute hash path. could just compute logs as usual, but then need to check inputs and outputs are consistent. but exponents are specified in binary, while results of logs are numbers. so we need way to verify equality. suppose v has binary form, and b has number form, and g has powers of base. we can multiply v and h to map to poly containing appropriate powers, then we can multiply them all together. 

we need to create a library for operations. each should consist of input, output, and code in the form of a constant number of polys used and their relations.
one should be for hash paths

what's worrying is the possibly large number of hash paths needed to be computed
all polys, not just two (as in QAPs) will need to be computed with logs for commitment. prover overhead would be massive, and not economical for small transactions. 
this project doesn't look worth it because it relies it's scalability is long-term focused, but it's use of logs is short-term focused.
a modification would be to use QAP version, where the prover just commits to witness and divisor poly. but all would need to evaluate the same circuit. but unfortunately this would not work because when evaluating at random point it would not be in form of univariate, so it could not be reduced with square root version. 
what about a super simple function, because verification is proportional to function complexity. this would be bootstrapping. lets think of the simplest verification. regardless the function, one or more univariates will be committed to with square root, and verification will require at least the exp computing functions above. also, to make this protocol worth something due to cost of decision of random points and evaluating root poly, it is necessary whole network commits, and this requires hash tree. for every log a verifier must compute, it requires at least 64 degrees. suppose witness size is like n, then number of logs for verifier to verify m proofs m*n^{1/2}, and this will require like 64*m*n^{1/2} degrees, so if n>64*m*n^{1/2} => n^{1/2}>64*m => n>m^2*64^2, which grows quadratically with m, so this would require large witness size, at which point overhead for both prover and verifier are probably no longer worth the cost.


STARK proofs, due to their succinctness, seem the only viable candidate for recursive proofs. Recursive proofs, where proofs can be made to verify other proofs, can provide a seemingly optimal scalability solution. Are community members currently developing a blockchain (or Ethereum extension) to implement such a solution? I dont know of one, and I see no reason to wait, because STARKs are already fully developed. Im confused why sharding, a tough concept to implement elegantly, is being pursued so vigorously when recursive proofs are already feasible. I dont even see much discussion on recursive proofs. Are there legitimate concerns regarding recursive proofs, or is the lack of activity due to a lack of sufficient mathematical expertise in the community?


x is input with length n
r and q are functions of n
r is a random string that maps you to random places in pi
proofs are at most q*2^r bits
PCP is where r = log(n), and q = 1


PCPs vs QSPs
circuit evaluation vs satisfiability problem
I think PCPs are particular to satisfiability problems

is there a standard set up techinques for constructing satis problems? 


Ligero
Pi is protocol. x is public input
learn about linear codes, and reed-solomon


https://web.stanford.edu/class/ee392d/Chap7.pdf
https://web.stanford.edu/class/ee392d/Chap8.pdf

RS
https://www.cs.duke.edu/courses/spring10/cps296.3/rs_scribe.pdf
(
	https://www.cs.duke.edu/courses/spring10/cps296.3/
	http://www.cs.duke.edu/courses/spring10/cps296.3/decoding_rs_scribe.pdf
)

questions:
only 1/2 soundness, so have to do proof average of 2 times? soundness error 1/2 "very high probability"
only log composition?
f is d degree poly, check that it is. 


must be low degree because prover could make it high degree to to meet all points

checking points shows that they formual is partly correct (at those poitns), and the constraint on it being a low degree polynomaila restricts the form such that is likely to have meet all other expected points as well. self correcting/redundeancy helps make it more robust, it amplifies errors spreading things around to multiple places so that if theres an error and you sample somewhere else you are likely to get a touch of that error. 
design of constraints must be stable such that if some of them are satisfied, it is likely all of them are satisfied
boolean constraint is a predicate psi:{0,1}^q -> {0,1}
for example, if a polynomial is a certain degree and it touches enough points you can conclude it touches other points.


http://www.ccs.neu.edu/home/jmitesh/pubs/intro-to-pcp.pdf
http://www.wisdom.weizmann.ac.il/~dinuri/mypapers/ICM.pdf
PCPs: https://www.cs.umd.edu/~gasarch/TOPICS/pcp/AS.pdf



suppose each matrix defines some operation. you can check each matrix 
if a matrix maps some points as expected, you conclude it maps all points as expected. 

or fourier. problem is find a function such that the relationship of its basis functions and it satisfy a valid constraint. check it by looking at different domain points for its basis, and check that they, along with their sum which is the final function output, have the correct relationship. assumption is that if they have correct relationship at certain points, and since the function is bounded by number of basis functions used, you can predict the relationship at other points with high accuracy. 
maybe frequency distribution is a point of interest. for example, the frequency distribution expresses some relationship of basis functions. 

'Program flow is controlled by multiplying each polynomial in Pop by a multivariate Lagrange selector polynomial that, based on the value v of the program counter (PC), annihilates all constraints that are irrelevant for enforcing the vth instruction of P'. so actually the multiplying effectively 'hides' the polynomials representing the opcodes that don't apply for an instruction. 


local testing is done by inspecting pairs of elements, where each element is a state, in order words inspecting a transformation. 
machine state is pc together with register values
the machine is an expression of input and output that yields 0 they are valid pairs of input and output. actually the expression is a set of polynomials. what is important is that it vanishes (equals 0) on valid input. 

maybe the output could be verified with no conditions on the input. the circuit itself verifies the input. this would be done by the circuit only accepting certain formats of output, and only after verifying them. 

singular value decomposition might offer a solution, because of the way it approximates a matrix. the idea would be the full matrix encodes the computation, and the the verifier checks it with the simpler, approximate matrix, under the assumption that it spreads out the essence of the full matrix and that 

in general, i think we want a problem that the prover solves and by doing such produces a computation. then the verifier needs to check the solution is valid. the solution needs to be encoded in some way such that it can be verified by only checking certain part of the solution, not all of them. 


RS: have k numbers, make a polynomial of degree k-1 out of them, then evaluate at k+2s points, and send results. at other end, use agreed upon x values together with the received k+2s y values to find the k-1 degree polynomial that matches k+s points. then take the coefficients of this polynomial
evaluation occurs for the prover at k+2s points. interpolation occurs for the verifier. 
computation tau is converted to satisfaction problem psi. 
prover encodes trace into RS codeword 'a'. another RS codeword 'b' = psi('a'). 'b' says 'a' encodes valid trace. PCPs of proximity pi_a pi_b are made for both 'a' and 'b'
verifier uses pi_a to check that 'a' is close to a RS codeword. same for 'b'
verifier then relies on 'locality' of mapping psi:'a'->'b' which means 'that each entry of b(P,x,y,T) depends on a small number of entries of a(P,x,y,T)'
an operation is a set of multivariate polynomials, and the set of common zeros is a valid input-output tuple
multiply each polynomial by a multivariate lagrange polynomial with degree logarithm of total number of lines in program.
a program is a line (list) of operations. 
significant prover time is spent making pi_a and pi_b

in short, you generalize the total, then send a redundant description of the generalized version. then 
the prover wants to prove membership of all elements in an RS code. 

the prover produces b = psi(a), and then the verifier checks that a = psi(b)

test if f is at most d degree asap. in other words, make sure f is not of degree greater than d asap. so test if f is of degree d+1. if its of degree d+1, then it is defined by d+2 points. with d points you can tell if f is NOT of degree d-2 or lower, that is that f is of degree greater than d-2.
	2 => deg !<= 0 or deg > 0
	3 => deg > 1
so if you want to know whether deg > d, you must have d+2 points. 
if f is delta far, it must be rejected with high probability. "f is delta far from every degree-d polynomial" means that f must be changed on at least delta-fraction of the domain to get a degree-d polynomial


phi:F[x]->F[x] 
P = phi(A)
wanna see if P(h)=0 forall h in H \subset F
computing P(x_0) depends on of course x_0, but also implicitly A at k points {AFF_i(x_0) | i\in[k]}, that is it depends on {A(AFF_i(x_0)) | i\in[k]}
where AFF_i(x) = a_i*x + b_i, where a_i and b_i \in F are specified
phi = (F,{AFF_i | i\in[k]}, H, C)
C:F^(k+1)->F, multivariate polynomial of first variable degree <= |H|
A satisfies phi iff deg(A) <= |H|-1 (ie A can satisfy at most |H| random points) and forall h\in H, C(h,A(AFF_1(h)),...,A(AFF_k(h))) = 0
phi \in ALGEBRAIC-CSP iif there exists satisfying polynomial
ALGEBRAIC-CSP_{k',d} is where k <= k' and and deg(C) in all but first variable is <= d

A is low degree
P is of slightly higher degree


take the right polynomial p, and get z by dividing it by d. a right polynomial will agree with d*z for all x. if it disagrees at any point, it must disagree at most points, so is likely to be detected.
z is constructed such that when whatever d, zp(x) will return right answer 0 for all x in domain.  
what if i provide the wrong polynomial. then 

if you can show that p = dz for all x then you're right.
why? for contrapositive, if you're not right, then p != dz

want to show that p(x)=0 for x in domain
but it is easier to check that two functions are equivalent than to check that a function and a value are equivalent.
instead show that p(x)=z(x) where z(x)=0 for x in domain
but problem is even though they are same for domain, they are probably not same for all, so they are not equivalent equations. and we need them to be!
we can solve this by finding a d(x) such that p=dz. and the new equivalent expression is now dz which still holds the right values of the domain because when d=0, dz=0. d can be found by dividing p by z. 
so now prove that p and dz are equivalent. 

now how to prove the two are equivalent?
we know that z by the way it was constructed as at least a certain degree. and multiplying it by d can only increase that degree. thus if P is different 

we now have a method to prove that the prover knows a function:F^m->F that evaluates to a specified subset of F for a specified subset of F^m. we can make certain assumptions about the function because of the way it is tested (eg that it is a polynomial of at least a certain degree). so the question is how do we make this computationally useful. usually we say, 'I know of an input of this common function that will give this common output.' But now we say 'I know of a function that with common input gives common output' Thus if we want to keep the former style, then the function must become the input, and the input becomes the function.

if you're not right, then p will not take proper value of 0 in domain, but z was constructed such that it necessarily takes proper value in domain. therefore if you're not right, p and dz will disagree over domain. if p and dz disagree over any part, eg domain, they must disagree over many other parts as well.


V has a representation of a function \tv_i but the representation relies on v=\tv_{i+1}(w) for a particular w and V is not confident this is the correct value. Suppose V has sufficient reason to be \tv_{i+1}(w)=v. Then V is confident she has a correct representation of \tv_i. 

P would like to prove to V that \tv_i(w)=v for a random w. but doing so requires iterating over all possible s_i+2s_{i+1} binary input combinations and evaluating the predicate of whether w predicate of whether they are the inputs to the gate label w, and if so what the value of the operation on their values is. but V does not want to do this iteration so V and P use the sum-check protocol. 
what if the iteration is not necessary, and instead the value is of \tv_i(w) is simply claimed to be v by P specifying 2 (perhaps more) input edges w1 and w2 and their values v1 and v2 and the addition or multiplication operation applied to them (perhaps other operations) that yield v. Then naively, V would need to verify the values of these two edges which means verifying \tv_{i+1}(w1)=v1 and \tv_{i+1}(w2)=v2, but we'd like to keep verification to one claim per round. It may help to encode \tv_{i+1} as a polynomial (that in turn is dependent on the values of \tv_{i+2} at some points). Then verifying \tv_{i+1}(w1)=v1 and \tv_{i+1}(w2)=v2 is a matter of checking that they polynomial indeed evaluates to those values, and then checking that the assumptions are correct. 

instead of going through interactive protocol to show a single gate value, why 
i think the iteration is necessary in order for V to not need to know \add and \mult (the structure of the circuit).
oh, maybe the 'regularity' of the wiring pattern is all in the effort to relieve V from memorizing the circuit structure. 

https://eprint.iacr.org/2013/351.pdf
file:///Users/josephjohnston/Downloads/TR17-108.pdf

for each round gi = sum g***
we want to know if the left side is a valid representation of the right side.
if we can get a valid representation of the right side evaluated at a random point, then we can compare it to the left side at the same random point, and by the lemma then the left side is valid.
well we can get the right side evaluated a random point be taking g_{i+1} left side and summing it over H. 

general method of splitting a sequential computation into parallel parts, and at the end comparing the output of part i to the input (starting point) of part i+1 to show that the parts indeed constitute a coherent computation. 

verifier could keep record of wiring patterns. each circuit type (each wiring pattern) would merkleized into an identifying hash. Then the verifier circuit could be provided with wiring predicates by merkel proofs. 


-- to write on summary (current status) page: it seems all existing schemes rely on the verifier choosing a set of random queries that the prover should only see one at a time, otherwise, the prover has opportunity to transform the proof so it fits all random queries. two ways to tackle this. one is to have the public random queries somehow only reveal themselves one at a time to the prover, despite their publicity. the second is to have the queries all present as once, but have the prover commit the proof ahead of time. but this requires the prover to provide answers for all of the possible queries, which is too expensive for a fast protocol. 

one possibility is making it so, like in the checksum protocol, the output of a query generates the next random query. 

matrix multiplication could be almost recursively verifiable, but the inputs wouldn't be, using Frievald Algorithm
https://arxiv.org/pdf/1705.10449.pdf
https://courses.cs.washington.edu/courses/cse312/14wi/freivalds.pdf

a problem with the proof's approach is differentiating between 


V has a full representation of V_d:{0,1}^s_d->F that P claims validly maps the input indices of the input layer to their values. Well V has an explicit map of this function too, cuz its just the inputs. V can check that V_d is a valid rep by testing V_d at a random point, and checking it against V's input table.
With V now having a valid rep of V_d, V would like a valid rep of V_{d-1}. P can claim a valid rep for V_{d-1}, which will be a polynomial that depends on (all) values of V_d. V could test the validity of the purported V_{d-1} if V could test it at a random point and compare the result against a valid value. 

P purports a rep for V_1 (the output layer). P makes a series of claims about V_1, ie that V_1(w_i)=v_i for some range of i's. But for any i, evaluating V_1(w_i) requires evaluating V_2 at some points x_j for range of j's. V can't evaluate for each w_i as this would result in exponential verifications. So instead V must do so for only 

we want a polynomial representation of the values at layer i, V_i, which takes an input an index specified via s_i=log(S_i) elements of F_i, where S_i is the number of gates at layer i. ...

V_i consists of \Beta, \add, and \mult, which we assume V can evaluate on its own, plus a black-box polynomial V_{i+1}. V wants to verify that V_i(z_i)=r_i. But V cannot compute this because it (takes too much time but also) doesn't have access to the black-box V_{i+1}. 

The sum-check protocol works without P showing V only the purported value, a series of univariate polynomials, and the info for V to evaluate f at a single random point.
V can validate V_i(z_i)=r_i by evaluating f_z at a random point. So now I agree the problem of verifying V_i(z_i)=r_i is reduced to verifying V_{i+1}(w_i)=v_i for two random points w_i\in F. 
we can't simply reduce verifying V_{i+1} at two arbitrary points down to one. but we can reduce it to verifying V_{i+1} at an expression, which is more powerful, because this expression can represent multiple points. Suppose the expression is \gamma. In this case the verification is that V_{i+1}(\gamma(x))=h(x). Just as for the points the prover claimed the values v_i, in the case of an expression the prover claims the resulting expression h. Just as for points, it is up to V to verify this. If V can verify this, then V can also verify for all points in the range of \gamma with results in the range of h. So to verify for a set of points, it must be that those points are in the range of \gamma. To test a point, pass the image x of the point to V_{i+1}(\gamma(x)) and verify its the same as h(x). So then what about verifying \gamma? Well we want to verify that two polynomials are equal, and so we can simply test at a random point. However, we h and \gamma, but not V_{i+1}, so we instead move on to verifying that V_{i+1}(w)=v where w = \gamma(t_1) is a random point, and v=h(t_1). 
If P claims an incorrect value it will be caught some time in the future. But if P knew ahead of time what random value w V would test for V_{i+1}(w) P could change V_{i+1} such that it still outputs the correct value, but gives the incorrect value for other inputs. Similarly, suppose P had access to the expression \gamma that V will test. ...


how about a continuous case of the sum check protocol, like evaluating over an integral. suppose it works. whats the benefit? in this case f would be a real function and we'd be computing its integral, which can compute other functions by the fundamental theorem of calculus. 


if we're going to have a deep proof network, then we must assume the possibility of provers and verifiers colluding. the prover could produce the proofs, send them to the verifier, and then the random checking points are revealed, all as planned. the verifier would feed the random beacon and the proof to the circuit and get out a proof, which it would pass to the next verifier. But that verifier's circuit would need proof that the previous circuit was verified on the correct random queries, which requires verifying that the previous circuit had an input accumulating the correct random beacon. It also needs to input the new random beacon. Then the next verifier needs to check that the previous circuit had input both of the correct random beacon as well as correct previous beacon for verifying previous circuit. Those two rnadom beacons could perhaps accumulated into one. have to work it out, but maybe this is possible. 

maybe circuits could output single accumulator value that packs its full output. consuming circuit would need to know this single output value as part of its verification process, and it could repack (or unpack) the actual output then compare it to accumulator value. this would mean a system for a circuit to verify that an output comes from another circuit it knows of, which may be sufficient, even though the consuming circuit is unaware what the input to the consumed circuit was. 

what about a verifier circuit looping back into itself for each iteration of the verification. this would rely on the sub-concept of a self verifying circuit.
or something like the polynomial submitted that gets evaluated at a random point deterministically and randomly generates the next random value. this would mean a global random beacon is unnecessary! this would be the most elegant approach.

a self verifying circuit: if the definition of the verifier circuit must have wired into it the definition of the proving circuit, 


p(x)=3x-7
p(a+b)=3a+3b+7

just as QAPs check multiple Qs which are simple relationships, maybe something similar could be done checking the relationship between spacing in the frequency domain of a function (fft).

linear pcp requires any fully homomorphic encryption scheme. 
arguments are for computationally polynomially bounded provers, while proofs are for unbounded provers. 
commitment was first done by hashing (as explained in this doc) and then with linear PCPs as shown in this doc. http://web.cs.ucla.edu/~rafail/PUBLIC/79.pdf


actually generally the verification is done by evaluating a polynomial at a random point. since polynomials can be presented to the verifier circuit in a succinct, expected way (eg coefficient list, or more likely FFT interpolation format), the circuit can use that list of F elements to generate a random element of F that will be evaluated on the polynomial. This would enforce the principle of the random evaluation point being dependent upon the polynomial evaluated. 

easiest scheme i can think of now is a polynomial that encodes all constraints and must vanish, which is simply done by evaluating the polynomial on a random point. the problem is the degree of the polynomial is about the same as the number of constraints, and it takes a constant factor of that degree number of gates to evaluate the polynomial, forcing the input, much more the total number of gates of the verifier circuit to far outweigh the total number of gates in the original circuit. 

W top layer, X bottom layer, and Y purported meaningful output layer. assume all have length S. Constraints are:
W_i * W_j - X_k = 0
W_i + W_j - X_k = 0
X_i - W_j = 0
X_i - Y_j = 0
transcript is a vector of W and one of X. these constraints are independent of input. 
but the last one ensures the purported output vector Y is consistent with gate values. 
there are basically 2S many constraints. try to encode them into a univariate polynomial of least degree. 
then the verifier circuit will take input about equal to the degree to evaluate the polynomial at random point, plus custom input not part of verification.
so challenge is i suppose how to design circuit so that constraints have pattern such that encoding constraints does not naively take degree S, but something much less. 

or suppose only the first two types of constraints exist, this way there are S constraints total, not 2S.
ring machines!
what about treechains where the tree is an arithmetic circuit. i imagine verification would start from the top, and move layer by layer to the bottom level, at each level verifying V_i(w_i)=v_i. i think the biggest challenge would be that at the bottom of the tree the polynomials would be too large. well, the add and mult at the bottom could be the same across leaves like Justin shows with the replicated circuit thing. this would be possible because we assume each cell has the same 'code' and thus wiring predicates. what's left is doing the sumcheck protocol for logS rounds where S is the number of gates at the bottom layer. but the hardest part is probably for the prover to compute such a huge sum, but maybe possible with dynamic programming and parallel processing if possible. 
So if circular circuits don't work, then can we do a DAG of composing circuits? and if neither works, we'll have to try the above where the tree is just a single circuit. all these approach (these 3 of them) are from GKR. 

is anything apart from GKR a good candidate? There's linear PCPs. 
I have a feeling that anything that requires materializing all the edge values, unlike GKR, will fail to compose. 


sumcheck for univariate. might there be some analog of the sumcheck protocol over multivariate functions for univariate functions? the sumcheck works by instead verifying a subfunction, so we would also need something like that, except instead of g(x_1,...,x_m) we would need f(x). but we would need to convert from
\sum_{(x_1,...,x_m)\in {0,1}^m} g((x_1,...,x_m))
to
\sum_{x\in {0,1}^m} f(x)
this basically means interpreting a multivariate argument as a univariate argument. if the univariate argument has range in F, and |F| is a power of 2, then ...
well i suppose the multivariate polynomial could just convert its input to univariate by always interpreting the parameters in a linear combination that yields the single variable. turns out the degree of the polynomial doesn't change! this is because the multivariable polynomial g makes a linear combination of its arguments (instead of multiplying them) and feeds the result to f. i think the degree doesn't change because of binomial coefficient formula. 
omg, i think this is all no use even if it works, because the last step is evaluating f on a random point. 

so we have a series of constraints. we want to make sure they all evaluate to 0. could we take all Q_i(W) polynomials (the constraints) and express them instead as a single polynomial f, such that if f evaluates to 0 then so does all Q_i? 

this univariate polynomial must vanish on a set H, and each root in H corresponds to a single constraint. this way, the degree of the polynomial is about the same degree as the number of gates. maybe this high degree won't compromise soundness of the sumcheck if the field is big enough. 

the way they have it, g needs to vanish on a subset H of F. they solve this with the polynomial equivalence method. 
what we want (if we're gonna use the univariate sumcheck), is to check that the polynomial evaluates to a certain value for a certain single input. probably we want the answer to be zero. we want the polynomial expressed as another polynomial evaluated over a subrange of the domain, because this is what the sumcheck is for. 

maybe f could have the form \beta * constraint similar to GKR where \beta is made for selecting correct term on any of the subset inputs. 

just an idea, what about doing low degree testing by looking at the integral of the polynomial, or actually some derivate or integral of it, and using the sumcheck protocol. what to look at? if its high degree we assume it goes up and down a lot, which means its integral actually changes more, and is higher degree. hmm,...

its too much for P to do anything with O(F), and too much for V to do anything with O(S). so like P can accumulate O(S)

maybe evaluate polynomial with only partial evaluation by expanding over the random point of evaluation with taylor series or the like. this requires a strong correlation of polynomial interpolation and taylor series. 

we want to verify the evaluation of a polynomial without evaluating it. the polynomial is the extension of the witness. 
current ways are:
1. the linear pcp way. this is where the function is linear (homomorphic). call it pi. suppose V knows, and P committed to, pi(r)=r'. then V wants to know pi(q), so it sends q and s = r + a * q, where a is random. then V can check that what P sends back, q' and s'. 
V accepts if: s' = r' + a * q'
q' is right, s' is wrong: q' = pi(q), s' != r' + a * q'
q' is wrong, s' is right: q' != pi(q), s' = r' + a * pi(q)
q' is wrong, s' is wrong: q' != pi(q), s' != r' + a * pi(q), r' + a * q'

so if P could find a, then it could assign arbitrary desirable &q to pi(q), and then V would check to see that s' = r' + a * &q. and P would have chosen s' by calculating r' + a * &q.



2. committing an evaluation of the polynomial over the entire domain and then simply revealing its authentication path on a query. but this requires the prover to manifest the whole polynomial range. when this way is used of only revealing evaluations of the function is when low degree testing becomes necessary. 

maybe self evaluating circuits have an advantage. in traditional P&V setting, this idea of a random evaluation point dependent on the polynomial being evaluated would look like this: P sends the polynomial to V, V calculated the random variable r, sends it to P, then P is supposed to evaluate at r...
i was thinking the fact the P and V must communicate back and forth as separate entities might be a limitation, whereas ...

using maclaurin series and geometric series it looks like the sigmoid can be approximated with an arithmetic circuit. 

what about committing the evaluation of basis vectors, probably the standard orthonormal basis. then to evaluate an arbitrary vector you utilize linearity and multiply the vector by the results for the unit evaluations and sum them. like this:
pi([1,0])=v1  pi([0,1])=v2  pi([a,b]) = pi([a,0] + [0,b]) = pi([a,0]) + pi([0,b]) = pi(a*[1,0]) + pi(b*[0,1]) = a*pi([1,0]) + b*pi([0,1]) = a*v1 + b*v2
well actually this is equivalent to V just computing the dot product itself. 

V sends the queries q1 and q2, and P responds with q1' and q2'. Then V sends s = \a1 * q1 + \a2 * q2 and P responds with s' which implicitly specifies q1 and q2. I think the necessary property is that P cannot choose s' by simply calculating \a1 * q1' + \a2 + q2', which means it doesn't know \a1 or \a2 when calculating s. maybe this is possible in that P could know \a1 * q1 + \a2 * q2 without knowing \a1 or \a2. i was thinking s would be a random value, and then \a1 and \a2 could be solved for. this effectively enforces that s cannot change while \a1 and \a2 remain static. but the problem is that \a1 and \a2 cannot necessarily be solved for. suppose vector length is L. Then this is like solving a system of L equations of the form s_i = \a1 * q1_i + \a2 * q2_i. I think to have enough leg room to find a solution you need more variables for flexibility, specifically just as many variables as equations, L. Suppose pi : {0,1}^s -> F where s=log(S) is the transcript, and is somehow linear. the the query vector length is log(S) so you only need log(S) equations and so log(S) queries. in the example of QAP and hadamard, the query length is >= S, so they're not doable anyway. this is because the function they are concerned with is the dot product with the full transcript. whereas in our case we're interested in the function that yields a single element of the transcript based on it label. as far as making pi linear, i think any multilinear polynomial with zero constant term is linear with the homomorphism property. and i think this is possible because in GKR V_i is a multilinear polynomial. 
to check the homomorphism property on multilinear polynomials. Well i think if the homomorphism property holds for every term in the polynomial, it hold for the polynomial as a whole:
\sum_{i=0}^m term_i(a1 * input1 + a2 * input2) = \sum_{i=0}^m ( a1 * term_i(input1) + a2 * term_i(input2) ) = \sum_{i=0}^m a1 * term_i(input1) + \sum_{i=0}^m a2 * term_i(input2)
now to show that it holds for each term for arbitrary subset k of the m variables.
v1 * ... * vk turns into 
(\a1_1 * q1_1 + \a2_1 * q2_1) * ... * (\a1_k * q1_k + \a2_k * q2_k) =
... oops looks like my assumption that multilinear polynomials are homomorphic is false, cuz the above requires distributing k components. 
so I'm left with how to make a linear pi that can take log(S) variables and output one of S values. well actually i think they are multiplicatively homomorphic. 

so this is all under the technique of P evaluating s before knowing the queries themselves. for the non-interactive setting, this means the queries must be dependent upon s. forget this for now and assume an interactive setting. 

so far we have s' committed to, and the alphas deterministic, leaving the prover to choose the q_i' values to satisfy the equation. the problem now is that the prover can still choose m-1 q_i' values arbitrarily, and only be forced on the last q_m' in order to satisfy that their linear combinations with the alphas equals s'. this implies the only way to force the q_i' prime values is similar to way we forced the alpha values. why did we force the alpha values? we wanted s' to be evaluated before q_i values and yet have complete freedom over the q_i values. to do this you need at least m free variables, which are the alphas, and their valued is determined (forced) to resolve the randomness of s together with the freedom of the q_i values. so we had a certain matrix height (the query length) and wanted freedom on the right side which required the matrix to have at least equal width. Now in the 'prime space', we have a matrix of a certain width, and since we want no freedom on the right side, so we must have the matrix with at least equal height. if this is so, this means pi must let go of old signature F^m -> F and adopt the new signature F^m -> F^m. this means pi has representation as a square matrix. 
oops, think this doesn't work cuz 

keep in mind since the q_i values are random, they are likely to not be linearly independent, in which case the Prover can choose between multiple alpha combinations. to aid this, we must ensure that the queries form a basis, and maybe this basis need not be for F^m but a subspace of it. One way is to transform the random queries to form a basis for F^m, using the randomness of the redundant queries to generate the new basis queries.

i suppose my approach to this linear PCP without secrecy technique using a system of equations had a fundamental problem from the beginning. it was that you need as many queries, and thus must form that order of arithmetic, as there are variables in the linear function. but the linear function is just a dot product, so you might as well compute the dot product with that same order of arithmetic operations. so this now simply reposes the problem, can a transcript pointer function somehow be encoded in linear function of logarithmic variables? i think the answer is probably no. but lets try:
i'm interested in how function of logn variables map to n arbitrary outputs. but with ploylogn operations. O(logn) operations would be the linear function, or dot product. O(log^2n) operations would be the multilinear function. 
i think we should take advantage of the fact that the arbitrary outputs are not completely arbitrary, but the trace of an arithmetic circuit. 

reducing from two evals to one, in binary tree, can it reduce n to one? what verification work involved? is this identical to sumcheck?
how does this work? you want to know that V(w_i)=v_i for i=1,2 where V is of multiplicity m and of degree d in each variable. you create \gamma such that \gamma(t_i)=w_i, taking O(m) operations. then you a univariate polynomial of degree d claimed to be Vo\gamma at two points, taking O(d) operations. then choose a random value and evaluate the polynomial again, and also evaluate gamma, taking O(1) operations. So it takes O(m + d) operations. Suppose you take 2^a points, then you must do it 2^a-1 times, resulting in O(2^a-1+d+m). Doing it manually would take O(2^a*d*m). If in a context where need to reduce more than 2 points to one, either use this technique, or make \gamma either multivariate or greater than one degree. worth calculating how many operations the latter 2 techniques would take...
is this identical to sum-check? no, cuz the sum-check is for a different purpose, not for evaluating a polynomial at a set of arbitrary points, but for evaluating them and summing the results at on a grid-like set of points. 

we now want a 'linear' polynomial with logn variables that can map to n arbitrary points, regardless how complicated and of what degree the function is. this is because the in this case the prover could commit to the logn basis vectors and then by linearity the verifier could open these logn results (with loglogn length authentication paths) and multiply them by the query values to get the true query result. as a polynomial, whats the most expressive form the function could have? it would have terms of one or more variables each of arbitrary degree, and the arbitrary degree means possibly infinite terms. wait, does the degree need to be restricted? degree restrictions happen when there is an expectation that the polynomial matches a certain number of points, eg 0, preventing it from matching those point but also matching lots of others its not supposed to. but in this case the prover can match as any points he likes. this is all assuming that degree doesn't matter for 'embedded functions' in the sumcheck protocol. this is worth asking about, because its the only part of the sumcheck i'm still confused about. actually i just now took a moment pause and used my common sense and realized that any function embedded in the sumcheck is interpreted as part of the function being summed, so yes, it must be of low degree. i'm no longer confused!
I think there is a fundamental conflict of interest here. that is the function must be 'low degree' (or some equivalent for functions other than polynomials) by the sumcheck protocol in order to reduce the number of times it can cross zero, and thus be equivalent to another polynomial. while at the same time we want it high degree in order to match the most number of arbitrary points possible.
what if we just take the V_i polynomial described in GKR, have it by multilinear, and then use multiplicative homomorphisms? this would mean in order to evaluate a random q from the committed basis vectors b_i, we would calculate t_1^q_1 * ... * t_m^q_m. let me make sure on paper this homomorphism works. nope, it doesn't, cuz the multiplicative homomorphism requires a single multiplicative expression, which is not in the form of a polynomial. so i think i'm stuck. for once i don't know what to do except to explore forms other than the polynomial for the sumcheck. 
well one last try. we want a linear function but low degree, so what if we make the function a matrix, and use its multiple outputs in the parent function to arrive at the arbitrary value. doing this or anything else to accomplish the goal like taking advantage of the circuit structure almost definitely required reordering the trace, but that would (I now realize) conflict with the original mapping of labels to gates that is persistent between inputs. i give up!

now for exploring beyond polynomials. 


thinking about GKR and having the final verification V_d(r_d) outsourced. i'm thinking the sender circuit would encode its output in a polynomial 



in order to calculate dot product of long trace with random query, why not just use cosine angle formula that only relies on the length of the two vectors and the angle between them. the verifier has to pick the random vector, so then the prover must prove to the verifier the angle, or cosine of the angle, or the distance between the ends of the vectors, is correct, without requiring the verifier to see the entire W vector. i don't see a way to do this.

spectral theorem, like low degree testing, only need to look at the major components. looks like the eigenvectors of a matrix can form a basis for the entire vector space, implying any vector in the vector space can be composed of a linear combination of eigenvectors, such that multiplying it my a matrix is just scaling its eigenvector basis.
Av = A(c1*e1 + ... + cm*em) = c1*Ae1 + ... + cm*Aem = c1*l1*e1 + ... + cm*lm*em

use forier series over larger integral range than -pi pi and only use the latter as the inner space in the sumcheck protocol. 

let the function that encodes be represented in fourier series, that is only represented approximately. summing the frequency components might be able to be done with the sumcheck in the same way i talked about a univariate polynomial over a range. in this way, the original function of interest can be evaluated at a single (eg random) point by the sumcheck, at the end only requiring the evaluation of one of the components. i'll have to flesh out if this is a possible way to evaluate a function at a random point.
check that it works. i bet it works, but the problem is the function that determines the frequency might be just as long as difficult to evaluate at a random point as the original polynomial. yup, looking at mathematica, it looks like the complexity of the function that yields the frequency amplitude grows exponentially on average with the length of a polynomial interpolating an arbitrary list. like for a list of length 6, the terms in the frequency expression is 42. i think this is a function of the law of information conservation. that is, a polynomial with N arbitrary terms, can't be reduced to a frequency function with a single term. 
actually this might still be possible where instead of computing the giant frequency polynomial and then handing it over to V to evaluate at a random point, it is kept in integral form until the last part where V gives the integral a form by choosing a random frequency value, and then evaluates the integral which is still a big integral so it is done with sumcheck. but at the end, this requires evaluating the polynomial inside the integral at a random point, which was the problem we started with. ugg, ok doesn't work.


GKR also has the problem of evaluating an S proportional polynomial, because in the last layer, V_i(r_i) must actually be computed to yield v_i, and so V must know V_i. However, I now realize if the input doesn't matter, then evaluating the last V_i would probably be independent of the inputs. 
what i want is a way to incrementally verify an arithmetic circuit, from input to output, where layer i+1 is correct if layer i is correct. 

what if circuits can only take an input of length m consisting of m variables each with p possible values. what i'm trying to do is not escape information conservation but rather process all the information before hand. for each possible combination, the extension is computed. wait, but V needs to evaluate the extension on a random value still. doesn't help. 
hmm, the discrete fourier transform can transform N discrete points and into N frequencies I think. so if the number of input combinations is doable for preprocessing, then for each, we can compute the the frequency components for it. i was thinking these components could be preprocessed only over O(input) rather than O(field) but i guess not. 
what about linearity in the fourier transformation? doesn't work.
if the sumcheck can be done for multiplication instead of summation, then verifying the polynomial could be done by executing the lagrange interpolation 

oh huh, i didn't realize, but the multiplication part of a lagrange polynomial can be computed before hand if the points of evaluation eg (1,2,3...) are known beforehand,a the resulting polynomial is only the dot product of these terms with the values. in the case of the univariate polynomial that maps the index of the input (an element of F) to the value of the gate with that index, this is useful. maybe there could be some accumulator method for which the y values can be committed to, and then given any vector x the prover can show the verifier that it has correctly computed the dot product of y and x. if this could be done, then perhaps we could make x be the evaluation of the lagrange polynomial at a random value r. so P also needs to prove to V that x is a valid result of evaluating the lagrange basis polynomial (known beforehand, only dependent on input length) at r. i think this is the next step and getting started means looking at accumulator schemes. maybe together this could be thought of a giant arithmetic circuit with a single input r. It is done by summing together the terms, each of which is one of the base polynomial terms multiplied by one of the y values. each base polynomial term is computed as the product of a constant with a series of factors of the form (r-x_i) for base points x_i. So first layer does subtraction from r, then multiplication of all but one of these resulting gate values, then the results are each multiplied by a constant, then they are each multiplied by a y value, then the final output is the sum of these results. but because we want polynomial commitment before evaluation, this must be done in two phases, the first doing the subtration and multiplication to compute the terms evaluated at r, and then the second doing the dot product and the summation to compute the particular polynomial. 
maybe this could be done by using the GKR protocol, where at the end V computes a value in the extension of the last layer, but the last layer extends the input which is r and the y values. but remember the circuit itself was designed to evaluate the extension of y. could this self-referential pattern be useful? suppose the input is just y for a moment, and suppose you could evaluate the extension at a random point r', then you would have verified that the extension verified at r yields value v. we know the procedure of reducing two verification points to one. so if I know f(r)=v, can I conclude f(r')=v'? yes. does this help? well i don't know f(r)=v unless i know f(r')=v'. 

accumulators... of a certain type. what properties do i want?
I want to make a commitment to a list of elements. then i want to do a dot product with those elements. this means, suppose i have an element w of the original list W, and a corresponding accumulator A(w). I want to take an element x of the other list X and multiply it by w and obtain a new accumulator A(w*x) but with the special property that I can verify with high probability given w, x, and A(w) that A(w*x) indeed accumulates w*x. 

assume we have an accumulator operator denoted & that given two inputs, A & B gives the new accumulator. what properties do we want from &?


going from input to output, GKR in reverse. Suppose you have completed layer i-1 (i=0 is input in this direction). You now do the sumcheck on V_i for layer ... how do you choose the random point on which to evaluate V_i because it is not a layer of a single output. 
why does GKR work? it may be helpful to picture it not in layers, but as one giant polynomial that extends the representation of the circuit. the polynomial has one input of the label of the output gate, and the polynomial returns the value of the output gate. 
take an arithmetic circuit, simplify it and you end up with a polynomial in terms of only the input gate values. 


you want to evaluate the transcript polynomial at a random point. instead you do the line method such that you can verify it at any point along a line by evaluating a univariate polynomial sent by P. verifying the line however reduces to verifying a single point. so you verify this single point, but along the way you are asked to once again evaluate the transcript polynomial at a random point. this random point is the choice of V, so V can choose a random point along the line. V can verify on its own that given the univariate polynomial h provided by P, the random point indeed evaluates to the value claimed by P. so now verifying this random point has reduced to verifying the original random point. but having completed this step, we can not proceed to finish verifying the original random point.

i suppose the multiple outputs of GKR are computed by taking the multilinear extension of the output layer and evaluating at all the label points of the output gates. but this only works if P can prove to V a valid form for V_0. i may ask Guy some questions like this. but first i should try to get the MIP version i want and i'll probably run into relevant questions along the way.

so the protocol i want has a circuit defined by L constraints, each of quadratic form in elements of the transcript W. W has length S. Proving a correct execution is proving that all the constraints are satisfied, which means testing each constraint on W should output 0. In this protocol the verifier doesn't care what the input is, but only cares about the definition of the circuit and that the output was correctly computed on it for some input. of course V cannot test each constraint. how can we make sure they are all 0? well we can use the expression for each constraint as the coefficient of a polynomial, either univariate in which case it would have degree L-1, or maybe a multivariate polynomial such as the L-variate multilinear polynomial in which each constraint is the coefficient of a variable. In either case we test the all coefficients are zero by testing the polynomial to be the zero polynomial, which we do probabilistically by testing it at a random point given the Schwartz Zippel Lemma. Suppose we sample the random point r from the entire field F. the soundness error is d/|F| where d is the degree of the polynomial. If we were to minimize d, we would use the multilinear polynomial mentioned earlier. But in that case the number of variables in the polynomial would be L so testing at a random point would mean taking time at least L to even obtain the random input. We want the running time of V of O(log(L)) or better. So the most we can do is log(L) variables in the polynomial. Suppose we have v variables and we want L terms. what is the minimum degree for which we can obtain L distinct terms using these v variables? First lets compute how many terms we can obtain of degree 1. that would be v. then how many of degree two? that would be (v + 1 choose 2). in general there are (v + d_i - 1 choose d_i) possible terms of degree d_i with v variables. this is the nature of 'multisets' and the multinomial coefficient. i can solve the optimal number of variables later. for now suppose v=1. 
so now how does V evaluate a polynomial with L terms at a random point is sub-L time? we could use the sumcheck protocol if we can represent each term with a single expression, because then the sumcheck can be used to sum this expression over a range that in effect evaluates the polynomial, only requiring V to evaluate the expression at a random point at the end. so how do we get an expression that can represent all terms?
...
this problem should reduce to evaluating the extension of the transcript W at a random point r to verify a claimed value. naively, this would take V time O(S) to do, which is unacceptable. 

So I think W (the extension) takes the form
W(z) = \sum_{p\in H^m} \Beta(z,p)*w_\alpha(p)
each of the m variables in this polynomial has degree |H|-1 or less so it has total degree at most m*(|H|-1).
evaluating it on \gamma will also be the same degree polynomial. 

i've been worrying about degree testing, that W since its never showed to V, must be degree tested. but now i'm thinking that V can enforce the degree by enforcing the circuit that computes the polynomial, ensuring it has width only of that degree for the dot product. it can do this i think because it can enforce of what range of labels the circuit is evaluated and this range of labels is what defines the number of gates used. 

now i'm thinking instead of doing lagrange interpolation for a univariate W, we could use the above version of the multivariate W and compute the \Beta basis terms instead of the lagrange basis terms. In both cases we will still do the dot product of the basis terms with the transcript. 

Lets take the case of |H|=2. then m = log_2(S). Then \Beta has the form \Beta(z,p) = \Pi_{i=1}^m (z_i*p_i + (1-z_i)*(1-p_i)). So first before the dot product the circuit will need to compute \Beta(r,p) for all 2^m = S possible p. But to avoid doing work, the circuit will take each r_i and compute (z_i*p_i + (1-z_i)*(1-p_i)) for each possible p_i where z_i = r_i. p_i is only 0 or 1. so this means for each r_i computing. for 1: r_i, for 0: 1-r_i. This takes m time. Then to compute \Beta(r,p) for a given p, eg 101 for m=3, we compute the product of r_1 * (1-r_2) * r_3. This takes m * S time I think. Finally we compute the dot product. now that we're doing multivariate I realize there is no longer just one input r, but m inputs r_i. this is ok.

So about the line. First suppose V want to verify that V(r1)=v1. 
V computes \gamma such t
\gamma(t):F->F^m = (a_1 * t + b_1, a_2 * t + b_2, ...) 
if we want \gamma(t1)=r1 and \gamma(t2)=r2, then for i in [m] we solve each of the m systems of 2 equations for ai and bi (each equation is solved separately)
a_1 * t1 + b_1 = r1_1
a_1 * t2 + b_1 = r2_1
...
a_m * t1 + b_m = r1_m
a_m * t2 + b_m = r2_m

actually for this scenario t1, t2, and r2 I think can be chosen by V randomly.

so gamma is computed. then V asks P for W(gamma(t)) which should still be of degree m just like W. P sends back h. V verifies that h(t1)=r1 and h(t2)=r2. Then it chooses a random t3 \in F and computes r3=\gamma(t3), as well as v3=h(t3). Then it asks P to prove that  V(r3)=v3. If P can prove this, P has proved that W(gamma(t))=h(t), so if V wants to know W(r) for some r in the image of gamma, it finds the preimage t, then evaluates h(t). 
when it comes to the dot product, V must evaluate W at a random point W(r4). ...

hmm, what if during the sum check when V is incrementally evaluating W(r4) with P, that instead of passing P the r4 components exactly (which is not random because after passing the first r4_1 P could determine via gamma the remaining m-1 r4_i values), V passes the gamma components? i'm afraid this would leave too much processing for V, but at the same time the polynomials passed back to V would still be univariate, so maybe. So what would this mean? if the sumcheck is happening of {0,1}^s and the last m of these values correspond to the m variables passed to W, then when the first of these m variables is reached in round s-m, instead of passing P r4_1, V passes a_1*t+b_1. P then plugs this in and send the result to V as g1. V checks that g1(e0)+g1(e1) is v3, where e0 and e1 are the \gamma's preimages of 0 and 1 respectively (or whatever values the sumcheck is summing over, for here is 0,1). Then P sends V g2(t') which is claimed to be equivalent to the g1(t) but with the second variable unwrapped and replaced by a_2*t'+b_2. So this polynomial has two variables, t, and t'. V checks that g2(e0)+g2(e1)=g1(t). so really g2 is a higher order function. Then it continues on to g_n which P claimes to be equivalent to g but with the first n-1 variables unwrapped and replaced by their respective components in t, and expanded, and the n'th variable replaced by its component but in t'. V then sums over t' ... i eventually figure this out but i'll instead summarize below.
this is basically a non-interactive version of the sumcheck i'm trying to do. i'm trying to make it where V only need specify all the r variables at the end. 

Generalization of Sumcheck, and its Non-Interactive Variant
keep in mind the alternate way to do the non-interactive version, which may have better performance
For now we will assume the operation is performed over the same subset S of each variable. 
let g be a function of v variables with signature F^v->F^n. There is a condition on g. That is, if r_1,...,r_v are sample uniformly from a finite subset S of F, then Pr[g(r)=0]<=z/|S|, where z is "always small enough". The previous condition is called 'soundness'. Such a function g is a v-variate polynomial of sufficiently low degree z where n=1. Soundness holds by the Schwartz-Zippel Lemma. Another function g is a vxn matrix of sufficiently low kernel dimension. 

omg, what about not only doing the operation over different subsets, but doing different operations for different rounds. this would mean a lot of applications, like polynomial interpolation, or matrix determinant (eg for determining properties of graphs). 


the left side consists of g_i for 1,...,v. these are the function provided by P. what V has consists of these as well as the original function g. V performs operations on these functions to find the desired result.
if V has a valid representation of g_1(x) it can wrap g_1 to over x to get a valid representation of the desired value. 
if V has a valid representation of g_2(y,x) it can wrap g_2 over x to get a valid, but inconvenient, representation of g_1(a_1*y+b_1).
	V can compare this representation to the convenient, purported representation of g_1, g_1' by evaluating the former and g_1'(a_1*y+b_1) at a random y value.
...
if V has a valid representation of g_i(y,x) it can wrap g_2 over x to get a valid, but inconvenient, representation of g_{i-1}(gamma_{[i-1]}(y)) where gamma_{[i-1]}(y) = a_{[i-1]}y+b_{[i-1]}. 
	V can verify g_{i-1}' by evaluating it at a_{i-1}y+b_{i-1} for a random y and comparing the result with g_{i-1}(gamma_{[i-1]}(y)) for the same random y.
...
if V has a valid representation of g()...


could we evaluate a polynomial in a single iteration of the sumcheck instead of multiple in a layered GKR setting like i was considering before?
we assume the x values are predetermined (and simply such as 1,2,3) and be hard coded into the extension, instead of needing to encode them in a separate polynomial. 
the first operation would be subtraction of every x value from every other x value, and also subtract every x value from r, the point of evaluation.
the second operation would 

y is a multilinear polynomial

sum_{j=[k]} 
	pi_{m=[k]} 
		y(j) * f(j,m)


f(j,m)
f(j=m) = 1
f(j!=m) = (r - m) * (j - m)^{-1}



suppose this works. our original goal is to evaluate the polynomial defined by y at r. at the end of the sumcheck V will need to evaluate y at a random point. i think the whole line scheme would work for this.
so it appears this could be an alternative to the GKR as i was thinking before.

Pi_{i=0}^{v} (1 - j_i*m_i + (1-j_i)*(1-m_i)) * (a-1) + 1
about the inverses, i suppose instead of having a formula right now for them, we could consider an arbitrary mapping to them from j-m. this means basically interpolating the points of j-m at the values of their inverses. how many possible values are there for j-m? if 

where a = (r - m) * (j - m)^{-1} but in multilinear form

about the

--------
abcd abc bcd cda abd ab bc cd da ac bd a b c d _
abc ab ac bc a b c _
ab a b _
a _
I think the above pattern shows that a multilinear polynomial of degree d (meaning d variables) can have at most 2^d coefficients. i guess this a basic result of the fact that there are 2^d distinct binary strings of length d, each corresponding to the variables in a term. a univariate polynomial of degree d can match d+1 points. 
--------


for the round where the first i variables are condensed into t and the i+1 variable is t', the degree in t will be i, and the degree in t' will 1, so treating t' as a constant, the polynomial can be interpolated in t at i+1 points, each of which will be expressed as a degree 1 polynomial in t'. thus in this round, P will send V (i+1)*2 field elements. V's job is to evaluate at t=r, and sum over t'=0,1. V can sum first turning a*t'+b into b + a + b = a + 2b. maybe P could even instead send a and 2b. Then V takes each of the a_j+2b_j values and uses i+1 interpolation points to evaluate it at r. 

P wants to prove that W(gamma(t))=h(t). 
consider this similar to the case that P wants to prove that W(r)=v

goal is to evaluate at random point r the polynomial W(z_1,...,z_v) = \sum_{p \in {0,1}^v} \Beta(z,p)W(p)
using the sumcheck means summing over \Beta(z,p)W(p) where z is fixed to equal r, and at the end this expression must be evaluated at random p \in F^v. 
\Beta(z,p) = \Pi_{i=1}^v [z_i*p_i + (1-z_i)*(1-p_i)]. 
There is the special property that \sum_{p \in {0,1}^v} \Beta(z,p) = 1 for any z. i don't think any multilinear poly can be converted to such a basis. In fact, the basis terms are just expressions of z_i and 1-z_i. It is quick, only 2v time, to compute these values. Then evaluating the rest is a matter of multiplying them, then doing the dot product with the defining vector, call it y.
each element of y can be expressed as a^n for some n where a is one of the primitive polynomials of the field. 
i suppose each z_i and 1-z_i values, or the products, could be converted to a^n form. 
it is the case that 1-a^n = a^n(a^{-n}-1). 
since every term contains either z_i=a^{n_i} or (1-z_i)=1-a^{n_i}=a^{n_i}(a^{-n_i}-1)=z_i(z_i^{-1}-1), we can factor out z_i. half of the remaining terms, those that had z_i, will have v-1 variables, and the other half, those that had 1-z_i, will have z_i^{-1}-1 instead. 
suppose we do this for every variable. All the z_i's will have disappeared. wherever p_i was 1, we had z_i, which will be converted to 1. and wherever p_i was 0, we had (1-z_i) which will be converted to z_i^{-1}-1. so we will be left with a polynomial:
sum_{p \in {0,1}^v} W(p)\Pi_{i=1}^v [p_i - (p_i-1)(z_i^{-1}-1)]
so it will have the same number of terms with the same y values, but each variable will be of the form z_i^{-1}-1
let x_i = z_i^{-1}-1. These can all be computed in time v. then the polynomial takes the form
sum_{p \in {0,1}^v} W(p)\Pi_{i=1}^v [p_i - (p_i-1)x_i]

i think we need to use a kind of dynamic programming, where we conclude the sum of the terms corresponding to p_i for a subset of i sum to 1. and from this we can conclude values of the other y values corresponding to p_i values for i's outside the subset. start small, get big. my worry is that this would work but would require an iteration for each i value of which there are 2^v. 

we could try the cosine rule for computing the dot product. we know the magnitude of the basis vector. we also need to know the magnitude of the y vector and also the distance between the two. note that magnitude of y and distance between y and basis do not uniquely define y, but that is ok because we only care that it uniquely defined the dot product and it does. P can easily compute the difference between the basis and y coordinates, and then what's left is for P to compute, commit, and reveal the magnitude and distance. hmm, so i think what we want is for the direction of the basis vector to be invariant so that P can commit to the distance between y and basis before knowing basis exactly. it would be the magnitude of the basis that changing depending on the evaluation point. this would basically leave the task of V computing or verifying a purported value of the basis magnitude. so now i should explore what vectors have invariant direction. the direction of a vector is a matter of the relationship between the magnitude of its coordinates. that is, the coordinate values of all vectors in the same direction have the same ratios, or coordinate pair ratios. that is, take any two coordinates and the ratio of their values should be the same? is this the case for the basis vector? writing it out shows no. but maybe the direction has some invariant possible values that can be represented by an expression. and P can commit to the distance between y and this expression. actually this is not the case because if it were the ratios would return an expression independent of the input. i now realize actually none of the coordinate ratios are invariant. 

in general its great to evaluate a polynomial at a random point using sumcheck, and at the end only having to evaluate the coefficient expression. its great when the coefficient expression, which itself is a polynomial, encodes less info than the original polynomial, making the previous sumcheck worth it. the problem is when we encounter the polynomial expression that encodes the transcript, because its coefficient expression must encode as many arbitrary points as there are terms as there are elements of the transcript. so it would be great if we could break the coefficient expression into multiple evaluations of a subexpression. maybe we could follow a similar pattern as for the circuit polynomial. we would partition the transcript points, so that each group is encoded in the same smaller polynomial. then there would be predicates multiply each instance of the polynomial returning 1 for those input that correspond to transcript points in that partition, and 0 otherwise. so the input is length v, and we want to encode 2^v total points, which takes 2^v terms. if we split it into 2^n instances of the subpoly, then each instance would have input length v/2^n and encode 2^v/2^n = 2^{v-n} points and have 2^{v-n} terms. 
let n=1 and m = v/2^n = v/2. so we have 2 instances of an m-variate poly that encodes 2^m values. total input length is v and there are 2^v possible inputs. we have permission to create an arbitrary map from an m length bit string to 2^m values. so if one instance always receives the left m bits and the other the right most bits, how many possible output tuples are there? there are 2^m*2=2^v such tuples. How many of these tuples will have the same values in both positions? 2^m. regardless what the predicates, this means the coefficient expression will return the same on 2^m of the 2^{2m} inputs, leaving at most 2^m(2^m-1) distinct coefficient values. Though these coefficients are distinct they are not necessarily arbitrary. In the worst case we must ignore the order of the tuples and instead treat them as sets of cardinality two. this means we have (2^m choose 2) distinct sets, each corresponding to an arbitrary point. big moment now when calculating how many points this is! great, so we end up with 2^{m-1}(2^m-1) distinct sets out of 2^{2m}=2^v sets, with a loss of 2^m/3 sets. so there are two distinct elements in each set, each elements of a set of 2^m arbitrary values. we need to use the arbitrary contents of the 2^m set to construct an arbitrary set of 2^{m-1}(2^m-1) values. oops, guess this isn't possible cuz in essence its turning 2^m arbitrary points into 2^{m-1}(2^m-1) arbitrary points. 

now is where we take into account that the points are not arbitrary but come from a transcript of a circuit. 
well for the case of two subpoly instances, we could think of them as variables in a bivariate polynomial defined over the space [2^m]X[2^m], the output of which would be the coefficient. or in general, for any n that divides v into n, m length bit strings, that is n*m=v, there could be n, m-variate polynomials, each encoding the same 2^m points. this still is trying to violate information conservation.

i've realized the unltimate goal is to evaluate a polynomial with s terms at a random point in less than s time. we do this with the sumcheck which at the end requires we evaluate the coefficient poly at a random point. this works for the circuit case because the coefficients can be expressed in terms of predicates and the transcript polynomial which has a third as many terms. 

if two multilinear polynomials are the same along a set of basis vectors they are the same everywhere. 
f(a,b,c,d) = ab + cd
g(a,b,c,d) = ac + bd

f(1,0,0,0) = f(0,1,0,0) = f(0,0,1,0) = f(0,)

so after asking on stackexchange, its evident multilinear polynomials don't have that property.

suppose we consider F a vector space where \alpha \in F is a primitive element so any other element other than 0 can be written as \alpha^n for some n. 
i'm trying to find the analog of https://en.wikipedia.org/wiki/Multilinear_map
so the domain vectors spaces are all F, which has dimension 1 and basis \alpha. so i think we just have to compute f(\alpha,...,\alpha) for v \alpha's. this would result in a single value s \in F. ideally, we then just multiply s by r1*...*rv for query r to obtain f(r1,...,rv). 

i think i just found out that two v-variate multilinear polynomials of the form i'm interested in are equivalent if they are equivalent over v univariate polynomials, one for each variable. oops, actually after writing it out for v=3 i see 7 polynomials so its not v but 2^v-1. but this is still a useful form for input extensions because they are easy to evaluate.

i think i now realize that basically the zhwartz-zippel lemma offers the ability for P to compute something many times in different forms as long as the definition of the computation is correct, and V has complete knowledge of this. this is basically amortization of a polynomial. an example is reducing two evaluation points to one, which is basically what GKR does. this is why we can't have V verify the evaluation of a polynomial without having complete knowledge of the polynomial. 
the problem with a transcript polynomial or input polynomial (like in GKR) is they will be different in every instance. 

it actually often the case that inputs will be largely similar. for example, computing the sigmoid will only differ in the actual sigmoid input, not all the other constants. does the similarly of inputs allow for input amortization in the same way exactly the same input does?
also keep in mind in cases like sigmoid all the necessary constants that do not change can in fact be encoded in the circuit definition and need not be treated as static input. 
but it is still worth exploring whether similar inputs can be amortized. 

trace of matrix as basis values. this was the idea of computing the dot product by multiplying the y vector by a matrix where the basis values are diagonals, and taking advantage of the fact that the trace of the matrix sums to 1. let me check online if there is any such advantage. hmm, given a square matrix, the trace is equal to the sum of the eigenvectors, and the determinant of the matrix is equal to the product of the eigenvectors. pretend that multiplying the y values by the eigenvalues vector is equivalent to multiplying by the basis vector. remember that any vector can be expressed as a linear combination of eigenvectors. this is all interesting, but its not going to help in reducing the work of evaluating a polynomial. but it can help think about other ways besides polynomials to encode programs. 

well now i know that a field has a normal basis consisting of m values that are linearly independent. any field element can be made by i think a linear combination of these m elements. despite having a basis and thus being able to use linear homomorphism on linear functions, the problem is still that a linear function can only encode as many points as there are parameters, so even reading the input takes as much time as evaluating. 

consider the vector of c-coordinates representing basis terms without the y values (the vector who's coordinates sum to 1). that vector can occur anywhere along the linear c-1 dimensional plane in c dimensional space. if P is supposed to compute the angle between its own y value vector and this vector i will call the basis vector, i suppose it can do this by means of first computing the angle for each coordinate. for the basis plane, whenever all coords are held constant but one it forms a linear line. so computing the difference between that and a fixed coordinate of y results in a univariate poly of one degree. then taking the norm of these expressions results in an expression for the distance between the two vectors that consists of the c coord variables, but these c variables can be reduced to v=log(c) variables that constitute the input of the poly. 
by law of cosines: let a and b be the vectors and let c be the distance between them. 
cos(angle) = (c^2 - a^2 - b^2) / (2ab)
so then the dot product is ab cos(angle) = (c^2 - a^2 - b^2) / 2
so computing the magnitude of each vector and distance between them is sufficient. 
so unfortunately in the end computing the cosine still requires evaluating an expression (not necessarily a polynomial) of v variables. actually the expression is c^2 which will take the form of the sum of the square of each of the univariate polys of one degree. this in turn results in a sum of univariate polys of 2 degrees. 
i guess what i've been trying to do is evaluate as much of the polynomial as possible before knowing the input to leave minimal work for V, but i've learned no matter what is done before hand, there are still O(c) operations to be performed after.
what about taking advantage of the linearity of the basis plane? 


March 5, 2018
i don't know if there is where i left of previously. but i want to revisit merkel trees as functions. what if instead of multi-variable functions for levels we used a univariate function. its degree would have to be as many points as it encodes, n. suppose the field size is 2^128. then log(n)=16, say, of the input bits select the index. oh, now i remember, the sum-check is only useful for univariate evaluations. but the univariate expression can be converted to a single value and so the input encoding function f could still be univariate. so if u_i for i=1,...,v are the inputs, then the input to f is \sum_{0}^{v-1} 2^i u_i where 2^i would be literally encoded as the element with all zeros but a 1 in the i+1's position. anyway, assume f can be univariate. in fact, we might even choose to just concatenate the u_i together rather than operate on them to reduce them to a 128 length string. a v*128 length string might be better as it is more compatible with hash lengths. so lets take path lengths to be 16, so 2^16 inputs points are merkelized. assume the former case of encoding using 2^i instead of concatenation. the the first 16 bits will determine the path, and the remaining 128-16 = 112 of the bits are zero on an index evaluation. thus we should use the rest of the bits to multiply sibling branches. suppose the hash size is 256. then we have 112 bits to multiply each 256 bit sibling. 256/2 = 16 + 112. we could pad the 112 bits with 16 0 bits then multiply by the 256 bits. but the padding positions should be random, perhaps determined by the first 16 bits. maybe go though the 16 bits. for each 0, pad on the front with a 0, and for each 1 pad on the back. so anyway, i'll assume some form of using the remaining bits to multiply the sibling hashes (or the path hashes). the function is evaluated by choosing the path, multiplying each hash correspondingly, and not multiply the leaf by anything. then adding all results. oh shit, i forgot about the whole idea of evaluating along a line. well if one of the u_i is a variable x we'll get a linear poly a*x+b where a is 2_i for some i. we no longer know the first 16 bits. when there is a variable P should have to evaluate over every hash of the tree. lets make a recursive model of evaluating a merkel function. let f be the root function let fl be the left hash function, and fr be the right. and let g(z) be some function that for an indexed z should output 0 or 1. this method using simpling multiplying, not path multiplying.
f(z) = g(z)*fl(z) + (1-g(z))*fr(z)
i suppose g should be different for each layer of the tree, because it will need to extract a different bit or value for each layer. maybe g_i could be made to extract u_i from the input. i think it's sufficient to suppose u_i is 0 or 1. we don't care what happens when its a random value. suppose the input is m = 100*u_3 + 010*u_2 + 001*u_1. how to extract u_3 for example? if we subtracted 010 and 001 we'd get 100*u_3 + b for an arbitrary bit b. if b is 0, we could then multiply by 100^{-1} to get u_3. if b is 1, we could multiply by 100^{-1} to get u_3 + 100^{-1}. i suppose in both cases we could then subtract 100^{-1}/2, to get u_3 +- 100^{-1}/2. and in this field addition and subtraction are the same, so we'd get the same value. then we could subtract 100^{-1}/2. i feel like i'm just going in circles, but logically it makes sense. oh, the fallacy is that division by 2 doesn't make sense in this field, because that means multiplying by 2^{-1}=0^-1=0. maybe we could separately reproduce the b bit and then subtract it. the b bit is u_2 + u_1. i don't see a way to get this. well we could do (100*u_3 + b)*(100*u_3 + b) = 100^2*u_3 + b. then subtracting 100*u_3 + b we get (100*100+100)u_3 = 100(100+100)u_3 = 0. what about instead multiplying by 100^{-1} and then subtracting to get (100*u_3 + b*100^{-1}) + (u_3 + b*100^{-1}) = (100 + 1)*u_3. then multiply by (100+1)^{-1} to get u_3. is this really possible? hmm, now it looks i screwed up early in the above paragraph, in that (010*u_2 + 001*u_1) - (010 + 001) = 010(u_2 + 1) + 001(u_1 + 1) != u_2 + u_1. 
why was there trouble with the multivariate method? we needed a way to select a path, and it had to be chosen by the first bit of each variable. there was trouble extracting that bit. using a bit 'and' operation would work, but that was different algebra that didn't seem to distribute well. 
ok, using freshman's dream, i can convert 2^i*u_i + 2^j*u_j to u_i + u_j. recursively doing this i suppose we could get \sum u_i. actually i could also get u_i or u_j. so then lets do this recursively with i and j = 1 and 2. OMG, this means you can find any u_i in \sum 2^i*u_i. or wait, does the fact that we relied on u_i being 1 or 0 destroy the recursivity? if u_i and u_j are both either 1 or 0, then 2^2*u_i = 0 or 2^2. then 2^2*u_i + u_j = 0 + u_j = u_j or 2^2 + u_j != 0 or 1. so yes it does not work unless u_i are either 1 or 0. what it does enable us to do though is express the input in half as many variables. oh wait, my initial assumption that it only needs to work for u_i 0 or 1 is wrong, because a single branch must be selected even when u_i is random. even my recursive definition of f is wrong, because when z is known eg is the random point for V to evaluate at, V should on have to evaluate fl(z) or fr(z), not both. 

001 001 = 001
010 101 = 111
011 110 = 101
100 111 = 011
101 010 = 111
110 011 = 101
111 100 = 011

so the core challenge is to take an element of some set perform an operation that will yield either the multiplicative or additive identity, each assigned to half the set. in other words, we must defined a 2-part partition of the set, with 0 and 1 in separate partitions, and have an operation over a field which can indicate which partition the element belongs to. what if our field is of prime order. then we could partition by even and odd elements. thinking of partitions as cosets, i think we could take an element, multiply it by itself a certain number of times, and arrive at either 0 or 1. 

odd * odd = even
even * even = even
even * odd = odd


p=5
0*0 = 0
1*1 = 1
2*2 = 4
3*3 = 9 = 4
4*4 = 16 = 1

origin of the pattern above. so we end up with {0,1,c} for an element a of field of prime order p, by computing a^{(p-1)/2} = 0 if a=0, or 1 or (p-1)/2. i will call (p-1)/2=c. how to deal with these three results instead of 2? maybe make a close term only evaluated for a=0 to handle the special case. we could also make use of x^{p-1} = 1. so we'd be able to reduce to 0 or 1 if we can take x to high powers where x is the input. but is this ok, or does it break some condition of low degree? the function would be:
	x^{p-1}( 
		((p-1)/2) - x^{(p-1)/2}) * ((p-1)/2) - 1)^{-1} 
	)
so x=0 would result in 0, and for non zero x: x^{(p-1)/2} = 1 would result in 1 and x^{(p-1)/2} = (p-1)/2 would result in 0. for example, x = 1 would be 1.
so this method should work for the multivariate version. each variable will be put through this function to determine a branch. we must write the entire function, make sure it works, but we also need to devise a way for the variables as say 64 length bit strings to be interpreted as elements of the prime field. A Mersenne prime p = 2^n - 1 for certain n's. suppose we find a suitable n. the bit string will be of length n. the prime field will have 1 less element than the extension field. this means two elements of the extension field would have to map to the same element of the prime field. so the bit string of all 1s for example, could map to 0, in addition to the bit string of all 0s. i don't think this is a critical problem, because the 1s bit string will not be accessed but as a random choice in the extension field. but maybe P could strategically choose 1s. actually i don't think this is a theat because we could work with the regular 2^n field as usual, and only have to map when evaluating f. and the inputs for f are always r_i, x, or c_i where c_i is 0 or 1. i need to check on this. assuming this is the case, the only possibility for an input to be 1s is for V to randomly choose r_i as 1s. if this is so, the protocol can have V recomputed a new r_i. possible choices for n are 89, 107, 127.

uh oh, looks like degree will grow since its not a multivariate poly. well, what we need to do is evaluate f(r_i, x, c_j) for multiple i and j, and then add the results. the result of each evaluation should be a polynomial, and adding will not increase the degree. lets review the structure of f. continue with f, fl, and fr notation. but add h, hl and hr notation for the hashes. then f is:
	f(x) = (1-g(x)) * (hr * x + fl(y))  +  g(x) * (hl * (1 - x) + fr(y))
where f is one layer of the tree, x is one input, and y is the next input. if x = r is known then we can reduce to
	f(r) = a * fl(y) + b * fr(y) + c for constants a b and c
so we will be able to crawl down the tree for every r_i. when we reach x, we will not be able to reduce. but we will, however, know that c_i comes next so we can execute fl(c_i) and fr(c_i) fully because we known all the c_i. thus for f(x) we will end up with a poly of degree deg(g)+1. deg(g) = (p-1)+(p-1)/2 = 3(p-1)/2, this is worrying because thats larger than the size of the field. well actually, x^{p-1} = 1, so we will only have degree (p-1)/2 + 1 = (p+1)/2. and this would be super easy to compute because we know it only has one of two values multiplied by x. so it appear evaluating at x will yield a poly of the form (a * x^{(p+1)/2} + b). this will be done for both fl(x) and fr(x), and then we'll put them in a linear combination from the r_i evaluations and so the whole thing will yield a likewise poly. 
since f is defined recursively we must define the base case. this will occur when we don't have any more variables, in which case f(any value) yields h, the vertex hash, which will be the leaves. so the degree will not in fact grow, but it is still high, (p+1)/2. i hope this does not destroy the security analysis.

now to review the security properties needed for f. well first, what properties of a function have i been trying to meet. P makes a commitment to a function f, not necessarily the merkel type. we want P to stay true to this function. 
suppose we just use sumcheck to sum over the leaves, so evaluating f(u_i) for i=1..v. moving backwards. we first know that V can obtain the correct answer for f(r_i) by evaluating directly. then we want V to verify the expression for f(r_i, x)=g(x). if f was a poly the argument would be, if if g(r)=f(r_i) then g=f because if they are the same at a random place they should be the same everywhere, because this is a property of linear polys. can we make a similar argument for the merkel f? let us first generalize the argument. it is that if f(r_1,...,r_{i-1}, x, c_{i+1},...,c_v) = g(r_1,...,r_{i-1}, r_i, c_{i+1},...,c_v), then f=g. we assume the evaluation of g is already known, and f is given as a poly with two terms. what about degree testing? this is only necessary when V can encoded more points than the poly should allow. if the merkel tree is only of appropriate size and can thus only encode as many points as allowed. It seems V can assume that the tree is of the right size, by only checking when evaluating the random point, because suppose the tree is too big. then if P evaluates correctly, it will need to ignore the extra part of the tree because it will run out of variables, in which case its as if the tree is of the correct size. if instead P somehow incorporates them then the evaluation is incorrect. the goal to establish is that P cannot afford to lie about f(r_i, x, c_i). 

The focus now is on how the sumcheck can be used besides just summing over the leaves of the tree. since the results must be added, it is necessary that the output of a partially evaluated munc (merkel function) must be algebraically compatible with the surrounding function that the sumcheck sums over. Suppose we don't try to convert the output of the munc to galois, but rather interpret the whole expression (the one being summed over) in the prime field. It would still be a multivariate poly, but it would just be over a different field. suppose V is the poly in the new field. evaluating it directly would just mean evaluating f at 0 and 1 for all arguments. using the sumcheck, however, it would be evaluated other ways, for always with at most one variable. the polynomial sent on each round would be univariate and of high degree (in that variable). but the number of terms would remain low. i need to prove that if you have a valid value of the poly at r, then you can test the poly at r with high reliability even though its high degree, because it has few terms. so don't focus on testing poly f, but on testing the a general poly with few terms of arbitrary degree. 


how about a transcript. forget about verifying a particular known computation. rather just verify the computation, with the code as part of the transcript. an instruction points to two indexes, a computation on them, and an index for the return value. i suppose to save space of intermediate values, an instruction could reference more than two indexes. for a proper integrand, the code will need to be referenced in a predicable manner, which could be at odds with conditional statements. for code, i suppose it could be held in a different merkle tree, in order to be easily verified as a 'program'. but in this case the program cannot be customized to the computation. so the next instruction would have to be dependent upon the computation. a program counter could serve this purpose. but returning to previous instructions seems to assume that the same indexes are writable, when they are not since this is an immutable merkle tree. thus in order to return to previous instructions, the destination for writing must not be specified as a register, but as a pointer to a pointer. a simpler method would be to just always assume the next index in the tree as the destination for the next write. so it looks like the program counter would need to be inserted along with every write value. the next check would reference this instruction to locate the appropriate instruction. 



Prover can encode each of the n spots into an n-degree poly. Then can create n taylor approximation, one around each point, each approximation of low degree but sufficiently high degree for appropriate error between points given a weighted sum. 

Suppose there exists a type of function such that the points can be interpolated with a maximum derivative of absolute value $\alpha$. The danger of evaluating at points with large derivative is if your point of evaluating is even slightly off, the result is hugely off. if the point is exact, however, i think the derivative should not be a problem. but of course when evaluating at a random point the argument can be unexpectedly off. So with alpha, we know that if the argument has uncertainty $\delta$ then the result has uncertainty $\alpha\delta$. 

if adding two funcs together gives another one, then if that funcs types has not many roots, than two different ones have few common values. 

attack occurs by prover giving wrong answer, justifying it with anything necessary until it means claiming a wrong mapping of the input poly. the assumption is if a random mapping is correct, then all mappings are correct.
we want to do polynomial identity testing. we want to make sure that the polynomial committed to has a certain value at a random point. If the polynomial committed to is of very high degree than it can have the same value as the used p
Assume computation must be done with an appropriate poly. The only problem is it may be the wrong one, that is not the one committed to. The one committed to can have the same values as the one used at arbitrarily many places. So evaluating at a random point, it may give the correct value, but be the wrong polynomial. 

So the attack would be the prover commits high degree, then uses a different poly that it shouldn't but one that agrees with the one committed in, say, half the points. 

an invalid C(P(x)) can match a valid C(P(x)) at many locations if the former is not low degree. 

suppose we commit high degree to the GKR. what attack can we make? lets re-frame this as the verifier now holds a high degree polynomial from the beginning. then the GKR poly testing from the very beginning is simply more at risk because the integrand of the sumcheck always contains a high degree poly. 

before i thought point of eval would be chosen randomly, which is it. i also thought the expected value would be chosen randomly. 
V must make sure P used a low degree poly. even with taylor series, nothing prevents 

at the end the verifier has a univariate linear poly, and chooses a random point completing the random domain, resulting in a completely random output completing the expected value. 
 

if same at random point, then same. if not same, then not same almost everywhere. 
suppose i have a valid value of f at (r1,r2,c1,c2). can i verify a candidate for (r1,x,c1,c2) with it? of course when i have the two branches at vertex x, i check their hash. is it possible to have the two evaluate the same while not being equal? 
(c1,c2,c3)
(x,c2,c3)
(r1,x,c3)
(r1,r2,x)
(r1,r2,r3)

(x,c2,c3)
(r1,c2,c3)
can get (c1,0,r1) and (c1,1,r2). need (c1,c2,x). you have a unipoly at two random points 

doesn't matter if high degree, sumcheck only relies on linear polys, and at the end it just must be checked that original poly has certain value, not certain degree.

	\begin{align*}
		P(f(r) = g(r)) \leq \frac{1}{deg(f)}
	\end{align*}
	\\
	Proof: todo
\end{Lemma}


replace each variable by the same one multiplied by an arbitrary constant. expand and now we have a degree v, univariate poly. hmm, maybe computing the coefficient of any of the terms could be done by the sumcheck over the arbitrary constants. anyway, the goal is to encode as a multivariate. 

we want to use taylor series on a univariate poly. 
we could choose that the poly disappears on certain points, eg the chevbsey nodes. using sumcheck with multiple variables to index into the nodes, the verifier can evaluate the roots polynomial in log time. the operation would be product, not sum. 
then verifier can evaluate G, the constraint checking poly. prover submits W and H such that G(W(x))=H(x)*h(x) where h is the root poly. 
so choose your constraint polynomial and choose the points at which it should equal zero. 
the constraint poly can be multivariate, and it can depend on x as well as on W(x) which is the transcript poly.
so the verifier evaluates W at a number of points, and H at one point, and checks for equality of the equation. 

what if W and H are of excessively high degree? then G is effectively of high degree, and so is H*h assuming H is still correctly computed. 


suppose we have coefficients and basis terms such that a linear combination can interpolate. given this, it is the case that the difference of two combinations form a third of the same degree. suppose a combination of degree d has at most d roots. then the probability of false positive is about d/|F|. if we assume (like with a poly basis function) that the interpolated function can only go up and down around d times. then each turn should representable by a 2nd order taylor series. the points of taylor approximation should be predetermined and probably evenly spaced. 

suppose the function had unpredictable distribution of up and down density. then the distribution of taylor polys would have to be unpredictable. this is because a fixed degree poly can only encode so many ups and downs. then the problem would be you could land on high density and no know whether its just one of a few dense parts or whether the whole function is that dense in which case its of high degree. so our first requirement is that the ups and down have as uniform a distribution as possible. we don't even want a predictably un-uniform distribution. because again, if you land in a high density area you don't have reason to assume the other areas are less dense as they should be. 
the prover can't increase degree by adding extra polys to the tree, because it is clear which of them are extra by their placement in the tree, and they will never be opened by the verifier, so it is as if they are not there. the prover also can't increase degree by increasing degree of the taylor polys, because doing so makes them invalid and the verifier will not use them. the only way to maybe cheat is to make incorrect taylor polys. what we'd like is there to be no such thing as incorrect, that is every possible commitment is a valid representation of some valid poly. the trouble is the edges of two polys might not merge. for example, two ends may both point in the same direction (eg down) instead of opposite directions as they should. if one looks at both edges its easy to reinterpret them as a valid curve. if you land right on the center of a point, just look at that point. if you land between two centers, look at both of their relevant edges, and interpret them as appropriate. so basically any commitment can be reinterpreted as some poly of appropriate degree. so now its just that for completeness we want the prover to be able to accurately enough approximate the function with evenly distributed taylor polys of fixed degree. 


feedforward network satisfaction problem. 
w * a + b 
go through every neuron and make sure:
if input: correct value
if other: is assigned activation function applied to appropriate weights, bias, and inputs from previous layer. 
the logic is the topology and the weights and the biases. a poly could encode the weights and bias. the constraint poly would assume a certain topology and query the parameter poly accordingly. the parameter poly would not be made by the prover, but assumed by the verifier. the prover would construct another poly for the transcript. 
i suppose there could also be an input poly. all polys encoding info would be oracles. 

a verifying computation take the proofs of two others, and by verifying them it is conscious of what they computed (as in the logic and the input, as well as the output). its job is to combine the results into something more meaningful. the input and logic of the verifier is responsible for checking the input and logic (via their root hashes) of the input and logic of the candidate computations. verification, however, requires hashing ability. suppose we take a hash function that can be computed by a neural network. 

how about RNNs? 
what about other paradigms besides nets? like an arithmetic circuit? one way to do this would be to eliminate the activation function, and give 0 weights for all edges that shouldn't exist in the network. 

if we are to embed proofs, we need the computational model to be able to handle a hash function. for now i will forget such a hash implementation, but one is probably well possible with a neural network. 

so given an expression suppose we can index into any polys. the challenge is to index correct combinations. the number of layers and the size of each layer can be hard coded in the constraint poly. we will proceed one neuron at a time. given the index we need to be able to map to the layer number. so really what we need is several expressions for mapping. all should be a function of a neuron identifier. one should be a weight function that takes a neuron and a weight index and returns the weight. another should return a neuron's bias. the last should return a neuron's value. the first two will need to index into the code poly, and the last into the transcript poly and the input poly. i suppose the transcript map can simply be 1, that is if neurons are identified by an index they can map directly to the transcript. biases could also map directly like this because there is only one for each neuron. actually we'll need a means of determining whether a neuron belongs to input or not. weights will be most complicated. each neuron will have a weight for each neuron in the previous layer. 

one way of mapping numbers periodically is with trigonometric functions. suppose we divide the unit circle into pieces, one for each layer. 

the prover must create a new poly for each round, so the verifier will not be able to just query the existing polys. does the verifier need to evaluate the new poly by itself, or can we use another commitment scheme? i suppose there's no reason the prover can't give the verifier the new poly in the form of a commitment. of course it could be wrong, but the sumcheck lets the verifier check this. the only requirement is that the prover give the verifier the poly in a form the verifier can evaluate. this is luckily not more work for the verifier, but it is significantly more work for the prover, now that for every round of the sumcheck it must not only create a new poly by evaluating the existing polys on affine transformations, but it must also make taylor series and commit to these new polys. 
so assume the verifier is able to evaluate the the dot product and the bias. keep in mind that that effective polynomial will be of degree of the product of the degrees of the transcript poly and the weights poly, which is an enormous number. then this poly will need to go in the sigmoid which inevitably multiplies the degree again. luckily the biggest poly, the weights/bias poly, will be pre-made and can 
hardest part will be for the prover to do the long division. doing this i think requires manifesting the poly on the left side. 

actually it looks like we might as well use the univariate with the sumcheck in the GKR since we'll have to do that anyway with the neural net method. 
actually maybe something like GKR doesn't even need sumcheck. well actually only the top level could go without sumcheck because it could be evaluated at a random point easily. but the other layers would have to be encoded in terms of the layer above. using this GKR style, each layer can be a different size. i suppose these layers would be the usual multivariate polys. its just the internal expression for each node that would be different. it would be the sigmoid applied to a dot product of that layer's weight poly which would be univariate, and the above layer's values. so if we use the sumcheck on the dot product, that means a sumcheck inside a sumcheck. whenever we use a sumcheck with the integrand containing a univariate poly, the prover needs to send intermediate polys as commitments. 
suppose we just used GKR as normal, and use univariate polys for the input poly. i think this is the first thing I will write down, and only later write down how to do neural nets or other models using GKR style or poly-div style. 

so first i need to define and prove the properties of the univariate poly commitment using taylor. this means choosing where the nodes are located, via what expression, and what basis functions we use for interpolation. 


if we used a bivariate poly instead of univariate, then the degree would be square rooted. but then the size of the taylor series i think would be squared. this could be a useful trade-off. 

assume no more about function than encoding shows. that is, consider function type not a polynomial, but as a series of equally spaced, fixed degree, taylor polynomials. 

i think its enough to know that the function is low degree in the area of the evaluation. it can be high degree elsewhere, and the two different functions can overlap elsewhere. so then its like 

For every point that the constraint should vanish on but doesn't, C/R=H will have a jump discontinuity. this is a problem, because if the evaluation point is not around that discontinuity it will be invisible, the test will pass, and yet C doesn't vanish where it should. a proximity test might detect the jump discontinuity, but simply randomly picking a point only looking at the taylor series on either side is unlikely to encounter the discontinuity.
one way around this i think is to not require any division, but to have the degree of C and R the same, so that the problem is checking that they have the same roots. Then V just computes R, multiplies it by a necessary constant, and then the result should equal C. We might have similar problem with sumcheck evaluating C, but for now suppose V is able to compute C knowing its of correct form. Then V evaluates C (using the transcript) at a random point and the idea is...

an attack would need to start with an incorrect transcript. As a result, C would fail to vanish on at least one point. Thus C would be different from R, so hopefully the would evaluate the differently for a random point. the concern is maybe if the transcript is incorrectly encoded, like with jump discontinuities, and the random evaluation point did not encounter them, then the test has opportunity to pass even when C and R are different. 

an idea is to accept any possible encoded function. maybe we can assert that any taylor series commit sequence, even if with jump discontinuities, corresponds to a valid form of some function, like maybe a rational polynomial. then we could try to make statements about rational functions in the same way we have for polynomials. but actually even this doesn't work because we know that two different taylor commits can be the same everywhere but one region. 

the hash function i had earlier satisfies the property that two different hash functions evaluated at the same point give different answers with high probability.
what about using a univariate version of this with the sumcheck? what we want is the contrapositive: if they evaluate the same, they are the same. is this true? i don't think so. and where the theorem breaks down is that a traditional function over a field can take many different forms and still be evaluated, but this particular function must be in particular form because evaluation also requires hashing, and some forms of the function (sums) don't reveal the necessary data for hashing. 

the idea about taylor series was it enables an honest prover to commit to a poly, but doesn't allow a dishonest prover the commit a poly with high degree. the problem is the prover can still commit something that doesn't resemble a poly. 

in complex analysis, if we have an analytic numerator, and a set of roots for the denominator, we can find the residues by the numerator and denominator. then summing the residues we can find a contour integral. we know that real integrals over infinite range can be done with residues. ...

i was thinking about a circuit that would use as input the same input as another computation (a taylor input). but it would evaluate it at different points and make sure that each item of the series leads to the next (no jump discontinuities). but this would be a recursive check. the input is in correct format only if the computation is correct, but the computation is correct only if the input is in correct format. how would a prover cheat? suppose the checking is done with GKR. ...

think about hash with sumcheck again, notice that degree 1 cases is what sumcheck relies on and this probably holds. 
i think it has the ability to recover (r,x,c) from (r,r,c). given (x,c,c) you can get (c,c,c). given (r,c,c) you can get (x,c,c). given (r,x,c) you can get (r,c,c). given (r,r,c) you can get (r,x,c). etc. 
(r,r,r) -> (r,r,x) -> (r,r,c) -> (r,x,c) -> (r,c,c) -> (x,c,c) -> (c,c,c)
ok, i think i found why it doesn't work. you cannot in fact recover (r,x,c) from (r,r,c). this is because (r,x,c) will include unverified data corresponding to the c of the branch not taken. 

SVD. can we make it so P can commit to the change of basis functions and V is able to compute with them? 

If we use fourier, think about predefined magnitudes, and instead P commits to offsets. if each amplitude is 1/2 the previous, you can evaluate eg any function over a 128 height range to within a 1 height range of uncertainty by evaluating the first log(2,128)=7 sinusoids. the protocol would be like the taylor, that is for a univariate continuous interpolation function, but this time by committing the offsets of sinusoids. the two conditions are that V can evaluate at any point with reasonable certainty. This was easy for taylor. The hard part for taylor is verifying the formating as correct. This could potentially be easy for fourier. to make sure its of proper form we want to make sure that any function that can be encoded as few roots. we also need to check that the difference of two such encoded functions of the same degree forms a third with the degree. i think the probability analysis would entail that there is not an infinite sequence but just enough to encode the full vector, probably one for each vector item, and the analysis would be about taking multiple (no more than log) claimed samples and analyzing the probably they could all be interpolated by the committed function. If infinite sequences were assumed then no analysis would be helpful because any point within the range could be interpolated, but it would still be able to at least throw out any claim outside the uncertainty range. 
so 


maybe we can enumerate of the weights with GKR with D_i(x1,...,xn) = sigmoid(sum_j D_{i-1}(j)*W_i(j)) and perform the internal sum using sumcheck. in this case the integrand is simply the multiplication of the source layer and the weights, no add_i or mult_i logic. 

whole other topic, but think about GKR RNNs, which how many the recurrence verification could be solved by reducing multiple points to one.  


we want a set of basis functions such that we can compute an uncertainty height. suppose we have a way of doing sinusoid interpolation, where the amplitudes decrease and are predetermined. i suppose we could have the frequency multiplier arbitrarily chosen by the prover, because it doesn't affect the uncertainty interval. 
looks like the integration of any such function is 0, so prover can commit to an average height as well. 


i think two functions can sum to another if 
a product of N sinusoids can be converted to a 1/2^{N-1} times a sum of 2^{N-1} sinusoids. the arguments would remain in proper form. So now we'll have the concept of a product, but only if we can have the concept of a sum. ugg, summing doesn't work. as a result, multiplying two leads to the sum of two sinusoids with different phase and frequency, but the same amplitude! this means that evaluating within a certain range of frequency now means evaluating multiple sinusoids for the same amplitude. 
so now it makes sense to resort to Euler's formula and use the complex exponential form. Then multiplication of two functions (each a sum of such exponentials with fixed coefficients) results in another one, but with square the number of terms, but it still takes same number of evaluations for same uncertainty. actually i realize we need a normalizer which I think is 2R for range R, which would properly increase the range upon multiplying two functions. 
taking conjugate means taking the inverse. adding a term and its inverse actually results in 2 * the real component. i don't know if this is useful.
i think the approximation still works. every term evaluated results in a vector (complex number), and the vectors are divided by increasingly large constants so the vectors become exponentially smaller, then the vectors are added together. so now our uncertainty region is circular, that is a radius of uncertainty. `
perhaps another operation we could do on these functions, which is valid, is the dot product. oh wait, the coefficients would not be valid. 
so at the moment we have a group of these functions that obey group properties. again, the functions are complex valued functions of real variables. 
for a function of t, plugging in a linear function of t also results in a valid function. 
might be time to give up on this. i don't think there's a way to interpolate. setting the problem up in matrix form shows the matrix must be such that the no only are the equations consistent, but a variable answer vector must exist where all the variables are less than one so they can be outputs of trig functions. maybe my last resort will be to ask Igor whether he thinks such interpolation is possible. 


now i'm going to try a different method where we don't solve a matrix, but instead we operate on the phase and frequency of p sinusoids in order to interpolate p points. This obviously works for p = 0 and 1. Given a change in a sinusoid's frequency and or phase, we have an expression for the difference in the resulting sinusoid from the original. For p points, we will assume the sinusoids already interpolate p-1 points. Then we will want to modify p sinusoids such that their sum remains invariant at those p-1 points. First we must find the set of operations that will have this property of invariance. Then we can select among these operations that which will let us interpolate one additional point. Using exponentials, it turns out this method involves solving an system of complex exponentials, which means solving a complex matrix and also ensuring that the chosen results all have absolute value <= 1 because they must represent complex exponentials. This is the same challenge as just solving the interpolation system directly. 
using real trig functions (even though sums of them don't form a group), the problem can be reduced to solving a nonlinear system of equations, but with the restriction that all variables are chosen between -1 and 1, because they must represent trig functions. 
I'm going to give up on this method now. I might to solve the equations I've been talking about, but my concern is that even if I could, it just seem so unrealistic to interpolate millions of points with the amplitude of these millions of sinusoids falling exponentially. The amplitudes would be so small, arithmetic would require exponential precision. And increasing the height of functions is not a solution, because often the parameters needed to be interpolated will have small values (eg neural network weights). 


HERE I LOST A FEW LARGE PARAGRAPHS OF DATA BECAUSE I MOVED THIS FILE WITHOUT SAVING FIRST
the content was about having basis functions, like polynomials, be officially committed via Taylor series slices, and then have the prover commit meta data about to modify those basis functions to reach the interpolation polynomial for their custom input. But both rungs phenomenon and chebychev's unpredictable roots made it difficult to do precision. 


MAY 4, 2018
don't know where i left off. just an idea. like before split univariate into bivariate, so have two polys of half degree. then only challenge is to verify their oracle proofs are in proper form. i think following reasoning applies. if they are in improver form, their sum will be in improper form, so only reduce problem to finding that their sum is in proper form (which is given as another oracle proof). of course the verifier must make sure the claimed sum is indeed the correct sum. do this by evaluating the two sides of the equation a+b=c at random points. assume c (the sum) is in proper form, and a or b are in improper form. Then c cannot be the sum of the a+b so they must be different, and hopefully there is the theorem that if a or b is in proper form their sum and c intersect only at a few points, this way its evident c is not the correct sum. 
the problem i think of right now is that c can indeed be the sum of and a and b at most places, but not at a few which happen to not be tested. 
lets review the overall challenge of encoding a univariate with taylor. 
so the problem starts by the prover encoding an incorrect transcript, with the wrong answer. the prover can encode the transcript however. this then gets nested in the constraint poly. divide this poly by

think about reducing the format checking of two polynomials to that of one. the risk i'm thinking of is just encoding the two like negatives of each other.

taylor doesn't immediately work for poly division because its form must be checked. what about for sumcheck with a univariate? at every stage, for every intermediate poly, the prove must construct a new taylor tree. i don't think this will work because the soundness holds under the condition that p_{i+1}(0) + p_{i+1}(1) = p_{i}(r) because the prover could construct p_{i+1} at 0 and 1 so that this holds, and yet randomly construct every other part of the poly to fit its needs. 


how about with GKR have top input encoded with a number of different polys. if this could work, then they could be standard, and proofs could reduce their points of evaluation on these polys to one. 
if input poly was piecewise, it basically means the whole poly would be piecewise. and at every stage in every sum check round, n smaller polys would have to be passed to V, ridding the logarithmic power of multivariacy. actually i realize passing piecewise polys would actually not be necessary until reaching it. so V's at D1. P has to evaluate the integrand at a lot of points where all variables are constants except one, and then sum them together. the integrand is a function of D0. but depending where you evaluate D0, it consist of different functions. but regardless, P can still sum them together into compact intermediate results. ...


i was thinking about have a small enough input that all versions of input could be evaluate able at every block, and an infinite number of computations could take place, with inputs consisting of a sequence of the basis inputs. 
one element of this idea is taking two GKR computations and combining the results. maybe this could occur at an intermediate row. so verification would take place from the bottom up, and then it would reach a splitting point, and each direction would become a regular GKR. 
for a moment suppose this worked. then certain computations with a lot of data could be done. for instance, one type could be a neural network but where each neuron only takes input from other neurons in a nearby region. however, if at the top every poly is to be evaluated, then the poly size must be tiny. suppose we're just doing bits, then something like 8 bits would be big enough. in this case the computations would really be limited. when it gets to the top, the points of evaluation would have to be combined. ahh, but unfortunately even this prevents anyone from verifying because to a result at any point, one must verify all paths upstream, and that is not doable.

let me revisit the idea of one giant poly that is collaboratively evaluated a single point. problem is, how can you trust others will properly evaluate they're parts? suppose somehow the community could arrive at an answer that everyone (at the moment, not newcomers in the future) trusts. for anyone that trusts the evaluation, that person could verify the computation, and the connection functions would probably be super redundant. 


regarding the poly transformation method, suppose Q, P, and phi are all committed, but Q and P only through a hash, and phi is the only one that gets evaluated. this way the input need only consist of one poly instead of two. so the phi would be input to a verifier which would evaluate it, confirm the right evaluation point, then reduce that the other one to a single eval point. the problem is V must also be sure that phi is correct. this could be done by confirming that Q(\psi) = P, which could be done by testing the two sides at (even just a single) random point. that requires evaluating Q, however. maybe one solution is to require that it be predetermined which Q and P will be merged, and phi be generated ahead of time so it can be falsely constructed but not maliciously. and maybe it could be that P and \psi (which then defines Q) are allowed to be computed to prevent contradictions on Q. evaluating Q(w2) would mean finding w3 such that \psi(w3)=w2, then evaluating P at w3 assuming Q(\psi)=P. in this case i guess \psi does not need to be checked as correct. but \psi must still be evaluated at w3. i'm imagining \psi to be a set of affine transformations, one for each variable. but wait, i don't think this is possible cuz it would only be using 2*v data points to translate 2^v points to another 2^v points. have to explore further what \psi would look like. 
well the simple affine with 2v points probably won't work. to keep it as the same degree poly, i think the following form will be necessary, and i'll check how many points it includes. i was gonna replace every variable with a linear combination variables but i don't think would work. and it would also increase the degree. 


(((c1X+c2)Y + (c3X+c4))Z  +  ((c5X+c6)Y + (c7X+c8)))W + (((c9*X+c10)Y + (c11*X+c12))Z  +  ((c13*X+c14)Y + (c15*X+c16)))
(((a1x+a2)y + (a3x+a4))z  +  ((a5x+a6)y + (a7x+a8)))w + (((a9*x+a10)y + (a11*x+a12))z  +  ((a13*x+a14)y + (a15*x+a16)))

c1X+c2 = a1x+a2 
	=> X = (a1x+a2)-(c2))/c1
(c1X+c2)Y + (c3X+c4) = (a1x+a2)y + (a3x+a4)
	=> Y = ((a1x+a2)y + (a3x+a4) - (c3X+c4))/(c1X+c2)
((c1X+c2)Y + (c3X+c4))Z  +  ((c5X+c6)Y + (c7X+c8)) = ((a1x+a2)y + (a3x+a4))z  +  ((a5x+a6)y + (a7x+a8))
	=> Z = (((a1x+a2)y + (a3x+a4))z  +  ((a5x+a6)y + (a7x+a8)) - ((c5X+c6)Y + (c7X+c8))) / ((c1X+c2)Y + (c3X+c4))

W = ((((a1x+a2)y + (a3x+a4))z  +  ((a5x+a6)y + (a7x+a8)))w + (((a9*x+a10)y + (a11*x+a12))z  +  ((a13*x+a14)y + (a15*x+a16))) - (((c9*X+c10)Y + (c11*X+c12))Z  +  ((c13*X+c14)Y + (c15*X+c16)))) / (((c1X+c2)Y + (c3X+c4))Z  +  ((c5X+c6)Y + (c7X+c8)))

at variable i, the numerator consists of a poly of i variables, the subtracted with a poly of i-1 variables (a subset of them).
so evaluating the numerator requires evaluating 2^i + 2^{i-1} = 3*2^{i-1}
the denominator multiplying something already computed by a constant, and then adding a degree i-2 poly
actually there's more dynamic programming. my hope is it only takes O(2^n) evaluations. 

new info for each variable (what must be committed to)
X: 2 + 1 + 1 = 4
Y: 2 + 2 = 4
Z: 8
W: 16
so doing this doesn't save any space at all so its not helpful. 

what about instead Q(psi)=P(phi) where psi and phi are different lines, and both sides are univariate degree v polys. 
both lines would need to pass through the initial point of evaluation. and then the direction of each line would be free to make the two sides equivalent. suppose this were possible. hmm, first should see how to construct the line. 

suppose P(phi) is already decided, so we must choose the 2v constants for the line psi such that Q(psi) has the same (v + 1) coefficients. 

a1*X + b1 = c1*x + c2 
	=> X = (c1/a1)*x + (c2-b1)/a1
(a1*X + b1)Y + (a2*X + b2) = c1*x^2 + c2*x + c3
	=> Y = (c1*x^2 + c2*x + c3 - (a2*X + b2)) / (a1*X + b1)

a1*(c1*X + c2) + a2 = b1*X + b2
	b1 = (a1*c1), b2 = (a1*c2 + a2)
(a1*(c1*X + c2) + a2)*(c3*X + c4) + (a3*(c1*X + c2) + a4)

this will probably work, but my concern is constructing the lines requires full knowledge of P and Q.
P(w1)=v1, Q(w2)=v2
to convert either to a line:
	start with poly(w)=v
	randomly choose a line as long as psi(t')=w for some t'
	ask for the poly evaluated on the line
	get univariate poly h in return
	check that h(t')=v
	want to know that poly(psi(t)) = h(t)
	if this is so then poly(w)=v
	to check that this is so choose random point r on line
	find psi(r) = wr and h(r) = vr
	then check that poly(wr)=vr
the idea was that if Q(psi)=P(phi) and you wanted to know that Q(w2)=v2 and you know that psi(t2)=w2, then you could verify that P(phi(t2))=v2 implying that v2 = P(phi(t2)) = Q(psi(t2)) = Q(w2)
but you must be sure that you have the correct psi and phi such that the two are equal. unfortunately these lines can't be decided until the random points are chosen, and unfortunately i think construction requires full knowledge of at least Q or P. 
if Q is included as input, and P(phi)=h is claimed by the prover, then the verifier can construct psi such that Q(psi(t))=h(t). the logic would then be that if psi(t2)=w2 and Q(psi(t2)) = v2 

wait, what was I thinking before. thought just the transformation phi would be committed, and the result would be true conditional on an evaluation of P. but the verifying circuit must then not only evaluate the P of the child circuits but their respective phi's. so this whole idea of reducing the evaluation of two polys to one poly doesn't seem helpful anymore.
maybe it would be possible for the prover to commit to a number of planes (idk what dimension), called psi_i, such that Q(psi_i(t))=I(t) for some fixed identity poly I. then hopefully when the random point wr is chosen, there would exist a psi_i such that psi_i(t')=wr. Then if I(t')=vr its known that Q(wr)=vr. the committed psi_i's would specify Q. more realistic is that the psi_i's are committed where the coefficients are not constant, but functions of the random point. 


STARKS
f is a univariate poly. 
g is the bivariate poly from f. g(x,y) = g1(y) + x * g2(y) and g(x, x^2) = f(x)
g1 has degrees 0,2,4,...
x * g2 has degrees 1,3,5,...

but let x be an element from a group generated by a root of unity w of order 2^n
let this group be called L
the image of L on x^2 is another similar group M but of size 2^{n-1} (half the size). with w^2 as the generator.

by sampling f at the two roots of y, supposedly one can obtain g(x,y) for x in C in y in M.

choose s0 and s1 in L such that s0^2 = s1^2 = s, which is in M.
sample f(s0) = a0, f(s1) = a1

choose r
sample g(r,s) = b
	that is g(r,s) = g1(s) + r * g2(s)

get line p passing through (s0,a0) and (s1,a1)

check if p(r)=b, that is p(r) = g(r,s) = g1(s) + r * g2(s) 
	= g1(s0^2) + r * g2(s0^2) = g1(s1^2) + r * g2(s1^2)


maybe want to show that
p(x) = a0(x-s1)/(s0-s1) + a1(x-s0)/(s1-s0) = g1(s) + x * g2(s)


this could be done with any other integer n > 2. my guess is that in this case you'd choose s0,...,s{n-1} such that si^n = s. Then you'd query f on all si and construct the n-1 dimensional plane connecting the results. then you'd query that plan at random point r, and compare with query of g on g(r,s). r is a multidimensional. 


PIECEING IT TOGETHER:
each row and column should correspond to certain degree poly, call it md, nd respectively.
randomly pick rows and columns.
for each row, say, sample more than md points. do this to get probabilistic proof the row is an md degree poly. 
but make sure one of the sample points is on diagonal, so f lies on same poly.
doing this, assume sufficient proof that all columns and rows are valid encodings, and f lies on them. Then f must have valid encoding.

each row corresponds to a value of x^d, each column corresponds to a value of x.
now assume small image of x^d. for if the are w wraparounds, the number of rows is divided by w. 
now lets try to repeat similar procedure, but now differentiating between sample rows and columns.
every row corresponds to f at w places now instead of just one. so at w of the md+ points sampled must be those w points of intersection with f. 
this intersection removes some randomness, but then it looks like only '+' columns must be committed to. the remaining randomness is in which row gets picked, then that point gets sampled on the committed columns. 
columns must be checked as before, but now they're randomness is also gone. in fact, it seems there's no randomness in the points a row is sample on. the only randomness is where the columns are sampled, and which rows and sampled. 

now lets go to d=2, so f(x)=g(x,x^2)
rows have values of x^2, columns have values of x
the image size of x^2 is half that of x. previously the table size was n by n where n is the size of the original field, the number of values x can take. now the table size is n/2 by n, since x^2 only maps to half those values. 
so w = 2, and to review the procedure: select n/2+ rows, and check each one by sampling the two points on f and the additional point on the committed column. use the n/2+ points on the column to check the column is of degree n/2. 
but of course we can't sample n/2+ points on the column cuz that's too many so we make the process recursive. so assume the column is known (with probability 1, through recursive analysis for total soundness will have probability < 1) to be a poly of correct degree. 
what i don't understand is which column is committed to, and the lack of row point randomness is acceptable (as well as just one column). 

each row check now must sample md+ = 2+ <= 3 points.

row check: pick a row at random, check three points on the row. 2 from diagonal. 1 from column.

(a1*x^2 + a2*x + a3) * k*()

Q mod (y - x^2) = P
K*(y - x^2) + P = Q
Q = (y-x^2)^k * P

a mod b = c  =>  b*k + c = a


k*y - k*x^2 + b4*x^3 + b3*x^2 + b2*x + b1 = (c4*x + c3)*y + (c2*x + c1)

f(x) mod (x-r) = f(r) => f(x) = (x-r)*K + f(r)

Q(x,y)/(y-r) => Q(x,r)

Q(x,y)/(y-x^2) => Q(x,x^2) = f(x)
Q(x,y) = poly * (y-x^2) + f(x)
f(x) = Q(x,y) - poly * (y-x^2)


Q(x,y) = sum y^i + x * sum y^i
y = x^2
p(z) = Q(z,s) = Q(z,t^2)

if Q(r,t) is a valid poly, then Q()


what if a computation just uses one element for input. the idea is we don't care how big the field is, just how many elements are in the inputs of verifying computations. won't work, cuz need a number of elements for proof meta data.
what if we don't include the inputs in the verifying proof input. but instead include just the sum poly of those inputs. this would make input grow at each level by 2 * length of proof meta data. i thought this would be exponential but growth is actually by addition, not multiplication. lets see, so suppose input length of original computations is n, and length of proof data for any computation is p. below lets list out the length of input for each computation from the original to the verifying without and then with using the sum.
without sum:
	0: n
	2: 2n + 2p
	4: 2(2n + 2p) + 2(p)
	8: 2(2(2n + 2p) + 2(p)) + 2(p)
with sum:
	0: n
	2: (n) + 2p
	4: (n + 2p) + 2p
	8: ((n + 2p) + 2p) + 2p
so without the sum its exponential, but with the sum its linear. in particular, with the sum the input length at the ith verification level is (n + 2i) at which point 2^i computations will have been verified. 
so this would be great, but how can we just include the sum instead of two inputs separately? suppose the inputs are A and B, the sum is C, and the proposed sum is C'. The verifier wants to know that A(r1) = v1, B(r2) = v2. This can be converted to a problem of A(r3) + B(r3) = C(r3) = v3

the idea is that the two computations being verified would already have constructed proofs, and then randomness of hashes would determine which two computations in the network of computations should be paired together. somehow, since the inputs have already been committed, they cannot be adapted for a malicious sum. for a moment i got worried that they new poly sum must be constructed and committed, but this wouldn't work because there would be no way to verify its the correct sum without viewing the individual polys. but luckily i think just the relevant merkel branches must be submitted and their values can be added. oh wait, no this doesn't work cuz merkel branches having values implies point wise commitment. pointwise commitment is hard for multivariate polys, but maybe possible on continuous field using short taylor series. if we use it we might as well use point wise commitment for individual proofs. and unlike for the univariate poly div technique, GKR might actually work with pointwise.

if point wise is possible, one way it might work is the two original polys are committed to point wise. Then even after the points of evaluation are known, a raw poly can be submitted to the verifier as the sum. The verifier checked that the point of evaluation correctly sums for the poly. But then i'm thinking maybe the two original committed polys could be verified to be in correct format, by randomly sampling (based on the hash of the committed raw poly) multiple points in each original committed poly, and making sure their sums also lie on the correctly formatted raw poly. ...

idea to yet consider is having original polys randomly summed with other original polys and committed to prior to any proofs. and then at the end of the proof its decided which sum to use. in this case there's no point wise commitment necessary. the points of evaluation are determined, then th...

so the original idea only makes sense if we don't use point wise commitment, but the whole raw sum poly is actually committed to. for the method to work, the sum must be committed to before the points of evaluation are known. so the raw sum poly is committed. then the proof is done. the danger is that it could be that A(r3) != v1 or B(r3) != v2 but C(r3) = A(r3) + B(r3) = v1 + v2. C is already committed to, so the value of C(r3) is fixed and can't be manipulated. At the end, the provers conjecture that A(r3) = v1 and B(r3) = v2. I think v1 and v2 are not able to be chosen by the provers. So the danger is A(r3) = v1' != v1 and B(r3) = v2' != v2 and v1' + v2' = v1 + v2. With r3 random and vi's un-manipulatable, what is the probability of this? so we know A + B != C. We can verify that C(r3) = v1 + v2. 

Let's get notation straight. 
Way 1:
	We can assume an actual and valid A and B exist but they are never manifested by the provers. Instead only C' is manifested, which is conjectured to be A + B = C
Way 2:
	We can assume C, which the prover committed, is the valid sum of two polys A and B. Since A and B are never manifested the verifier has the freedom to imagine them as anything. If the inputs used do not correspond to a pair of polys A' and B' such that A' + B' = C, then its viewed as though the provers committed an invalid C. 

One notable point is there is a large number of polys A and B such that their sum is C. In fact, A could be chosen to be arbitrary input and B would then be C - A. In other words, the proof cannot confirm that the outputs of the computations came from certain inputs. The provers could have near complete freedom in choosing inputs. In traditional applications this would not be acceptable because the verifier wants the computation done on a certain input otherwise it's useless. But in this particular case we don't care at all what the input is. As usual we don't care about the witness of the computation and most of the input. But now I'm taking this attitude a step further. Now we don't even care that a certain hash is contained in the input. Instead, I'm thinking either we're doing neural networks and any input is valid. Or, if the input data must be validated, then its hashed against, not a hash in the input that is checked by the verifier, but a hash that is hard coded into the logic of the computation. With this attitude we can use the notation of Way 2, where the question is whether there exist A and B such that A + B = C.
Suppose we do a blockchain. If we want state to be a merkel tree, then each computation could take the place of a vertex, and thus output a hash of its child vertices. Unfortunately we'd have the constraint that the merkle tree is binary. In all of this I'm imagining GKR, but maybe it would work with the division method. Suppose its GKR, and the single output is the hash or 0 in case of error. Danger would be if prover could force a non zero output when it should be 0 (ie there was error). This shouldn't be possible (on the other hand the converse is possible by finding input to the hash function that outputs 0). So soundness is not a problem, but on the other hand, the non-zero probability that the hash function outputs 0 reduces completeness below 1. so a merkle tree state seems doable. 

so let me confirm its possible to verify two proofs by just evaluating one when we don't care about input. raw poly S is committed. verifier proceed through the two proofs as usual until reaching the top. provers claim P1(r1) = v1 and P2(r2) = v2. verifier chooses line L passing through (r1,v1) and (r2,v2). prover passes univariate polys claimed H1 = P1(L) and H2 = P2(L). verifier finds a1 and a2 such that L(a1) = r1 and L(a2) = r2. verifier checks that H1(a1) = v1 and H2(a2) = v2. Then verifier picks random point t and computes r = L(t). Then verifier computes w1 = H1(t) and w2 = H2(t). Verifier must be convinced that P1(r) = w1 and P2(r) = w2. Verifier checks that S(r) = w1 + w2. Is this convincing enough?

---interlude---
how many ways are there to assign values to a and b such that a + b = c is the same? There are 2^n values for a, and for each, b = c - a is different. Thus there are 2^n ways. 
On the other hand, consider that w1 and w2 are random, yet C(r) is fixed. What is the probability w1 + w2 = C(r)? From above, there are 2^n ways this could happen. What is the total number of ways to assign to w1 and w2? w1 and w2 can have 2^n values, so total number of possibilities is 2^n*2^n = 2^{2n}. Thus the probability is 2^n / 2^{2n} = 2^{n-2n} = 2^{-n} = 1 / 2^n. 
Suppose w1 and w2 are not random but actually chosen by the provers. This is somewhat possible by sending H1 and H2 that have small image spaces. For the extreme case, suppose they know w1 + w2 = w. r, however, remains random because it is the output of a line. Thus what's the probability that C(r) = w? We know C can only cross w at most d = deg(C) times. Since r can take 2^n values, and at d of them C takes the value w, then probability C(r) = w is d/2^n. d corresponds to the input length, so the input length must be chosen to be 2^m such that n-m is sufficiently large for soundness, eg 32. 
it would be helpful to do analysis on the image sizes of H1 and H2, though the above analysis of d/2^n gives sufficient soundness already. Due to the maximum degree of the Hi, we may be able to obtain a lower bound on the image size.
---end---

Let P1 and P2 be the un-manifested polys that encode the input the provers actually used. Let w1 and w2 be the claimed values of Pi at r. Suppose P1(r) != w1 or P2(r) != w2. What is the probability that S(r) = w1 + w2? 
Keep in mind that S is arbitrary and need not be P1 + P2. 
Suppose S = P1 + P2
	Suppose P1(r) = w1' != w1 and P2(r) = w2. Then S(r) = w1' + w2 != w1 + w2. So probability S(r) = w1 + w2 is zero.
		This can be reduced to probability w1' = S(r) - w2 = S(r) - P2(2) = P1(r)
	Similar if P1(r) = w1 and P2(r) = w2' != w2.
	Suppose P1(r) = w1' != w1 and P2(r) = w2' != w2. Then S(r) = w1' + w2'. So what is probability w1' + w2' = w1 + w2? Recall that w1 + w2 is random. Due to possible reduced images sizes of H1 and H2, suppose w1 + w2 can take m possible values. w1' + w2' is the output of the polynomial P1 + P2 at the random point r. So the question becomes, what is the probability that P1 + P2 take the value w1 + w2 on r? P1 + P2 can only take the value w1 + w2 at d = deg(P1 + P2) places, yet the size of the domain is 2^n. So the probability is d / 2^n.
Suppose S != P1 + P2
	S can take the value w1 + w2 at d = deg(S) places, yet the domain size is 2^n. So for random r, the probability S(r) = w1 + w2 is d / 2^n.
	It appears the actual values of P1(r) and P2(r) are irrelevant. 
In both cases above it appears the image sizes of H1 and H2 are irrelevant because even if their sum maps to a prover chosen element k, the probability S(r) = k is still d / 2^n. 

It occurred to me summing over more than two inputs may be possible too with reasonable soundness, decreasing composition depth, and giving more flexibility to the merkle tree. 


Now assuming proofs can verified as above when the input is arbitrary, we now investigate what computations are possible for arbitrary input. We want to validate the data by hashing. 

Constants:
The code needs to support constants. The constants cannot cascade from the input because the input is unknown. Maybe a piecewise input is possible. 
Another possibility is that somehow a constant such as 1 is generated by the input. Then its added and multiplied by functions of itself to generate a set of constant basis values, which then can generate any other constant. This is hard and costly.
Actually counter to my intuition, its easy to combine two interpolations, one X for input the other Y for the constants, into one interpolated function Z for the entire input such that Z(r) can be obtained from X(r) and Y(r) and a little arithmetic. There might exist a way to do this for univariate polys too. Will this work? so Z(x) = func1(C(x)) + func2(P(x)), so the verifier has a claimed value v of Z(r) to verify. Verifier can evaluate func1(C(r)) and then it remains to check that func2(P(r)) = v - func1(C(r)). func2 is such that this reduces to a claim about P(r). In other words, Z(r) is as claimed if P(r) takes a certain related value computed by the verifier. So this should work. 


uh oh, might have a problem that poses constraints on computation. when RAM model is used a transcript is necessary, and this transcript is considered part of the input. For the original computations this is not a problem, but a verifying computation that uses RAM will need its own transcript in addition to the proof data inputs. so now suppose n is regular input length, t is transcript length, and p is proof data length for one computation. then total input length grows as follows:
	0: n + t
	2: ((n + t) + 2p) + t
	4: (((n + t) + 2p) + t + 2p) + t
	8: ((((n + t) + 2p) + t + 2p) + t + 2p) + t
	16: (((((n + t) + 2p) + t + 2p) + t + 2p) + t + 2p) + t
that is, if d is the depth such that 2^d = total computations, then the input length at the dth level is (n + 2dp + (d+1)t)
So since transcript (t) are long they would significantly add to the length but they could possibly still be supported. 


What happens to soundness as we increase the number of inputs that are summed?
Suppose inputs are P1,...,Pn, and arbitrary S is committed. wi are the claimed values of Pi(r)
Suppose S = \sum Pi
	Suppose m of the n Pi are such that Pi(r) = wi' != wi, while the rest are Pi(r) = wi. All orders should have same probability.
	What is the probability \sum_1^n wi = S(r)?
	Well S(r) = \sum_1^m Pi(r) + \sum_{m+1}^n Pi(r) = \sum_1^m wi' + \sum_{m+1}^n wi
	Then to what is the probability \sum_1^m wi' + \sum_{m+1}^n wi = \sum_1^n wi
	Then to what is the probability \sum_1^m wi' = \sum_1^m wi, keeping in mind wi' != wi
	Regard the right side as the random element w. How many ways can \sum_1^m wi' sum to w?
	We must subtract 1 from the answer due to the exclusion of the possibility wi' = wi.
	w1' can take 2^n values. Then the problem reduces to how many ways can \sum_2^m = w - w1'. 
	Applying this reasoning recursively, it appears that wi for 1,..,m-1 can each take 2^n values, but wm must take value, thus there are 2^{n(m-1)} satisfying assignments. Excluding one we get 2^{n(m-1)} - 1
	In total there are 2^{nm} assignments. For simplicity forget the subtracted 1. Thus the probability is less than 2^{n(m-1)} / 2^{nm} = 1 / 2^{nm - n(m-1)} = 1 / 2^{n(m - (m-1))} = 1 / 2^n
Suppose S != \sum Pi
	S can take the value \sum wi at d = deg(S) locations, yet the domain has size 2^n. So probability is d /2^n independent of values of Pi(r).

This would imply the shocking result that you can sum as many inputs as you like with no change in soundness. One caveat is that the polys must be evaluated on the same point, and we need a way to reduce evaluation of all polys to one, and that could mean an really large plane, probably even bounded by the low dimension of the space. Maybe its still possible recursively using lines and such. Of course the more you sum the more proof data you must also take. The state tree could be way more flexible this way if proofs could verify inputs of different length, which would work if soundness holds when the degrees of the input polys are different from each other. 


Computation type
computations will definitely include hashing and evaluating polys and doing other verifier tasks, which can probably all be amenable to the arithmetic circuit model. But the model of the blockchain also matters, so we should think of that before settling on a certain computation model. The blockchain should be a chain of state, each one a merkel tree data structure. We want to use the state transition approach. So every new state uses arbitrary data from anywhere in the previous state to compute the new state. For now, and hopefully forever, I'm not considering pipelining, but assuming a single chain. The only pipelining option involves proving the use of state data that has not yet reached the head, and then proving later that the data used eventually made it to the head. But the state transition model is problematic when a slice of state is non-existent, which may occur if a slice of the tree doesn't get verified or processed during a round. So different from like redux is the fact that multiple actions occur in each round, not just one. With this, maintaining consistency could be challenging and when one slice depends on another, it cannot update if the other is unavailable. This is the data availability problem. State transition applications like redux have each part of state responsible for a different purpose. Now I'm thinking that each part of state could be identical in purpose for two reasons. The first is maybe it can be designed such that if one slice becomes unavailable it only affects that slice and does not harm other slices, while at the same time slices can cross reference data when it's available. The second is that identical slices mean identical transition functions, such that circuits can be smaller because instead of evaluating the code functions of children most of the code functions will be the same an evaluations on them can be amortized. With this model, perhaps a single application can have multiple chains, each serving a different purpose and they all operate in sync, and one chain can definitely use data from other chains. For now I will research with this model in mind.


i like the constraints model, and it seems like functional programming is good for it

Enforcing Constraints:
how to enforce constraints in an arithmetic circuit? make sure all equations evaluate to 0 given inputs from transcript. Do this by evaluating each equation and then combine the results such that the result indicates whether any of the equations did not evaluate to zero.

Utility to use:
for element a, a*a = a only if a is 0 or 1. Proof: this clearly holds for a = 0 and 1. For a = b != 0 or 1. Then b^{-1} exists and thus a*a = a => b*b = b => b = 1. This is a contradiction. 

One possibility for the expression is a product, one factor for each ei, and any ei that is not zero causes its factor to become zero, reducing the entire product to 0. This could be done with an expression of ei that evaluates to 1 on ei = 0, and to 0 on ei != 0. Using the above utility we can first enforce that ei is either 0 or 1. With this enforced, we can then use \prod (1 - ei) such that we get 1 if and only if all ei = 0. So how to enforce that all ei are either 0 or 1? Using the theorem below we actually don't need the utility above. So we evaluate the product \prod (1 - (ei*ei + ei)) and if we get 0 then some ei is neither 0 nor 1. Now since both products fail if 0, we can multiply the products themselves so we only have to evaluate the product \prod (1 - (ei*ei + ei))(1 - ei) = \prod (ei^3 - 2ei + 1). But 2ei = ei + ei = 0 for any element ei. Thus we're left with \prod (ei^3 + 1)... this doesn't work for 1 because of the -2 i'm a little lost but overall the technique should still work.

Theorem
The map ei*ei + ei maps to 0 if and only if ei is 0 or 1. Proof: 
ei*ei + ei = 0 => ei(ei + 1) = 0 => ei = 0 or ei + 1 = 0 => ei = 1.
I guess a better way to show this would be through the roots of the poly (ei - 0)(ei - 1)

NEURAL NETWORKS:

don't forget about neural networks and even trying them on finite fields
it would be like \sigmoid(\sum_k P_{i,k}*W_{i,k,j}) = P_{i+1,j}
the proof is correct is this equation holds for all i (rows) and j(columns).

One possible way is to check that \sigmoid(\sum_k P_{i,k}*W_{i,k,j}) - P_{i+1,j} = 0, by checking that the univariate polynomial
\sum_i \sum_j (\sigmoid(\sum_k P_{i,k}*W_{i,k,j}) - P_{i+1,j}) t^{index(i,j)} 
is the zero polynomial. This would be done verifying that the value of the poly at random t is 0, with evaluation via sumcheck. Oops, unfortunately the integrand must be a single expression, so either P and W are single polynomials encoding all rows and weights, or a separate sumcheck is done for each i. The latter may be doable, and each P would be appear in two sumchecks (as source and sink) so the two poitns of evaluation would be reduced to one, and hopefully this could ensure the two Ps are identical. A bad part is I think each P must be committed separately. The former would possibly not limit the number of rows, but of course P (and W) would be enormous. 

Can we use GKR? GKR seems good because the intermediate polys need not be evaluated directly, and the logic is contained in the m (row number) weight polys which are the same for all computations. This way of a common logic poly (the weights), though n^2 in length, makes me think it might be useful for the discrete case of none neural networks, such that it can express arbitrary logic (but with the redundancy of hashes and such we won't need arbitrary logic). 
So for GKR, check each row at a time, that is treat i as constant. GKR goes by evaluating P_{i+1,j} at a random point, so let j be random. So the prover claims a value for P_{i+1,j} and the verifier evaluates the left side of the equation to confirm. To do so, they engage in the sumcheck over the inner sum, and end with the verifier needing to evaluate P_{i,k}*W_{i,j,k} at a random point k. The verifier does this by evaluating W and then moving on to the next iteration of evaluating P_{i,k}. The verifier passes the value of P*W through the sigmoid and confirms it equals P_{i,j}. In the last iteration, P_{0,j} (the input) must be evaluated at random j.
Does this work? I'm a little worried because if I actually use the sigmoid the right and left sides of the equation are not equivalent expressions, because one is a polynomial and one isn't. So I guess we can't use the GKR reasoning that we can evaluate the right at a random point by evaluating the left at a random point. The only way to do this would be to make both sides equal, so somehow use an activation function encoded by a low degree multivariate polynomial, and then demanding that the prover encode the right side using this extension.

END NEURAL NETWORKS


think about remote storage, 

why not just have a transcript with all variables used in the computation. have static code with everything including conditionals represented by static variables. The constraints will have no pattern, so the logic is not redundant. This method avoid use of main memory.
one thing important in this model is that we don't care how large the code functions are because there will only be a few of them for each chain, and they're points of evaluation can be reduced to one. inputs lengths, on the other hand, are different for every computation so they build up and we want to minimize them. so counter to existing systems, we don't care if the code functions V must verify cost more overhead than evaluating the C directly. 
On the opposite side, suppose we don't have registers but only main memory. State then consists of a program counter and maybe a flat. So maybe the trace could be just of instructions, each consisting of op code, the two addresses of the operands, and the address of the result. It may be possible that each such instruction could be represented with a single element. Now I understand why they have registers: in order to verify arithmetic operations the values operated on must be present as in registers, whereas they are not in main memory. Also, having two operands from memory is challenging because memory checking means checking both sources. Whereas if instructions only allow one load at a time, that single source can be checked.

...So instead if we just do main memory instructions would have to specify not only the value obtained ... think about what it would have to specify, like input addresses, output address, and output value, or same but just input values. Cexe i'm thinking would just look at op codes and make sure each one is right. for conditional jumps it would look at input values (or value). if not both input and output values are specified, Cmem might do the arithmetic checking. eg if only output is specified, Cmem would look at the two previous stores of the inputs and take those output values as the inputs and check the operation yields the purported output. on the other hand, if only inputs are specified (longer transcript than with just outputs specified) then Cmem would compute the operation on them and make sure the result was loaded by all loads before the next write. I prefer the first method, though Cexe suffers with unknown inputs for conditional jumps, but I suppose the previous instruction's output could be expected to be the result of the condition, and Cexe could just check that. Keep in mind this method requires replicating the elements of the transcript, because each must appear twice in the memory sorted list. This list is sorted by write-to memory, and each element appears after the previous write-to of each of its two inputs. We need to impose the condition that every load not only loads the last stored, but that the value stored is indeed the LAST value stored. Suppose the sort checking forces every element to appear after two writes. of course Cmem checks that the destination address of the write is the source address of the read. then the only error possible is that an element appears after the wrong write. I think this can be solved by examining op codes... more to think through, and i think to handle op codes in the presence of conditional jumps, having jmp instructions preceded include the element of the conditional value would be helpful....
... i like this idea, more to investigate, but i should learn more first

the input could include a separate poly for the program logic, that is a vector of elements, each representing one instruction in the order of the code. this would be in addition to the transcript where the instructions are specified again, each perhaps multiple times. its point of evaluation would be reduced. this could make Cexe's job easy, just looking at pairs in the transcript (from now on i'll call it trace) and making sure the two instructions are either ordered next to each other in the code, or the conditional jump has the appropriate conditional value and jumps to the correct location. Again, it would be helpful for conditional jumps to specify the conditional value, because examining a pair of a jump followed by the next instruction, determining whether that next instruction is appropriate requires looking at the conditional value. 


a whole other technique would use constraints and the vanishing polynomial problem. the divider (the roots poly) would have arbitrary roots depending on the arbitrary constraints from the arbitrary code, but it would be the same for every computation so evaluating it could be reduced to one point (first converting it from univariate to multivariate). The transcript V and division H could be summed I suppose, converted to multivariate, then evaluated (already at the same point). Verifying two different operations, the polys would be summed but they would have to be converted to multivariate in order to evaluate them at the same point. So the constraints satisfaction, as well as the circuit satisfiability approach is an option. 


random comments:
We could have each item in the transcript specified with two elements, the instruction and the write value. the instruction would be divided into 4 parts, an op code (4 bits, giving 16 instructions), two source addresses (20 bits), and a destination address (20 bits). Thats a total of 64 bits, and the address space is exactly 1MB in size. 
Luckily the idea of only main memory (no registers) goes well with flatlang. 
Code should be flat, no nesting, so conditional jumps are "if <cond> go to <line num>". 
i think not pipelining is not a problem, because after each round the users must organize themselves and obtain the right data from the new state, and this could take just as long if not longer than the depth of the network times the time of a computation. after the first layer is computed, the network could already begin to seek the data for the next computation, though they should verify that data is indeed included in the new state before computing. 


We know there are advantages, like sumcheck or reducing two points to one, to evaluating the same circuit of many different inputs. What if we try this for C_mem and C_exe? 


It would be great if two separate users could engage in a protocol to obtain the sum of their inputs (to order to commit to it) without revealing to each other their inputs. This is not possible because if a + b = c, and a user knows a and c, the user can obtain b. But this might work if we use the sum of 3 inputs because knowing a and d in a + b + c = d does not reveal b or c. But unfortunately if two of the three collaborate and share their inputs, they can obtain the input of the 3rd user. 


Important theorem I did not know: the multilinear extension of a function with domain {0,1}^* is unique.

the majority of a verifying circuit's input is the summed input of its children. The only computation it does on this, however, is evaluate it at a random point and this is easily tailored to an arithmetic circuit. if hashes are also friendly to arithmetic circuits, then all verifying logic is, and so verification proofs would then require no trace. but if verifying computations perform any logic, eg analyzing results of child computations, they would require one. I'm imaging the recursive structure. 


OPERATIONS ON F
keep in mind, verifying is sometimes easier than computing. 
NOT: to verify that b is the NOT of a, check that b + a = 1^n or b + a + 1^n = 0, because b XOR a should equal the bit string with all 1s.
squaring an element abc... produces a0b0c0... this may be helpful. for example, you can isolate the first bit this way by repeatedly squaring.
shifting: If the left bit is zero then shifting left can be done by multiplying by z. I think when the right bit is zero, likewise, shifting right can be done by multiplying by 1/z. apparently shift correspond to affine functions. 


i think using bit operations to simulate a regular machine is not worth it at all. i think a programming model tailored to the field should be used. the only reason we need bit operations is for hash functions. 


operatins on quasi string paper


worried now that hashing requires witnesses. and the bad news is hashing must be used many times by a verifying proof, at least once for each layer of each proof being verified. 
lets think about the input to a verifying computation. of course the summed input of its children. then, for each proof it verifies, what does it need? 

to reduce input we'd like each layer of the tree to have the same code, and even better for all verifications to have the same code. 


hmm, going from log2 to log3, for say a billion computations, decreases depth from like 30 to 19, and it looks like this would still be worth it even with increased input sizes. going to log4 may even be worth it too, because 2*log2 = 4*log4, but log5 is not worth it.

apparently cryptographic primitives such as encryption and hash functions have been studied for neural networks. so if I can figure out to prove neural networks, recursion could be used. i'm thinking given the nonlinear sigmoid, it may be helpful to try functions other than polynomials. To avoid blow-up, maybe something like trigonometric polynomials would be good. I could also explore using complex rather than real numbers, which could still be specified via a single number as the exponent of the complex exponential. Its also worth exploring the difference in applications for arithmetic computations versus neural networks. Doing RNNs I don't know how to do yet. 

It may be worth sondering alternative models than just a straight circuit, eg a circular circuit. One problem we want to solve is avoiding placing intermediate results in the witness, which is turn is done to avoid deep circuits. RNNs, and the intermediate results could be solved if a circuit layer was able to refer to its input as from a layer further down in the circuit, though not any child of it. Or perhaps even just referring to its input as from a different location on the same layer. The immediate challenge with this in GKR is that when evaluating a layer poly at a random point, at the end of the sumcheck you must evaluate it at yet another random point. 
what if at each step of the sumcheck, a random affine function (assume surjective) line was chosen instead of a random value for each variable. Then bivariate, instead of univariate polys would be returned. At the end, hopefully the verifier would have verified the poly evaluated on a random line. The line would have to pass through the initial point of evaluation. In other words, at each step instead of choosing a random value r', the verifier chooses random c1 and c2 and then sends the random value x solved via c1i*xi + c2i = ri, where ri is the original random ith variable of evaluation. since c1i and c2i are random, so is xi. So actually the verifier doesn't send an affine function but just a derived random value. In the beginning the verifier wanted to know that P(r) = v. At the end, the verifier is assured of this as long as P(x) = v'. Although a line connects them, P would have to be evaluated again, so this doesn't work. No matter how we try, at the end of the sumcheck or the end of evaluating over a line, P must be evaluated at a NEW random point, and if this point lies on a line or something already verified than it is not random. I don't think its possible to interpolate a poly in terms of itself anyway, and as we know the two sides of the equal must be identically equal, not just on the interpolation points. Actually it doesn't have to be. Instead just previous points of interpolation would be used to derive new points of interpolation. So the poly still can't be expressed in terms of itself. Actually I'm wrong again, yes it can, just like in GKR where the function in the body however, is applied to constants (and is thus a constant), not to the variables. 

Hmm, one possible thing we could do is have wires skip layers, that is have any DAG circuit. A layer would point to its sources as any of the layers above, and when a given layer is reached, the necessary points of evaluation from all layers below would be reduced to a single point. 

The fact that the intermediate values do not present new information makes me think there must be a way to more efficiently handle them. 
This should be my focus now, because its my current bottleneck. inputs would be so small, and not even transcript would be necessary! And the benefits would apply to both circuits and neural networks.
Think about complex multivalued functions. You can return to the same point and the function can take a different value. 

hmm, here's an idea. suppose the sum of two consecutive layers is committed. you evaluate the 


and we need a lot of hashes, eg each of the n coefficients of the committed poly, and I think actually 2^n-1 hashes because the only way to obtain a unique single value from them is via a merkle tree. I think must be hashed. if m witness elements are required for each hash, eg each of the n coefficients of the summed input, then the new input will require proof data as before but now with an additional m*n witness elements. if m != 0, then the input length will grow exponentially which is unacceptable. thus avoiding hash witness elements, and hopefully all witness elements, is mandatory. 
the division method also has the problem that the transcript must include hash witnesses for the commitment of the summed poly. 

i see now how the right side equals the left side in GKR. first of all, even if interpolation is not unique, the verifier can demand that the prover choose the appropriate interpolation. second, using \beta, is only \beta that accepts the function's input. and beta is of correct degree.
let each layer's poly be composed of two. the first is expressed in terms of any layer polys above it. the second is an arbitrary poly of appropriate (the same) degree, chosen by the prover, which is supposed to encode values copied from polys below it (there is no use is copying its own values which it could have just computed). First, the code of the computation specifies which ranges of which polys should be copied to which ranges of which polys above it. For each of these ranges (perhaps multiple of them at a time if they are the same length), the sumcheck is performed over a univariate poly where the coefficients are the difference of the two relevant polys evaluated at the points the points that should be copied. The polynomial should be identically zero. At the end, the verifier must evaluate each poly at a random point. Finally the regular GKR takes place, but at each layer before evaluating at a random point, the random point of evaluation is reduced together with any points from the sumchecks for copying to a new single random point. What does all this mean? It means proof data will now include extra sumchecks, as well as extra instances of reducing points (ie a lot of univariate polys of length around 30). Of course this all means a lot more hashing. But the benefit is that the input need only contain the actual information, not any copied values such as intermediate values. 

but when you evaluate the copy-poly of each layer poly, you don't want to evaluate it as an arbitrary poly. at worst I suppose, the prover could commit the sum of all copy-polys and at the end the verifier could evaluate that sum at a random point, but this requires reducing more than two polys to the same point which I don't yet know how to do. so better if the copy-polys are expressed in terms of other polys, even if just those below them they copy from. But of course this is has the problem that it requires evaluating the constituent polys (in this case polys below them), which results in the recursive problem.


hmm, i think i know how to recursively reduce polys to evaluation on the same point. you start with pairs, and for each you construct a link connecting the two points, get the two univariate polys, check them, then choose a random point on the line and the result is the new common point. then with two pairs of two polys, you can do the same. you simply construct a link connecting the two points, get 4 univariate polys, check them all, then choose a random point on the line. You can continue this process for arbitrary numbers of polys on either side, as long as all polys on a given side are being evaluated at the same point. before i was stuck not thinking straight but thinking the line had something to do with the purported value of the polys, so it could only be done with two polys. 

just have one sum of all inputs. then all inputs must be reduced to evaluation at the same point. at each vertex have the sum of two child sums. each sum is a sum of all children below it. at the time of proof, reduce evaluating the two child sums to a single point. maybe a merkle tree of the sums should be made, but this merkle tree doesn't need to be circuit friendly. the sums are just of the original inputs themselves, no intermediate proof data. but i guess at each merkle tree vertex should include a third hash containing the sum. first verifier reasons is children are correct if their sum is evaluated at a certain point. the prover of this verifying computation should check to make sure this is indeed the case, by looking at the sum of those inputs he previously committed. then second verifies its child verifications proofs, and reasons similarly about reducing points to one. At the top, the top proof outputs that a single point at which the root committed sum must be evaluated. we rely on every vertex of the proof tree having more nodes on it taking responsibility. if at any vertex the nodes realize either of their children proofs are wrong in any way, including the possibility their proposed point of evaluation does not match the sum they committed, they are blamed, their proof is cited to be wrong, and to compensate for their input already committed the provers compute some correct value for the committed poly though upon further thought I may find this is an opportunity for attack. maybe when a group finds the verifying proof did not include a child proof they should double check to make sure its not exclusion. 
maybe we could take this concept further, where for each layer of the tree, all inputs at that layer (which are all computed at the same time) are committed into a sum that is passed up the tree. this way the layers above it need not include them as input, but just calculate their points of evaluation, reduce them to one, and claim a value for the sum evaluated at that point. all sums above it pre-committed. at the top the entire network will have to evaluate the sums of polys, one for each layer. maybe somehow there could only be one poly, but committing the sums of polys from different layers.
the basic condition for the summing idea is that the sum is committed before the points of evaluation are determined, which means before the verifying proof is computed. so at each layer, before the computations are verified, their inputs are summed and committed. 
of course using this method instead of the additive growth puts more pressure on network organization to keep nodes accountable. for example, it might be that the entire network can't verify a block because one of the groups doesn't respond with a point of evaluation, preventing the network from obtaining a valid evaluation point for the entire committed sum.
but if we can do this then each verification computation doesn't need to include any inputs of its children (or all the hash witnesses required for verifying the commitment), only the proof data (with its commitments), which means a witness may be acceptable. 


think about copying again. notice that though the witness would not be needed for intermediate computed value, it would be needed for actual 'witness' values that are easier to verify than compute, eg to verify the NOT of a bit, the element must be found such that it has bits in opposite positions, and this can be easily verified by XORing the two together and seeing the result is the all-1 string. oh wait, haha, the NOT is actually easy to compute, just add the element with the all-1 string. but still, witnesses may be helpful in some contexts, even when copying is possible. 

take a poly, have it divided into two and have the sum committed. when evaluating it at random point, prover provides two values, one for each poly, and the verifier adds the result together to get total claimed output. verifier connects the two points of evaluation, reduces evaluating those two polys at two different points to a single point. Then verifier evaluates sum at this single point and compares result with outputs of univariate polys. What if done recursively? that is, at the very beginning the prover commits not only the sum, but also divides the sum into two polys and commits the sum of those, and divides that sum, etc. since verifier doesn't actually verify any of the sums but the last one, a small poly, maybe only that small poly (maybe even a constant) need be committed. 
if this works, maybe its no use copying because not only will it incur costs from the extra sumchecks, but any data copied might as well go in the witness now. so we could have enormous witnesses, but luckily they don't grow on each other except i guess logarithmically. If it works, the commitment would be a single element, the final sum. So assume we do regular GKR with giant witnesses. The input to a verification would be the regular proof data, along with the univariates for the evaluation of the input. When verifying multiple, I suppose the sum should be of all the inputs, though it would mean more data for reducing the respective evaluation points to one. 


1 round on degree 2 (2 degree 2 univariates)
1 round on degree 1 (2 degree 1 univariates)
1 round on degree 0 (2 degree 0 univariates)

(n-1) + (n-2) + ... + (n - n) = n * n - (1 + 2 + ... + n)

oh shit what? too good to be true, due to the format of the multivariate poly, the evaluation points of the left and right sides are already the same.


((a1*x + a2)y + (a3*x + a4))z + ((b1*x + b2)y + (b3*x + b4))

want to evaluate the below at x,y = a,b
(a1*x + a2)y + (a3*x + a4)
P claims left at a is v1 right is v2
V computes v1*b + v2 to obtain the answer
V is left to verify P's claims are correct
V computes the committed sum at a: (a1 + a3)a + (a2 + a4)
V checks the result is equal to v1 + v2
One evident problem is P cannot know 'a' when reporting v1 and v2 or else P will know what V will test against v1 + v2 and thus pick any pair of v1 and v2 such that their sum is equal. So can P report v1 and v2 before knowing 'a', no. the problem is in order to report the evaluation values, P must know the point of evaluation. so we want P to have to report v1 and v2 independent of 'a'. 
we might be able to overcome this by evaluating over a line, even though P must know the area of evaluation (a line) before reporting the values (by a univariate). Suppose the univariates, due to their bounded degree, map to at least H different values in the field, and assume uniformly. Also suppose the line maps to L different values....


problem: prover take line its given, evaluates the sum it committed at that point, obtains a univariate poly, then splits the poly into two in whatever way it pleases, and no matter what point on the line the verifier chooses, the sum of the split polys will match the summed poly at that point. my analysis went wrong assuming the random point of evaluation of the sum was determined independently of the claimed values, but in fact both are dependent on the line chosen. 
so i suppose the nice hopes are gone and what I have gained in the meantime is that even if they were true it's still difficult to handle computations with hashing because they have so many intermediate states their would either need to be a large witness or a way to copy values inside the circuit, and both are difficult to handle. 
i must return to the original inspiration. That is, have proofs fully done where the only question remains is whether the input evaluated at a particular point equals a particular value. Then, randomly pair proofs together and verify the sum of their committed inputs is the sum of their committed value. But this either requires evaluating both committed inputs, or evaluating their committed sum which must declared prior to the chosen point which allows adaptiveness by the prover. 
so now we're back to where we started. 


the only related possibility to summing I see now is that which inputs are summed together is determined after the univariates are returned, but of course those sums must be already committed to. the only way I know to achieve this is through pre-committing the sum of a poly with a number of others prior to any proofs. but a prover could just bet on a particular sum being chosen, use that sum, and the probability they would succeed is 1 / (number of committed sums). So for sufficient soundness it would require committing to nearly every input vector. and this would require some public algebraic commitment scheme if its possible, which is doubtful. 


what if we did taylor series for the multivariate input of GKR. in the poly div problem, taylor encoding didn't work because although it limited the degree, it did not enforce a valid poly form. since the problem was about comparing polys, bad forms could pass. but GKR is not about comparing polys, so maybe form is not important and only degree patters or not even that. at then end the random point of evaluation is chosen via a line and the expected output is chosen via a univariate. Like before, the chance for manipulation is to return a special univariate dependent on the line chosen. especially since the univariate will have a large image size, i think even degree is irrelevant and pointwise commitment is fine, but I need to investigate further. Anyway, multivariate point wise commitment is hard.
maybe it could be encoded with sets of linear taylor series (so no need for a derivative), but linear may not be enough and the concept of approximation with lack of formal notion of magnitude is challenging.


i realize even if it had worked, there would still be significant network latency passing around new data, but the benefit is most traffic would remain in the same region. Whereas with treechains, the same amount of data must be passed around but it would be passed across oceans. 
comparing about privacy, at first it seems proof chains would be great because the first layer of computation would be private. But then how are results combined? Either those private results are passed further up the tree with increasing public visibility, or computations cross reference private data from other computations. treechains doesn't let data be private. so whether proof of tree chains, how should privacy work if its to scale to the size and applicability I want? 


i think hashing may be applicable for a commitment in the form where you hash each of the coefficients, and then add them together, instead of creating a merkle tree. 


now i'm reconsidering what i had before, because i forgot that upon receiving the univariates, the verifier doesn't just evaluate them at a random point, but first checks that at the certain point they evaluate to the previously claimed values. Satisfying these evaluations may make it unlikely for the prover to find a valid splitting of the sum, in which case the prover must choose two univariates that don't sum to the committed sum.


G = L*x + R
G(r) = L(rb)*rt + R(rb)
P claims L(rb) = v1, R(rb) = v2
V verifies G(r) = v1*rt + v2
V chooses line U = a*t + b passing through rb, that is a*t' + b = rb
P sends Hl = L(U), Hr = R(U)
V checks that Hl(t') = v1 and Hr(t') = v2
V picks random t, rt, and compares S(a*rt + b) to Hl(rt) + Hr(rt)


P computes S(a*t + b) and splits it into two polynomials Hl and Hr
So what is the probability that S(a*t + b) can be split into two univariates such that Hl(t') = v1, Hr(t') = v2 ?
We know that Hl(t') + Hr(t') = S(t') since the two sides are identical.
We know that t' is random with respect to rb due to the randomness of the line. 
As a result, t' is random with respect to v1 and v2, thus the probability S(t') = v1 + v2 is the probability S take the value v1 and v2 at an independently random point. 


Take two cases:
Hl + Hr = S
	If this is the case, the prover will succeed. the question is what is the probability of this happening?
Hl + Hr != S
	If this is the case, what is the probability the prover will succeed?
	Suppose Hl(t') = v1 and Hr(t') = v2



Proof:
Assume G is an m-variate linear polynomial with the special encoding. G(y) = L(x) * z + R(x). 
P first commits to S(x) = L(x) + R(x)
V chooses random point rv and wants to verify that G(rv) = v. Let r' be the first variable of rv, and r be the remaining variables. 
The proof reduces to L(r) * r' + R(r) = v.
P claims L(r) = vl, R(r) = vr
V verifies v2 * r' + v2 = v
Proof reduces to L(r) = vl, R(r) = vr
(r is random, but vl and vr are chosen by P after knowing r and thus are not independent of r)
V chooses random line q = s1*t + s2, passing through r. There are v * 2 variables to choose. m + 1 of them can be independently random, the rest are dependent upon them.
Depending on the choices, there will exist a unique random t' (dependent on the line) such that q(t') = r
V computes t'
(due to the randomness of the line, t' is only dependent on r, and independent of vl and vr because it was chosen after P sent them)
(if V had valid form of L(q) and R(q), V could just evaluate them at t' and compare the results to vl and vr)
Prover sends Hl and Hr supposedly L(q) and R(q)
(note that Hl and Hr are dependent on q)
Proof reduces to L(q) = Hl, R(q) = Hr
V checks that Hl(t') = vl and Hr(t') = vr
V picks random tr and computes q(tr) = qr
V compares S(qr) to Hl(tr) + Hr(tr)



Suppose G(rv) != v. Then L(r) * r' + R(r) != v. Prover must find L' and R' such that L'(r) * r' + R'(r) = v and L' + R' = S. Suppose the prover picks L' and R' such that L'(r) = lr and R'(r) = rr. Then its must be that the two following equations are satisfied.
lr * r' + rr = v
lr + rr = S(r)

rr = S(r) - lr
lr * r' + S(r) - lr = v
lr * (r' - 1) = v - S(r)

lr = (v - S(r)) / (r' - 1)
rr = (r' * S(r) - v) / (r' - 1)

top equation:
(v - S(r)) * r' + (r' * S(r) - v) = (r' - 1) * v
=> v * r' - S(r) * r' + r' * S(r) - v = (r' - 1) * v
=> v * (r' - 1) = (r' - 1) * v
=> 1 = 1

bottom equation:
(v - S(r)) + (r' * S(r) - v) = (r' - 1) * S(r)
=> S(r) * (r' - 1) = (r' - 1) * S(r)
=> 1 = 1

so the prover can easily solve the set of equations and set L'(r) and R'(r) accordingly. Then the prover can choose any appropriate degree poly L that passes through lr and then obtain R by subtracting L from S. One possible defense is that the prover must report the value of L(r) and R(r) before r' is chosen by the prover. But then the prover can't claim v. So the prover could just obtain L and R as just described with much flexibility over choice, and then proceed honestly. So this method wouldn't work.
But wait, v is not up to the prover. It is implicitly demanded a certain value by the sumcheck. Instead, v is obtained by the verifier by evaluating the last univariate at a random point. Actually this univariate will be affine. Call it g(x). Then it must be that g(x) = vl * x + vr = L(r) * x + R(r). This means vl and vr are already specified by the prover. For a moment, suppose the prover has no flexibility in choosing vl or vr. ... Then I suppose the verifier can just compare S(r) to vl + vr....


But would it work if we were summing two different polys?
prover claims that P1(r1) = v1, P2(r2) = v2
q is line passing through r1 and r2, points of evaluation. q(t1) = r1 and q(t2) = r2 
prover evaluates S(q) and splits it into H1 and H2, that is such that H1 + H2 = S
but it must be that H1(t1) = v1, H2(t2) = v2
thus the prover must choose H1 and H2 such that
H2(t1) = S(t1) - v1
H1(t2) = S(t2) - v2
thus each poly just has two points of interpolation, and otherwise they must just sum to S. this is entirely possible for P to do.

so the idea that this might work again after remembering that V checks the univariates upon receiving them, doesn't seem to be worth anything. 


could the process of challenging not be a separate process but instead be part of consensus?
	would this be implemented by validators only being considered valid via proof of deposit?
		would entail that anyone could be the validator of a task, and all adversaries could collude on it?
			would the consequence be that adversaries can easily submit corrupted tasks instead of waiting until a task is included in the chain and the subsequently challenging it?
			would it be better that challenging occur subsequent to initial implementation, and occur as a separate process?
are cells in a special state when they are being disputed?
	can the side process in fact continue execution of the cell and propose the resulting value in its dispute?
	can we suppose the cell was corrupted, and the network would like to fix it?
		should the cell be suspended before it contaminates the network with corruption through its side effects.
	can we suppose the cell was not corrupted, and the network would like to keep the existing answer?
		should the cell be suspended?
		can the dispute involve calculating state so the suspended cell is in effect not suspended?
	can we suppose the cell was not corrupted, and the network wants to force another answer?
		should the cell be suspended?
		will the cells final state be determined by an assertion of the community?
	does this imply cells should be suspended during a dispute?
do disputes arise when a node sends a challenge to a special cell?
	does disputeCell (the cell handling disputes) emit a signal that a dispute has started?
		is it only after this that the cell should be suspended?
			could a cell be suspended by in fact being killed (destroyed but from the outside)?
could disputes occur not only in a separate process but in a separate network?
	does this raise the topic of availability and out of network dependencies?

are pushed messages validated?
are pushed messaged only considered valid upon the completion of the tree containing the cell that pushed?
	
should internal cell state be designed such that it is easy to encode assertions about it that fit nicely in a merkel proof?
	will one common assertion be the address of a message's sender?

can validators compensation be in the form of funding cell evaluation?
	can this occur by cells holding the public keys of the validators that fund their evaluation?
		would the validator need to map the public key to some proof of task completion?
			is this near impossible?
	can this occur by the validators sending the cells a special merkel proof of their task completion that the validator of the cell can interpret as funding?
	does this pose a problem of singular redemption, xor a scenario of validators only able to redeem their funds a fixed number of rounds in the future?

is it evident to a validator when to stop processing a cell?
	can the cell emit a special signal captured by the validator to says it is done for the round (similar to a generator `yield`)?
		does varying CPU speed mean validators will be at different points of execution when the timeout occurs?
			can the timeout be rough enough that the probability of inconsistent results due to CPU speed is negligible? 
		if the cell does not emit a signal without a network-wide global timeout variable, can the submitted state be the original state?


what is the conflict resolution process?
how are trees stored?
cell communication
	each push or pull could cost a fixed amount, maybe dependent upon the size of data.
	there could be a simple api for pulling so that the cell can customize what it wants to pull, but not add too much filter processing work to the node doing the pulling
		I'm thinking pattern matching for example
	for reasonable performance messages will have to be sent without one or two rounds. the trouble is it take logn rounds to confirm the messages are authentic, so consuming messages before then bears risk those messages are not authentic.

reducers send their merkel proof which contains their public key. they also send their answer.

at the message level, collectors submit their message list along with their proof and signature.
the evaluators verify the signatures and proofs of each applicant, and then choose the messages from the lists that fill a majority of the lists. they create this new list, evaluate the cell. they also hash the list of messages they infer. they also decide which of the collectors should get credit. they make a list of these collectors. they then send the result has and contributor list up to the first level reducers. 
the reducers review the evaluator applicants and infer an answer. they also infer a list of contributers who submitted that answer. they also take the contributor lists of the collectors that the evaluators assembled and infer the majority subset. they then hash the list. they then pass to the second level reducers: the

at reducer level n, you have a list of subreducers to learn from. each subreducer gives you a proof, a list of contributers, and hash of a list of subcontributers. you infer the answer and a contributer list. you also infer a majority subcontributer list. hash it, and then hash it with the subcontributer list hash. 

is it necessary that contributer lists are inferred one layer higher? can they not be inferred at the previous layer and hashed and included in the answer due to a high probability that though those contributers lead to a consistent answer the list of contributers will be different making the answers different? yes. can the contributer list be hashed and passed separately? is the problem here that the node assembling the contributer list could simply insert as many contributers (many aliases of himself) as he pleases without anyone knowing, and then use these as permissions for an attack? yes. 
does this imply the top level will submit its hash answer as well as a create a hash of subcontributer lists, and well as lists of contributer lists? will it be the job of the next collector generation to select a majority subcontributer list? will they also need to assemble a contributer list for top level reducers. will the evaluators select the majority top level reducer set, hash it with hash of the subreducer list.
does this imply the contributer tree is not ready until 2 rounds after the cell tree? yes.
maybe instead of wrapping the contributer tree around and completing it after the start of a new tree, it can be the job of everyone 
maybe instead of passing up contributer lists an extra layer they can be pass among peers just once, and then select the majority contributer list and hash it with the subcontributer list hash, and then pass the contributer list hash together with their answer to the parents. this takes advantage of the fact that nodes will already be networking among themselves and contributes to share data. this seems like a cleaner model, but i need to explore the concept of (similar to paxos) of sharing answers with peers and then changing your answer. if they include wrong contributers in the list to their peers their peers will remove it, and if they instead are inserted in the submitted list different from the one shared with peers, the list will differ from that of peers and so be considered incorrect. 
using this model the collectors can decide which top level reducers to include, complete the contributer tree hash on their own, and link it together with the cell tree hash to the previous tree. 
actually deciding the cell tree hash and the list of top level reducers would be a kind of network wide task, so maybe the collectors need not handle it, and the need not link their messages to the previous tree. and the top level reducers can create a proof of contribution not by showing their position in the contributer tree like the rest (because they are necessarily at the top), but by waiting until next round when the next generation of top level reducers infer the top level reducers of the previous tree, construct the hash, and complete the contributer tree of the previous round. so actually each round the top level will publish a pair (root hash cell tree round n, root hash contributer tree round n-1). 
i think this last idea of the bottom not handling the completion of the contributer tree, but leaving it to the next generation of top level reducers, is best. 

one round peer consensus stats: hard because can't assume fixed number of nodes participating, or number of proposals, or number of answers in a proposal.

if a majority message list is empty, it need not be considered an error, and the state can still be evaluated. this is just an indication that more time is needed for messages to converge and it can be tried again next round by the next generation. 
for reducers, an implicit null hash can always be considered as submitted by each applicant in addition to the hash they submit. this way the null hash will always exist in the majority hash selection, and in the case a non null majority hash is not found, the null hash by itself would be considered the result. 
suppose a scenario. the messages converge, the states converge for the first reducers, but unaware they are selecting different majorities. this way, the result of the first reducers does not converge. can the same method be applied as with messages? that is, can it just be left to the next round to try again?
there are two things that must be consistent, the cell vertex hash, and the contributor vertex hash. 
reducer scenario: at level L there are N nodes and for a particular incoming vertex there are M contributer nodes. 
	The cell vertex hashes of these M nodes do not converge. 
		Insert the null hash in their place. And while the subcontributor hash is kept, the contributor list is not hashed and instead is replaced by a null hash discrediting them for not being able to agree. 
		Will the state continue to be valid? yes. Does the error propagate to following trees or remain contained in its own tree? I think it is contained. 

	The contributer vertex hashes of these M nodes do not converge.
		Discredit the contributors by giving them a null hash, 

have list of cell vertices. 
	for each vertex, have list of contributors C. 
		for each contributor have answer-hash, contributors-hash, and subcontributors-hash
	for each vertex find majority answer-hash, majority contributors-hash, and majority subcontributors-hash
	make hash of list of contributors, make hash of list of contributors-hash with subcontributors-hash



evaluator scenario:
	the states transferred by previous generation do not converge. as the first reducers should do, the evaluators submit a null hash. 


what about sending answers to peers for a single round instead of passing them up, in order to achieve higher probability of consistency.
regardless whether passed to peers or up, what if node passes different answer to different peers. the danger with peers is that a node might trick other nodes into wrongly changing their submissions, whereas submitting to parents there is no peer communication, so no risk of this.

1. messages are collected
2. those messages are evaluated a layer up, and they are also used by the collection layer to infer where the next message collection should begin. 

when the contributer tree is complete, every contributer 

pushes messages could be verified by the first reducers reading the output headers of the cells (the messages they emitted and where they are located), then navigate to that space, and query for them like anyone else, with the cell evaluators unaware that the one querying them is the one who will be grading them. then the reducers can make a separate query to the cells asking for their proof (in effect revealing themselves as first reducers). 


LINES:
generating the cell tree and contributors tree. a topic of pipelining
design of cells, how they communicate, how they are executed, 
network communication, finding info you need
assigning those in a contributor tree to tasks


suppose the cell tree outputs a mapping that dictates the location of collection tasks for two rounds in the future (it take one round to learn and organize for it), which doesn't have to do with collecting messages as much as giving a location to the cell being collected for. This way all 
I suppose each level of assignment could be dictated by two things. One is the 
Imagine the list of existing cells at a time in a list, and once these cells are executed this list changes due to divided and died cells. This list is implicitly broadcast with the new cell states. Suppose the previous cell list (an integer range) had a mapping to a space. There exist those cells in the new list that remain from the previous. Let each of these cells be mapped identically as for the previous map. By networking, a new consensus to the map is reached by shifting around cells in the new map to account for the removed ones and added ones. the cell map i suppose is a spaced list. from the order of this map, and perhaps the spacing as well, first reducer assignment tasks should be inferred. perhaps a result of the shifting for the new cell map, it becomes evident how much spacing is before the cell, how much after, and what index it holds. From these variables it could be inferred which cells will go to the same reducer, and the position of that reducer would be averaged over the positions of its cells. Now the reducer task has been given a position, and thus it can be mapped to the assignment space. Likewise, second level reducer assignments can be inferred by the previous level of first level reducer assignments. So the effect is all tasks can be mapped to the assignment space. 
how about message collectors? they can also take info from the new cell list.

can cells be executed back to back? does cell state need to be transferred. can the next generation of nodes be waiting around in the assignment space ready to receive the new cells, network out the new cell list, and obtain their final assignments? does this take some time? yes. does it take a full round? not necessarily. does the 

to prevent errors (or rather interpret errors as part of the protocol) can money not be treated as a fixed material like usual, but be thought of something that might break the typical rules of money. eg it can appear out of nowhere, and disappear.
in order to prevent money problems (steeling large sums), can we rethink the concept, and maybe have a system where it doesn't exist. isn't the purpose for it in the first place to enable the system to operate, like to represent `right to resources (such as execution)`? yes. is the basic scheme that by contributing as a node, you can fund other nodes for execution (such as your own node, or community nodes). instead of sending an amount of fuel from you contribution to another node for execution, can contribution give you right to vote on the distribution of node executing cells for next round? can your contribution be somehow validated and used to influence the spacing of the next task assignment map? can your contribution be emitted in message space. can evaluators check a particular message space for available votes to fund execution. can they cite those votes when executing. in fact, should those votes be collected along with messages by the collectors. should messages be processed only once? likewise for votes? 

could nodes evaluate states they want (that way supporting them). would this break a fundamental principle that the entire network can trust the valid state of any cell on the network? yes. is the problem that nodes only have incentive to take care of those cell they care about? does this leave some cells unattended such that they can do what they want endangering the entire network? could this only be prevented 

conflict resolution. before did it occur by people putting down money on a betting competition, and did this money represent previous contribution? yes. 
now if money doesn't exist and previous contribution is instead interpreted as voting rights to cell execution distribution, then can conflicts be resolved by the voting instead of betting? if nodes execute cells of their choice, can the voting in effect execute the cell by as many nodes (all needing proof of contribution) as want to vote, and the result of the execution could be an arbitrary cell state (even wrong for valid hard forks), and then reducers are responsible for choosing the majority. is a trouble with supporting cells by executing which one you want, singular right of contribution redemption? yes. 

may a wrong output arise? must this wrong output be addressed? can an alarm be spread to prepare others to vote on the right answer. within one tree time, can both sides be expected to have submitted their votes. must the result of the vote be referenced by the next evaluators of the cell. 

can reputation be built up. every time a node contributes can the parent attach in the contributor tree to the public key an integer counting the number of contributions made. can this integer be referenced in a merkel proof. 

can an entirely different treechain handle errors for other blockchains. would the error chain need to explicitly depend on the handler chain. could the error be broadcast to the handler chain. could the handler chain read the error chain and decide the outcome. could the handler chain send the decision to the error cell. could the message be used by evaluators to reset the error cell. would the handler teer need 

is the essence of a dispute to represent the opinion of the entire network. does the network opinion involve the opinions of many nodes. can we assume the network is able to mush these opinions together in a reasonably sized data structure. 

was an important idea today that of the social chain, where the network together cares for every cell, instead of private parties caring for their own cells and the network being indifferent to everything about the cell except its interaction with the network-wide currency. in a core benefit of the social chain that all cells are considered necessary for execution and care. does this throw away a lot of problems regarding subsidies. will cells in this case not need to pay for their execution as it is considered part of the larger cost to running the chain. does this model better represent price than one of charging cells individually and not accounting for all the subsidized cost (networking). should nodes be incentivized to participate for the same reason as before: that cells can be executed as a result of their effort. before was this incentive represented by cells receiving funding with a notice of who generated that funding, and then the cell takes an action that benefits that generator node. in social chain is this incentive represented by the network as a whole seeing who has generated value by examining the contributor tree. can cells take account of the contributor tree and take beneficial actions towards contributors. 

good idea i just had when writing the last sentence of last paragraph: can dispute votes be collected, validated, and reduced by reducers. can nodes keep a watch on the existing disputes (maybe by watching certain areas of network designated for dispute alarms). can nodes at any time include their vote as part of their submission to their parent. if their parents decide the node is a contributor can the reducers combine those votes with others and reduce them. could they include more than one vote. could this depend on how reputation is recorded. would this mechanism allow for vote losers to give their reputation to the vote winners. next time the merkel proof is used of a loser, can the reducer accepting the proof be responsible for examining the previous tree headers to determine the result of the vote and then decrement or increment reputation. does this idea need more precise specification, for eg how much to change a reputation by. would evaluators also be responsible for reading the headers that contain a list of vote results, each result consisting of the hash being disputed and the hash chosen to take its place (which may be identical). 
is a problem that vote outcomes are not fully determined until they reach the root. and before that they are spread all across the network. could this be aided by only a subset of the network corresponding to some local area being expected to share their votes, this way the outcome can be determined faster. but would the outcome citation still need to be a merkel proof through the root head, and would this proof not be available until the top is reached. could determining how much to increment a node's reputation by be a function of its vote which should be visible, as well as the total vote count, as i guess just divide the total number of loser votes by the total number of winner votes to figure out how much each winner vote should be incremented by, and then multiply that quantity by the number of votes (assume winner votes) the node has. likewise if the node has loser votes. 
might it be helpful to delegate to a separate chain. would the other chain need time as well to collect and reduce the votes. 

maybe contributor trees could be used as julia trees (together with assignment function) to allow singular redemption while also flexibility of redemption time. 
if i remember, a julia tree is actually just a merkel tree with 

general idea taken from truebit is using a merkel tree to specify the steps in a computation (maybe even the elements of an eg message or contributer list) instead of just a single hash, this way it can be determined where differences arise. 


is one benefit of separate treechains that they can operate at different speeds (eg a storage chain can operate slowly)

what are applications?
each tchain is an application. 

cells can customize how they pull messages, including how long they wait before pulling one. waiting longer means more confidence the message is correct.

does the reputation tree need a means of applying reputation reports. should this means take the general form of weighing a report by the reputation of the reporter. do nodes gather info about their peers. do they then send a series of reports to their cell does weigh their reports and then publish them, each one published in the area of where the receiving cell can find it. do reputation cell continue pulling any reputation messages in order to update their own reputation. do nodes need an incentive to collect and publish their reports. could the incentive be that if the cell does not receive reporting from them, the reputation inside will decay. does this need to be further detailed. is it a problem that nodes could offer alias of themselves reputation. does this require reputation flow analysis. 

do cells need to communicate beyond other cells, like with other chains and the outside world. do cells do all communication through pushing and pulling in channels. if a chain trusts and is designed to depend upon other chains, can it pull from those chains as usual and read the headers as usual. does this mechanism answer how chains can interact. do cells need to communicate with the outside world through servers. could outside queries be solved by simply expecting cell executers to arrive at the same answers, and leaving it to the implementation the chain to make this possible by making queries only ask for deterministic data. another approach is to set up middleman servers to read and write from the outside world, enabling more customized side effects. unlike the previous method, this method would allow for writes. also, it would remove the burden of the protocol for making queries deterministic because the query would actually only be executed once (by the server) instead of multiple times with possibly different results (by the evaluators). explained under 'calls' in 'pipelining'.


i think physical locality of nodes should be considered to reduce networking across large distances (eg oceans). this might best be done by creating splitting an application into separate tchains for different regions, and letting them communicate when necessary. another possibility, more complex, is to build node location awareness into the peer to peer networking protocol. 

simple solution to untrusted exchange problem: assumption is that every node will need content that it will not receive without asking. solution is that the node asking for data must be the first to serve, then it can ask the other node for help in return. but this scheme discourages 'seeding', where as helping others 'optimistically' (expecting debt repaid) encourages seeding. The idea of tit for tat, or more specifically of helping your peers in compensation for them helping you, could be done by expecting your peer to send you a node within a certain distance, proportional to the distance you send him. and the node could go about getting this distance however he pleases, and if he can't find it he can give up, and not expect anything from you in return. in fact, the node software might best be ready to use multiple strategies. i like the idea of not just doing a one time exchange with another node, but keeping contact with that node for a short time while both of you try to help the other continue on his own path. 
'leeching' or 'freeloading'

storage. imagine a storage tree is made for storing large pieces of data, eg trees. 

maybe reducers could go at their own pace. each reducers would (as is already the case) seek their inputs in a certain space and post their output to a certain space. 

i like the idea of a DAG where transactions are never finalized, but just justified with increasing probability. but when i used to think about them i realized they are not scalable, because of the chaos that would ensure with such high transaction rates would make it impossible to verify the validity of dependency transactions. So those dependencies must be verified first, and then their verifier must be verified, etc, and the only way I can think to do this is reduction (a tree structure). 

Pls let me know there's a better place to ask technical questions.
In the validation tree each validation tower will have a certain number of incoming edges from other validation towers. How many of these input edges are you thinking (eg 2 would constitute a binary tree)? Also, what values are you thinking for `d`, the depth needed for validation.

https://vectr.com/roadmap/
https://www.draw.io/

communication of cells. two possibilities. one is messages. where cells specify for each round what messages should be pulled from where and what messages should be pushed to where. in this case i think the message space is separate from the task space. the second possibility is that cells can read from other cells (in effect publishing a message by dividing). in this case the cell space must be navigable for cells, and they would specify where to read cells from and where to push their divided cells. i prefer the message technique. with messages as cells i think there would still need to be an extra space (analogous to message space) separate from task space.
what if cells could move. don't think worth much, and would require cell space.

i see no reason not to have evaluators collect messages for next round (according to state's initial 'pushing' status) instead of a having an extra layer of collectors. the message collection would be submitted alongside the state submission.

could cells communicate with the outside world solely with messages?
	would cells write to the outside world by pushing to a space where the outside world reads from?
		would this mean anything outside that is written to would need to be chain aware?
			yes.
		yes.
	would cells read from the outside world by pulling from a space where the outside world writes to?
		would this mean for chain A to read from chain B, ...
		yes.

could messages be tagged with their intended space such that an attack cannot occur by providing a message in the wrong space?
	yes.

selectors. would selectors decrease state size? yes. they could be implemented as cells. or...

state differences could be determined by having the state as a merkel tree for data structures.

cells could evolutionize their code as they go. this might allow for holographic cells such that by partially encoding the whole network state, they can give better messages responses faster. research holographic properties. 

a fundamental basis assumption. a distributed application must have data. this data must be distributed into units, call them state. states must be dividable, merge-able, and transformable. hence the design of cells. 

alternative ideas.
cells replace messages
modeling nodes as cells
leaf nodes (processors) should reference top level reducers of previous round.
use another treechain (instead of a contributor chain) for assignment. 
extra layer of collectors below processors
pull messages and process them during processing instead of waiting for next round (must be deterministically pulled)

theoretically all that must be stored is outside messages, and the rest can be derived. 
how to use minimal bandwidth, transfer as little state as possible. 
what if servers hosted by individuals or organization could hold the data, and act as a database. the goal would be for the nodes to never have to hold state, only to record the transformations of state. they would collect messages, send them to the server, then update the state hash with the response of the sever. 

SELECTORS

merkle tree. at each vertex there could be meta data describing the data below it in the tree and thus how it should be accessed (eg an array, map, etc).
there could be a designated area for every cell (such as the cell's location itself if selectors are particular to cells). 

selectors. they offer a strong foundation for the chain that was missing. they are great for immutable data, making it easier to rollback mistakes. two approaches. 
1. one is for each cell's state to be stored somewhere else with replication persistently. the state  is merkleized. the root state hash it passed between processors, as well as the network location of the selectors. processors access the relevant parts of the state requested by the function as it executes by requesting from the selectors, and at the end they send the relevant selectors the relevant state slice updates. processors should only access those state slices that are hashed, ensuring they are valid elements of the previous state. eg if an individual list element will be accessed alone, it must be hashed (and then probably all others elements should too). a con is that all derivations of the state must be computed by the producers. 
2. two is all communication between cells occurs via selectors. state is a single giant merklized data structure. selectors store this structure and derivations of the structure. 

how are selectors incentivized? perhaps it could be as simple processors giving a referral (by submitting them as contributers) to any node that successfully serves selectors. special in the case of selectors is they need not be trusted and thus may not need assignment, like processors or reducers. however, there must exist certainty that there are enough nodes storing the data that it is safe.

the challenge of incentivizing nodes to serve the data they have. the general scheme I think is to have every node able to verify the data they receive (assuming its served) by comparing the hash against an official one. and also to incentivize them to serve the data because their grader will anonymously request from them. 

maybe the whle tree including the reducer tree could be the state, and at each vertex there could be meta data that contains derivation stats like 'counts' as well as hashes of derivations that contain a lot of data. unfortunately this still means the processors will need to compute the derivations themselves.
there must be an official hash for derivations. thus the nodes submitting the official data (processors) must be sure of this hash. one way for them to know this is to request the derivation dependencies, verify them, then calculate the derivation themselves. 

assume have a bare minimum chain ordering outside messages. from this everything can be derived. anyone can process any cell they want, and apply outside messages, but cells can only be so large and so they will need to send and receive messages from other cells. this means they need to know the state of other cells, etc. 



DERIVATION
yet another approach to blockchain scalability

the below is in format...

there must be events from the outside world. the frequency of these events is far too high for any single node to handle. events often relate to other events. 
there is an internal state. the internal state it too much for any node to handle. state must be distributed. 

events may influence state. events may trigger a transformation of a part of state. a transformation of a part of state may depend on another part of state. or, a transformation may influence two disjoint slices of state. two nodes must trustfully interact for this.

nodes do not trust each other. a subset of untrustworthy nodes must compute a transformation. collusion is a risk. to reduce probability of collusion, nodes for a transformation can be randomly selected.
the output of the nodes must be verified in order to be trusted. apart from snarks, the only means of truly verifying is by computing. therefore the output must be trusted solely on the basis of other nodes' answers. the level of trust can be dependent upon the nodes' identities such as their reputation or means of selection, as well as the number of nodes. suppose a node would like to know the output of a transformation without calculating it, and thus relying on the previous bases of trust. this node must obtain from each node (or as many as possible) the output, as well as meta information regarding identity (such as a signature) to obtain as much knowledge as possible. given this information from the nodes and comparing it or analyzing it however, the node must decide whether to trust the output. ...

alternatively, with snarks, suppose the verifier can engage the processor to prove the output is correct, without computing himself. make this recursive...

there is a group of nodes that have decided on the validity of an output using this method. by the processors broadcasting their answers, this group could in fact be the entire network. but then every node would have to verify all the transformations, not possible. we use the idea that multiple (eg 2) verifications can be merged into one. a node produces the validation by getting the transformation from two processor groups, deciding their validity, putting the answers together in a data structure, then hashing the data structure.

it is an important point that the validity of an operation is a function of the consistency of the outputs, not their content. for this reason, the content can be anything, such as a hash. 

a polynomial is a function with variables as inputs, and polynomial value as output. prove that a set of inputs result in an output without having to evaluate the polynomial yourself. 

zero-knowledge-authorizations.
assume contributions are recorded in contributor tree. in order for them to be recorded in this tree, the origin of the contributions (the cells contributed to) must be public knowledge. it is possible to find all information produced by a node that contributed to a particular cell. 
the contributor trees produce a lineup. for next assignment we'd like to map the assignment space via an ordering of previous contributions.
we want the contributor tree order to map to the new order, but the new order not to map to the contributor tree order.
i think this means a one-way permutation function. an example of a one way function is a hash. in this ...
this idea of making nodes anonymous might not work. because nodes could prove themselves as loyal colluders upon every round by submitting not only their right to task, but their contributions to previous tasks, which the cartel could then judge whether they were loyal in those previous rounds or not. the way to fix this to make it so nodes cannot prove their previous contributions. this means that a submission cannot contain info (eg right to task) only generatable by the node, because then the node could use its sole right to generating this info as proof of its contribution. this means right to task cannot be a function of success of previous work (acceptance of previous tasks). 
we can make the parameters such that the probability of a reasonably sized cartel gaining an entire task without a single space for an honest node such that they can submit the answer without an honest node sounding an alarm is negligible in that it will not occur within the next few decades (eg with a block per 2 minutes and securing the next 40 years corresponds to a probability of one in like half a million). and we want a probability a cartel getting the majority of a task to be small enough that it will occur infrequently enough that a global alarms can sound. hypergeometric distribution.
in a model where there is no snarks and nodes' history is traceable such that the cartel can identity its loyal members, what happens after an alarm wherein the cartel knows one of its members is not loyal? the node 
a spy, anonymity, or pretending to be bad when not, actually doesn't seem to help, because whether or not there is a spy, all that good node can do is ring an alarm. actually spies help just to prevent censorship, where the cartel works to prevent the spies from submitting their responses. the separation of IP and public key could help. this general scheme requires that nodes and reducers check the output just as their parents do, and submit an alarm if they are confident the answer is wrong. the alarm could be sounded by the node citing its right to task as well as identifying itself so that it receives credit. the node, having been identified by the cartel, then exits the system and re-enters on a disconnected public key, and must start from scratch. but this is ok because in return the node gets compensated for this alarm. 


if the order is simply the index of contributions, then one can iterate over the ordering and find those indices that map to a particular assignment space (for the coming round), and the locate the node identities associated with those indices (by walking down the contributor tree).

snarks are limited by the need for messaging. ... actually not! the output of a cell should include its messages, and the inputs of any cell can be checked as valid by checking the output of the relevant cells. cells receiving a message would verify the sender. 
in general (for both snark proofs and consistency proofs) why not simply check those cells upon which you depend. the proof that a cell is valid includes the proof that its inputs (its only dependencies) are valid, and this would be done with recursive snark or by the nodes consistently reporting that they verified the inputs. 
the goal is that you can verify the output of a cell. but the validity of output depends on the validity of input. so the proof must include proof that the input is correct. the input can be the output of another cell. well, maybe they are limited: suppose a cell only outputs a single message (not multiple, and not a new state). ...

ooo, idea. to prevent the spread of cancer, when an alarm is sounded, the alarm can be passed around the space of the cell as well as the space of any messages the cell is posting. this way other cells when they pull messages will be entering a space with an alarm, and it is their duty to regard that alarm by not intaking any of those messages, citing the alarm. then the only cells that must be cured are those that received messages from the cancer cell before the alarm was sounded. as soon as the alarm sounds, these cells should be sought and an alarm should sound them too. seeking these other cells will involve going to the message area where messages where posted, but then finding the cells that pulled from that area might entail a need to keep a history of pulling cells from that area. this history could be kept by the cells posting in the area. but such a history would probably mean identities for cells (like index) which is possible but complicated given the transformation of idices round to round. another possibility is having every cell use the sound of a global alarm to inspect the alarm and see if it pulled any corrupted cells. another one is cells seek not only messages from an area, but alarms about the corruption of previous messages it pulled. 

instead of ringing an alarm given sufficient reputation, could you ring it with a proof cheap enough for anyone to verify? the proof would say, this is the output of the cell, and it is wrong given the previous cell state and its inputs. the proof would need to reference the first operation that was incorrect. could this be succinct with the merkleized state? 

whats the economic difference between gaining authorization by staking, or by working on the chain to earn reputation. 
in staking you're gambling on token prices. 


matrix. the benefit is that randomly testing the linear mapping of a points gives a good idea of the overall transformation because it is linear and can't have hidden and so has good soundness. i think it wouldn't be the case that the prover maps a bunch of points through the matrix as a program, and the verifier just checks a few of the points. this doesn't take advantage of linearity. instead, the linearity says, if it maps these test points to these areas, then likely it will map all others similarly so this is the correct matrix. this way, its the matrix that is being tested. so suppose you can test a matrix by mapping sample points, eg by multiplying another matrix, perhaps of different dimension. you could check two matrices this way. the goal is to ...

i understand how computational integrity is of the form proving you know an answer to some problem. then the verifier checks your answer instead of finding the answer. then there is probabilistic proofs that make you only check part of the answer. then there is IVC where you can merge them. what I don't understand is how to encode a computation as finding a solution to problem in the first place. 


problem: must have sufficient reason to believe that another derived state is accurate. first way, with proofs. second way, trust itself. techniques to increase trust. assign a random node to task. assign more than one node to task. check the consistency of their outputs. trust that at least one honest node sound an alarm at the event of a corrupted task. try best to assign to each task at least one honest node. 
for proofs, need assignment mechanism and recovery strategies. one question with proofs is whether it can be proven that a node not only accurately processed input, but made the necessary effort to obtain that input. the node stand idle, not computing, and yet still output valid proofs. a solution is for input availability to be guaranteed, by requiring the node to constantly process an input, even if empty. 

thinking about dfinity model. bottleneck seems to be that if n nodes process at each vertex, then it must accept inputs from n*b^d nodes where d is the depth and b is the fan of the tree, eg 2. minimum fan for scalability is 2. i want to know if increasing n has the same effect on soundness as decreasing d. the nodes at the vertex must process from each of the d-1 layers below, while taking input from the d layers below. in my model d=1 and transactions are only processed by n nodes. with d>1, transactions are processed by n(d-1). so if to calculate the tradeoff of n and d, lets compute their relationship by n(d-1)=c for constant c. or (n+1)(d-1)-n(d-1)=(n+1-n)(d-1)=d-1 and n(d-1+1-d+1)=n so basically increasing n or d by one increases number of nodes processing by d or n respectively. however for the processing model n*2^d=input_to_process, increasing n by 1 increases processing linearly, and increasing d by one increases processing exponentially. suppose we want processing constant that is n*2^d=c. so increasing n by one increases result by 2^d, so increasing d by one gives the same effect: 2^d*x = 2^d+2^d=2*2^d = 2^{d+1} => x=2. 
... this needs to be rewritten more clearly. 


I don't think there is reason to continue thinking with distinct, intertwined, recurrent chains, but rather a single pipelined chain of trees.

Maybe cells could take as much time as they like to process. Maybe cell leaves could be ordered in terms of execution time for efficiency.

process state, transfer, process state, etc. The fewer rounds (previously called 'cycles') between load observation and load balancing the better. Suppose it is 1 round, such that load observations made during state evaluation are used to transfer that state to the next generation. This entails the nodes of the next generation (in essence the final assignment function) will not be known until the state is evaluated and ready to be transferred.

I have a rough picture where nodes have a rough idea where they are assigned a few rounds in advance, and on the last round they find out exactly.

By load I'm referring to how many cells there are that must be processed in a given round, based on the info of which are scheduled to be evaluated, how many are newly created, and how many have died.

To aid against targeted attacks, a generally good pattern is only pay them and credit their evaluation if they successfully completed one in the past 1 or two chances they had. This way they can't wait around not doing work for the perfect task they want to corrupt. Another way to say this rule is if two or so rounds pass without you successfully submitting, your next successful submission will not count, only one of the two after that.

The real problem I came across last night, is that regardless of how previous the authorizations, these authorizations can be sold and bought by those who would like to target an attack. Unlike collusion, buying authorizations cannot be shown to have occurred in the blockchain  its invisible from the chain's perspective. I was thinking a market for authorizations could make the workforce more dynamic, with on demand work possible because nodes will not have to prepare the authorization logn rounds before. Actually I realize this type of corruption can be tracked and handled in a similar way to the collusion. Nodes can report any selling of authorizations they observe, and the tasks those authorizations point to can they be observed for collusion. Maybe the best solution is just to forbid the selling of authorizations, and report any seen. Maybe better solution is to allow selling, but

Last night in bed I realized work for further assignment cannot be one round previous, because it takes another round to verify it, which takes two consuming actions. One is to calculate one or more merkel proofs along with the assignment function that to verify that the task is authorized. Calculating once would only point to a task the node did that has gone up one level, which only gives a certain level of confidence. Calculating two would show two levels, more confidence, etc. I know I'm not being clear here, and I'm not because this won't work anyway. The reason it won't work is the second consuming action which verify that answers are right which entails searching for the majority answer to compare to, which likely means as many searches as there are nodes submitting. The farther back the authorization the more probable the majorities you seek will be duplicates. This is too much overhead for collectors. The other side of the spectrum from authorizations one block previous, is authorizations previous enough they have reach the top and therefore the node will only have to search the majorities of the blockchain headers, far less overhead. This introduces the pattern that a node instance can only do tasks at least logn rounds apart which means a lot of idle time. Idle time can be use by nodes to run multiple node instances. It also gives rise to recurrent trees, partitioning up the network making it less fluid and providing more opportunity for targeted attacks. This can be resolved by an authorization in a tree pointing to one of logn trees at least logn trees ahead of the authorization. This introduces uncertainty about load for node instances, in that their authorizations could all point to the same tree at times, causing inconsistency in load the node may be unable to handle.

Can homomorphic encryption play any role? A desired effect would be you do not know what cells contain. The challenge is then how do you read them at all. A benefit to encrypted cells is that each instruction could be made into a homomorphic circuit applied to the data of the cell.

To aid (only slightly) against the two types of collusion, its is worth making sure that final assignments are only made last minute, which should be the case anyway for load balancing. But the collusion should still be fought against by reporting and betting. To make this defense mechanism more dependable, perhaps instead of being implemented within the protocol using cells, it could be part of the protocol itself. Anonymous reports would be broadcast in the local network 'space' on the task that is being pointed at. Or maybe they could be broadcast in other areas of the network.

Maybe there could be a way to buy and sell authorizations where the buyer doesn't know what he's buying until bought. This way honest nodes could sell and honest nodes could buy with no suspicion on bad intent. This scheme will depend on the design of the assignment function. ...

# Problems

Authorizations can be sold without public notice, then the attacker having bought enough authorizations for an evaluation can take complete control of that evaluation and only the authorization sellers could possibly be aware and they probably won't because they cannot know whether collectively they have sold the same party of not. Their only hope is to judge the correctness of the evaluation of the buyer which they may well not do. Maybe we could make the protocol so that buying authorizations is 'prohibited' in the form that honest nodes will not do it. This way, nodes can offer to sell their authorizations and if someone offers to buy it, the seller node realizes the buyer node must not be honest and up to no good, and therefore it may be worth the unpaid processing to verify what the buyer is doing in hopes of catching something wrong and winning a bet reward. Opting in to a system to sell your authorizations would be like opting in to a system to accept bribery.

If good guys keep using the same key they can be identified by the bad guys as the nodes that continually appear in their bribery circle on occasions they get reported. If good guys keep changing keys, the bad guys can come to trust only those keys that persistently prove themselves as being members of bribery groups that do not get reported. I could do the stats to figure out how easy this type of identification is. If bad guys identify themselves this way with persistent keys, maybe the good guys could track them just as well as the bad guys. This is a game of identification I need to flesh out.

# Assignment

I will try to flesh out some details of assignment.

Every round each level observes its traffic from children.

Nodes organize themselves and find their place in the round prior to their task. Its an important assumption that nodes can organize themselves within the time of a round. Best if evaluations happen back to back. This means time to organize should be less than the time of a round, or less than the time of two rounds.

The info, such as load info, of a level should be emitted after (or while) processing, and it should go both to the next generation (so they know what to process) and to the parents.

It could be a random function where you take the pool of applicants and their related ids which are ordered and the tasks all have a certain probability of getting chosen uniformly by an id. then the random ids get mapped to the tasks. They could get mapped to the probabilities if they are distinct. Or the ids could map to spaces at which the tasks are located. Wherever they are mapped, the mapping needs to be influenced by the load needs. I was thinking about using modulo the id number, where assuming the ids are uniformly distributed, modulo certain numbers will be common than others, so the ids that need more validators can be assigned those numbers whose modulo will be more common. of course ids can modulo multiple numbers, so maybe greatest divisor would be better. and actually tasks would be assigned a set of numbers, not a single one.

Another option is take the integer range of id (a uniform distribution), give it a random permutation, then map it over by ranges of numbers to the tasks. The hard part is assigning the number ranges to the tasks. The tasks can also be represented, just like the nodes, by a range of integers.

Another option is mapping the ids into the network space the tasks will be located in, hopefully mapping them more densely in those areas where there are more tasks (which can maybe be done by restricting the space and then mapping uniformly). Then tasks can choose (accept the authorization of) those nodes that are closest to them in distance. Nodes may well fall within the distance of multiple tasks, so they may be used by multiple tasks. Those tasks that request more nodes can be given more area. The trouble with getting multiple assignments is then those multiple assignments may map out each to multiple assignment, exponentially. But I realize this would likely happen for most nodes equally so the mapping would get crowded enough that tasks would not be using all those assignments. This seems like a simple solution. But to avoid the same authorization being given to parent and child levels, tasks should be mapped randomly to the space. So the ids and the tasks should be mapped to the same space and then matched up, whatever that space.

Another better option is randomly map the available nodes onto probably a cone (or some suitable space for the shape of a tree). And map the tasks on there too. Then the mapped nodes will look around the space and see what tasks are closest to them and that they are willing to do, maybe taking into account how many other nodes are looking around in the space. The tasks are spaced apart so as to give the tasks asking for more security, more space, that way encompassing more available nodes. Your parents that inspect your submissions will have to look at the previous tree space and verify your authorization.

Lets try averaging the nodes in a round over the nodes of all previous trees. There is a 'last' variable representing the divide between those nodes that have redeemed their authorization and those that have not. The way it works is each round you calculate the average of all passed trees, assign the nodes between (last, last+average], then last=last+average. See how this algorithm behaves for varying sequences of tree nodes.

I just realized that I don't know exactly how the tree will be mapped to the network space. Before I thought it would be mapped as in the previous round, and then self adjusted by looking at neighbors and shifting around until the tasks are spaced out appropriately. The trouble is proving your authorization, because it won't point to the task that maps directly to the same place due to the shifting. Perhaps it will be sufficient for the authorization to show that the mapping is in close proximity and that the others who are making the same submission all have the same authorization mapping.

How the shifting itself would occur efficiently, like waters seeking its level, I don't know yet. One algorithm I think is possible in logarithmic time is sharing info with peers (nodes one space away), then nodes 2^i spaces away, and do this (n-1) times. There is room for harm by reporting false traffic. Or perhaps the traffic info could be shared during the transition process. Actually I realize that can't be done, because the transition process is logarithmically big to small, whereas the adjustment process of logarithmically small to big. Actually maybe it still could, where as the previous generation holds the states and seeks who to pass them to, they could also be seeking the shift by talking to each other, and perhaps it could even be that they must find the level before passing to the next generation and must include that info in the submission by hashing it last minute, and this aspect of the hash could be a component in their grading. Once this is done they would actually adjust the mapping of their tasks (using the adjustment as a component of the determinist mapping), and this adjust component would become a component of the authorization for the next generation.

Random is not the same as uniform, and uniform mapping is what we are striving for, so if able, nodes should be uniformly mapped to the assignment space instead of randomly mapped. This may be doable because nodes will known their index in the tree because the tree will have completed, but of course the positions should be randomly shuffled.

# Payments

How do cells pay for dynamic security? Perhaps the amount of security could be part of the cell state, and it could be made sure after each round that the cell has enough points to pay for another round of evaluation with the security level it is requesting. While states are evaluated the balance can be reduced by the requested security amount. Then the next level collectors can decide what payment messages to send to the evaluators.

Different cells take different amounts to process. It is fair if an authorization points to a cell that is tiny and doesn't pay much because it doesn't have many instructions? Should there be a base fee for processing? Yes. Could there only be one type of score, which comes from (partly by luck) which cells you evaluate, the fees you can charge them, and how successful your evaluation is considered to be by your parent processors? Yes. This score can be regarded as stake. Is the only way you can earn stake by processing? Yes. Is the only way you can spend stake by cells getting processors. Is the stake of the processed cell just transferred to the processors? Yes. Can users trade the stake? Yes. This be possible by giving away the key, or more securely by transferring from one account to another.

Payments should be done by taking the total (agreed upon) amount the cell should be charged and distributing it among the processors based upon their submissions.

payment could be in the form of a merkel proof of what you did and could be used to fund the execution of another cell. a general problem regardless how funding works is how assignment works. if its that you must first do uncompensated work for a round, then how does on demand work work? another similar option is stake. What if the work (or stake) could be stored, like it could be earned the past, and then redeemed any time in the future. This way there could be no need for selling authorizations and on-demand work would be a matter of when users decide to use their earned authorizations. This could also help node control, spreading them out more evenly when an error causes a pile up. Of course the challenge is preventing multiple redemption. there would have to be a cumulative redemption tree.

i suppose every successfully completed task, which should result in both the right to do another task, and compensation, could be stored in the round following in a convenient way to store it for unique redemption in the future. It could go to a cell for each node, which would keep a score of how much the node should be compensated, and how many evaluations they have done. It can be a special cell with simple contents that are recognizable from the outside, this way a node could hash it and prove its contents and presence in the most recent tree as an authorization. The cell that is accepting your authorization and processing submission would then send a message to the referenced cell, hopefully along with all its respective parent peers, and either increment or decrement its score. since this info would be kept in a cell it could undergo more complex changes, eg time decay.

With this type of authorization (sufficient stake) ids would no longer correspond to positions in the previous merkel tree. instead, positions would need to correspond to the cells holding the stake, which could be done in a number of ways, such as the position of the cell in its respective tree, but these stats would not be as distinct as with the previous model with all ids for positions in the same tree (which is actually not as simple as i imagined because each node would be referenced by all of its parents, thus having as many merkel proofs with distinct positions as there are parents, though likely these position would follow a pattern).

Maybe this mechanism of authorization would require more complexity, and special attention in the protocol which is deserved because it belongs to the protocol, not to users. The reason it can't be so simple is it would take several rounds to show that stake has been used as an authorization, letting the user use it multiple times. Maybe it could be that the one requesting authorization can give some proof of unique redemption.... Another approach is the stake can be split up into a few separate sections (as many as it takes rounds to process a stake deduction). Upon each authorization request, depending on the block number, only one of them should be referenced, and then not reference in the following rounds so that the deduction has enough rounds to be processed. How many rounds? One for the parent to process your authorization and send a deduction message. Another for the deduction to be processed. So two. Therefore there should be 3 (or more) sections.

Maybe nodes should not be able to use their authorizations whenever they want, because they can see what task the auth maps to and then decide if they want to do it, enabling more targeted attacks. The argument for use-when-you-want authorizations that on-demand work is helpful may not be true. It is true that the network will be volatile in traffic, but on average the network as a whole will use about the same computing power round to round. Thus deterministic assignments may be ok, but problem they pose is losing a bunch of nodes (or having too many) for a round due to dropped branches, though this might be solved by assigning any tree of nodes to any of a few following rounds, not just one. Another way is calculating the average of nodes in the last n and giving that many nodes to the next round. I think this technique would basically be make a deterministic mapping of nodes to the assignment space given the propagated-up info of previous trees. On the other hand, make the mapping of tasks to the assignment space based on the outputs from the last round. This is elaborated upon above under assignments because that really where it should go.

What should be the criteria for an authorization, regardless how authorizations are assigned assignments? Originally it was that you must have immediately previously successfully completed an assignment. In general the idea is that it requires a certain amount of previous success (points, stake, whatever). It is this success that is deducted from cells and transferred to cells specified by their processors. You can buy this success from others or create it by yourself by doing evaluations that require no authorization that are not counted but result in success. These points naturally decay over time because they will sit in cells that must be processed every round (regardless how easy the processing), and it can be created dynamically by more computers processing. Since the resource is in essence computer processing power, it will be represented as so with some unit.

# Calls

Maybe cells could be suspended in time where the cell indicates it is willing to do so, and then anybody (hopefully managed outside the protocol) downloads and stores the contents of the cell, while the hash of the contents of the cell replaces the cell. This way storage storage and bandwidth costs would be minified. All that would be required is a processors node to store the hash, and keep decrementing from its payment for the tiny cost of storing the hash. In fact, this need not be part of the protocol, but could be implemented in cells. The cell requesting hibernation sends itself to a hibernation management cell along with details of what parties it would like to be stored and reactivated under. The managing cell would store the cell hash, and relaunch (by dividing) the cell when it receives the activation message from the right party (probably via key). The manager cell also must have a mechanism of finance where it changes those cells it is hibernating. This is appealing but it may end up being cheaper to just let the cell die, and relaunch it, although this requires the trust of a central party, whereas the previous method everyone can see that the relaunched cell is identical to the one hibernated. Any changes that should have taken place during hibernations, eg score decay, will need to be applied before relaunching. Its important to remember that protocol sensitive stats, like scores, cannot be manipulated by cells, only read and requested writes can be issued. So when merging cells, these stats are merged, and when dividing cells, the stats are divided.

In general (beyond hibernation), an important interface is the internet. cells should be able to make read and write from servers, and it is up to the protocol of the cells how they manage these servers. The servers will either be public APIs, or servers managed by the party managing the cell. The responses will not be verifiable when the server disappears, but it should be verifiable enough that anyone should be able to compute the same result within the same time. The trouble is that if responses are different for processors, it is unknown whether it is the cell's fault (which includes any fault of the server) for not querying correctly, or the processors' fault. Typically with deterministic state computation it is assumed to be the processors' fault for any difference of outcome.

Here's how to fix it. The managing party of the cell is responsible for setting up the server, and specifying the public key of the server. The server returns signed data, this way the processors have no opportunity to alter it. The server should also sign the response instances (via a request id created by the cell) so that processors cannot reuse previous responses. So every node is expected to deterministically make the same requests, and post responses. If responses differ (not including absence vs presence of response), it is assumed the fault of the cell, which should probably result in charging the cell as if it were process, compensating the processors based on their evaluation except the differing responses, reporting the conflict of responses to the cell, discarding all that is needed in order to reach a convergent state, and moving on to the next round. The internet info, just like the stake, should be kept separate from the cell code. General APIs could probably still work just by judging from the majority response, and of course trusting the server is not malicious as it could be with cell-made servers. Public VM images would be great, but autonomous deployment can be tricky.

A bit unrelated, but its important to note that a feature that will not be available to cells in a pipelined environment is merkel proofs of recent events because it will take a number of rounds for those proofs to become available at the top. Like the internet, reading the blockchain should be an available interface for cells.

# Storage

Trees will have to be stored. They could be stored in cells, but if cells get passed around every round this will cause too much overhead. Two options. One is use a cell's protocol to actually do the storage externally. The other is have cells that only must be included in the tree periodically, suiting them for long-term storage. The latter would be more complicated, but would make the tree storage part of the protocol (which is suitable) instead of leaving it to user cells. But these special user cells could be regarded as system cells, and they might need special privileges. The former could work by expecting each node to send its data once the chain the header has bee created and the data has flowed back down the tree. The storage cell would matched storage sections to volunteering parties. The content would be hashed and passed to the volunteers for long term storage (or maybe an autonomous external database could hold it), and financing of rewards and punishments would happen inside the storage cell. The stored content would need to be queryable for other computers (eg to rebuild the chain) and it would need to be proven that those untrustworthy volunteers are indeed serving the queries. This brings up the need for a cryptographic tool that could be useful elsewhere. A tool to prove to party 1 that party 2 has sent something to party 3. Call this the outsourced-server problem.

Outsourced-server problem. Let 2 (party 2) prove it served the content to 3, or let 3 prove 2 did not serve the content. This boils down to the problem of how two untrusting parties can exchange information.

# Error

Branches may get dropped. Nodes should keep what they have and pass it to the next round.

# Conflict Resolution

In order to settle conflicts quickly they should be handled as an extra part of the protocol. Timeouts could occur by real time instead of block time. Claims could be hashed together in a chain. Then once finalized that chain could be included in the actual blockchain.

Or it could occur inside a cell, perhaps created dynamically with the first report. This would be more elegant, and I don't see any extra cost to it.

When it is determined a change should be made, the evaluation make the changes and points to the conflict resolution cell for justification. Potentially even this part could be implemented at the user cell level instead of the system level. How? Those cells that are would like to participate in a conflict resolution program would check the resolution cell when the execute to make sure they have no conflicts to reverse, this way the evaluators need not keep track of the resolutions themselves. This user level way would enable there to be multiple programs of conflict resolution, each with their own policy. But it would come at the cost of checking for conflicts during execution, instead of the evaluators already having in mind the resolutions that must be applied.

This aspect of the protocol is independent enough from the rest that it can be openly discussed at the time of presentation.

# Honest Majority

The possible problem is that relying on nodes to submit the right answers because they expect everyone else to is a bad idea. If a node thinks it can profit more by deviating from what others do, this can be problematic, because every node might take that action, resulting in an answer where everyone deviated (a wrong answer). We just create a scheme where nobody can benefit from deviated answers. This way, nodes are not motivated to deviate, and more importantly they know better than to accept bribery from others who request deviation, because they know there is no legitimate reason the briber should be seeking deviation, and instead must be trying to deviate everyone, in which case the original bribery reward for deviation will not be fulfilled. Briberies should be offered only dependent on the action of the one being bribed, not dependent also on what everyone else will be doing. In the case of deviation (only a few deviate from the majority), it is those that do not deviate that benefit. I see no way any party involved in deviation could benefit.

# Address Transformations

Every round the identities of cells will have to be maintained in the presence of cell devisions and deaths. This transformation should be able to be passed up the chain. But it will have to be known every round, so after cells get evaluated and passed to the next generation, the transformation will have to be passed with them. This will take a form of "if a message was sent to that space where the cell previously was, it should go to over here instead because the cell has changed locations."

# Language

I will probably use flatlang. So before releasing this I should probably have flatlang more fleshed out. This is located in the Flatlang page.

# Communication

Last night I realized I still hadn't settled on how messages are passed, because the location of the recipient cells will change depending division and dying, and even the cell itself will change it if divides or dies. Then I thought that instead of proactive communication (like in object oriented programming where objects send signals to each other, ie message passing) there could be reactive communication where cells emit signals and other cells decide what emitted signals they would like to consume. That is, instead of messages being pushed, they are pulled. Cells could pull outside-chain events from servers and IPs via the internet interface. But even pulling messages has trouble because a cell must know the location of other cells to pull from them. The solution is for cells to emit signals to a 'space' and for other cells to pull signals from this 'space'. There could be multiple spaces for different kinds of signals, perhaps even some of them could be permissioned in that only signals containing certain content (eg a certain signature) may enter the space, saving pulling cells from having to filter them all. If a space is to change, eg size of shape or existence, a cell could be in charge of it by having the ability to emit a special signal that changes the space, and all cells that pull from that space would look out for that signal to keep track of the space. Knowledge of spaces can be passed around through other spaces. I don't know yet how spaces would be financed or how they would fit into the tree, but I don't think there are any fundamental problems. With this notion of intermediate spaces for communication, perhaps a cell address transformation is no longer necessary.

The more spaces a given cell uses (either pulling from or pushing to) the more networking will be needed for processing. Could spaces be discrete, totally separated from each other, or should they continuously form one big message passing space?

I'm having trouble answering how the space is maintained. Assuming a continuous space, how is it allocated? Maintainers can be paid by pushers or pullers, but what is the payment process?

If addresses, corresponding to the space the cell occupies, exist and are static then there must be a global address allocator that places cells in order to prevent congestion, but the global stats would need too much time to propagate (rise the the top and back down). But with these static addresses messages could be pushed and pulled between cells, and some cells could function as channels to minimize the number of cells that must be queried for pushing or pulling. But maybe a global allocator would be possible by using information round to round given from above that came from other states in rounds previous, in which case it wouldn't be an optimal allocator but perhaps sufficient.

If addresses, on the other hand, are non-existent then all messages must be passed by 'spaces', which I might for now better call 'channels'. Could these channels all be reliably implemented with private servers? Of course the benefit here is static addressing, and performance. The risk is that the servers would not be trustworthy. Also, since the messages would go out of network and then back in, redos would be much more difficult. Most problematic is that passing messages is such a core part of the system, that resorting to centralized servers is purpose defeating. So back to where we started, how to maintain channels. Actually servers can be used but they can't be trusted, so can host the message content and make it high performance queryable but the network must still handle the hashes of everything.

I don't think I mentioned it yet, but the way I think messages should be sent-checked is the evaluator sends out the messages directly, and the parent then queries for them to see which evaluators sent, which gives both evaluators and parents incentives for the query, but unlike other schemes where eg the evaluator passes the messages to the parent then the parent sends them out, it requires to cycles of querying instead of one.

Channels could be managed by a series of system cells that assign volunteers channels to manage and pay them based on reports from queriers. This approach encounters the outsourced-server problem which I still haven't solved. But in section below I realize solving it may not be strictly necessary.

Suppose channels are discrete. A channel manager can always be searching the network for pushed messages and pull requests. These channels need to be in a metric space to be navigable, but this space cannot be the tree space, because items in that space constantly change location, whereas these channels must maintain location which is their identity. That is, unless the channel items are the only items not to change location. Otherwise, the channels must be in a different space. If channels are discrete, this could be like a lattice space.

# Untrusted exchange problem

Will probably use some form of homomorphic encryption.

The general problem can be stated as why should either node have incentive to send the last message. The last message must be of concern for the node that sends it. Maybe there could be a third anonymous party C, to which both A and B send their ...

Maybe for now we can just trust that enough reports will be considered the right answer.


maybe 2d is special due to its particular relationship with the 4 color problem. maybe this can justify using 2d instead of other dimensions in certain contexts.


code concatenation in a non communtative way to map two codes to a third of less error.
this could be converted to a set with an operation, or alternatively view each code as a morphism
for inverses to exist, codes should be generalized to also compress rather than only expand.
regular coding theory assumes a vector over an alphabet, but maybe that could be generalized
note that alphabets can change with composition, eg a code with one alphabet can be use in the composition to make a code with another alphabet.
maybe if two codes use the same alphabet they are commutative, cuz the params of the resulting code is already invariant. this means a vector space could be constructed. maybe it would integrate well over a complex field if the codes mapped complex vectors (list of numbers) in some subspace (eg k roots of unity) to vectors of a larger subspace (eg n roots of unity).
or maybe make each edge a vector. for directed, consider mapping the edge to all outgoing edges of the vertex it points to. this would yield a matrix is a similar way. would it specify the whole graph? could it contain any contradictions? other variations are possible. yes it would contain contradiction while not even specifying the whole graph. what about undirected? i don't think it has contradictions, but it doesn't specify a unique graph. this is all due to the possibility there are more edges than vertices. 


i think the concept of having a large space spotted with special elements (eg codewords) could be generalized to the graphs concept, and other spaces like vector spaces in general, eg lattices fit well. then consider local testing in these new environments, similar to codeword testing in coding theory. 

make graph structure with vertices as codewords and edges only between adjacent codewords, and edge values as distances between them. 


computing the determinant of a matrix seems like a good problem for a proof because it uses the same data over and over in a tree structure of computation, so it forms a great arithmetic circuit. The question is how useful this proof could be. What is the use of proving to someone whether or not a linear map is invertible?


how to calculate a determinant?
how to cac

since lambda is all about functions, and all you do is pass them around and evaluate them, executing a program would be well suited for a distributed environment where function are represented as oracles. could evaluate oracles using homomorphic encryption or maybe proofs of somekind or simply querying a server. so it may or may not require the interaction of another party. 

consider a lattice as a k-regular expansion graph and maybe mix cryptography with proofs. 
and a path on the graph can be thought of as the linear combination of basis vectors. 


i think multivariate interpolation is very hard. 
eg given ability to evaluate a multivariate (in CNF form) its hard to determine a given coefficient for any standard encoding format. 
eg adding two poly in factor form and converting again to factor form

basically factoring is hard.
learn and think more about general pattern of factoring, eg it requires two operations.
in what cases of these operations and arguments is factoring hard.
build hard problems out of this


3SAT reduces to this factoring
closely related to multivariate polys. 

maybe a decisional versional is deciding whether one can obtain a factored poly form from adding two given polys in factored form.
think of relation to number theory and prime factorization, where all numbers can be factored. 

and think of relation to 'encoding complexity'. eg numbers can only encode 1 item, while polys can encode many.


in general think of owf as random generation of multivariate poly in factored form. its succinct, easy to evaluate, but hard to invert. and it obviously compresses. but it has no obvious homomorphic features. prove security by random reduction of CNF, somehow. 

i began this by thinking how dividing a product of two primes by a small prime might give insight into what where the two primes are more likely to be found. this way in effort for RSA type factorization. 

consider the outer product of the vectors of prime numbers, so you have a symmetric matrix of all products of prime numbers.
take any such product and divide it by a prime number p, eg 2, and figure out the modulus. continue this for an entire row or column, and you end up with a single fundamental sequence particular to the prime divisor you chose. this is an infinite random sequence and from it you can derive the other p-1 sequences that will fill the rows and columns of the matrix if you found the modulus of every product in the matrix. 

using these 'modulus' matrices, you can take any product, compute its entry, then eliminate all possibilities with other entries in the corresponding matrix (this eliminates about 1/(p - 1)) of the possibilities. so if you had the sequences stored, you could factor in about log time. of course you can't store the sequences (which seem completely random, ie with maximum entropy) for when the product is too large.


so given a product, the moduli show its 'trace'. its an interesting way of indexing information in the case index can be infinitely long, because it doesn't index in lexographic order, but in the order defined by these random sequences. maybe in inspiration for information storage. 


thinking of integer factoring as decomposition in a vector space, maybe generalized hard problem could be made of decomposing vector in very high dimensional space, into its components. should look at relationship between something like this and lattice problems (small dimension but no continuity). 


maybe use the sequences to prove its purely random so randomness does exist. maybe show that its an infinite sequence that never loops, and within any segment (across all segments of that length) the number of each character is approximately the same (make a distribution of this), and show that any sequence with these properties must be a random sequence. maybe show the last part by saying if a sequence is not random, then it must either loop or have non uniform probability on the number of each character type.
hmm, maybe show that a sequence is random iff measuring character frequence across all segments of all lengths of the sequence gives some kind of distribution (eg uniform) or be some constant, and at the same time requence it doesn't consist of a repeated pattern. i think this would be the best approach. 
eg 101010
	segments of length 2
	(10) -> 1/2
	(10) -> 1/2
	(10) -> 1/2
	(1/2 + 1/2 + 1/2)/3 = 1/2
	semgnts of length 3
	(101) -> 2/3
	(010) -> 1/3
	(2/3 + 1/3)/2 = 1/2
	segments of length 4
	(1010) -> 2/4
	(10) -> 1/2
	(2/4 + 1/2)/2 = 1/2
... actualy i should be doing it over all sements of a certain length, not just partitioning. the point for this one is it would converge to the constant 1/2 as desired, but it is not random because it repeats a simple pattern. however, it still could be random so we need a way to measuring probability of a pattern if its truly random and grade according to that. 

i think the way to approach this further would be to first investigate the natural prime generated sequence and see what can be proved about its frequencies for different segments, and then think again about how to model a purely random sequence by frequence in segments. the latter is of secondary concern while the former is of primary concern. 


ACTUALLY
i realize these sequences are a lot easier to generate so I should focus on this simpler model for now. just take sequence of primes modulo some other prime.
maybe worth investigating primes modulo composite numbers too. 

maybe if these sequences can be shows to be random it can be extended to say the primes themselves occur at random places, assuming we normalize for they're gradual decay in frequency. 

just realized a better way to think about fields, or multiplication with addition in general (eg polys) is to consider multiplication as a formula of repeated addition that depends on the input. maybe there are other schemes of input dependent addition that also have nice properties.
addition is a single operation regardless of inputs. but a multiplication is also a function of addition, but the number of additions depends on the input. this is neat perspective on complexity. for example, consider those formulas that can be written down before inputs are known only in terms of addition. that includes addition and multiplication by constants. this means multiplying two unknown inputs gives a new complexity class.
i'd be interested to explore in general how a single operation (eg addition) and give rise to other operations (eg multiplication) with higher complexity. an obvious area to look at is lambda calculus. 
maybe a good measure of complexity could be how many points a function can encode, also considering how many unknown inputs there are. for example a univariate poly can encode as many point as its degree. whereas addition by itself i think can only encode one point, no matter how many unknown inputs. encoding points means the function contains arbitrary inputs that cannot be reduced. addition can always be reduce to unknowns plus a constant. but when the unknowns have dependencies that cannot be reduced, that is when there is opportunity for encoding constants. so we have a tension of complexity. the more reduction possible, the fewer encoding points possible.
maybe i'll try to create a general theory of this based on an arbitrary operation, and talk about derived formulas and complexity. the operation could be binary, but i could consider other possibilties to see if they result in different complexities. 
thinking about the commutatuivity of multiplication, we could have two interpretations of mulitplication by a constant. for 2*x, the first is x + x, and the second is 2 + ... + 2, so in the first the unknown is what will be added, and in the second the unknown is how many addition will take place. this comparison could help build a complexity relationship between unknown variables and unknown operations. 
vector spaces are a nice place for limited complexity. 
think about how to make an item of the set describe an operation. this is why integers and real numbers are great sets, because eg 2 naturally describes something plus itself. this relation between the operation and set is necessary to make multiplication naturally follow from addition, otherwise multiplication is considered a separate operation. so eg consider how a graph could describe an operation on another graph. this way an addition operation on graphs could give rise to a multiplication operation.

actually a field achieves the connection of addition and multiplication (instead of treating them as seapare operations) through the distributive law. This is another approach. That is, think about nature of distributive law and find two operations that satisfy it. 

remember, we don't want multiplication to just be repeating an operation a number of times, because then we are forced to use an integer to represent how many times and then the multiplication operation must be compatible with integers. 
but at the same time, this might be useful for vector spaces. 

can operations specify sets themselves?
usually an operation would be specified by an explitit map, eg (2,3)->3,...
but maybe an operation could be specified by an operation itself, eg (x,y)->x+y
fact(n) = n*fact(n-1), fact(0) = 1
 
one way interactions
these would be extensions, actually alternatives, to one way functions. 
would be based on average case complexity of PSPACE = IP
so the complete problem would be a QBF, and input a solution to it, and output the instance. 
it would be hard to invert (find a winning strategy), but evaluating it would require interaction, or another source of one way functions (maybe these could be composed in order to do that). 

think of how can represent an integer by a sequence of powers of primes. 
maybe comparing info entropy of integers versus entropy of this sequence give insignt into primes.


(drawing from quantum mechanics, try complex numbers. I think quantum mechanics has restricted the final outcome to bits (eg spin up or spin down).)

So we have a probabilistic process that proceeds through time (maybe later generalize to any dimensional space), and results in one of a possibilities of 'outcomes'. We'll leave the intuitive notion of a 'probabilistic process' unspecified. At any point in time, the process has a certain 'state' that describes its 'probability' of ending in the various 'outcomes'.
(any fuzzy word will remain in quotes until a precise definition is found.)

Note at this point no complex numbers have been introduced. But for sake of analysis, suppose we try to make a (perhaps complex) vector space out of this.

Hmm, we have 'outcomes' and 'states' to represent. What's their relationship? A 'state' describes (the 'probability' of) 'outcomes'. We could map a 'state' to a single 'outcome' to show the 'probability' of that 'outcome' occuring given the process is in that 'state'. Or we could map a 'state' to a set of 'outcomes' to show the 'probability' of ANY ('outcomes' are of course disjoint) of those 'outcomes' occuring given the process is in that 'state'. Suppose we draw from the traditional theory of probability, and assign the 'probability' of an 'outcome' a number between 0 and 1. In this case our first map would have domain the set of 'states' and codomain the interval [0,1]. Our second map would also have domain the set of 'states', but codomain would need to represent the probability any 'outcome' in a set occurs. With the traditional definition of probability, we note that since these 'outcomes' are disjoint, we can simply add up their respective probabilities. Thus the codomain of the second map is also [0,1].

The general map encompassing the two above is a map from a subset of all possible 'outcomes' to the interval [0,1] the describes the probability one of these 'outcomes' occurs. This map is essentially a probability mass function. If we had multiple such maps perhaps we could treat them as real valued vectors. But a single probabilistic process seems to only have a single probability distribution, so more than one map doesn't seem to make sense.

Could we make the 'outcomes' the vectors? Addition would correspond to an 'outcome' of either of the added 'outcomes', that is the union of the two 'outcomes'. The inverse of a subset would have to be the complement of it, ie all the other 'outcomes'. In this case the identity would be the whole set. But the union of a subset with the identity would result in the identity, not the subset. If instead the identity was the empty set describing 'no outcome', there doesn't seem to be a proper corresponding inverse. So we abandon this approach too.

Let us pause for a moment and see if we can clarify assumptions. Could we model an 'outcome' as a special case of a 'state'? It could be the case where the 'state' has no uncertainty, and all probability mass resides on one 'outcome'. So 'outcomes' are 'states' of completion, and thus we can narrow our focus to 'states'. This leaves us with little choice but to make 'states' the vectors of the vector space. So from here forward, let us unquote 'outcomes' to be special cases of 'states' described above.

At this point is it not clear what 'addition' or 'multiplication' mean in this vector space. What are the basis vectors? Would it make sense to make them the final outcomes? How many dimensions does this space have?

Recall how a matrix encodes a graph with no hint how vectors really represent vertices, and instead focuses on the relationship between the vertices. We could try analogously to ignore how vectors really represent 'states', and instead focus on the relationship between the 'states'. From here forward let us unquote 'states' and simply identify them with vectors, leaving the rest to intuition.

The next question is what is the relationship between states, and how do we use it to shape this vector space?
One natural relationship is how one state can transform into another, and with what probability it does so. 
Suppose we can enumerate the final outcomes. Suppose we use this subset of vectors as a basis. We could then model each vector as a probability mass function, where the components with respect to this basis are the real numbers in [0,1] representing the probability this state results in corresponding base vector outcomes. There would thus be a constraint that the components should add up to exactly one. Neither natural addition or scalar multiplication of vectors, however, would respect this constraint, and thus this space would not be have the required closure property.

How can we change the vector space to obey closure? Given a list of components, come function of them must always have the value 1. Addition of two lists must also obey this property, as well as scalar multiplication, where as scalar is from the same field as the components. I think we can reduce these two conditions to closure under a single addition and multiplication. To obey the usual laws of a field, such as commutativity and associative we should draw our field elements from a known field, either R or C for the full benefit of vector analysis. To obey distributivity, we should maintain the natural field operations, rather than introducing a modified addition or multiplication in order to obey closure. So now the question becomes which subset of which field obeys closure under a function that maps elements of the field to the interval [0,1]?
In other words, we want the following implication, where a,b are from R or C, and operations are the natural ones.
	0 <= f(a) <= 1, 0 <= f(b) <= 1 
	=> 
	0 <= f(a + b) <= 1, 0 <= f(ab) <= 1
suppose we consider the function of absolute value. The multiplication has closure but not addition. Hmm, I don't know.

When I think of inequalities in this context I think of Cauchy-Schwartz. This could be applied to lists of components. Suppose for each state the sum of squares of the components (suppose in R) adds to 1. Then the dot product of two lists is bounded by 1. However, this operation maps a list to a scalar, whereas we're considering mapping a list to a list. In this case we might consider a matrix. We want this matrix to preserve the sum of squares equal to 1. But the sum of squares is a common norm, so we'd like a matrix that preserves the norm, and indeed, such matrices exist and are called unitary. In such a matrix the entries of each row and column can be interpreted as a state. 

Above we found a way to map states to state, but what we were looking for is addition and scalar multiplication. Perhaps we must abandon the idea that all vectors are state vectors, and accept that addition and scalar multiplication of state vectors will not result in state vectors in most cases. Instead we may be content with using linear operators to maps states to states. Using this convention, we summarize that that standard basis vectors are the outomes, and are presented with the standard basis component lists. The square of a component's absolute value for a state is the probability that state results in the corresponding outcome. Since we square components to get probability, we can imagine the probability space as measured by area, rather than length. What absolute value squared results in probability 1/2? |x|^2 = 1/2 => |x| = 1/sqrt{2}.

It should be possible to do a change of basis, so instead of mapping standard vectors to other state, one can map arbitrary states to other states. Since we can take absolute value, and the Cauchy-Schwartz inequality still holds, we can use this approach over the complex field instead of the real field, so that there are many possible amplitudes for a single probability, rather than just 1. This implies many different states can have the same probability distribution, implying states can encode more information than just probability.

Perhaps a more straightforward way of deriving the above is to first consider linear transformations from one state to another, rather than first considering addition and scalar multiplication. 
In quantum mechanics they call each component an 'amplitude'. Also, the outcomes are resticted spin-up or spin-down, so the dimension of the space is 2 raised to the number of qubits. 


this seems too simple, but I think I can make a full succinct nonineractive proof given a vector commitment scheme that enables addition and scalar multiplication.

a vector commitment scheme. P sends V an element as a commitment to a vector (list) of elements. assume basis vectors are random. V can add two vectors or scale a vector with an operation. P can reveal the vector by sending all the elements and V checks. 
The assumption is that P cannot manage to make a commitment that is consistent with two different vectors. 
However, it is possible for the prover to present a commitment without having a vector. 

P presents vector
V asks for split vector
P presents two half vectors
V makes sure they combine to form the original vector
	V changes the base of the second vector
	V combines it with the first and compares to the original
		(
			this is where I think the flaw is
			remember basis vectors are supposed to be random
			so how do you convert one random basis to another? you can't with compromising randomness.
		)
V now reduces evaluating the original vector to evaluating the two half vectors in some way.
V reduces evaluating the two half vectors to evaluating a random linear combination of them.
V generates this random linear combination and proceeds to the next round.



---
but wait originally i was considering something else. the above only fails for making a succinct proof.
i was considering conditional aggregation of proofs. 
let V be a proof validating proofs P1 and P2. P1 and P2 make vector commitments (representing poly coefficients). V is to evaluate each of them at a different point. V reduces to the two polys to one, and reduces the two evaluation points to one, and these two reduction can be done in either order. 
1. polys, points
	P1 presents its evaluation result on the evaluation point of P2 (as well as its own), and vice versa. 
	In one case, first V reduces the two polys to one by generating a random linear combination of the polys using the vector commitments.
	V uses these claims to calculate claims about what the new random poly should evaluate to on P1 and P2's points.
	V connects these points with a line, and asks for the univariate evaluation.
	V checks the univariate is correct. 
	V chooses random point on line for new random evaluation point, and evaluates uni poly on that point to get claimed result of new poly.
	V outputs new poly along with evaluation point and claimed result. Proof validity is conditional on this.
2. points, polys
	V connects the two evaluation points and asks each for its uni poly
	V checks each uni poly. 
	V picks random point on line, and calculates new evaluation point from line and also calculates new claimed evaluation results from the two unis.
	V then creates random linear combination of the two polys using the vector commitments. 
	V calculate claimed result. and outputs new commitment, evaluation point and claimed result.


consider the arithmetic circuit or constraint system that would best describe 


----
while this would work theoretically, using a regular cyclic group for the discrete log problem would mean a huge field size of at least over 1000 bits, which would take way to much time for the prover to computer a generic circuit and polys with.
but then I realized elliptic curves have keys that are much smaller and faster. but elliptic curves and complicated and i'd need to learn them first. there's also the question of how much would be done in the elliptic curve versus outside of it. if the curve is over a finite field maybe the elements of the curve, 2d coordinates, could be interoperable with the general field. an issue is that composing one way functions requires raising a point in the elliptic curve to the input, which in composition would be another point on the curve. this might be possible by instead requiring two points to be raised to each of the respective corrdinates of the input point, assuming those coordinates are from an ordered field, ie one of prime order. 

for now i'll leave it to think more about vector commitments and learn more about algebra, including elliptic curves, before I decide whether this implementation is with pursuing further. 

since this is a vector space, we are maybe given the holomorphic effect. if two vectors are the same, then their difference (another vector) is 0. the converse is also true. so we are really doing zero testing, though a vector is not necessarily a assuming the basis is orthogonal, this means we want to testing that all components of a vector are 0. maybe using decompositions there could be a number of ways to do this. maybe these ways of testing all the components to be 0 could be independent of the vector space. on the other hand, zero testing may have particular benefits for particular vector spaces. 

think of FRI (and hopefully others) as subcase of proofs over vector space.
for FRI, basis vectors are x^n
in general, we take a vector that is claimed to be in the span of a limited basis (eg x^n for 0 <= n <= d)
and the prover decomposes it into two vectors of half the size, such that when one of them is 'shifted' (eg multiplied by x^{d/2}) and then added together, they form the original vector
now the problem is reduced to testing both of the half size vectors. 
then testing two is probabilistically reduced to testing one. 
then the process repeats.
notice I have not specified how the reduction from 2 to 1 occurs, and whether it necessarily occurs within the vector space. 

now what about multilinear poly? this structure also decomposes well. but again, i don't have in mind a way to reduce the testing of 2 to 1. 

I think the general task of reducing 2 to 1, was a problem before. reducing evaluation of two points to one, on the other hand, was easy. i should look at the way FRI does it. maybe it should involve a random linear combination of the two vectors to form a third vector. the point of the randomness is to prevent the linear combination of two invalid vectors from forming a valid vector. this is justified because an invalid vector includes an invalid basis vector B, and r1*B + r2*B = (r1 + r2)B = 0 occurs with low probability. the verifier chooses the random coefficients. the prover returns a vector and by recursively checking, let's assume that vector is valid. then what remains is to check that it is indeed the requested random linear combination. this returns to the question of zero testing, when the third vector is subtracted from the linear combination. actually we missed the point that the original two vectors might be of invalid form. we would like to solve this by using an oracle model that forces the prover to encode functions using the basis vectors, in other words a linear-only encoding. of course lattices is a natural place to look for this. but it seems FRI was able to get around this without assuming linear-only encoding. 

it would be great to relate this theory to that of eigenspaces and spectral theory.


solve average coNP => solve average NP
PH is where the number of quantifiers is constant
pspace is where the number of quantifiers is polynomial. but each quantifier eliminates one variables so it seems the number of quantifier equals the number of variables. and the number of variables is < size, so this implies quantifiers < size 
think of a game, like chess. the problem size would correspond to 

seems like initial position is most important, its like the formula


what determines the Sigma or Pi is the first type of alternating quantifier. 
if Sigma, it asks if there is winning strategy, 
	in this case a simple round can refute this claim
if Pi, it asks if there is no winning strategy
	in this case a simple round can refute this claim

oh each variable can be of poly length, not just a single bit
oh, and the input is x, and serves as a variable in the formula. the other variables are all quantified
the formula is thought of as the verifier algo. and the question is whether it outputs 1. V must be poly time.
i think x is thought of as the initial position of the game.
Q1y1...Qkyk V(x,y1,...,yk)
when yi is chosen the position, or state of V must be updatable in poly time. 
think of x as the starting position of the chess board. then first player moves, creating y1, and instance reduces to a Pi system.

this makes sense for constant k, that is for PH, but what about poly size k, that is for pspace?
so PH is the set of all problems in Sigma_k for some k. the point is that if a problem is in Sigma_k, then all instances of the problem are in Sigma_k for that fixed k, and k cannot depend on the instance size. 
in contrast, with pspace, 

any problem in Sigma_k means there exists k-move winning strategy for first player.
any problem in Pi_k means there is not k-move winning strategy for first player, or alternatively there exists k-1 move winning strategy for second player. 

so PH doesn't necessarily consider x as a circuit, just as some object the verifier can modify given yi. 

Pi_k = \forall x !Sigma_{k-1} = !(\exists x Sigma_{k-1}) = coSigma_{k-1}

now for pspace k can depend on size of x. which means V can accept a poly number of moves, make a poly number of modifications. but V can always be converted to a circuit. I think there exists poly such that once the size of x is determined, the number of alternations is upper bounded by that poly. and also the size of each variable is determined, so at that point a boolean circuit can be setup. but the crucial point is the size of the problem is NOT the size of the circuit. in fact I don't think the problem can be necessarily represented as a circuit without first incorporating the quantified variables. the circuit size can be polynomially larger. furthermore, the number of variables used can be poly in the size of the problem. 
the time of each move can be poly in the size of current position and previous move, this is equivalent to poly in initial position. 

for PH the size of the circuit, or rather the number of possible circuits is 2^{kpoly(n)}
for pspace the number of possible circuits is 2^{poly(n)poly(n)}
so in both cases there are exp number of circuits, and the number of variables is poly in n
the difference is PH has constant number of alternations among these variables, whereas pspace could have as much as every other one alternating.

so the core takeaway is problem size may be much smaller than circuit size. 

maybe proof of being in PH can be done inductively. show in Sigma_{k+1} by presenting y such that (x,y) in Pi_k. This assumes one can identify if in Pi_k. 

maybe there exists better IP for pspace. for a Sigma, P sends y1, V modifies x to x_1. for a Pi, P and V engage in sumcheck. the goal is to show that for all y2 applied to x1, the resulting x3 is in Sigma. sumcheck does this by asserting that the sum of all applications of y2 to x1 is correct only if all applications are correct (not even just probabilistically). Then it reduces to probabilistically computing this sum. but in order to do this sumcheck must represent the instances as multivariate polys, who's degree depends on what sigma level you're on. we'd like a reduction that is independent of the sigma level.
to restart, to show that x is in Sigma_k for some k, present a y such that the result of efficient reduction function f(x,y) is in Pi_{k-1} only if x is in Sigma_k. This is an efficient reduction from Sigma_k to Pi_{k-1}.
Now we'd like a reduction from x in Pi_{k} to some x' Sigma_{k-1}. suppose there exists efficient function f that decides whether (x,y) in Sigma_{k-1} for some y. If f(x,yi) for all yi, then x is in Pi_{k}. So we want a way to efficiently probabilistically evaluate all f(x,yi), or merge all (x,yi) such that the result is in Sigma_{k-1} only if (x,yi) in Sigma_{k-1} for all yi. the sumcheck does this by representing x as a poly, and yi as argument, and output zero only if (x,yi) in Sigma_{k-1}, and saying sum_i x(yi) is zero only if all x(yi) is zero, non-probabilistically. then it reduces to computing this sum, probabilistically. challenge is to represent x succinctly. 

QBF is PSPACE complete. its in P, and it can emulate and pspace computation. so now goal is to relate QBF to IP. 
and the size of the problem is the size of the formula, and the number of alternations can be at most linear.
both satisfiable (winning strategy) and unsatisfiable (no winning strategy) QBF are both pspace complete. 



in addition to the owi using IP, maybe it could be used without IP where there exists a winning strategy that the 'holder' knows who generated the game, so the holder can beat anyone else at the game, and anyone else can challenges the holder to the game could win but will probably lose. breaking the crypto assumption means an adversary could beat the holder at its own game. so actually this seems to yield a falsifiable computation assumption. nobody can feasibly beat you at your game, but they can always challenge you and show you they can. in fact, this formulation may not even require that the 'holder' be able to beat anyone, but simply that the 'holder' generate one that has a winning strategy. if an adversary doubts the formula has a winning strategy, the holder may need to prove it with a probabilistic proof. for now assume its a valid QBF.
so make a crypto scheme that reduces to finding a winning strategy. 



if the formulas are multivariate polys, then the random linear combination of two of them would also be a game. 
for a moment consider the simpler case of adding together two boolean formuals. 
think more about the homomorphic properties.

\Ax\Ey\Az f(x,y,z) 
= (\Ey\Az f(0,y,z))(\Ey\Az f(1,y,z))
= ((\Az f(0,0,z) + (\Az f(0,1,z))))((\Az f(1,0,z)) + (\Az f(1,1,z)))
= ((f(0,0,0)f(0,0,1) + (f(0,1,0)f(0,1,1))))((f(1,0,0)f(1,0,1)) + (f(1,1,0)f(1,1,1)))

eg first univariate is
= ((f(x,0,0)f(x,0,1) + (f(x,1,0)f(x,1,1))))((f(x,0,0)f(x,0,1)) + (f(x,1,0)f(x,1,1)))

each universal quantifier doubles the degree. 
oh i see. first take the function of n variables and evaluation at all points and construct the corresponding multilinear poly. then for x_n quantify it however, setting x_n to a constant and doubling the degree of all other variables. then repeat with this new poly (which could be of any degree but is of degree 2 in each variable) of n-1 variables. 
this works because any boolean formula can be made into a multilinear poly. more generally, any poly over field size N can be made degree N-1 in each variable. eg field size 2 means degree 1 (linear) in each variable.
this way a quantified boolean formula can be arithmetized to any degree but then linearized into a form suitable for the sumcheck. 
what burdens does this put on prover and verifier?

maybe the other method of converting to the non-canonical form and then arithmetizing would be better. 

suppose there is some efficient way for the holder, the one who generated the random QBF, to generate an IP proving its a QBF (there exists a winning strategy), that is also efficient for the verifier (eg in final random evaluation). 
so even though its a QBF that in principle anyone could be a prover for, efficiently generating a valid proof requires the trapdoor information of the holder. 

suppose by network wide interaction or commitment (using non-math hash functions) every node could generate a non-interactive proof for its formula (QBF) so any other node can check its a valid formula. how to use this?
a formula is basically a random multivariate poly in some succint form (eg CNF).
what are the homomorphic properties of these formulas. 

Ax f(x) = 1, Ay g(y) = 1 => Az f(z) + g(z) - 2 = 0
Ex f(x) = 1, Ey g(y) = 1 => Et f'(t) = f(tx + (1 - t)y) = 1, Et g'(t) = g(tx + (1 - t)y) = 1
	which can be reduced to verifying there exists single unique t for which both evaluate to different values
Et f(t) = a, Et g(t) = b => Et f(t) + g(t) = a + b, use random linear combination if necessary
so above it shows both A and E quantifiers for two formulas can be reduce to checking the corresponding quantifier on a single formula. 


for composing proofs it is useful to distinguish between those conditions which the proof must check from those conditions that only the node creating the proof must check.


what about doing interaction with 2 parties? so instead of half NP, half coNP, its all NP, just alternating users. 
Ex1 Ax2 Ex3 Ax4 f(x1,x2,x3,x4)
= Ex1 !Ex2 !(Ex3 Ax4 f(x1,x2,x3,x4))
= Ex1 !Ex2 !Ex3 !(Ax4 f(x1,x2,x3,x4))
= Ex1 !Ex2 !Ex3 !Ex4 !f(x1,x2,x3,x4)
= Ex1 !Ex2 !Ex3 !Ex4 (1 - f(x1,x2,x3,x4))
idk, but basically users go back and forth together, each picking a value (going over the variable in backwards order)that will give them a winning strategy, last player to pick of course has winning strategy. 
i don't know what i'm talking about.
its like they setup a common key. to use the key they send each other the values they picked in forward order, as if finishing the game authenticates the other party. so a signature scheme like this could probably be turned into an encryption scheme.
or maybe a public key (a short seed to generate a stream cipher) could be used where holder (who has the public key) and user (who only want to send the seed to the legitimate holder) interact back and forth. eg both parties generate a formula, then they combine it into a common formula, then they agree on a random sequence of variables to assign to the universal quantifiers, then they both execute (assuming they each have the capacity to execute on the common formula), and a unique set of existential variables in order to win the game would be determined between them.


before pursuing further protocols build on QBF or any other pspace complete problem, I need to learn first about the foundational assumption of the hardness of average case pspace complete problems. 
supposedly average case hardness is established by the random self reduction of a problem like QBF. Can I find this reduction myself?
this means decide whether a formula is a QBF by generating a poly number of mostly random formulas, and upon receiving answers on whether they are QBFs, determine answer for original problem. Maybe another way is find a winning strategy by being given winning strategies to others. I think the condition is queries must be pairwise randomly independent. hmm, just as multiple independent QBFs can be combined into one, I suppose a QBF could be decomposed into multiple independent QBS. since combining has a soundness loss but not completeness loss, I think decomposing could have a completeness loss but no soundness loss. ie if random case is true, worst case is true => if worst case is false, random case is false. but we don't care whether a case is true or false, we just care whether it is solvable. so it seems this random self reduction would be valid with high probability. 
consider only boolean, not arithmetic
Ex f(x), Ey f(y) => Ex f(x)f(y), Ex (1 - f(x))f(y), Ex f(x)(1 - f(y))
with SAT I saw combining two adds a variable, or decomposing reduces by one variable. but in this case its adaptive. also, the instances queried are not random.


how is discrete log randomly self reducible. I suppose solving g^x, could be done with a single query by solving (g^x)^r = g^{xr}, then dividing by r. that was easy.
of course it would make sense to model this with a struct, and hence even define the PH with structs. in fact probably all complexity classes could be defined similarly. 


idea: generalize poly hierarchy where each level is existence of at least some number that satisfy. E is the special case >= 1, and A is the special case >= all. this would be a good framework for probability, where the progression of events in a protocol is like the moves in a game. and instead of winning and losing strategies we can talk about them probabilistically. 

QBF may be reducible to coNP in poly time
take the arithmetization and expand it using 1-f*g for A and (1-f)(1-g) for E. this way for all variable assignments the formula should return 0. 

to be in PH means for every instance of the problem set (different instances may be of different size), there will be k variables in the QBF. to be in pspace means for every instance of the problem set, the number of variables in the QBF may vary polynomially depending on the problem size, eg if the problem size is an nxn chessboard, the number of variables may be cubic in n. this means the variance of the size of a problem in a distribution will grow by a polynomial factor in the size of the corresponding QBF distribution. 

OWF EXISTENCE
Goldreigh
owf => problems in NP hard in average case => problems in NP hard in worst case => NP !in BPP

question is whether converse holds. ie can cryptography (owf) be based on a worst case assumption (NP not in BPP)? the big question is the middle implication (average case vs worst case complexity). 

what worst case assumptions are necessary for owf? since NP in BPP => !owf, NP !in BPP is the minimum necessary. is it enough?

all known worst case intractable problems with average case intractability are in NP & coNP

?
NP !in BPP => owf, !owf => NP in BPP
show that if you have an inverter (!owf) then you can solve SAT (NP in BPP)
	given a circuit, apply the inverter to the output 1, and if the inverter returns any answer it is a satisfying assignment.
	what if the inverter has negligible error? then the SAT solver has negligible error. 



WORST CASE TO AVERAGE CASE
list decoding. what's the connection? 
suppose in the codomain, each string is an instance. suppose list decoding is possible. this means given any instance you can recover all yes-instances (codewords) that are in close proximity. then given these results can you decide whether your codeword is a yes-instance?
instead consider the code the probabilistic algo. domain is instances, codomain is solution space. the algo should map each instance to its solution. but since the algo can be faulty, an instance might get mapped to a wrong solution. if that solution is close in distance to the actual solution and list decoding is possible, the actual solution can be obtained. so this reduction to list decoding exist if and only if the errored solutions are 'approximate' in the sense of distance from the real solution. 
but thats more about helping approximation on all cases. instead we want to help completely incorrect answers on the worst cases assuming the algo can solve average cases. 
instead of considering the code to be the algo that operates on all instance, consider the code to be an instance of the problem itself. i'm not sure about domain and codomain.
instead consider the domain the set of instances with worst case hardness, and a code that maps to a codomain with average case hardness. ...
what the paper says doesn't yet use codes, but says given any instance f, output an instance g that is solvable on negligible fraction. then show if g can be solved on more than negligible fraction (its not average case hard) then f can be solved on all inputs (is not worst case hard). this is a reduction from f to g. it says consider domain as set of all instance solution pairs to f, and codomain as instance solution pairs to larger instances. 

PERMANENT
suppose you have circuit C_n to solve permanent size n. If permanent(n+1) is randomly reducible to permanent(n) then C_{n+1} can be constructed givn C_n, and continuing downward permanent would be in BPP. similar to SAT that I tried with factoring, this shows that a reasonable conjecture implies problems like these of size n are not poly time reducible to problems of size n-1. In fact, one might argue that all problems solvable in poly time must have poly time reductions from size n to size n-1. showing that no such reduction exists would imply its not solvable in poly time. 
In fact I think this theorem is easy to prove. again it would mean that a problem is in class L if and only if the problem of reducing from size n to size n-1 is also in L. At least this holds for L=P, and it can probably be shown to hold for other classes, even for different kind of resources. so making assumptions about the instractability of one side implies intractability of the other side. eg for sat, factor => reduce => solve, so intractability of solving SAT implies intractability of factoring (or multivariate interpolation), but unfortunately this is focused on worst case, not average case. An important part of the theorem is that the reduction is a Levin type, not a Cook type, in particular it can only reduce to one instance, not two, or overhead grows exponentially (at least with time). 

make at least 2, hopefully n, or even n^2/2 entries of your matrix your data to commit to. then compute the permanent efficiently by choosing the necessary remaining entries of the matrix. then the permanent value is the commitment. I think there's efficient way to combine permanent of matrice sum, at least probabilistically provable combination. Revealing the data is done by a probabilistic proof, assuming prover has enough advantage. 


RSR: suppose there exists reduction from all cases (worst case) to algo that's correct on noticible fraction of domain (1/poly). then existence of average case algo implies existence of worst case algo. and non existence of worst case algo implies non existence of average case algo. i don't think 'randomness' is necessary. ie if by solving any noticible fraction of instances you can solve all instances, then inability to solve all instances implies inability to solve on any noticible fraction. actually its important that the reduction algo is randomized. this reduces worst case assumption from NP !in P to NP !in BPP.
the fraction that the average algo works on matters a lot for constants. it must fail on a noticible amount. suppose it fails with probability 1/p(n). We must invoke it on enough instances it fails with high probability, which is 1 - (1 - 1/p(n))^x for x instances. of course we want x=1. this means 1/p(n) is large, so p is small. 
maybe this can be reformulated where the average algo only needs to operate over a small subset, idk


TV02 -- worst case to average case for EXP in uniform setting
STV01 -- save as above but for non-uniform setting



http://twistedoakstudios.com/blog/Post2644_grovers-quantum-search-algorithm
http://algassert.com/post/1718



So given the formulation of a state space with unitary matrices for transformations between states, how would a quantum computer implement this transformations? 

Wires carry quantum bits, and gates perform the matrix operations on them. So I think the dimension of a gate matrix is 2 raised to the number of input wires. 

Consider a single qubit. A NOT gate needs to subtract 1 from the probability of each component. In this case the probabilities switch. But actually we want phases to switch to, so it makes sense to just switch the amplitudes. Thus the NOT gate corresponds to the 2 dimentional non-idenity permutation matrix. 

The Hadamard gate has no classical equivalent, and switches both 1 and 0 to mixed, but can uniquely convert either mixed state to the original outcome. Mixed means both outcomes have probability 1/2. But since the mixed states have different phases, they are still unique and the original value can be recovered. For 1, the mixed phases are agree, and for 0 the mixed phases disagree.

Now suppose we have two wires, and we want a gate that only affects one wire. the circuit should naturally be expanded by 2. 

Instead of 'outcome' they use the word 'pure state'. 

Any two transformation can be composed by multiplying the matrices. 



how to represent sparse graph
make vertices an othonormal basis vectors
any sum of the basis vetors give a unique subset of vertices
make a linear operator by mapping each of the basis vectors to the vector representing the vertices it is connected to (or only those it points at, or only those pointing at it). 
actually this would yield the standard matrix encoding!
what we want is to represent graph with bounded edges per vertex, eg 2. previous contructions encodes 2^v information per vertex (total (2^v)^v), but we only want to encode {v choose 2} = v!/(2*(v - 2)!) = v*(v - 1)/2 information per vertex (total (v(v-1)/2)^v. assuming we use bits, this can be done with a v(log(v) + log(v-1) - 1) size matrix.
i would be more interested in small matrices with bigger entries than just bits. eg every element of the field represents a vertex. extreme is 2x1 matrix, input as vertex query, output as its two edges. the structure of the field would impose limitation on the graph shape. but exploring such limitations would give interesting insight to graph and field shape.

vectors don't need to be considered ordered lists. the order only serves to enumerate the basis vectors which could be replaced by any map from basis vectors to components. 

make vector space of colors
or take vector space and convert it color space to be used in real world


can think of lattice problem as list decoding problem. given a codework (long basis vector) find codewords closest to you (closest lattice points). 


NP hard problem of making best plots/stories. easy to verify, hard to make


from now on use the word 'struct'

maybe a struct with loops (circuit with feedback) could 


similar to how arithmetic has variables, think of structs with variables. its analogous with similar implications, and should allow for expressions to encode data like a poly. but the difference is arithmetic separates expressions from data, the numbers, whereas structs like lambda calculus only have one type. in arithmetic once evaluated an expression is no longer an expression and can't be evaluated, or you could say evaluation returns a constant, ignoring the argument. with lambda calculus i think there exist similar 'constants'. for structs, a constant would be one with no head. 


a disconnected struct cannot have its parts reconnected as a function. its like the parts are in different universes being processed in parallel. but when as an argument they might get reconnected once applied. 
a clique as a function discards all inputs, and as an argument removes all heads and all edges connected to them, creating a disconnected struct with one part the clique that will follow the struct around until forever. 
disconnections, rather called isolations, can help model protocols. when as a function, two isolated parties 'receive' common info (the argument), but they remain isolated. when as an argument, the two isolated computations get 'sent' and whever they get sent they might intersect and get reconnected. so it could model not only protocols but distributed comuting. 
an interesting question is when to consider two structs as different. an extreme would if two structs are not connected they are different. this might be a helpful formulation, because in this case a disconnected struct would be thought of as a set of structs, and since a computation might result in disconnection, the result could be considered a set of structs rather than just one. similarly, passing a disconnected struct as an argument could be thought of as passing a set of arguments. note that these sets are unordered. 
maybe the union of structs could be thought of as 'addition' to form a vector space, and scalar multiplication could be application, but this woudn't be quite the same, yet it will still mean a space with two operations, but they wouldn't relate like addition and multiplication. but perhaps a subcase could model a vector space. 

since matrix permanent of 0,1 matrix #P complete, consider how to compute corresponding value of a struct. this problem in turn would be #P complete. the struct would be undirected. 

think about defining operations on the struct as isomorphic to another mathematical structure. eg model each struct with v^2 bits for the matrix and map these bits to integers then perform arithmetic operations and translate result back to struct form. ways to represent real number could be interpreting each of the v-long bit strings to represnet a number in [0,1] and the computing the result with a basis of other carefully chosen numbers. one basis vector would probably be the integer 1. basis vectors don't need to be rational, eg e^i. the question is whether struct functions necessary to compute these representations given representative inputs would reasonably sized. 

think computability. like turing machine there probably exists no struct that can compute a function for all inputs. but like circuits there probably exist family of structs to compute a function for different sizes. think of how to generalize uniformity, like the concept of advice. a family of structs could be defined as the union of structs, because when operating as a function they would operate in isolation, and an argument is applied to them some may give garbate result or never halt, but the claim is there exists at least one struct that will halt with the correct answer. 


notation and vocab should be formalized to allow for better communication than using 'func' and 'argument' and 'applied'. this is good opportunity to use left to right ordering of argument and function. so maybe 'left' and 'right' could replace 'argument' and 'function' respectively. and x 'applied' to y could be rephrased x 'left of' y, or y 'right of' x. 
does associativity hold?


maybe it could even emulate sets. but a set of sets would be necessary.
i think it could even emulate infinite sets using isolation.  
and to go infinitely small (or large) don't think of a struct as pre-existing in a set, but rather as the opportunity exists to compute any number to any desired precision. 

what can be reduced to the graph isomorphism problem? how hard is graph isomorphism? one way to do it is to consider every permutation of rows (together with columns) of a matrix
if GI is hard, it might make sense to randomly generate a collection of differnet ones, and duplicate some with random permutation, so some in the collection operate the same as others but its hard to figure out which ones do. this would make an 'indistinguishability' problem.
of course determining if two structs compute the same function. ie compare the result of two different joins (applications) with different lefts (arguments) and different rights (functions). they will have different representations of the output matrix, ie vertices in different order, but those they may be isomorphic. 
so what can be reduced to whether two functions compute the same function? 
maybe determining if two CNFs are equivalent would be useful. determining unsatisfiability can be done by isomorphism with the 0 function, if done over GF(2). to make this work it means there must be way to extend a struct (eg 0 struct) such that it computes the same function but grows in size. 

property testing for graphs has a lot of possibilities and would be useful. 

if structs are associate they can form a category. momic and epic structs are evident.
from a first look they might be associative. in this case they could form a group.
as for a category, they would naturally form morphisms, but the objects could take many forms, eg classes of structs. 

how to study structs? I don't see natural adjacency matrix algo for application. matroids would be most useful. 

could have category with objects as vertices, and morphism as set of all paths between two vertices. there would be identity morphisms. of course associative composition would hold. In fact, I think any category could be a struct. 

what about using undirected graphs, and modeling each vertex with a single edge as a 'port' (but entry and exit). joining ports for application would mean any applicaiton results in a graph with 0 ports. this might be useful if we want to limit to a single operation.

this calculus is not associative, even under the condition all structs are dags. 
but this 'higher circuits' concept still interests me because circuits interest me as a model of computation.
unlike lambda calculus that seems like it must remain pure, circuits could remain pure but it seems eventually evaluating them with some operations for vertices and same values for edges makes sense. this requires defining a set with a commutative operation. the a edge value could specify an operation, eg conjunction, applied to other edge values. its important that partial evaluation get explicity absorbed into the circuit. modelling with a single operation i think is sufficient, and of course more elegant. 

in a regular circuit not all inputs must be assigned the same argument, whereas we made this the case for structs, for the sake of simplicity and imitating currying. in this case, does the func or the arg have control over where the next head will appear? we know all the following heads will be on the arg. but an instance of the arg will only have a head if the tip had no sources. so the arg decides what to do with the next arg, but the func gets to decide which args accept the next arg.
if a circuit is to support non-commutativity the only way I can think of is by the order of arrival of arguments. 

is it isomorphic to multivariate polys? well kinda, but polys have two operations. and polys can encode constants. and they have no 'recurrences'. 

circuits are appealing but we need to get straight the model of giving inputs and outputs, distinguishing among inputs and outputs. 
an important question is whether an edge disappear after it assumes a value. if the circuit is not absorbant, it cannot disappear but must either become a constant or have the gate operation changed to account for it. 
(the case where it can be absorbed into the operation without changing the operation could be the definition of a boolean circuit, where AND and OR would be the two possibilities, either eliminate all other edges and take the value of the edge, or wait for all other edges and take their value. these would be natural definitions of AND and OR in the context of circuits.)
on the other hand if the circuit is absorbant either the edge vanishes or it remains. the natural answer would be that it vanishes, but then recurrences can become difficult. 

using order of arrival to distinguish inputs I think is an ugly model. since inputs may have different path lengths anyway, to make them arrive in a particular order one may need to artificially expand a path to an extent dependent on the path length, and thus the computation, of the other input. this means the computation of one input depends on the computation of the other input, even if the inputs are supposed to be independent. 
it may be better to just assume the ability to assign different inputs to different heads, rather than assign the same input to all heads. when a circuit occupies a gate it would need to be implicit from context how to match inputs and outputs to the external sources and sinks respectively. 

what is the relationship with knot theory? graphs don't respect dimension, whereas knots depend on how many times the edge enters the 3rd dimension. graphs can be more complex with the 3rd dimension (nonplanar) but it is insensitive to the direction and so no knots are possible, ie its like 1+1=0 instead of 1+1=2 so its like a different modulo. 
generalizing knots means taking edges as planes of dimension d in a d+1 dimensional space (eg dots on line, lines on paper, walls in world). this is like a planar graph. to create a knot, these edges must enter dimension d+2, so they can rearrange themselves in dimension d+1 without colliding. this is a neat concept but it doesn't seem to relate directly to circuits in a helpful way.
but it does provide a rich-looking binary map that may be helpful. 

what about twisting theory, like the belt example. a belt could be modelled as as a plane that twists in 3d space, or maybe better as two lines in 3d space that form a knot. this fits well with knot blocks which I'll now describe.
think of a polygon (ie a shape that can have at most n similar shapes on its boundary). the match it with others and reduce. i'm pretty sure this would make an associative operation so it could form a category. 

maybe a crossing could be made to a gate.


maybe a better way to represent structures would be with constraint systems. a constraint system can express a cirucit, and more, like recursion, that a circuit can't. Recursion basically means the number of time a computation repeats depend on the its result, whereas a circuit with no loops must repeat the same number of times regardless of result. I think constraints could be a nice general framework because they are independent of the types of constraints and the things being constrained. 


a space can be reused over different times (but used only once at a given time)
similarly, a period of time can be reused over different spaces (but used only once in a given space)
so in this sense space and time are not too different

I tried thinking of the lorentz transformation for converting between time and space resources. the loretnz transformation is about relating how two observers see time and space when observing the same object, given their separate velocities relative to that object. the first challenge is deciding what 'object', 'oberservers', and 'velocity' mean in the context of computing. 

one difference is how NP!==coNP but NL=coNL. i think this is just due to the design of nondeterministic machines and has little significance. with non deterministic turing machines the following is partly false (the latter part), but with practical computing the following is true. when using a single space over many times, part of that space can be used to gather all of the results across all times. when using a single time over many spaces, part of that time can be used to gather all of the results across all spaces. the last sentence is what is not true for a non deterministic machine due to its 'independent branches'. a non deterministic turing machine for space complexity analysis should instead be designed where a computation takes place at a single time on a single machine, but instead of cloning the spaces and running them all at the same time with different choices, the times should be 'cloned' and they will all run on the same space (at different times) with different choices. well actually this machine seems isomorphic to the regular machine so i guess there is no difference between 


time t, space s = 2^t
space s, time t = 2^s
this shows are both time and space are upper bounded by each other
the first result comes because with time t, the maximum space you can access is the range of the address you can write down in t steps, which would be a binary address of length t, which sits in a space of size 2^t.
the second result comes because with space s, the maximum configurations is 2^s before looping starts. of course this assume the machine terminates. 
it may be worth further extending these ideas to meausre not just 'how large' of a space a computation can be used, but 'how many times' the space is used (eg how many times memory is accessed), because likely the more times memory is used the results of memory from those different times can be combined into a more effective result. likewise, measure not just 'how long' a time a computation takes, but 'mow many spaces' are being used within that time for separate computations, because likely the more parallel computations those results can be gathered into a more effective result. 
this should be some pretty useful and tight results, in light of the latter correspondence, on a way to measure both time and space by the same metric. 


regarding the lorentz transformation, before i thought the two coordinate systems were with respect to a certain object, but now i realize they are with respect to all of spacetime. so there is no need for an 'object'. consider the two observers as two approaches (one with more time and the other with more space) to the same computation. think of the limit. if v=c, then gamma=t'=x'=infty. if v=0, then t'=t, x'=x. 
first we need to relate how in physics we're referring to coordinates of an event (which may be negative), whereas in computation we're referring to the amount of resources used. we could only consider positive coordinates. another problem is with relativity, greater time displacement means greater space displacement, to keep velocity constant, whereas with computation they are traded. so for this primary reason lorentz is probably not a good idea, and i had to concrete reason to believe it would work.


limiting time is about limiting how much or in how many different ways you can examine the instance.
limiting space is about limiting how much you can remember at a given time about what you have examined. 




maybe all objects can be relations, maybe even don't ever have to touch objects
analyze a relation is done by composing it with others and seeing what properties the result has
universal properties take the form "for all compositions of this form with this object, there is a unique composition such that the whole simplex commutes."

the same relation can appear in multiple places in a simplex, unlike objects in categorical diagrams. this enables unrolling higher dimensional relations on the same vertices. 

unary relations will be isomorphic if they are the same size
binary relations isomorphic up to permutation, means both unary relations must be isomophic and also the connections. 
ternary relations likewise... this will allow comparing groups
quotiens can be said with universal property. equivalence relation will be binary relation on same set. 

notice relations of different dimensions can be connected, just like a morphism connects two objects. of course in multiple ways, and universal properties can be stated by any such connection. 

higher order relations, in particular feeding relations as input
unlike distinction of objects and morphisms, it appears theres no distinction here. as any simplex can connect via any simplex to any other simplex. 
unlike vertices and objects which are unique, a relation is not treated in the same way as unique. instead it can be duplicated and show up in many places. every instance might be thought of as unique. 
homomorphisms. its an implicational type of structure. assuming a mapping, it says all instances satisfied in the source must satisfy in the target under their mappings. i think a relation rather than just a mapping could also be used under the same conditions. 
maybe we could consider simplexes lacking explicit sides, eg a square, where the diagonals are implied, eg by compositionality with the other sides, or just assumed empty (actually i think these are quivalent conditions). 
functions are like morphisms.
but natural transformations seem more like given two homomorphisms, map the output of one to the output of another (so it takes place all in the target). it doesn't seem to create anything new, but rather to select existing connections in the target. 
how to deal with subsets. this is related to higher order relations. 
will higher order relations be useful even if not necessary for subsets? eg will recursion be useful? maybe could be used for infinities like uncountability of reals.
regarding higher order arguments, a relation that accepts one could perform instance querying or test for properties. any analysis requires logical operators, like quantifiers. all statements in our language would have this form, not just queries and testing statements. so we want an elegant way to make logical statements. 
maybe we could unify all quantifiers to the form "it is the case that for x-many ...". universal would mean x=all. existential would mean x>=1. unique existential would mean x=1. !universal would mean x<all. !existential would mean x=0. would it be better to use absolute or fractional values? by fractional I guess I mean the probabilistic version where you say "the probability I sample one with this property is x". 1 is universal. 0 is non-existential, 1> or 1!= is non-universal, 0< or 0!= is existential. as for uniqueness, if we know the cardinality of the set we could say x=1/card to mean there is exactly one with the property. i suppose cardinality t can be measured by the propability eg the same item is sampled twice in a row, 1/t*1/t. but I suppose this probabilistic approach fails for infinite sets, as probability is always 0. even just thinking of a fraction not as probability we have same problem as 1/infinity = 0. i suppose the best approach is to convert between the two forms, maybe call the fractional form normalized form. of course conversion occurs by multiplying or dividing by cardinality. 

i suppose we can have higher order relations. but it would appear as an opaque block, and one must look inside to see the definition. all definitions should be of propositional form and i'd like to always express it in the form of a simplex with the likes of universal properties. I want to rely on symbols like quantifiers as little as possible and instead encode that info with interaction via simplices. the input is unknown and opaque and is only 'decided' based on the known simplices encoded in the definition. this is the perspective that an object (the query) can be completely understood by querying it rather than by looking at its construction. since we can't look inside we can can't get direct access to objects that satisfy it, but rather we are allowed to create object variables that represent supposedly what it accepts then work with those variables (without knowing their construction). a relation is completely characterized by what it accepts. and almost always, what it accepts is determined by what other relations accept. at the base level we just create unary relations and some properties it satisfies with other relations. or maybe we could begin by declaring dots, but whatever we define should be in terms of what already exists, so clearly its a cyclic dependence. this means we'll have to define something in terms of other things not yet formally defined, but only up to intuition. what we want to avoid is defining simply by name, or even by construction. notice that construction requires identifying that which it is constructed from, and identification can only be done by specifying properties, ie how it behaves, which boils down to what it accepts. notice that whether a relation accepts an object depends on what that object accepts. rather than just naming things we should define them by properties, and only have names as optional convenience.
i think a relation should be categorized by how many inputs it can take independent of order. 
think of a binary map, one input a group element, the other a coset, the output another coset. 
every vertex must be assigned an input. some can be variables, some can be constants, and some can be quantified. 
need to think about higher order variables. when an item eg a vertex is unknown, any higher items built on it are unknown, eg edges, and then a triangle. every relation can have a 'signature' that dictates where it can go. signatures, or types, consist of the 'boundary'. eg two vertex types are the signature for an edge, 3 edges (which include information of 3 vertices) are the signature for a triangle. a unary relation has no signature. 
maybe we could forget dots, and just say they are all relations, leaving a cyclic dependence that shouldn't matter i think. 
what's important is to take into account that most of the objects we are interested in will be complex mathematical objects represented by simplices, rather than just opaque dots. but we may still want to feed these complex objects to unary relations just like we would a dot, contracting the simplex just like a dot would (assuming the unary relation accepts it, or else the simplex doesn't contract but rather immediately returns false).
so its like a relation is a hoop, with the hoop as the boundary as a signature, and then something gets thrown into the center of the hoop like a ball, and it is rejected if the boundary of the ball doesn't match the hoop, and otherwise, a new relation or a truth value is returned. so note that signature/boundary appears two places, on the hoop and on the ball. the concept of the boundary is different from that of arity. a relation can have multiple arity, each param type of a possibly different signature. if a relation nests hoop variables, each should be a separate param rather than grouping them together, though a single input for the whole thing can be given and interpreted as a complete collection of inputs.
should we have multiple instances of a variable indicated with the same letter or something, or rather declare an isomorphism between them, probably the latter. 
must still decide how to express logic and quantifiers, etc. 
oh wow it seems relations have direction (or orientation). a binary relation that connects two unary relations is not the same as its converse, therefore orientation matters, and there's two possibilities. for an n-ary relation, there's n! ways to order them (though the relation may be invariant under certain reorderings, eg in the case of a binary relation this is commutativity). but I think all such ways are induced by the orientation of the faces, of the dimension below, which are in turn induced similarly, until reaches edges, which induce direction for everything above. we will use the notion of 'direction' as orientation, to have a notion of vertex order on all relations. this will naturally give an intuition for functions, eg where for binary functions the first will be the input drawn on the left, the second the input drawn on the right, and the third the return value. the arrows drawn on edges, can reveal quickly the ordering on vertices of all dimensional simplexes. 

intersections, unions, etc by composition
identity as default interior
self composition
converses
complements within scope. 

given a universe, can make more things in it by either joining some of them in an ordered or unordered way (subset and tuple) but all tuples must be finite. ordered corresponds to instances of relations (the arguments are ordered), while unordered corresponds to the set of instances, ie the relation itself.
suppose the value true is interpreted as the nullary relation that always returns true (within the implicit existing universe), and at the same time still being a truth return value from a query. Then we could express relationships between relations in terms of this total relation. For two relations to be equal would mean for their 1001 operation to be the total relation. That the two are complements in the universe would be 0110. But the problem is if the two are not equal the return is not false, but rather a non empty relation that is not total. 	
using enumeration to check for existence or universal satisfaction can compare a relation against another and return a boolean. for equality simply univerally enumerate the 1001 of two relations. 
note enumeration may be uncountable.
need binary negation, which can be special case of complement for nullary relations. 

need way express object is accepted by relation. maybe by functional notation, arg on left, relation on right. but for multiarity relations arg would have to be tuple. what if you only want to submit a component? args could be submitted in any order, but each must indicate its position. in place of an arg one could submit a unary relation and a way to enumerate over it (disjunction or conjunction or parity). 
in general we want to specify how feeding relations (we will never feed dots) to relations reduce. every statement will have a truth value, we never write expressions in isolation or we don't know what to make of them (unless context is making a statement about the expression). when we wish to characterize an expression, eg the relation resulting from this expression is monic, we should feed it to an appropriate unary relation that decides monic relations. 
maybe best to think of the framework as relational lambda calculus, but where args can be fed in any order. 
we should enforce that an n-ary relation return an n-1-ary relation on any input, implying exactly n inputs should be fed to give a truth value. this holds for higher order relations, though in the body recursion may occur, but that should not require extra inputs to execute. so it seems we can say everything, except maybe the opaque dots, is a relation. then at every instance we are viewing a single relation. often it will consist of many smaller relations joined with boolean operations, and convenient notations and shorthand might make sense, eg a typical case is a lot of conditions for a theorem, it a lot of conjunctions between smaller relations. when looking at a relation, i mean we are looking inside it. when it is presented as parts of other relations we view it from the outside as a single shape. regarding single shapes, they will all be simplexes, but for convenient notation as shorthand we can leave many faces implicit because they will indeed just be trivial default faces. a body could just be a single formula but for convenience may be allow assigning variables and reusing outputs. an important question is how we address the arguments within the body. remember arguments may arrive in any order. arguments will need to be analyzed in many cases, but first we need to characterize it. I think a restriction and probably the only restriction we can impose on arguments is arity. any other filters applied to the argument should occure inside the body. the reason arity should be a promise to the body is the body can't easily determine the arity on its own, but once known it can query faces given orientation and everything necessary to gain all info from it. so every relation has a signature that includes not just arity but arity of arguments. but then this chain continues because for the body to query the argument it must supply the arument with its own arguments of the right arity. so we may need a full type hierarchy. or maybe second order is enough. or maybe no type checking. most of the time we'll be dealing with first order relations (that accept dots), and second order that work around first order relations. but do we need third order? eg a quotient group will be second order as the elements are cosets which are first order. we'll need third order if we pass around quotient groups with access to their internals, which actually seems likely, but if access is not needed to invernals it can be reduced down to a first order group isomorphic to it, or really what we're doing is treating the first order relations it accepted before as arguments now as dots, which can still be the cosets but now they are opaqe. maybe type checking can be done 'run-time', where if you feed somethign the wrong arity (the first possible kind of error) and error propogates. actually it seems one should not be feeding to a relation without knowing the appropriate type so I think we should have a full type system, but recursion and stuff can still happen because a relation can accept and operate on an argument of unknown type and return a related type. this is i guess like typed lambda calculus. so a type consists of an arity, and for each parameter the expected type. actually its dependent types cuz the type is a variable. so type checking can be done inside the body. or one could have a 'family' of relations, each one devoted to a particular set of type types, similar to a family of circuits. 

note that vertices of simplex projected to 2d can be arranged in any order, such that any fact, edge, triangle, any face can be on the edge. 
what should steps of proof be?

a problem is the graphical notation is cumbersome. like before, even given written notation with symbols in relational style, i underestimate the convenience of shorthand like exponents and index. oh, but I suppose I was thinking of making the app easily convert between formats. 

invertible matrices are isomorphism homomorphisms operating on the same group, which is the additive group. linearity, homomorphism property, and distributivity can all be seen as the same thing. 

permutations can make knot with intersections. use the element that induces the permutation to determine the layers of the intersection to eliminate them. eg, if element is g, let edge going from g be on the top, let get from g^2 go beneath it etc, but this depends on period of g. 

turn existence of a type of relation between n things into an n-ary relation between those things. 

relations universal properties

maybe svd, with concept of bad basis is like expansion graph with bad generators, or like discrete log problem which apparently is a hidden subgroup problem, so maybe all related.

one approach to multiplication is just repeated composition of relations. actually this itself wouldn't have the right type system as it would need an integer, btw negative integers could be mean composing the converse, and 0 could mean the identity. But where this could work is a poly, where coefficients are relations, and variables get replaced by relations and then composed an integer number of times (and negative exponents are possible). oh wait, this multiplication is the composition we had before for addition and now we lack an addition operation (for some reason I thought it could serve both simulaneously). note converse only functions as an inverse (as implied by negativity) when in group setting, ie permutation, but extending the concept to non-permutations could still be useful as long as its accepted that z*z^{-1} != z^0

we need to take advantage of homomorphisms and try to immitate linear algebra and linear maps on how they can form groups by homomorphisms with just a single underlying additive property (I think, because I think multiplication is usually inherited from addition). 
homomorphisms between relations could be like multiplication, and when bijective they could form group. is this true? are homomorphisms distributive? well the homomorphism property for functions f(a + b) = f(a) + f(b) is indeed distributive. continuing with this functional version over some additive group for the moment and leaving relations, how does this compare to linear algebra? 
first I need to confirm that multiplication is always inherited from addition, meaning addition must always be numeric such that multiplication can be in the form of repeated addition. note poly multiplication is not inherited from poly addition but rather from the multiplication and addition of the underlying ring. 

note how functions from any set to a group can form a group, thus maybe relations from any set (eg relation) to the group of relations can be a group. the function group is actually isomorphic to a direct product of base group. 

for graph with no self edges, draw circle.
which groups have structure such that there exists an ordering of the elements so when in a circle, every instance corresponds to a equilateral triangle. actually non, cuz of identity.

can make a 'curve' or even 'circle' by considering a large set and thinking of those as points in space and following making a binary relation where lines form a 'path' through those points. if set is uncountable must instead use 'direction' or something. anyway, the pose the problem of finding something like a square on this curve. of course this requires an order on the points, but with that it just means find 4 points along the curve such that they have certain symmetries. assuming its always possible to find one, this could be a hard problem but easily generated. actually curve must be embedded in higher dimension. 


an underived relation is that which can be defined as if it was the first thing in the universe to be created. these can be defined only by enumeration basically. well the first ones to define are those like empty and complete relations. in the beginning we should derive everything from our boolean assembly language, and there should be no need to have some basic relations that couldn't be constructed from booleans. 
so there are two types of relations. the first is a definition which has an identifier on the right together with its wrapped meaning on the left. the 'value' of this relation is always true in that the defintion can always take place. the second type of relation is one only involving existing relations. this is also assumed true as a statement, that is though we are viewing it in its full form, it should reduce to the nullary relation true. if it doesn't this is a false statment that should be identified. if one wishes to show the negation of a true statement, one should make a true statement identifying the negated statement. 
every relation works with its arguments in its own way. it may pass an argument to another relation, in which case it needs to enforce that whatever type the subrelation accepts is matched by the type of the argument. so relations have signatures, and the 'depth' and specificity of the relation depends on what the relation does with it and what its subrelations require from it. to do anything with an argument, it seems the minimum necessary is the argument's arity. thus arity is the minimum signature. if one knows arity a lot can be done. relations that only need to know this, ie have this minimum signature type, should probably have an identifier name, and that of course can be the signature type. these might be called 'second order' relations. 'first order' relations are those that accept dots, but we will never be looking internal to these relations. then 'thid order' relations are those that don't just compose arguments, but feed explicit things to the argument, thus requiring knowledge of what types can be fed. this creates a recursive definition of signature. 
in lambda calculus functions don't have identifiers, so recursion requires the combinator. for relations this a relation that feeds it argument to the argument and then feeding this relation to itself. so regarding signatures, if a relation feeds an argument to the argument, the agument must have a type that accepts its own type. when combining unknown relation with known relations, precise type must be known for the unknown. but when combining unknowns with unknowns, only their relative types must be known. 
so I think signatures of relations and types of relations are synonymous and a type is a tuple of types. base type should be determined by what we want to the type to look like for a first order relation. an n-ary first order relation has n slots for dots. so it could be given by a tuple of n empty tuples, or rather just the natural number n. remember arity specified how many arguments a relation receives, while order specifies the type of those arguments (the one with highest order). thus if we use a tuple of empty tuples, we are saying dots have type empty tuple, that is dots are of order 0. so in this case order of a relation is the nesting depth of the signature. also in this case we are saying dots have arity 0, that is they are nullary, which conflicts with our notion of nullary as truth values. if on the other hand we use the natural number this is problematic because a relation may accept one param of first order and another of higher order, eg a binary relation accepting dots for one param and unary relation for another param. thinking intuitively, it seems we deserve and need a special symbol for the dots type, because they are indeed not nullary relation, but rather relations of unknown type and simply opaque. in this case an n-ary first order, that is a relation with n slots for dots, should be an n-tuple of this special symbol, call it _. 

so nullary is technically first order by having one layer of parenthases, but intuitively it has undefined order because it has no arguments. well officially its actually called zeroth-order logic. this is the sense that dots are first order variables, and n-th order relations are those that range over n-th order variables.
but i don't want to distinguish variables and relations, simply between 'known' relations and 'unknown' relations (dots). we can define dots as 0 order, then define something as order n if its arguments are of order at most n-1. with this definition, a nullary relation is of all orders.

when we have variables, order is known because we are unable to calculate the full depth.

nullary: ()
first order n-ary: (_,...,_)
second order binary accepting: ((_,...,_),(_,...,_))
third order 4-ary (third order in first arg, second order in second arg and fourth args, first order in third arg): (((_,...,_),(_,...,_)),(_,...,_),_,())
?>=2 order binary: (T,(_))

use recursion for induction

ways to make relation involve a combination of these methods.
1. any of the 16 boolean operations of two relations with same arity. type is induced.
2. enumeration using the 3 boolean operations that are associative and commutative: conjunction, disjunction, exclusive disjunction (maybe not allowed as it is undefined for infinite sets).
one way is enumeration takes place over a single vertex within an n+1-ary relation to produce an n-ary relation. 
another way is simpler and that is enumeration takes place over all relations (remember everything is a relation including dots) that don't contain this relation in their ancestry. in this case we could say only unary relations can be enumerated. the enumeration does not need to worry about the internals of the unary relation, which may use another enumeration. so while booleans preserve any arity, enumeration strictly take arity 1 to arity 0.
i don't think unique existence will be supported. remember duel between absolute and fractional form. will need convenient way to express order and manipulating placement of variables. enumeration requires more thought, but we know its outline.
3. composition. actually i don't think this should exist on its own, but rather be a convenient wrapper of enumeration. however, it requires the notion of simplex which we haven't defined.
4. of course the simplest method, which only reduces arity, is plugging in arguments.
5. the opposite of plugging in I think is expanding arity via simplices. That is, to expand arity, one must take multiple relations and connect them somehow.
6. operating on simplices, like converses, complemenets.

the above methods simply pass the arguments to other relations and then perform boolean operations on the results. none of them require labelling arguments within the body. booleans preserve arity, while enumeration and plugging in reduce arity. now simplices can expand arity. 
what we're going for is existential composition of simplices, but we'd like to only present it as a convention, and offer a more formal and comprehensive way of composition and expanding arity. 

one way to proceed would be to avoid the need to label arguments. this means one cannot arbitrary assign the arguments across multiple simplices. this means to combine two simplices they must produce a new simplex. this process we could call composition. but we still haven't defined exactly how they can be combined. a first constraint could be the faces must line up. another is any two faces that become one must be the same. note that with defaults, any two simplices can be joined in any such way to make a new one, with no elimination required. 
what constraint or bias are we imposing by having relations represented as simplices? there is fundamentally no constraint, but rather a bias on relation construction. the bias is that all restrictions particularly (n choose m) of the inputs must be encoded in the corresponding subsimplex/face. eg if one wishes to impose an argument belongs to one of two unary relation, one must first take the union of those relations and make it the vertex, rather than expressing that disjunction within the simplex itself. 

--- lots of unproductive thoughts about quantification

Elimination can be a separate step, and be either existential or universal (maybe fractional), and be applied to any vertex, or I suppose any face (set of vertices). it seems we must use scope for enumeration, which is equivalent to using the universe but then an implication on scope. this may be a justification to use scope for the enumeration mentioned earlier. So when we enumerate over a face, we are enumerating over all instances that satisfy that face. we can highlight that face in isolation, that is not the adjacent faces too. the existential case is standard composition.
can the enumeration from before be converted to this type so we only have one type of enumeration? before, when enumeratin over the universe, implications would be used everywhere. I think it instead makes sense to use scope and enumerate over instances satisfying something. if one wishes to enumerate the universe, then enumerate over the nullary relation true, as everything satisfies it. enumeration basically means enumerating over a unary relation and plugging in the answer to another unary relation, likely a reduced form of the one enumerated over. 
so its a little tricky.
first we begin with a simplex with no enumeration.
then we take a face and consider all its instances satisfying the current relation. 
then we expect to be given another relation with arity of the face, and we think of this as a version of the original.
then we enumerate, considering if all or any (or a fraction) of the original instances remain.

hmm, actually i'm thinking now quantification could be done via operations like inclusion. if we take seriously that quantification occurs only for unary relations, then we can reduce the notions to existence (recursive def) of certain binary relations between the unary relation R enumerated over and the unary relation P representing the proposition. for existential quantification we enumerate over every r in R and take the disjunction of P(r). Likewise for universal quantification but we take the conjunction. I think existential means there is a non-empty binary relation, ie at least one edge drawn from R to P, while universal means there is inclusion binary relation, ie and edge goes from every element of R. 

maybe a good shorthand to show how small relations make up a larger one would be to animate through the sequence of steps of evaluation. 

actually maybe a better approach to considering quantification in the context of composition of unary relations, we should simply consider quantification like plugging it, as an operation that reduces arity. for an n-ary relation, when we quantify a vertex, we consider the n-1-ary relations that result from plugging in each value enumerated. now we have a set of relations and we must combine them somehow independent of order. so it only makes sense to use conjunction or disjunction on the relations themselves or on some identical transformation applied to all of them. 

in all cases considered we always enumerate over something, plug results into a proposition, then take conjunction or disjunction. the only problem with this is the two relations, the enumerated one and the propositional one, can be arbitrary, whereas we wanted to structure them around a simplex. 
a common pattern is to enumerate over one face on a simplex, and then do some proposition on the simplex that results from plugging in for the selected face. 
another common pattern is enumerating over all relations of certain signature, then making the proposition that when composed (another enumeration) with a fixed relation yields a particular kind. 
we could impose unary over unary like thought before, but not sure.

--- end, below some more useful thoughts


how to indicate visually plugging into a higher order relation?
note that arguments may be plugged into arguments as well as fixed relations.
simply a directed edge from one to another won't work at least for the reason arguments need to be passed in a certain order.
we could indicate it via highlights. when the instance or the parent relation is selected the other lights up in some indicative way.
multiarity relations, instead of constructing a sequence of arguments, we could construct a simplex from them with the default for all faces (true) and let this simplex indicate order of arguments.

every statement can be interpreted a combination of relation acceptance statements. we could use shorthand for ones that are convenient due to associativity and commutativity likes conjunction and disjunction, but in general we could modularize such that every statement is a single acceptance statement. definitions would be special case, probably not formal. so every statement would have distinguisheds argument and a relation, eg arguments on left, relation on right. doing so would mean creating a lot of intermediate definitions, but this is easy for computer and can be unravelled algorithmically for visual appeal. its like an assembly language. 
maybe quantification could be as simple as given a unary relation and not providing arguments but rather making the statement that the relation is non-empty (existence), or is equivalent to false, that is empty (non existence), or is equivalent to true, that is accepting all objects (universal), or not accepting all objects (non-universal), or even counting absolutely how many objects it accepts, eg 1 means unique existence. this could be done for multiarity relations too, but arguments would not be distinguished, eg useful in monomorphims when need universal quantification over binary relation. this makes sense because complexity wise identical quantifiers should be grouped into one.
definitions require two steps and can't be reduced to 1. first step constructs the value, second step assigns to it.

definitions:
@ one of the 16 binary operators between two relations, with an indication of which face of the larger relation the smaller relation gets combined with. 
the result can probably be visualized. and probably shorthand for multiple at a time would be handy and easy.
for more power and to cover the need for an additional special definition type, it could be allowed that any two faces of the relations get combined, resulting in a relation with arity equal to the arity sum minus the number of vertices combined, ie minus dimension of glued face - 1. 
the above method is a bit ugly. for elegance and more structure, impose that the two faces combined into one must have identical boundary. unary relations have nonexistent boundary, so any two unary relations can be combined. 
this type of definition takes towo relations to produce a third with arity greater or equal to the sum.
@ elimination. this definition takes a single relation and reduces its arity. any face can be eliminated with one of the 4 (or more for absolute)quantifiers.
@ plugging in. the definition take a single relation and reduces its arity. argument(s) must indicate their corresponding parameters.
statements:
@ eliminating all vertices, which i think is enumeration.
@ plugging in all arguments.

i know the above can be improved, at least by unifying the two types of plugging in and the two types of elimination.

we should restrict ourselves to use scope more. 
i think enumeration could be as follows: use a face of the simplex as the enumeration set, then plug in sufficient arguments to reduce the simplex to the selected face (now a subset of the original face), and use this new face as the proposition. thus the proposition is defined via the arguments for reduction.
existence would mean the new face hasn't been emptied (still accepts something from the old face), and universality would mean the new face is still full (still accepts everything from the old face), and similar for the other two quantifiers.
of course if we select the 'face' of the whole simplex, then existence and universality return true, while nonexistence and nonuniversality return false.
so really the act is comparing a face before and after reducing the simplex by another argument, or rather how arguments affect the relation.
i think this definition of enumeration is equivalent to composition. select the face to eliminate. to characterize the relation, suppose it is given arguments. plug those arguments in to the rest, then depending on which of the 4 composition models, use one of the ways described above.
in both cases of enumeration and composition, we can consider the fraction that remains. for enumeration this corresponds to the fractional model. but for composition, the fraction depends on the argument of the relation so it may not be such a useful notion.
we may like to specify plugging in a class of arguments rather than a specific one (probably represented by a simplex identical in size to the face). what we mean is that this simplex is a containment, not necessarily tight, for all arguments that satisfy the quantification. if it was tight, that would mean all arguments satisfying would be in the simplex and none outside it.
the contrapositive of the notion may be just as useful, depending on circumstance. that is, if you don't plug in certain arguments (the complement of a containing simplex), 1 - other_fraction will remain. to turn this into a positive statement, say fraction remains if plug in not-not-arguments.
maybe another way to state it is by specifying the face to enumerate (and what quantifier to use), then considering the simplex with that face reduced and making a proposition about the remaining simplex. then the problem reduces to representing a poposition on the simplex. the original form above was simply the proposition that a set of arguments satisfy it. the more sophisticated form above was that any satisfying arguments belonged to another simplex. so we can expand to more generic ways.
the traditional form is enumerating over a simplex, then using the current argument within a proposition regarding another simplex. the method here make the trade-off of eliminating the need for arguments in exhange for the need to encode the enumerating simplex as a face of the propositional simplex. i think this may be justifiable, as it requires more structure. but this can be challenging and I think contrived.

just like scope is important enough to make rules by, the dimension of a simplex, ie arity of relation, may be worth making rules about. for example, one may iterate over all simplices of a dimension instead of iterating all objects then filtering. one reason this is useful is filtering for arity seems difficult. in higher order relations, where faces are simplex variables, this will be useful as the notion of quantification requires iteration over faces.

heck, what if we just do the standard of enumerating over a set (a simplex) then plugging current arguments into another simplex. seems simple enough, and with managable power. for higher order relations, one may iterate over all simplices of a dimension as specifed directly above.
enumerating over a simplex and plugging in args to another simplex. each time we plug in we get a new relation. since we consider the iteration set unordered, we can only combine all such relations using conjunction or disjuction. but if the enumerated relation has an order maybe more sophisticated merging can be done.

actually whenever we enumerate over a face, the arguments will probably end up back in the same face, so this leads back towards the previous idea. can we strike a balance between them?
suppose we specify a face to enumerate, with the quantifier, then something to do with the reduced relation, and the value returned is the output of the whole quantification. thus arguments are hidden as before, but we are not confined to the propositional form before. I suppose we can allow quantifying over multiple unconnected faces simultaneously, as long as quantifier is same. whatever transformations the reduced relations undergo, they must all return a relation of the same arity, and then those relations are merged. the transformation i suppose can always be in the form of passing the simplex as an argument to another relation. if the receiving relation is unary then the quantification is a statement, and if multi-arity then quantification is definition. thus an enumeration operation consist of the subject simplex, the quantifier, an indication of the faces to be quantified, and what higher order relation to pass to. 

my own symbolic, written, syntax to define the language:
say 'shape'. say 'n-shape' and 'n-side'. say 'input' and 'output'. start with n=1.
definitions i suppose could be indicated with an arrow from the value to the new alias. the value must be evident at the time of definition, and if the value is a 1shape it should be indicated with the corresponding special symbol rather than given an alias.
identifiers should be atomically recognizable such that they can be put together in a sequence and identified without separators, eg each can start with a capital letter and have all following letters lower case.
write inputs in a seqence with the shape last (on the right). indicate input number by exponent.
use parenthases when necessary if using nesting. use left-associativity by default.
represent a side with an increasing sequence of digits. use as subscript on a shape for projection onto the side.
indicate the 16 binary operations on 1shapes with special symbols. 
let the two quantifiers be treated as shapes, and indicated with special symbols, probably the regular ones. these shapes will have output type equal to output type of the proposition.

note composition is a process of first joining and then eliminating

we start with relations/simplexes. we want to make new ones from them. essentially two ways to do so. first increases dimension by joining them together. second decreases dimension by plugging in. however, there are two approaches to plugging in. one is plugging in a single argument explicitly. the other is plugging in a set of inputs you don't have direct access to, and then combining the results in a certain way. the natural way to form a set of inputs is to consider the satisfying inputs of another relation. to this extent, the set is unordered, so merging the results must be done independent of order. 

use recursion for induction. each input is a problem instance. if input is base case, solve it directly. if not base case, use implication on a subcase using recursion to solve current version.
our goal is to show that certain relations will accept certain inputs with certain properties. 
can we do quantification with recursion? quantification requires enumeration which requires access to whats inside a relation.

i realized implication can replace the need for universal quantification, but universal quantification can only partly replace existential quantification. the problem reduces down to proving negation of a statement. the problem is a statement can be false in many ways, while there is only one way to be equal. 
I'm supposing that for universal quantification we'd axiomize the 'total' relation, because I see no other way to state it without quantification. But then for existential quantification, we'd need to axiomize all the 'non-total' relations. I felt uncomfortable with this but now I suppose having an intuition on what a total relation is necessarily requires an intuition what it is not. (in place of totality we could use emptiness instead).
I suppose this is the most reasonable and simple approach to quantification I've encountered. But still, the odd axiomized statement of whether something is empty or not would show up everywhere in definitions and statements and it feels like an adhoc addition to the language. more acceptable would be if the axiom remained mostly hidden, but that is not the case as quantification will be used extensively. i suppose it may not be so awkward if we make 'empty' and 'full' higher order. note neither of these relations are either empty or full themselves.

using this method I suppose all statements and definitions could be done purely with the 16 binary and the 4 unary operations which operate on nullary relations, together with the two axiomized relations above that operate on all non-nullary relations, and of course plugging in. The first increases arity, the latter two decrease arity. 

i think we mean empty and full of a relation with respect to its boundary. the boundary of unary relations is the universe. the boundary of nullary relations is undefined I think. or maybe with respect to the type makes more sense. 

converses simply mean switching the orientation. n! ways to do so for an (n-1)shape

for now simply use nn and nnnn and E and F to designate the operations.

aG 0001 a'G
1101
faComp 1001 fa'Comp
1101
a 1001 a'

fgComp
	f.1 g.0 0001
abc
abf 0001 bcg
a_f 0001 _cg E !

!E f(a,_) 0001 g(_,c)

x,y,z
f1(x) & f2(y) & f3(z) &
g1(x,y) & g2(y,z) & g3(x,z) &
h1(x,y,z)


i'm wondering if we need boundaries at all, or whether types are enough. we would still have orientation. or like considered before, maybe boundaries but no types. i think we should pick only one, and we should have emptiness with respect to it. I think boundaries may have a difficult time compensating for types. I also think boundaries can be implemented manually (and probably should be). As far as combining faces, we could require types to be the same, and I think still implemented by simply plugging in the args to both simplices (common args in both faces) then taking the binary operation. complements would be with resepect to types, but of course they could also be programmed with respect to boundaries. thus we pick types, and leave boundaries as informal. 

formally, what is a type? it includes arity, and for each input includes the type of that input. however, the hierarchy doesn't need to go all the way down. variables can be placed instead, and variables that appear multiple places are forced to agree. note types are not arbitrarily complex or able to encode anything other than arity and order.
to avoid un-needed flexibility we could enforce the order of inputs according to arity and order. probably order first then arity second. but what about variables? i suppose we can consider their relative order. oh wait, haha, order of arguments is not arbitrary but gives the orientation so is relevant to notions like converses.

how should we index the inputs and in what order do we feed them? if we use postfix and index in order of input then reading left to right the numbers descend, a little ugly, but I suppose tolerable. but also by default the first input gets fed to far right, so if it's intended for something further left it must be 'pulled back', a less intuitive concept than 'shifting forward'. 
i think the header that lists the args should list them in increasing order for intuition. for plugging in, we somehow indicate which place an input is suppose to fill. I suppose by default, the first input provided goes in the first slot. thus 'shifting forwards' is appropriate. Or better than supporting this kinda unintuitive default, we use placeholders for all those slots that remain unfilled, no numbering or shifting necessary.

0:_ 1:_ 2:(_,_) 3:(_,_) Compose
	%1%3
	0001
	%0'%2
	Empty !

0:_ 1:_ 2:(_,_) 3:(_,_) Compose
	%1. %3
	0001
	.%0 %2
	Empty !


for now I think we should start with no variables allowed in types, only explicit types all the way down. i don't think we need to protect against self dependence, as types prevent a shape from being fed to itself. plugging in is then the act of param instance replacement and then further plugging in if possible. as long as we don't allow recursive types, I think the universe of atoms need not be well defined.
all statements are of the form of plugging in, including passing to Empty and Full and boolean operations. definitions take the form of a type, a reference number/possible names, and the body. since we are not officially supporting simplices, there is no need for official support to access faces. the notion of faces, however, may still be possible. a face would be with regard to a subset of vertices, and be defined as existential elimination of the other vertices. this is more restricted than the faces considered before, where not all face instances may be partial valid instances. there were what now looks like an extra unnecessary guard.
lets say 'subshapes' rather than 'faces'. also say 'passing' instead of 'plugging in', 'pass' for an instance of 'passing', 'input' instead of 'argument'. maybe use word 'props' instead of 'params', noting how 'passing props' is intuitive. maybe say 'selection' for a pass that results in a non-nullary subshape, because the idea of selecting a boolean is not so intuitive, but selecting subshapes is very intuitive. say 'subshape' instead of 'output', and use 'result' for the case of a nullary subshape.

definitions are not optional aliases but rather the only way to create a shape other than passing (eg to expand arity). definitions are the creation of new shapes, whereas passing can be said to be reveal subshapes, rather than construct entirely new ones. Apart from a definition (the act of defining/constructing) and a pass (the act of passing/selecting), everything is a shape.
props will be identified by index.
we need to decide how bodies are represented. we'd like a DAG. in fact, maybe we'd like a graph, if its possible to make loops. is it? what we mean is that a subshape get passed to its parent shape (which should result in the subshape itself), but now I realize this is non-sensical. in the tradiational model of loops and recursion, every evaluation is different, so in our case that means a shape can be evaluated multiple times, but each evaluation is a new instance so it should appear on its own in the DAG. I suppose we just program the body as a DAG, in the form of a set of vertices, each pointing to its dependencies. in a vertex representing a pass, props and or the shape may be variables or constants. no need to represent definitions, as they can all be predefined. so the body can serve both as a statement, in which case anything use is already defined, or as a definition in which case some things used may be undefined props.

how do we perform verification? statements are with regard to only existing shapes. so the task is to verify new statements given existing statements. any substatement, eg the condition of a implication, is about existing shapes so should be able to be inferred as true or false. 

xyR^2 = (x.R &1 .yR) !Empty
xyR^3 = (x.R &1 .yR^2) !Empty
	= (x.R &1 \z = ((z.R &1 .yR) !Empty)) !Empty <- confusing

. y R^2 = x (y R^2) = (x.R & .yR) !Empty

x R S &1 = xR xS &0
x y R S &2 = xyR xyS &0
	= x .yR .yS &1
	= y x.R x.S &1

the above shows how our notation can be confusing and actually just inlining may not be valid, hence the need for the anonymous z func above. 
I think we can characterize permutations as there existing an n such that R^n is identity. seems this requires disjunction over n. seems this requires recursion, which means either recursive shape definition or recursive type (for fixed point). 
its a bit worrisome that our definitions seem a bit too constructive. I was more excited about the likes of universal properties.

x y R^2 | = ((x.R | .yR) 1001 R) Full = (x.R | .yR) == R

S R == = (S 1001 R) Full

and the constructions and formalization seem to make analysis harder. 
encoding everything formally seems to introduce too much structure, though theoretically this structure is necessary, just cumbersome.
i'm afraid i'm trying to do too much and i will inevitably fail and end up wasting my time. 

if we continue with these shapes/relations, the next step is deciding on types and notation and to what degree we will have an inference/verification engine. 
do we need types? without types the problem is an inability recognize how to handle the props. 

it may help to remember some of the most significant reasons to explore relations. one is the representation of rings and such using existential composition for additivity and homomorphisms for multiplication. the insight was that distributivity is like the homomorphic property / linearity, and that's why linear maps can represent rings. so what would a homomorphism be for n-ary relations? well for linear maps we need an underlying ring. actually i'm not interested in a ring necessarily but a general structure with two binary operations satisfying distributivity. it seems linear maps expand the dimension of an underlying structure that already satisfies this condition. with conjunction and disjunction we have a structure with disributibity, and relation existential composition is defined as a linear map on top of this operation. but our addition, disjunction, is not a group, so we can't make this a ring. we want to generalize both addition and multiplication, so we need to move higher, like into homomorphisms of relations.
first consider binary relations. the relations can be heterogeneous. then we have two relations, one from domain to domain, the other from source to source. the rule is for every instance in the first relation, all images of domain and source in the second relation must form instances. what about ternary relations? I think the general pattern is to relate instances of one relation to instances of another. since we are relating instances rather than mapping them, some instances can collapse to the same one, while others can expand. generally we think of a homo as relating two structures. but we could generalize to higher arity, but I think this would be extremely complex with no application in mind. it would be like relating 3 ternary relations, via 3 ternary relations (like how two binary relations relate two binary relations as above). the rule would be more flexible this time, like a ternary boolean operation describing the relation of how instaces relate between the relations. for binary relation this would mean a binary boolean operation describing how instances relate. standard boolean operation is implication but of course we could do more.

there would be several benefits to reducing study to binary relations. one is they can be clearly illustrated, where as ternary and higher cannot. we could even study unary relations by binary relations, which may make the universe simpler. another benefit is the importance of graphs. another is the applicability of matrices. but note all of these are also available for higher arities. of couse we'd have higher order, and really what we're doing is compensating arity with order. we need large dependence on order anyway. another benefit is many problems we're interested in seem fit for binary. another is we're unlikely to go beyond ternary really, so higher arity isn't worth much. another benefit, maybe best benefit, is we could simplify notation because it would fit more naturally with our convention of line writing. another may be that homomorphisms work best for binary relations as explored above.
if everything is binary, we could use the single word 'net' instead of 'relation'.

if reducing to binary, why not reduce to unary? is it possible? consider for ternary. I think the only way is for input to be a terary relation and for output to be the truth value of whether thats the single correct ternary relation. reducing to nullary is not possible. 

suppose we focus on binary relations. we must develop semantics and notation. what we had previously was a universe of atoms together with any relations created. relations were defined with no guard and only types, where a type can be an atom or a relation of a certain arity or a relation of a relation, etc. we could keep it similar now, or maybe take advantage of the new restriction. 
i'd like to use recursive types. for fixed point considered before, we used unary relation. 

instead of considering relations like queries, maybe we can consider more like multivalued functions both ways, where plugging in for one we give ourselves access to the elements of the remaining unary relation. but how to represent this? 

is homomorphism distributive over existential composition for binary relations? this would be a good exercise.
Suppose all are homoenous relations. to begin, suppose the homo is a function. 
suppose we think of the homo as mapping the boundary sets to the boundary set of another relation and letting the structure carry over in such a way that the homo property is satisfied. wait, but is this a binary operation on relations as it should be? what is the result? I suppose the result could be the set of instances that are the image. then forget about a preexisting target, and think of the homo as a function that creates the target. the homo relates the domain and source (same set since homogenous) to themselves. For homo G', argument G, suppose aGb and aG'a' and bG'b'. Then it must be that a'G'b'. The binary function G'G satisfies this. Consider homo G and args A and B. Lets check that a(G(A + B))b iff a(GA + GB)b. 
a(GA + GB)b means exists j such that a(GA)j and j(GB)b.
	a(GA)j means exists a',j1' such that a'Aj1', a'Ga, j1'Gj.
	j(GB)b means exists j2',b' such that j2'Bb', j2'Gj, b'Gb.
In total this means exists v,w,x,y,z such that 
	wAx, wGa, xGv.
	yBz, yGv, zGb.
a(G(A + B))b means exists a',b' such that a'(A + B)b', a'Ga, b'Gb.
	a'(A + B)b' means exists i such that a'Ai, iBb'.
In total this means exists x,y,z such that xGa, yGb, xAz, zBy.
So it seems this is not by itself distributive, though maybe with some restrictions. 
Transforming the former
wGa, zGb, (xGv, yGv), wAx, yBz.
let x = y
wGa, zGb, (xGv), wAx, xBz.
x -> z, z -> y, w -> x
xGa, yGb, (zGv), xAz, zBy
so if x=y then the former implies the latter (forget the v, its not necessary). The restriction x=y means j1'=j2'.
And I'm pretty sure the latter implies the former if there exists v satisfying certain conditions.

I'm wondering if we should use boundaries, ie unary relations over which a binary relations takes place. the type would simply then consist of the source and target unary relations, and be written S -> T. Before, when we were using types, we did the same for each vertex, which I guess means we were implicitly giving the unary boundary, while not making any other boundaries. So I suppose we are simply adapting to the binary case. 
So to complete our definition of types, we allow variables for a source or target. now what about recursive types? this means we have a type with variables and we assign the whole type to one of those variables. 
Do we need to support unary relations? What happens when we plug in? we get a subset of the remaining type. Instead of supporting the separate concept of a unary relation we could just consider them to be some partially evaluated binary relation.
With arbitrary arity we had the luxury to plug in in any order. But now for a ternary relation only plugging in the first arg directly seems straightforward. how can we make this more doable? Maybe clarify how one form can be transformed to another while preserving the whole ternary relation. 

I now realize a problem with the binary approach is that a binary relation of the form A -> (A -> A) only corresponds to a ternary relation if its functional in the first arg. multi-arity didn't have this problem, or the problem of flexibly plugging in described above, or the problem of definitions/transformations by relations (cuz this requires extra arity for the relations being transformed). Would it still be possible to focus around binary relations or must we resort to all arity as before? maybe we can take advatnage of the functionality just described to solve some of the other problems. So for every left input there exists exactly one completing right input which is a binary relation. when we write (a + b) = c we're using two binary operations, but the top relation doesn't relate an element to another binary relation, but rather an element to instances of the other binary relation. omg, I've been thinking we'd need higher order where one unary relation (a side of a binary relation) is over all binary relations of a certain type. but rather it should be over all instances of another particular binary relation. now plugging in more flexibly makes sense. the def problem isn't directly solved, but ability to treat binary relations as unary relations of instances may help. 
For types, I suppose we could give a relation for both the source and the target and we the unary relations of instances for each relation. I suppose the relations would either be unary or binary, displayed as A or A -> B respectively. Binary relation is the minimum to expand arity, so reducing to unary for arbitrary relations wouldn't be possible.

So now we are more intuitively iterating over instances of relations. So what if we want higher relations? The we need a relation that contains them. I suppose we could artificially introduce a relation containing all unary relations and one containing all binary relations, but of course this would introduce paradoxes. Maybe instead we just have two special types, one for unary and one for binary relations and we could iterate over them.

No matter what we decide, its probably overkill and not worth very much to study regular real number stuff like probability inequalities (which will involve things like power series) in terms of relations. but our study interests will very much involve this, thus it will become a leakage for a verification system. also, it will become contrary notation. best possibility is making a translation interface that can at least enable postfix notation. yet I think complex math may be worth verbose notation. 

in our new model, fullness and emptiness make full sense because our domain (instances of existing relations, which are well defined since the relations have been proven to be true relations) is so clear.
suppose we start without recursive types. then the full definition of the type of a relation is either A or A -> B together with the relations (and their types) A and B. Suppose B is has type C -> D. Then when we iterate over A -> B we receive instances of A and instances of B which means instances of C and D. 
How do we express quantification? I think fullness and emptiness may suffice.
How do we create higher order relations? a higher order relations means we iterate over relations, meaning our domain must itself be a higher order relation, so its a cyclic dependence. What if we axiomize a relation that contains all unary relations up to isomorphism, ie one of each finite cardinaltiy, then one of countable and uncountable infinities. Is this enough to also iterate over all binary relations of a certain type? construct the binary relation over the axiomed relation that accepts all source and target pairs that match a particular type. Instances of this are source and target pairs of a particular type, but what we want are instances that also encode binary relation, not just the source and target.
we could go back to previous type model where iterating over binary relations would mean type ((_,_)). But this has no guard, and it also doesn't directly support iteration over instances. Notice this type model is not entirely consistent, as iterating over _ is over atoms, while iterating over anything is over relations, and relations are different from atoms, though I guess we could consider all atoms at opaque relations. To impelment a guard, I suppose we could I support could use constants like (A) where A is not a variable but a known relation. If A is unary, it means iterate over the subset A of _. In general it would mean iterating over the instances of A. This would also implement ability to iterate over instances. 
In this case by A -> (B -> C) which is (A, (B, C)) we mean iterating over instances of A and instances of (B, C) which are binary relations with boundary B and C. To get back what we wanted, we first need to define a binary relation D with type (B, C) then we iterate (A, D) meaning instances of A and instances of D. So in a type a constant means iterating over the instances of the relation attached to that constant, whereas a parentheses expression means iterating over all relations with that type. When we say D has type (B, C) we mean D is an instance of (B, C), which we interpret as the unary relation of all binary relations from B to C. So when we say 'A has type B' we mean 'A is an instance of relation B', even if B is multi-arity in which case A can be represented as a tuple or single symbol depending on needs.

T means all instances of the relation T. 
if we say D has type T we mean D is an instance of T.
(T) means all unary relations that decide given an instance of the relation T.
if we say D has type (T) we mean D is unary and given an instance of T it decides.
if we say T has type (T) we mean T is unary and given an instance of T, itself, it decides.

maybe we might as well include recursive types.
we write x,y, is instance of binary relation R as xRy, and x is instance of unary relation R as xR, in both case using parenthases as necessary.
bodies will still use identifiers to access props by index. 
So I think we define a relation by identifiers and what we do with them together with constants. Then we think of an unofficial binary relation 'has type' which takes a body and determines whether it has a given type. one should be able to determine the type of a body just looking at it. 
we could interpret a definition as a statment on whether the body fits the type, together with an alias for the body (which is not part of the statement).

For example, %1%1 has type T = (T).  It has one prop so is unary, thus has type (T) where T is the type of the prop. But it feeds this prop to itself so the prop must also have type (T). Thus it has type (T) = ((T)) => T = (T).


I think we're just facing the fact that its impossible for our single notational system to be convenient for all situations. 
I feel like I should ditch this project, but I'm still interested in relations for all the reasons I started this project.

one problem is we want to be friendly to functional infix notation, but here our infix notation only has truth values. Functions would be defined with the full relation on the left and the target on the right and with the property of one-to-one. Support R is the function. usually we'd want to write xRy to mean the element R maps xy to. But actually we never want to write xRy with that meaning in isolation, but always with respect to something else, such as saying its equal to wRz. Depending on how we want to employ the value, we could construct a new binary relation to support that notation. we would use * for the full relation on our domain and write x*y R=R w*z where R=R means R maps the two pairs xy and wz to the same thing. Of course this is different than the traditional xRy but it still seems convenient and may even have benefits. We could write x*y R=R' w*z to mean R maps xy to the same thing R' maps wz to. It may even be possible to use the misleading shorthand xRy = wR'z without confusion though this depends on x and w being recognized not as instances but single elements indicating R and R' are not meant to function in this position as they normally do. 

if for functions we put the domain on the left and then we compose with another function we get postfix function notation. 
ab+c, dc*e => dab+*e or d*(a + b) = e

I think supporting access to the constituents of an instance may be complex. the posfix just described means composing with one of the constituents. i think better would be to make the composition as separate step. Take the converse of the AB->C relation and compose it with the D->C relation. Then we have a D->AB relation. Now we can replace the D->C in the DC->E relation by D->AB. 

Suppose we have a binary relation (A : (B -> C)) -> D and we want to obtain the binary relation by fixing an element of C. If we are only allowed to submit full pairs (b,c) and not partial pairs (.,c) we could still create the wanted relation B -> D by defining it as (b,d) satisfies it if ((b,c),d) satisfies the original. 

what we have done by restricting to binary is just imposed that arguments are either singles or pairs (of singles or pairs, etc). bodies definitely need access to constituents. 

notice binary relations with one side instances of another relation will often be too large to illustrate, and that side will be quadratic the size of the other. we may consider drawing the instance side as its own binary relation rather than as a unary relation. or maybe draw the instance size as a matrix.

when we write 1001 do we really mean global equality? we mean that on any 'appropriate' input the two are equal. By appropriate, I suppose we mean of right arity. we still have confusion about the global environment. how does a unary relation not just recognize by decide objects in a global environment containing atom not even defined at the time of the unary definition. this task may in fact be undecideable. Maybe we should not have a universe of atoms but rather start with unary relations defined from scratch. All other relations would be created on top of them, containing either instances, or relations themselves. intersections and unions and such of base relations would be straightforward, and it could be possible two base relations overlap. when comparing two relations, it seems we must take the intersection of their domains, in order to make a domain compatible for both. or actually when an input doesn't match the domain we can interpret that as a false response so I think we can compare any two relations regardless of domains. 

what about quantification? i think we could still use fullness and emptiness of a relation given its domain is evident, and now we measure with respect to that domain.

notice that 'is an instance of' is itself a binary relation. this is how we do set membership. we could maybe use this for notation but it seems to present problems.
we need notation for unary and binary satisfaction. we want to use infix for binary, but then the question arises on how to show partial application. 
while I suggested before we don't want to support nested partial application, it may be helpful and the definition is so straightfoward we would just be using shorthand. 
somehow I want to take advantage of the way our nesting tree always has no more than 2 branches. 
suppose we find notation for plugging in...

a question is whether we must access instances in expanded/unfolded mode. If we access in compact form eg x instead of (x1,x2), what can we do with it? we can pass x to a unary relation or maybe to a binary relation if we can clearly treat it as a unary relation of instances. maybe we could just have a special marker on the identifier if it stands for a pair rather than a singleton. we could also just simply require always unfolding. 


maybe we should insist all base unary relations are isomorphic to [n] or N or R. In fact we could we could insist those are the only base relations. of course can create other unary relation isomorphic to them but those would not be base relations. With this standard we could construct relations using notions of numbers. we could assume prior notion of just natural numbers construct all finite and countably infinite relations from that, and model uncountability of the set of all subsets/unary relations on these natural numbers. Remember we will make extensive use of real and even complex analysis so either we assume those structures as a foundation, or we build them. Actually to make use of natural numbers to build relations would also require the basic operations of natural numbers. 
due to our extensive use of analysis, I think an important step in developing a system is deciding how to support analytic concepts. 
If we chose to support them as a foundation, how would we? we'd assume a unary relation for natural numbers, a ternary relation for addition and multiplication and division and such, some of which could be developed from others, but which we might as well support out of the box, and similar for rational and real and complex numbers. 
If we wanted to build them, how could we? Remember we want to stay away from constructions as much as possible and focus on properties. But we define properties by interaction with other relations and we can only characterize so much without interaction with explicitly constructed relations. In general i'm confused how much meaning we're attaching to atoms versus relations, like whether a unary relation by itself contains any structure and if not is the identical to another unary relation of the same size. can we make a simple rule, like anything that is equal up to isomorphism is entirely equal? yes, sure, but that doesn't answer how much meaning atoms carry. 
Maybe we support any unary relation, that is any representations for the elements, and then support any binary relation defined via these representations. these representations are purely for means of construction and are not part of the official language. any (finite) construction can be represented generically by illustration. I feel uncomfortable giving atoms distinct IDs as then operations apply which we don't know how to interpret. but if we have a binary relation on sets then that induces. Note that any identifiers attached to a unary relation induces a complete ordering, thus we assume nothing more if we allow only the natural number representation. another issue with supporting multiple representations is composing two relations requires matching up via ID the elements of the merging unary relations. If we support only a single representation this is solved. Now to what extent would we support arithmetic? to support it elegantly, we could consider ternary relations of standard operations already defined (axiomized). same for rationals, reals, and complex, and of course we could establish our own isomorphisms and inclusions between these.
I realize one way to boostrap is to consider relations over (), that is the 2 nullary relations. over () there are 2 inputs, over the full ((),()) there are 4, etc and these inputs correspond to binary strings, so using our boolean operations we could theoretically construct any relations. 
Maybe the most economical boostrap would be to take the single unary relation of natural numbers, and define others like finite ones as finite sub-unary relations, a real number as any sub-unary relation, complex as paris of reals, etc but this all requires construction. 
Suppose we prioritize minimum constructions and any notions already defined we can 'import' externally and assume properties about it. this would be arithmetic, analysis, and maybe others, even like integrals, measure theory, random variables, etc.


I have to be real clear with myself why I'm pursuing this.
Is it mostly because I'm interested in binary relations on finite sets and how they can compose in various ways and how I could introduce homomorphism and possibly use these operations to represent algebraic structures? If that is the case, I can use standard notation, introduce some of my own, but mostly study informally, that is without developing this whole system of notation and reasoning.
Is it mostly because I want a consistent, unifying notation with which to reason about all kinds of concepts? This was indeed a primary motivation for starting the project. But as I explore how to do this I continually realize no matter how I reformulate the system, that such a formal system that is convenient to work in such a wide range of scenarios is basically nonexistent. At the same time, I remind myself how functional languages with S-expressions can be so powerful and convenient to so many DSLs simultaneously, and I convince myself that the DSL's of math should be no exception. I guess indeed they are no exception, but I guess I view the syntax as heavy wrapping that requires careful handling and add much to meaning/intuitiveness. 
The first reason above is amenable to illustration, which was a primary motivation for the browser interface. The second reason above is not amenable to illustration, though it was still motivation for an interface for the purpose of keeping a db of concepts and transpiling between notations.
I'm beginning to think the second purpose is not worth it, while the first may be, in which case I should pursue the first without the second. Maybe in pursuing the first I can learn towards the philosophy of the second by not hesitating to invent my own notation when helpful, and carefully writing out my proofs in a format of reasoning I find clear, and record my own proofs and maybe display them on a web interface.

Another argument for abandoning the second goal is that there is a lot of variance in the statements of proofs, and formally justifying every inference step would be a pain, and due to the variance making helper macros won't be useful. 


But what if I continue...

I love infix, so even though many things will not be illustratable and thus ability for illustration doesn't justify binary, infix justifies it.
eg first requires right side to be ternary. X and X' are binary, while second restricts to binary.
X (X Converse) ==
(X' Converse X) & (X' == X)

Note there are two evident ways to do a ternary relation
;
one accepts an atom and a binary relation. to fully evaluate it one must separately plug in the second two arguments into the submitted binary relation. this method actually encodes more than a ternary relation, as a ternary relation requires every atom to correspond to at most one binary relation. If we further enforce that not only is the first relation functional in the first prop (the atom), but the submitted binary relation is also functional in the first prop (another atom), then together they form (or technically the first relation alone forms) a binary function.
;
the other accepts an atom and an instance of a fixed binary relation. this is the method I considered before, and it was presenting hard questions like to what extent to support pairs. I now realize the previous method is sufficient and in some ways more elegant. We can thus disallow access to internals of instances. Suppose that we define a binary relation. If we choose to iterate over it in another relation, we must treat it as a unary relation. Before, I thought it would be useful, even necessary, to allow representing the instances as pairs and thus giving access to the constituents. But now I think that is not necessary, and instead we only allow assigning a single symbol to an instance of the binary relation. Thus we truly treat the binary relation like a unary relation. All we can do with an instance as a single symbol is what we can do with other single symbols, feed them to other relations. This restriction simplifies a lot. Before, there could be an arbitrary number of props inside a body, due to the unfolding of instances. Now, there can be at most two props, and we can designate them 'left prop' and 'right prop' with special symbols like 'lp' and 'rp'. For a unary relation we can just say 'prop' and use 'p'. A body then consists purely of constants and these props. I think theoretically we don't need intermediate variables inside the body, but I think it would be helpful to enforce them by enforcing that all passing occurs in the form of 2 or 3 symbols in the cases of unary or partial binary, and binary respectively. For partial binary, I think we use a special symbol in the place of the absent prop. All that remains in the syntax for a definition. unfortunately we can't represent this as a binary relation. this is the single time when the operation requires that one part not be a single symbol but rather an expression (the value assigned to). I suppose we can still make it look typical placing the new symbol on the left, infix an assignment symbol like <- or :=, and on the right the multi-symbol expression (required to be multi-symbol). Again, this is not actually a binary relation, though it appears as one. It doesn't return a nullary relation but rather like an assumption.
;
How is an in-body definition different from an isolated relation definition? 


suppose X,Y, and Z are permutations, addition is exitential composition and multiplication is permutation homomorphism
a(X(Y + Z))b means exists a',b' such that a'(Y + Z)b' and a'Xa and b'Xb
	means exists a',b',c' such that a'Yc', c'Zb', a'Xa, b'Xb
a(XY + XZ)b means exists c such that aXYc and cXZb
	aXYc means exists a',c1' such that a'Yc1' and a'Xa and c1'Xc
	cXZb means exists c2',b' such that c2'Zb' and c2'Xc and b'Xb
	together means exists c,c1',c2',a',b' such that c1'Xc, c2'Xc, a'Yc1', c2'Zb', a'Xa, b'Xb
	now since X is a permutation, there exists c1' and c2' and they are equal, c1'=c2'.
	thus we reduce it to exists c,c',a',b' such that c'Xc, a'Yc', c'Zb', a'Xa, b'Xb
	now all that remains is the additional constraint c'Xc. This is the only constraint on c, and since X is a permutation, c exists.
Thus we have actually established the equivalence of the two only under the condition that X be a permutation, no need for Y and Z to be permutations. 
In fact, X may only require some properties of permutations. 


back to completing the language. an isolated definition consists of multiple lines, while an in-body definition consists of a single line. an isolated definition is the building of a relation, expanding arity, while an in-body definition is the evaluating of a relation, reducing arity. would it be possible to replace in-body defintions with isolated ones? The in-body definitions depends on the props of the body. thus a fixed isolated replacement would depend on these dynamic props, which is not possible. would it be possible to replace isolated definitions with in-body ones? our process of building is to continually build new relations on top of existing relations using isolated definitions. if isolated definitions are replaced by simply passing to existing definitions (no free variables), then if we want a binary relation we will need to generate it from a ternary relation. This method conflicts with out commitment to binary and unary relations. But it is still interesting to think how useful this would be. we could define basic but nontrivial n-ary relations then specify what n-m props to pass in order to construct a particular m-ary relation. The basic relations could be tree structure to refect that all formulas are a subgraph within that tree. So indeed all relations could be defined by passing to existing relations, and the base relations would be written without any existing relations. As said, doing this will not enable sticking with binary relation but will require higher arity. But the tree perspective may still be perspective, and shows how relations are stucture like boolean circuits and are thus amenable to complexity analysis and interpretable as a program. We will stick with isolated definitions and we can think of them as partially evaluated trees.

i thought we might have to choose between concrete constructions to make statements about, or distinguishing assumptions from conclusions. But maybe we can make implication statements without concrete constructions or assumptions. we create a definition with props as the variables we'd usually make assumptions about. then we show that this definition is full or non-empty.

now the question of base relations and to what extent we need explicit constructions. if we want props to be the only things unknown, then our first binary relation constructions will have to be over base unary relations. how can we construct base unary relations? we could axiomize some like natural numbers, and also axiomize some binary relations on top of them like addition. the only known objects without axiomization are I think the two nullary relations. we could make the unary relation of nullary relations, and then binary relations over that unary. But of course these are our most basic binary relations, the boolean operations. From here I'm sure we could proceed in many directions to construct anything. But we want to avoid that route. I'm wondering if instead we might take the route of assumptions but on a more global level rather than the level of theorems. So we can 'import' relations from a 'library', as if each relation is a function for programming. and that 'library' has an API that gives the type of its relations, that is what prop types they accept, together with semantics on what to expect in return, like a programming API would. in particular, our semantics are in the form not of written documentation, but language statements. you import from a library not knowing whether those statements are indeed true. your job is to use them to see what you can accomplish, maybe even a contradiction. it may be there exists a valid construction/implementation behind the interface (like regular programming libraries), or it could just be an interface with nothing to back it up and thus it may be false. unlike programming libraries which present themselves independent of other libraries (but may rely on other libraries in their implementation), it would helpful in our case for an API to present how the relations interact with the relations of other libraries so they do not present themselves independently. Now how to interact with relations from a library? I suppose all you can do is pass to them and use assumptions to infer behavior, and combine them with other relations to create new behavior, which can be justified from the assumptions. Thus I suppose a library statement (a theorem) can have a dependence graph of assumptions (other theorems). One may like to only import (and depend upon) certain assumptions, rather than all of the library. Should we then even have the concept of a 'library' as something that lumps statements together that regard common relations? rather we could regard every definition and every statement as an isolated 'library' together with its dependencie. what would be the form of a 'proof'? I suppose theoretically it can be a single large statement, but we'd rather modularize it to depend on lemmas. So we can think of it as the final statement that draws together many others. I suppose isolating definitions and statements and relying on a dependency DAG would be better than lumping together. a definition would have its own internal DAG as the body.
What are our rules of inference from a collection of statements (dependencies) to a concluding statement? I think we'd like to work with logic. What's neat about the nullary relations which represent logic is the boolean operations which we think of as ternary relations will be represented as binary relations (with the last prop inferred as true). So if I want to say x and y are both true as my conclusive statement, assuming x is true and y is true, then I write x AND y and looking at the explicit form of the AND relation which has a single connection from 1 to 1, I can infer that indeed since x is 1 and y is 1 they are related by AND as claimed. All statements will be binary or unary passing, but I'm not yet sure of the explicit verification algo. 

definition consists of
	type signature
		a type signature can be either unary or binary. 
		if unary, it consists of a known unary relation
		if binary, it consists of two ordered unary relations.
	a body which contains lines of passing assigned to a new name. 
		regardless how we present it visually, we need way to encode the body.
		for encoding we need a way of identifiers. 
		each pass is a node in the dag and consist of identifiers for its parts. 
pass consists of
	identifiers for the parts, 2 parts means unary, 3 parts means binary

what if instead of having definitions include a type signature, we infer the most general type signature. this is more minimal. then when it comes to asserting fullness and such we don't use a unary relation Full so assert the relation is full in its type signature as we planned previously. instead, we use a binary relation FullIn to assert that the relation, the left prop, is full in another relation, the right prop. But the fullness relation has no definition and is axiomized, and its type signature must also be axiomied. Ability to infer the types of props assumes knowledge of the constants types. Thus when a relation is imported without a definition its type must be given.
could we then just think of definitions as not isolated but part of the statement dag, except that certain parts are unknown (the props)? I suppose so, as it would allow assume maximum polymorphism implicitly. This borders on the previous idea of all definitions made by passing. Consider an n-ary relation containing no constants in the body, and we can model it as a sub-tree (maybe even model as partially evaluated full tree with props that somehow reduce it to its current form). Then pass in all but 1 or 2 props to make it a binary or unary relation. Not restricting to binary is essential for this method due to the need for n-arity. But restricting to binary (or some finite arity) is necessary for standardizing a tree form for all computations. I don't think the tree method is worth the complications. 
hmm, tough to choose whether to impose types or not.
Suppose we did and types and had a way to determine whether the body matches the signature. Could we easily convert this a typeless system? When we wish to pass we need to do a search on the maximum compatibility of the two relations combined. this seems tough. Its like this is a max-search problem whereas matching is a decision problem. so how can we reduce the search problem to the decision problem? 
What would a type matching verification algo work anyway? from the signature see the type of the props, and then using the dependence link see the type of the constants the props interact with, and check whether the type being passed exactly matches the type passed too. Its too hard inferring subtypes, and we leave that as explicit theorems to be proved, so we go with exact matching necessary. 
type restrict what you can do, but at the same time with no time plugging in something also restricts capability. If a relation is to fulfill the purpose it was intended for its best to put a type on it to guard and make sure no plugging in inhibits its ability to do that.
For now, without further reasoning, we'll just go with types.

Could we turn the following 3 prop expression to one where everything is a prop, and express it somehow within the bounds of binary relations?
a X0 X1
   b X1 X2
      c X2
sure, any n-ary relation including the one here can be made from a binary relation that accepts the first arg on one side and the next relation (which accepts the second arg etc) on the other side.

recursive types. we will need type variables anyway for things like monomorphisms where a unary relation is variable. I suppose we could restrict recursive variables to be presented in the form (T : ...) where T is the type and ... is filled with an expression containing T, and if it does not contain T there is no reason for T and the colon. A relation can have multiple recursive types connected with each other, eg G:(G->T) and T:(T->G). For Y we have T:(T) we can think of colon as a meta binary relation 'has type'. 
T:(T)

would it be possible and helpful to make every relation a binary relation. We'd need some kind of a univeral element. 
hmm, For sets X and Y there are 2^{|X|*|Y|} binary relations on them. Now if X is empty, then there is 2^{0*|Y|} = 2^0 = 1 relation on them. What is this relation? I think its the empty relation, as any instance would require an instance of X of which there are none. 
hmm, Remember for a fixed set X of size N, and there are 2^{N^n} n-ary relations. There are 2^{N^2} binary relations, 2^{N^1} unary relations, and 2^{N^0} = 2 nullary relations. But now suppose X is the empty set. Then for n>0 there are 2^{0^n} = 2^0 = 1 relations on X. Then again this is the empty relation. 
We need to note that there are many ways to specify a unary relation with a binary relation, all isomorphic. I suppose we want a unique, standard way to do it. The empty relation doesn't seem to help. I don't think this will be easy or worth it, especially the paradoxical nature of eveyrthing being a binary relation when binary relations are defined on top of unary relations and resolve to a nullary relation, which then must also be binary relations. 
Maybe we can find a standard for making all definitions binary relations, but we still keep the notions of unary and nullary relations. Usually we've taken binary relations as defined over unary relations, but what if at one end (say left), instead of a unary relation we have a nullary relation? this wouldn't be 'universal' because there are two nullary relations, but it may be close. Oh, hmm, now this would not be even isomorphic to having a singleton on the left, because a nullary relation is not the same as a unary relation with a single element. But we'd have to change the definition of a binary relation to be defined over not just unary relation but other relations, including nullary. We already have some boundaries as binary relations, but we are treating them as unary relations of instances. It would seem elegant to establish that an m1-ary relation on the left and an m2-ary relation on the right make an m1+m2-ary relation. We already have this for positive m1,m2, but we could extend it to the cases of 0 too. This gives us our hope as then a binary relation with a nullary relation on one side and a unary relation on the other side gives a 0+1=1-ary relation. Pass in 0 for the nullary and you get the unary relation that always returns 0, while pass in 1 and you some nullary relation on the other side (this is due to the conjunctive nature of relations). In addition to the ability to express unary relations (almost uniqely but not totally becuase its symmetric) as binary relations (and also nullary relations, though there are 3 ways to express 0), we can also now standardize a body form like a tree with branch count 3, that is a form with everything as a binary relation (though again not uniquely). And if we use the notion that we draw lines from whatever satisfies one side to whatever satisfies the other side, then putting in 1 for a side, it will be connected to all satisfying things on the other side. We can view any unary or nullary relation as a degenerate version of a non-unique binary relation. And remember we now have the 'left-right-sum' rule. 

algebraic data types
this only gets interesting with recursive types.
I think the question is, given a type, how many instances are there of that type?
So if two sets X and Y give the type of a binary relation, we ask how many instances are there of binary relations with that type. we don't ask how many instaces are there of a given binary relation of that type.
if we have a type Z, a binary relation over X and Y, then we say |Z| = 2^{|X|*|Y|}
if we have a type Z, a unary relation over X, then we say |Z|=2^|X|

types are confusing, like the difference between the type of a relation, and the type of a relation instance. remember we have generalized relations of all relations with a certain signature. how to indicate these? suppose we use the notation x:y to say x has type y. if G is a unary relation, I suppose x:G means x is an instance of G. What about x:(G->H)? does it means x is a binary relation from G to H? Then how would we say y is a unary relation on G? Would we say y:(G)? If we do this, parenenthases are being used for more than indication, which is undesireable. 
maybe better, x:G would mean x is a unary relation on G. Then x:y would always mean x is a newly defined relation that has signature y. This notation would not be able to express being an instance of, which I just realized we have always decided would take the simple form of adjacency, ie x y if x is an instance of unary relation y, and x1 y x2 if (x1,x2) is an instance of binary relation y.
But a notation still unclear whether (x1 y x2) which is equivalent to (x1 y .) x2, is also equivalent to x2 (x1 y .). from the previous explanation, we said x2 should be applied on the left of unary relation (x1 y .). the problem is the asymmetry of unary relations.
if we used directional infix 'instance of' operators, say <: and :>, would could write x:>G or G<:x to mean passing x to G. to stick with infix operators we'd then say (x:>G)<:y or the other way (proving associativity, parenthases could be dropped) to mean passing x and y to binary G. Note if G is binary, x:>G and G<:x mean different things as x is passed from different sides, but for unary it is the same. But this is verbose.
But we still have the confusing distinction between x:G and x:(G) or x:G->H and x:(G->H). Thinking of function signatures, to say x:G->H means x is a function accepting instances of G, whereas x:(G->H) doesn't even look like a complete signature, lacking a target, and it seems say the input would be of type G->H, meaning a function from G->H. So I suppose we must take parenthases to mean more than grouping indication, and (G) would be the unary relation of all unary relations on G, while (G->H) would be unary relation of all binary relations from G to H. 
So : (or something else) indicates 'has signature', while :> (or something else or just adjacency) indicates 'is instance of'. 
Would x y actually present confusion or would it always be inferrable which is being passed to the other by examining the signatures? If x is passed to y, then the left (same as right if y is unary) input of y should have signature (sig(x)) where sig(x) is the signature of x. Similarly, if y is passed to x, then the right input of x should have signature (sig(y)). Then sig(y) = (sig(x))->sig(rightY), sig(x) = sig(leftX)->(sig(y)). So if there is ambiguity it means both ways are possible meaning both this equations hold which clearly induces a recursion. Consider the subcase that x and y are unary, so we have sig(y) = (sig(x)), sig(x) = (sig(y)). or for show T = (G), G = (T). From this we get T = ((T)) and G = ((G)) implying G = T. I would hope to establish for the general case that when both directions of passing are valid, either neither direction will terminate, or both will terminate to the same. For the current subcase we've reduced to the case that both are unary having equal signatures T = (T). Suppose they both terminate but x(y) (functional notation) accepts while y(x) rejects. Rewind both recursions to the last pass before termination. This means x accepted without evaluating y, and y rejected without evaluating x. ... idk, and notation still visually ambiguous, maybe best to stick with a convention. Formally, probably best to encode using binary :> relation so only one pass happens at a time.

I'm realizing now an implicit plan I had before. That is, that isolated definitions be all pre-processed, that is the body of a definition cannot construct another isolated definition at runtime. I had the idea of only plugging when evaluating a body. But this is easier with n-ary relations, but we are trying to restrict to binary, which makes it harder, maybe not feasible. 
Can we have a constructivist way to convert an n-ary relation to a series of binary relations. we already know its possible, but is it straightforward? Suppose we have a ternary relation. Note we know the constants in this ternary relation, so given any partial application we can reduce it to a known binary or unary relation. Now construct the following binary relation. Its first prop, called p, is the same as that of the ternary, while its second, called phi, is a binary relation from the second prop to the third prop. Now for the body of this binary relation. First plug in p to the ternary relation, obtaining a binary relation. Then compare this binary relation with 1001 to phi to make sure they are equal. The problem here is that the body of the ternary relation has 3 variables, but our binary relation only has 2 props. so it seems necessary either to allow isolated ternary relations, or allow run-time binary definitions in body.
the problem with runtime definitions is they cannot be checked. it would be defining a top function in elm with type signature and implementation, inside the body of another function. This increases complexity as they could nest arbitrarily deep, and its ugly. i'd rather forgo the symmetry of binary relation to n-ary relation. 

example below that turned out not to require ternary or runtime-defs.
Suppose we want a unary relation with signature (A->A) that tests whether the relation p (the prop) is symmetric. The intuitive way to do this would be to construct the binary relation that is the intersection of p and its converse. Now the converse of p can be constructed in the body by passing to the binary relation for converses. Then take the intersection of these to construct a new binary relation, then feed this binary relation to the unary relation Full. 

can we confirm this is indeed a need a binary relations don't suffice? consider testing associativity from on (v->v->v) that is the set of all ternary relations on any set corresponding to type variable v.
we construct and compare for equality the two 4-ary relations 
x1x2x3x4R1 = ((x1x2_p) & (_x3x4p)) Empty Not
x1x2x3x4R2 = ((x2x3_p) & (x1_x4p)) Empty Not
so can this be done in binary?
before we can test for associativity, we need to represent the ternary relation as a binary one, say one with signature (v->v)->v, modeled with lp being the pre-image of rp. 
	btw, given this model, can we easily obtain alternative models, corresponding to the other two sides of the triangle?
	consider the binary relation v->(v->v), and suppose the original binary relation described above is the constant phi. now this new one is defined as 
		(lp phi .) 1001 rp. similar could be done for the last orientation.
the unary relation that tests for associativity then has signature ((v->v)->v). it seems doing this requires alternating orientation as shows above, but that requires defining a relation with p as a constant in it (corresponding to phi). but that would be an in-body definition. 	
what about testing for a commutative ternary relation. if in-body defs were allowed, we'd create ternary relations xyzR = xy(. p z) and xyzR ConverseXY. Then we want to say for all z, they are the same. This would be by considering the unary relation that plugs in for z and tests if the resulting binary relations are equal. We'd then say this unary relation is full. 
The inevitability of the need for in-body or more arity is the need for variables so that we can characterize with quantifiers. For example, I don't think we could even establish that binary relation is a permutation without first making it a constant embedded in another binary relation which we then quantify over.
But I still haven't established that binary relations are provably limited. Suppose for every ternary relation there existed a binary relation as above, with signature (A->B)->C. can we obtain a contradiction from this? what's important to note is the body of this relation can only feed pl and pr to more binary or unary relations, or feed pr to pl. supposing A and B and C are distinct, we cannot feed pr to pl. instead we must feed both pl and pr to another binary relation, in which case we have just move the problem to another binary relation and not solved it. Or, we feed pl or pr to unary relations and we combine the results. This would not work because intuitively, lp and lr may need to depend on each other more than just a boolean amount. Or, feed pl or pr to separate binary relations, get back unary relations then compare those. but it can be shown this is equivalent to transforming each prop without information from the other, then feeding both transformations to a binary relation. since the transformations are independent, i think nothing meaningful can be deduced, and then feeding results to a binary relation we know as before is just relocating the problem. this is a very informal argument/proof. 

Maybe we can return to n-ary relations but to some extent preserve the elegance of binary relations. one way would be to stick with binary, but allow unfolding the instance. we need a new notation for this. we could access props by prr,prl,plr,pll etc. But what about signatures? I suppose same as before, just that now when we have eg A->B where, say, A is a binary relation, we don't just have the symbol pl to refer to its instances, but instead pll and plr to refer to instance constituents. So the 'p-tree' will always be a binary tree. So now that an instance of A consists of a pair, an instance of the A->B relation consists of an pair with the left being an instance of A (another pair) and the right a instance of B. So we could consister instances of a relation as a binary tree. with leaves as atoms, and vertices corresponding to relations. 

relation variables for signature: tuvwxyz

Product : x->y
	1

Apply : Product<x,y> -> (x -> y)
	pll pr plr

Converse : Product<y,x> -> (x -> y)
	plr pr pll

IsConverse : (x->y)->(y->x)
	pl 1001 (Converse pr)

Alternatively, if we used n-arity and postfix this would look like

Product : x->y
	1

Apply : x -> y -> (x -> y)
	p1 p2 p3

Converse : y -> x -> (x -> y)
	p2 p1 p3

IsConverse : (x->y)->(x->y)
	p1 (p2 Converse) 1001

both are more unreadable than I thought. one has to constantly resolve by backtracing what kind of relation a given symbol represents and thus what rolw it plays. 
The goal for the project is an elegant and simple, readable way to succinctly represent expressions and statements. What makes regular notation readable I think is that concepts become familiar and prevalent enough that shorthand is applicable. But relations can serve like macros and also offer shorthand, so why does it still seem inconvenient? We need to keep all the bodies small, and that means minimal nesting in the signature so minimal props appear in the body.

how do we even compose two binary relations when restricting to binary?
for 4-ary we'd have
Compose : x -> z -> (x -> y) -> (y -> z)
	(p1 . p3) (. p2 p4) Empty Not
reducing to ternary would be
Compose : Product<x,z> -> (x -> y) -> (y -> z)
	(p11 . p2) (. p12 p3) Empty Not
then reducing to two and using infix
Compose : Product<x,z> -> Product<(x -> y),(y -> z)>
	(p11 p21 .) (. p22 p12) Empty Not
at this point its the identification of the props and the type variables that are ugly.

what about composing a ternary relation? gee, the way I had in mind means an n+m-ary relation to compose relations of arity n and m.
gosh, and I don't know how to compose a ternary relation in binary form with another, as is needed below for associativity. Given a binary relation where the source is another binary relation, how do we access the binary relations formed by the target with the separate left and right constituents of the source?

LeftEdge: (Product<x,y> -> z) -> (x -> z)
	q1 = pl ValidSource
	q2 = pl ValidTarget
	q3 = q1 ValidSource
	// somehow obtain all instances of x and z that connect via y, then form a binary relation from it, then compare that with pr
	// informally:
	let binary relation x->z be: 
		let unary relation y be: ((px, py) pl pz)
		this unary relation is Not Empty
	this binary relation 1001 pr

D: x -> z
	q1 = \p => ((pl Product p) D pr) Not Empty
	q1 Not Empty

EmptyIn: x -> y
	x xxxx y

Associative : (Product<x,x> -> x)
	...
	q1 = p ValidSource // set of all valid source instances <x,x>
	q2 = p ValidTarget // set of all valid targets x

ValidSource: (x->y)->x
	pr pl Empty Not
ValidTarget: (x->y)->y
	pl pr Empty Not

I think most of the inlining above would be compensated for by the common pattern of relations with a source or target of products.

would it make sense to replace Name<x,y> with x Name y ? Name<x,y> is the binary relation called name with source and target variables x and y. x Name y actually doesn't make sense in the case that x and y are relations and Name is defined on instances of them.
Could we increase readability by changing our signature notation? Instead of arrows, use infix of the relation name and its source and target. Then it must be known to be a signature/def or else it could be confused with a statement. 
we would need a notation for the generic unary relation of all binary relations from X to Y. We can't write something like X Rel Y because that would an instance of the relation Rel consisting of an instance of X and an instance of Y. Instead it should be a unit to signify a unary relation. We can't write X Y either because that would signify an instance of one consisting of an instance of the other (remember we haven't fully resolved unary passing notation). If we used Rel(X,Y) (which is not a good choice) then for the Compose signature I think we would write
(x Product z) Compose ((xy) Product (yz))

Maybe I should switch notation to using capitals for variables and lowercase for constants

Lets try again to formalize the way of extracting an edge from a ternary relation.
its so easy with regular notation. for ternary relation XYZr we just want binary relation XYr' such that xyr' => exists z such that xyzr. this was easy because we implicitly passed all the information from our premise to our conclusion, rather than formally passing with relation (in particular only binary relations).
to start, suppose we have r
(X * Y) r Z
	some def
then we make r'
X r' Y
	((pl, pr) r) Empty Not
but this encodes the constant r, which is not constant, because its the original relation we want to extract an edge from. so we need a way to put r in there via a binary relation. I think the only way to do this is to expand the signature of r' to include a variable. But now we must rearrange the signature to maintain binary form, and we subjectively choose X and Y to be together on the left, and the variable on the right (somehow we must always with a ternary relation, which two signature constituents to group together)
(X * Y) r' Rel((X * Y), Z)
	(pl pr .) Empty Not
which can be simplified to
X r' Rel(X, Y)
	(pl pr .) Empty Not
but this is actually just the validSource of pr.
now we pass the given ternary to r' on the right side, receiving a binary relation, then we compare that with the candidate binary relation that should match it.
Rel((X * Y), Z) getTop Rel(X, Y)
	(. r' pl) 1001 pr
which reduces from the observation above to
Rel((X * Y), Z) getTop Rel(X, Y)
	pl validSource pr

we should use the convention that if its natural to make a function, we should put the inputs on the left and the output on the right, making a binary relation. Applying from the left is then evaluating it, while applying from the right is inverting the function. If we made the convention that all unary relations are applied from the left (resp right) then the second evaluation would only happen in the natural order when the right (resp left) evaluation occurs first, which means we naturally always invert (resp evaluate) the function first. This is counterintuitive so if we have a convention to always apply to a unary from a fixed side, it should be from the right side. We could say whenever we see (x y) it means y is applied to unary x. and otherwise we'd see forms like (w z .) for the evaluation z at w, or (. z w) for the invertion of z at w, both forming unary relation, or lastly (w1 z w2) for the full evaluation of z at w1 and w2. eg from now on to say a relation is not empty we say Not Empty X instead of X Empty Not as we have been.

I'm tempted to use the notation X -> Y for the unary relation of all binary relations from X to Y despite its appearance as a particular binary relation labeled -> from X to Y.  

I think we can say a prop variable should appear in the body for every leaf of the signature tree. If any variable does not need to appear, its leaf should be removed.

How to treat unary relations of binary relations. Normally we'd say only binary relation instances can be unfolded as pairs, and unary relation cannot as an instance is only a symbol. But if an instance is a binary relation, we may want access to its source and target.
This brings up the the subjective question of how much we want to restrict via the signature versus the body. We considered before the possibility of only having a body, but the goal there was minimizing type assumptions. suppose we omit the signature and only keep the body, but equivalently require that each variable be delcared inside a unary relation. if its a unary relation we delcare p, if binary we declare pl and pr. But this would decrease readability.

A main motivation for this project is a readable system of reasoning for complex cases involving many parts. But I realized before, to make this system readible we can't be looking at too many parts at once, but instead need to encode appropriate macros to only look at a few at a time, eg in each definition body. Now I'm realizing regular verbal and written math notation does the same, only looking at a few parts at a time and that is why it is convenient. My assumption that more complex math would mean looking at more at once and require more readability may not be true. Rather, readability and complexity of notation may remain constant as topics increase in complexity. Sometimes that complexity of notation (eg lots of sub-subscripts) is too much for me, but now I'm realizing that complexity is constant (eg we rarely see sub-sub-subscripts). At this point this project would provide an alternative, more consistent form of notation, and maybe a way to formally verify proofs. but it would not necessarily provide more readiable notation. 
What I'm most in search of is a succinct, readiable notation, not necessarily one that is provable. I know I wouldn't convince others to use it, but I would be able to translate to and from it. But this extra work may not be worth it.
Looking at notation below is something that I've been aiming for, but its still hard to decipher compared to the shorthand of standard notation which requires implicit conversions which are not so simple to code.
(((p - 1) / 2) / p ) approx (1 / 2)

I'm so tempted to drop this project again as I have been before. The alternative would be using standard notation but with my own symbols when appropriate, and my own proof format, like noting which proof statements rely on which lemmas clearly, and which statements can be stated in 'parallel'. And I'd write the formal proofs on a site together with illustrations and interactive diagrams, especially with binary relations.

On the other hand, what about compromising by using an existing functional language. This would basically forgo the benefit of infix notation. Relations would be functions mapping to boolean values. here we'd be do n-ary functions/relations, rather than strictly binary and unary. I would have to axiomize full and empty, though not the unary relations of all functions with a certain signature. Now that we use n-arity I think we don't need to think about instances and the signature can consist of opaque sets but in nested format. I think the only way to prove emptiness or fullness would be by contradiction, assuming non-emptiness or non-fullness.

any quantification we observered before can be reduced to fullness or non-fullness of a relation. for a boolean valued function this would mean all outputs are true or at least one is false. Similar relationship for emptiness and false. 

I'm beginning to wonder if it would be just as convenient if not better to use functions instead of relations. suppose we did. would existing functional languages or lambda calculus be exactly the platform we need? I suppose so. But an important distinction of purpose is ours has no performance goals and can be executed slowly, only with the condition that it can reason about types and compute output. Developing our own lambda calculus would then take about the same effort as a relational calculus. And since the functional language we would have requires nothing by performance and just minimal functionality, it or a relational versional version should not be hard to implement. 

Using functions would probably mean dropping infix. We would be using univariate function, which is a binary relationship but i don't know that transforming to infix would make sense, like f(x) = y into x f y. This would be f1(x1) = f2, f2(x2) = y transforming to x1 f1 f2, x2 f2 y which isn't helpful.

I still like most the binary relations. maybe we can resolve the previously stalling question about signatures by considering function signatures. the function signature A -> B means given an instance of A, an instance of B is returned. (A -> B) -> C means given a function with signature A -> B, an instance of C is returned. But we can also say that given an instance of A -> B one of C is returned. Putting two together we can then say an instance of A -> B is a function with signature A -> B, so A -> B can be thought of as the set of all functions from A to B. Why do we run into trouble when translating this to saying A -> B is the unary relation of all binary relations from A to B? Everything was fine until we converted product<X,Y> into X product Y. 
before we had r : A -> B signify that relation r is from A to B, or in other words r is a relation consisting of an instance of A and an instance of B. we wanted to convert this to A r B. But then A -> B would appear as -> is a relation from A to B which we didn't intend. 
for unary we would translate r : A to r A.
the infix A r B is a bit misleading anyway since (A, B) is not instance of r.

another confusion is when to treat a binary relation as a binary one versus a unary one of pair instances. this gives confusion on whether to sumbit pair P to binary relation R by either Pl R Pr or R (Pl, Pr). The former can always be enforced, but what about the case that R is not known to be a binary relation? In that case it cannot be that P is known to be a pair either, so we write R P. 

we want an elegant, unified way to write the declaration r : X -> Y as well as the parameter type r<X,Y> which consists of instantances of r when over X and Y. we realized X r Y is not a unified way to do so. what about something similar but a little more specialized, like <X,r,Y> or (X,r,Y). 

hmm, what if we write a signature as a regular n-ary signature, using () for brackets, but then use a separate set of brackets [] to indicate the tree that make its all binary relations. but we still have the question of how to specify type variables. 

I realize function and relation signatures are fundamentally different. A relation signature consists of other relations, where we iterate over instances of those relation. But a function signature consists of function types which are like sets and not the same as other functions.
Its the fact that we interate over relations which themselves have signatures that make relation signatures complicated. A core difficulty is that when relation signatures have variables, when using that relation in the signature of another relation, we may like to have the type variables of both signatures in the same namespace. this was the motivation for notation like product<X,Y>

this is all simply too complicated to try to work with. we need to have the simplicity of function signatures. function signature can have variables. but we only iterate over instances of unary relations, not instances of binary relations. when iterating over a unary relation, one should only get a single symbol to refer to the instance. So if its a binary relation, one can only refer to the instance as a whole and that instance cannot be submitted to another binary relation and we really treat it as a unary instance. If we want to iterate over the instance of a binary relation in binary form, having access to the constituents, I think we must iterate over the source and target of the binary relation in question and then subject the two instances to satisfying the desired binary relation inside the body, leaving the signature generic. Maybe we could insist on the most generic form, that is only allowing variables in the signature, but still allowing nesting, eg (A -> B) -> C. the types of the signature constituents would only be determined by passing in something. I think this could work. simplest would then be to go with n-ary relations, maybe supporting infix as sugar. alternatively we could try to pursue this model with only unary and binary, but it would be more difficult. I think the sugar is better because we will be transpiling notation anyway. 

I think 'mixfix' notation may be more elegant than infix and also support for n-ary relations. mixfix notation in our case means for an n-ary relation, a symbol goes between each adjacent element. or we can generalize further to using placements, like exponentiation. in the case of a binary relation, mixfix can be infix. for a unary, it can be on the left or the right. I suppose partial application would be done similar to before, just placing a placeholder for any variable not passed to. mixfix could be converted to prefix or postfix. 

Remember in the constructionist way, base relations are on the two unary relations, eg the 16 binary relations with signature () -> () or the four unary relations with signature (). Maybe we can mantain that signatures are always of the form consisting of arrows between previous signatures. eg (() -> ()) -> (). So at the base we have () which is the unary relation of the two nullary relations, or we might not think in terms of unary relations but just call it the set of nullary relations. every signature is a set of relations, with constituents that are existing sets of relations. I think infinite sets can be made via recursion. but recursion requires variables in the signature. variables are what we will need anyway and one stands for a set of relations.
so we always iterate over relations, not instances of relations (except when that instance is a single relation). not iterating over other relations is a huge departure from what we had before.
now relations (except for the generic ones which we may not need to consider as relations at all) never appear in signatures.
when constructing a relation we are creating an instance of a generic relation.
we will have axiomized empty and full unary relations for all relation sizes.
at all points we should know the signatures for all constants. there should never exist a constant with unknown signature. in this case when performing any pass, it should be straightforward to check for consistency.

now for mixfix notation. we need a way to state the name for a relation for how it will be identified. we need a way to indicate partial application. suppose we use variable symbols (like single lowercase letters) and other symbols for signatures (eg ->), and symbols like words or math symbols (other than ->) for constructed relations such that the two are always distinguishable. then a relation name together with its signature could be stated by a sequence of relation symbols and sub-signatures, with the condition that exactly one relation symbol separates each subsignature, with the possibility of a relation symbol on either boundary (eg in if.then.else. where there is one on the left but not the right).
parenthases can be used to nest relations, but officially we will always use intermediate variables.
mixfixing may require a rule like the left boundary must always be a relation symbol in order to indicate what is being passed to what, eg in the case x y we don't know the direction of passing.
i think its fair to say that mixfixing is sugar and we can safely go with prefix with little to lose until we figure out mixfix. hmm, but trying prefix out, I realize its inconvenient, especially with partial application in any order. maybe its worth fleshing out mixfix now.
now using n-arity I suppose we use prop identifiers p0,p1,etc. since q follows p in the alphabet (and the two are symmetric), we could use q0,q1,etc for the intermediate variables. similarly symmetric, since the symbo -> is reserved for types, we might as well reserve the symbol <- for assigning to intermediate variables. another indexed letter like t0,t1,etc could be the type variables in the signature, or we use r for 'relation' and cuz its next in the alphabet. we'll also need to reserve a placeholder like the period. of course () serve a special purpose but beyond that we may not need to reserve any more symbols. in an official body all of lines with intermediate variables assigned to single passes, no parenthases should be necessary in the body, only in the signature. 

it seems the design of the best mixfix depends on the underlying language. I would like to use chinese.
chinese sentences have subjects verbs and object.
chinese is object-prominent, meaning that the object comes first in a sentence.
another way to say this is that chinese has a topic-comment sentence structure. that is, sentences are of the form of a topic followed by a comment on that topic (containing a verb). 

oh gosh, wait. relations will be passed as props and we will only indentify them with single symbols, eg p0. yet we will be passing to them. so how is mixfix supposed to work when we are pass to them? if we have a standard such as a sequence of relation and prop symbols starting with a relation symbol, then we can treat the relation symbol as a sequence and index over it, eg (p0[0] x p0[1] y p0[2].p0[3].) but in a much more elegant notation. and since we know the prop's type we should be able to know exactly how long the sequence is. in fact, the notation could just be using the same symbol p0 over and over with no ambiguity provided the whole pass is isolated with parenthases as necessary, eg (p0 x p0 y p0.p0.) or even (p0 x p0 y ..). 
I think the idea of a sequence is good, but the questions of boundaries remains. when we pass to a relation, we're making a statement, and the topic of our statement is what we pass, and the comment of the statement is what we pass to. for an object-prominent language then it makes sense to put the prop symbol first followed by the relation symbol, eg (x p0 y p0 ..). I suppose both symbols sequences should be the same length. notice this presents no ambiguity for unary relations. 
the other desireable mixfix design is having both boundaries as prop symbols. then we need to choose a side for the unary case, which would probably be the prop symbol on the left, relation symbol on the right.

what about something like composition where all mixfix words are not straightforward.
t0 <_> t2 <_> (t0 -> t1) compose (t1 -> t2)
this is because we intuitively think of compose as a function between binary relations, but we are forcing ourselves to create a relation from it.
maybe t0 <_> (t0 -> t1) compose (t1 -> t2) <_> t2 would be better, as then we could say
x1 (. y1 compose y2 .) x2 (but only if we're allowed to drop the <_> symbol notations all together instead of finding words for them).
i suppose we don't need a full sequence of relation symbols, and require at minimum one and it can be placed anywhere in the sequence of prop symbols. for example, in the case of one relation symbol on the left boundary we have prefix notation. we still require that all placeholders are present and that is what allows this with no ambiguity. in this case we could use the above form of 'compose'.
I suppose different relation names can contain identical words, even the same set of words in the same order if they differ in placement of props. its our job to distinguish them writing out every time all words with all props or placeholders (call them propholders) present. and we must keep in mind that no matter how similar two relation names are, if they differ in any way the may be completely unrelated, but if they are the same of course they must be identical. we could add the convenience rule that when two propholders are next to each other with no relation symbol between them they may be adajacent rather than separated by a space. we should not use a mixfix name as an identifier for a relation officially but rather use some reference id. a reference id referring to a relation can also refer to not just one but multiple mixfix names given to that relation by different parties. any new relation created should be compared for isomorphism against all existing relations, and it may become evident that two relations with drastically different names thought to be different are actually the same. testing for isomorphism is only difficult due to permutations of the prop order. but standardizing prop order would be inconvenient. instead we make prop order, just like names, a custom choice of metadata associated with the officially stored relation which will have a standard prop order.
maybe we can make mixfix even more flexible by allowing multiple relation words next to each other.
note that mixfix names can only be used for known constants, that is globally constructed relations rather than ones made from passing. in the case that a relation name is not known, ie the relation is in the form of a prop, we need a way to identify that in a pass expression it is the relation and not other symbols. one way is placing it between all prop symbols as said before, or we could just place it once on either boundary, eg left to make it in prefix form.

i think we should use the word 'type' instead of 'signature'. so type of a relation is what we previously called its signature. a type is a set of relations. 

Now I'm thinking about the trouble of things like composition that are standard but with our current construction would require a different definition for each arity and probably even for combination of vertices to be composed. this motivates a way always be able to decompose to binary relation, and the first idea to do that is iterating over instances of other constructed relations, but we went down that rabbit hole before and decided to return. without instances we must decompose eg a -> b -> c to either a -> (b -> c) or (a -> b) -> c which is not isomorphic but have much larger spaces of possibilities. they are only isomorphic to the original if they are functional. I liked this method and at one point was pursuing it, but there were problems. Look at line 507. I think the problem was mostly inability to build complex relations by passing due to the binary restriction, where normally n-arity would allow for that. 
Functions offer a solution, but i'm not yet sure why they would if relations do not. an n-ary function can have the binary form of input and output, both of them functions, so we can support n-ary complexity but only needing to operate on any function one part at a time. in other words, the benefit comes from the fact that eg a binary function can be reduced to a unary function from a set to another unary function. Consider the three sets of this to be A, B, and C. the number of binary functions is C^{A*B}, and the number of unary functions from A to unary functions from B to C is (C^B)^A = C^{B*A}, and the equivalence of these two reflects how one form can be converted to the other. For relations, on the other hand, I don't think we can convert a ternary relation on (one involving three sets like a binary function) to a binary relation from one set to a binary relation between two other sets. This is reflected in how the former corresponds to 2^{A*B*C} whereas the latter corresponds to 2^{A*2^{B*C}} which does not reduce to 2^{A*B*C}. 
So what is the 'cost' or 'loss' of using functions to solve the problem. One is that a signature for an n-ary relation now has n+1 parts, the newcomer being the last part which is Bool. 
I think the way it would be is having the single base set be () (which we mean to be {0,1}) and above that have all types be function types. so a function that we interpret as a relation is one with signature ending in (). () denotes intuitively, all functions that take 0 parameters, that is nullary functions, which are constants, and we decided our set of constants is {0,1}, so () = {0,1}. i think we will need symbols for these, i guess using 0 and 1 for now.
Inside the function I think we could either go like lambda calculus with abstraction or like we did with relations, just having an identifier for each input.
practice
lets try again composing the equivalent of ternary relations the usual way, then let us try taking advantage of function decomposition. we use right associativity for brevity. also i'd like to try posfix below. below we assume we compose last part of p0 with first part of p1.
(A -> A -> A -> ()) -> (A -> A -> A -> ()) -> A -> A -> A -> A -> () maybe write (A^3 -> ()) -> (A^3 -> ()) -> A^4 -> () for short
	q0 <- p2 p0
	q1 <- p3 q0
	// actually we have to 'flip' or rotate p1 to right by 1 first
	q2 <- p4 p1
	q3 <- p5 q2
	q4 <- q1 AND
	q5 <- q3 q4
	q6 <- q5 Empty
	q7 <- q6 Not
actually with posfix signature and assignment direction and arrows should be reversed.
now with decomposition

(A -> A -> ()) -> (A -> A -> ()) -> A -> A -> ()
	q0 <- p2 p0
	q1 <- p1 flip10
	q2 <- p2 q1
	q3 <- q0 AND
	q4 <- q2 q3
	q5 <- q4 Empty
	q6 <- q5 Not
hmm, this doesn't seem to work because the signature doesn't fit when we plug in ternary relations. dang, this is the 'cost', namely it simply remains not possible! its hard to explain why this seems to be the case, but to remind oneself it may be helpful to ask oneself the question of what the appropriate signature would be for this problem (composing two ternary relations using only binary).
(A -> A) -> (A -> A) -> A -> A
	q0 <- p2 p0
	q1 <- p3 p1
	... idk
i thik the problem stems from the fact that the binary decomposition form of a relation in function form consists of one part on the left, and the rest on the right, together with the fact that the left is what is accepting the argument. so when we want to compose, we want to submit to 'the rest' of the two ternary relations, ending up with two unary relations which we then take the conjunction of. but 'the rest' is on the right and we can only submit to the left, hence the problem. but we can flip the order we submit arguments, by creating a wrapper around the two relations that submit to 'the rest' first, leaving two unary relations to conjunct. but we still have the problem of yielding a function with a singature we can neither explicitly construct since we don't know 'the rest' parts, nor can we construct from 'the rest' parts implicitly it seems due to the un-associativity of signatures. eg suppose we want to receive two functions A and B and return one that accepts the inputs of A then accepts the inputs of B then does something with them all, even supposing we know the arities of A and B and even that they are the same. But we only want one function to perform this operation, we don't want to construct a new function for every possible arity. i imagine recursion can be a solution, but at this point I'm afraid our solutions are getting too constructive, where we are doing too much construction and it will lead our focus on details away from the main points. again, I'm tempted to just return to standard, ugly, implicit notation. heck, maybe chinese standard vocabulary (though the written part stays the same) would make it fun. 

i realized on the plane back to the china that what we would need to solve this is precisely dependent types, so we could compute the output arity from the input arities.

just as functions have a decomposition, relations have one too that we already visited, iterating over other relations, that may not have the same problem. last time we got stuck there with notation and concepts of how to specify signatures, thinking it was all too complex to even read. but maybe its worth another try. a relation is either unary or binary and iterates over one or two other relations respectively, and we treat those relations as the unary relations consisting of their instances. now the unary relation of all relations with a certain signature is also one we'd like to iterate over. maybe we can consider of such unary relations as sub-unary relations of the global, unique, unary relation of all relations, which we can denote * for now. Thus what we previously treated as (A -> B) we now treat as a unary relation over the unary relation of all relations. we will need special notation for the relation of all relations, and to avoid paradoxes we won't even consider it a relation, just some axiomized thing. thus the things we iterate over are always instances of some existing relation, with the relation of relations at the base. clearly we have a type hierarchy. the 'type' of a relation is its signature, which is its 'mother' relation. so only the relation of relations has no type.
Now we may be able to proceed with similar signature notation as we did before. first, recall the convention of which side the input goes on in the case of a unary relation. we decided the input should be on the right, to be consistent with modelling functions as relations with inputs on the left and outputs on the right, to in turn be consistent with thinking from matching the direction we read with thinking of functions from inputs to outputs. 
what makes this a little tricky is type variables. this is when a relation is a set of instances of another relation as usual, but that set can change depending on a parameter. so the relation together with values of those parameters must be given when the relation is used in a signature. elegant notation for this might be tricky. the reason types are useful is a relation may not depend on exactly what it iterates over, but just the relationship between those things it iterates over, in particular which ones are the same and which may be different (though we don't also allow for specifying how they may be different). is there a way to support this without types? i can't think of one. 

suppose we have a ternary relation by some relation R1 on A and B then some relation R2 on R1 and C. suppose want to create the relation R3 from B to C consisting of valid instances. it would be something like 
R3 : B -> C
	(\x => (x,b) R2 c) Empty Not
but that method uses an in-body definition, which we want to avoid. the reason is we don't have a method of partial application to instances, but rather require the instance in entireity. this may be a fatal flaw to the approach of using instances. 

i'm really about to give up, because i can't dream up another simple scheme and returning to any other (of which n-arity is the most attractive at this point) will yield troubles I've already encountered. For the case of n-arity the trouble was the need for redundancy between standard operations on relations of different arity. while there may well be a solution somewhere, I think it will not have the elegance that makes motivated this project and made it worthwhile. 


make matrix
make an array for each row
make matrix an array of pointers to rows
---
initialize an i size array of pointers
for each i, initialize a j size array and set the i pointer to it

set matrix
to set entry i, j
iterate through column i
  to remove entry, if encounter j set it to nil
  to insert entry, if not encounter j set it to entry

print matrix
print out grid
iterate through rows and print each corresponding character
---
get_screen reference_to_screen
print_character reference_to_screen character

print_character:
  c_code

multiply matrices
make column j
  iterate through all rows to complete column j of new matrix

make matrix, modify matrix, copy matrix, multiply matrix


do we want to test a network connection scheme by matrices?
does the matrix represent the network?

do we start with a random network?
does each node have the same number of contacts?
is this number N?
does every row in the matrix represent the contacts of a node?
does this mean we choose N random columns for each row, excluding the column corresponding to the node number?
is this how we construct the initial network?

  do we need to implement this?
  do we set aside the memory for the matrix?
  do we go through each row, choosing its random contacts?
    do we first choose a random number from 1 to colNum-1
    do we then choose one from 1 to colNum-2 and increment it if it falls above the first chosen number?
    do we continue like this until we have chosen N numbers?

does each node in the network then contact some of its contacts to get new contacts?
does it contact those of its contacts that are closest to it?
is there a max number of contacts a node can send to?
is this number S?
do we locate the S closest node in each row?
is the distance a function of the row number and the col number?
is the distance dependent on the type of space used?
do we construct a matrix just with the sending connections?
is this matrix called the sendMatrix?

do we need a matrix that represents the new contacts nodes will get after their inquiry.
is this matrix made by multiplying the sendMatrix by the currentContactsMatrix?
is the resulting matrix called the newContactsMatrix?

does every node need to re-choose its contacts among its current contacts and the new contacts?
can this be done by adding currentContactsMatrix to newContactsMatrix and trimming the resulting matrix down to the threshold of N?
is the first step adding currentContactsMatrix to newContactsMatrix?
is this sum called availabilityMatrix?
is the second step trimming every row of availabilityMatrix down to at most N contacts?
should a logarithmic distribution of contacts be chosen, based upon minimum distance?
does the resulting matrix become currentContactsMatrix?

do we continue this process over and over observing the evolution of connections?


do we need an operation that sets aside the memory for one of these matrices and returns access to its rows?
contactsNum * rowNum

to loop through the rows, do we start with the base address
base + 0 * num
base + 1 * num

do a task n times, with index i
if(n < 0) {
  give error
}
if(n == 0) {
  do nothing
}
// assume n > 0
next_trial_num = 1;
task:
  actual_task
  next_trial_number++;
  if(next_trial_num < n+1) {
    goto task;
  }



AKI: Amazon Kernel Image -- "Kernel loaded by the Amazon "boot loader""
AMI: Amazon Machine Image -- "Everything post-boot, including loadable kernel module. An AKI & ARI can be specified when starting an instance of an AMI"
ARI: Amazon Ramdisk Image -- ""Disk" used by boot loader during kernel load"; initrd / initramfs => initial ram disk / initial ram file system
loadable kernel module: object file with code that can be loaded to extend the running base kernel.
EBS: elastic block store



AWS forum: https://forums.aws.amazon.com
http://erlangonxen.org/blog/making-amazon-ami
http://www.nongnu.org/ext2-doc/ext2.html

i realized an autonomous system may be doable on AWS with flatlang. it only requires an amazon account, with of course payment (no identity info, eg proof of citizenship). previously, i thought signing up for this would require using something like phantomJS on linux, and having the another autonomous, or partially autonomous (where watching parties agree what it is doing), do this task. But of course a program like phantomJS running webkit is not doable in flatlang as it requires basically writing a browser. Instead, I was wondering if the autonomous system could simply be a unique receiver of the webpage and then broadcast it to a set of people at their laptops who would then enter the appropriate info and take appropriate action as if they were receiving it directly from Amazon, and then the autonomous system would receive this return web signals from the set of people, decide which signals are best, and then send those back to Amazon. the task could be broadcast through the blockchain. first problem i see is difficulty of acting as true intermediary, because the webpage brodcasted will have links back to amazon, instead of the intermediary, and replacing these links could be dangerous because some of them may be best kept as they are in order to do the task correctly. but perhaps this problem could be solved by it being decided before hand how the autonomous system should modify the page before sending, and this decision on automation could take place through the blockchain. interesting about this is the tasks that would go through the blockchain are not for computers, but for people, so maybe nodes would be told to redirect the task to a system with people ready to do tasks, the result of the task would be chosen and sent back to the nodes, and then the nodes would include that result in the chain. 

blocks: basic unit of storage
inodes: keeping track of stuff
block groups: split disk into manageable sections
directories: for hierarchical organization
bitmaps: keep track of allocated blocks and inodes
superblocks: parameters and global state of file system


different block sizes, probably specified in superblock
blocks put into blockgroups to prevent fragmentation
info on blockgroups put in discriptor table immediately after superblock

block groups:
two blocks near start of each group are for block and inode bitmaps
each bitmap only one block, and i think each bit in each byte of the bitmap represents one block in the group
after the bitmaps is the inode table, one for each group
then rest are data blocks

directories:
has inode
file records associating names with inode numbers
use singly linked list to store filenames in directory

inodes:
each object has one inode
has pointers to blocks which hold its data, and metadata, but not object's name
little endian format


MNT = /tmp/mirage-ec2
IMG = mirage-today.img

make directory MNT
rm IMG
fill image with a bunch of zeros
make ext3 file system inside IMG
i think it means mount the IMG filesystem into MNT
make MNT/boot/grub
make menu.lst and fill with:
  default 0
  timeout 1
  title Mirage
    root (hd0)
    kernel /boot/mirage-os.gz
move menu.lst to MNT/boot/grub/menu.lst
zip up APP and put in MNT/boot/mirage-os.gz
unmount MNT
make directory ec2_tmp


image is ext3 file system
  boot
    grub
      menu.lst
    mirage-os.gz

see https://mirage.io/wiki/xen-boot


----MEMORY----
pseudo physical page frames are for the kernel map with no evident ordering to machine page frames. these pseudo physical page frames are contiguous in kernel memory.
grant table: concerns access to memory pages, for shared memory
xenstore: a tree-like filesystem, can be used to discover shared pages.
ring buffers: for passing requests and responses between domains for IO. defined in documents, not code.


----CONTROL----
kernel runs in ring1
applications run in ring3
uses paging protection instead
...in x86-64, only two protection levels, so apparently kernel and applications run at same level


----HYPERCALLS----
interrupt number 82h used
... pass arguments in registers
event port binded to IRQ
generated via "hypercall page"
issue a hypercall by "calling" an address within the hypercall page
registers and memory are clobbered upon hypercall return
for macros look at extras/minios/include/x86/x86 32/ hypercall-x86 32.h


----PAGING----
...pages are 4kB, 2^{12} bytes
...page directory, what is it?

----ENVIRONMENT----
boots in protected mode, 32-bit mode i think or maybe 64-bit mode
real time vs CPU time
minimum needed is xenstore and console device drivers, better with also block and network interfaces
...start info page, shared info page, xenstore
#pages provided by start info struct
#vCPUs provided by shared info struct
use hypercalls to map machine addresses (eg those for pages given in start info struct) into guest addresses. these should be remapped on resuming the VM.

----EVENTS----
"register callback for event delivery", actually two callbacks, one for an error
when events are delivered, flags are set
...events can be masked
"all upcalls are delivered to the same address"
events delivered via "channels"
to send event:
  receiver creates new outbound port
  receiver advertises port (prob by xenstore)
  sender creates new port if none free
  sender sends
the bitmap i think is in the shared info page


----SHARED MEMORY----
two operations: sharing and transferring
...transferring expensive, maybe has been replaced with aliasing mapped memory
all shared memory manipulated at page level
done with grant table
shared memory region identified by "grant reference", an integer
grant references are communicated prob via xenstore
start info page:
  mapped by domain builder at guest boot time
  maps console device, whereas others must be found by xenstore
  address is store in ESI for x86
"store_mfn" references shared memory page for communicating with xenstore
"store_evtchn" gives event channel for notifications
the union is about the console; the console page is a shared memory page; an exception as other devices must be located via xenstore;
all fields above and including "console" are updated on resume, the rest are not and should not be relied upon.
...modules, files loaded into guest space?


----GRANT TABLE----
a grant reference is an integer
grant_table_op hypercall, three args: type of operation, array of structures containing operations, and number of operations.
can map or transfer a page
mapping is a pull operation
transferring is a push operation
gnttab_map_grant_ref ref and dom identify the grant reference and the domain offering the page, flags give options, host_addr gives pseuo-physical address in guest space page should be mapped to, status give result of map request, handle gives reference to the mapping use to unmap it later, and dev_bus_addr is a machine address for device IO which I don't think I need to worry about.
copying data between domains sharing memory may be more efficient than transferring pages.
gnttab_copy : a copy operation. source and destination pages can be grant references or machine frame numbers.
handles permissions
this table is an array of grant_entry structures
...do exercises starting at 4.4, page 69
each domain has its own grant table
grant references (integers) index into the grant table, identifying corresponding grant entries
grant references are the capabilities the grantee's use to perform operations on the granter's memory

----IO Rings----
requests and then responses placed inside, structures for each
power of 2 sized
start and end pointers for producer and consumer
...don't understand the procedure, who writes where


----SHARED INFO PAGE---
unlike start_info, this struct is dynamically updated
info about global state
vcpu_info_t.evtchn_pending_sel indicates which word of shared_info_t.evtchn_pending indicates the event
shared_info_t.evtchn_pending is 8 machine words long; each word in 64 bit mode is 64 bits, so this is a total of 8*64=512 bits, each one used to indicate if that corresponding event channel has events waiting. these bits are set by hypervisor and cleared by guest.
shared_info_t.evtchn_mask is checked when the evtchn_pending bits are set, and if the corresponding evtchn_mask bit is 0, the hypervisor makes an upcall to deliver the event asynchronously. this allows switching between polling and interrupt mechanism on a per-channel basis. (tip: use polling for busy channels).
CR2 holds address of last page fault
arch_shared_info:
  max_pfn: maximum physical frame number
  pfn_to_mfn_frame_list_list: machine address of frame that contains the list of frames that each hold pseudo-physical to machine mappings.

----XEN STORE----
implemented as a split device, with machine frame number given in start info struct

----RING BUFFER----
producer and consumer, a pair of these for each of two rings, one ring for xen, the other for guest


----TIME----
wc: wall-clock
...learn about time again later, and go through the exercise



----DRIVERS----
front end driver implements page with ring buffer and exports it to grant table, then advertises grant reference in xen store where back end driver retrieves it
front drivers use grant table to give access to back drivers

----VOCAB----
page frame: a segment of memory that can store data
page: the data stored in a page frame
IOMMU: imput/output memory management unit
DMA: direct memory access
HVM: hardware virtual machine
domainU: unprivileged domain
IRQ: interrupt request
PFN: physical/page frame number, a catch-all term not particular to guest or machine
MFN: machine frame number
GPFN: guest pseudo-physical/page frame number
GMFN: guest machine frame number, same as MFN i think for x86-64
TSC: time-stamp counter
i think xen_pfn means a machine frame number (mfn)
...GTF: grant table reference / frame, maybe, idk
GFN: probably grant frame number
WMB: write memory barrier
PTE: page table entry
...pvt: private

read 6.3
grant table gives shared memory
ring buffer builds on shared memory
xenstore builds on ring buffer



base instructions are actual hardware instructions
all instructions are base instructions or built upon combinations of base instructions
each instruction has an associated estimated cost
every instruction makes certain changes, equivalent to the writes it does, affecting the environment (aka state) which includes registers, execution mode, cache and other hidden stuff, and memory.
an instruction can be executed in any state, but should take care to only execute in a state in which its changes will not be harmful.
an instruction may make an assumption about the state it will assume. this assumption should be about values in registers and values in memory.
every instruction is an operation with parameters that should be able to tell you how it will affect state based upon your parameters. the parameters can be provided at compile time (eg hardware instruction operands, or a memory layout pattern for higher order) or at runtime (eg JIT for hardware, or a data structure in memory for higher order).
the program as a whole is an instruction. so the question is always, how is it best possible to achieve the functionality of this instruction using lower order instructions?
the amount of registers and the amount of memory are part of state.
a critical point of instruction abstraction is where instructions are no longer necessarily hardware instructions and don't reference hardware components (eg register) but instead operate solely on abstracted flat memory. hopefully these instructions will be most of what software is built upon. these instructions are distinctive in that their parameters are always memory locations.

the other possibility is that there is an explicitly divide between the front and backside of the language. the front side is simple, with instructions of any level of abstraction that will operate on memory. the backside is where the instructions from the front side are translated into combinations or hardware instructions. if one would like control over the hardware (to write drivers) one can write those hardware instructions and then expose them through the interface to the front end by wrapping them in custom-front end instructions. this would be similar to the way the higher order language C can call external functions written in assembly.

in both cases there is a central assembler that takes the instructions and decomposes them, optimizing them and outputting a sequence of hardware instructions. the job of the optimizer is to take those instructions that reference hardware instructions, and optimize their use of registers.

maybe i should design the front end first. maybe i can attract others to building the backend this way, and i can test for myself how much this front end language would be worth. it also forces me to consider portability. there are data movements, data modifications, data conversions, and of course combinations of them. absolute addresses can be 8 bytes, but to save space, a large part of their addresses in common can be put somewhere else, or implied. maybe i can forget floating point in the beginning. But multiprocessing would be a highlight so I should probably include it. given the front end as a memory interface, its probably displayed as shared memory, but if processors can send signals and use other means of communication and just like other operations, this could be abstracted mostly out of memory, but at least some part of memory must remain in order to expose the interface to the program since the programmer only has memory. instructions will be entities with hidden length. but they can be moved around to a certain extent, but not like data. for this and security reasons, areas of code should probably be distinguished from areas of data, though code areas can appear in different areas and be coupled with data for like an OO pattern. there are instructions that create other instructions, which can be used for self-hosting or JIT.

_____
realized a new design after a few important points:
1. data types will not just be simple known types (integers, masks, floating points) or composites of them, but even a simple byte may have a very particular purpose like in ocaml.
2. the size of a type should be part of the type. minimum size in one byte.
3. a type can be designated with a color to reveal its size and how it should be interpreted. types can be unknown when size or meaning is unknown and such unknown types are a particular type that belongs to the language.
4. instructions are types. only difference is they should only be put in executable areas. also, they should be able to modify themselves at compile time (depending on environment), but other data types should too
5. value of data types might not be known until compile-time or runtime.
6. there is no distinction of an item and a segment, because a segment is an item, so it is a data type.

so editing is about creating new items from scratch, by combining existing items. also the same can be done but with data types. but we don't need the explicit difference of classes (data types) and instances (items). it should be more like optional to save a data type as a template for future items. in other words, classes should just be data types the user chooses to save, so that they can retrieve it later and customize it. in this case, the user should document what they save and what parts of it are fundamental and what parts are intended for customization, so it can be used again later.

i guess then there's no distinction of data types and items, because data types are just items that are common enough to have been saved as a template. at the base are the machines the program executes upon. of course there are different machines and even future ones to one. so the bottom layer of items should take into account the best data types for the machine. the language assumes a common set of properties to all machines, namely digital data and a flat memory model (the way of using bytes, or two's complement, or a floating point format are not assumed and can be thought of as machine dependent, though bytes may be worth assuming because they will probably change no sooner than the large flat memory model, think about the parallella board). in the end, all we are doing is laying out bits in memory. i think the base should be we create items out of bytes with control at even the bit level.

for instructions: one item would be an actual x86 instruction, and several of these in sequence could form another item that is a higher level abstract instruction that is microprocessor independent. an item is defined by its memory layout (what items it is made of) and the metadata that accompanies it describing how it should be interpreted and what function it serves. thus two items are only the same if they have the same layout and the same metadata. items can carry references to other items. ...hours passed... these references should define adjacency of items, operands and results of instructions, metadata on complementary functions of items, the constituents of an item by reference to the item's constituent items and metadata on their purpose. with this design, the program is a graph where vertices are metadata with a byte pattern, and edges are metadata describing the reference. of course edges can be filtered (as can vertices), eg only see operands of an instruction. i like the idea of having minimal restrictions on format of vertices and edges, that is complete freedom, and relying instead on the tools one chooses to use to interpret those things. this is in the anyone-can-do-anything-at-own-expense philosophy of the blockchain.

collaboration: so that many people can work on the same program, maybe there should not be a model of everything always being in agreement at the global level. vertices can remain as they do because each one is already unique, no-one is privileged, but instead of global bidirectional edges, edges should unidirectional and declared by the vertices. this way two people can create conflicting references to the same item and the program must decide between which to use (eg two difference instruction items have an edge declaring below-adjacency (next instruction) to another instruction). the tough part then is entry points, eg how do you locate an isolated vertex that references something by nobody references it? i guess its not difficult, just show it along with the rest.
with this model, i suppose when an instruction writes to an item, it must designate how the item is to change, with the constraint that the replacement be the same size as the original item. this could mean the item is changed, or that it is destroyed and another is created.

maybe better a graph represents a certain state with a certain total size, and the only thing instructions can do is change bytes within that size, but not changing the total size. even if you only want to change part of an item, like the bottom half, you must replace the entire item, and it is up to you to keep the top of the item identical. but this may not be ideal, eg for performance when replacing an entire item when only a small change is necessary.
instead it may be better for the notion of 'changing' and item. so an instruction points to the item it will change, the new meaning the item will have, and what part of the item it will change, even if just at the bit level. eg changing a random bit in a bitfield is only changing part of the item but may change the meaning of the entire item.

an item should be a collection of pointers, one to its memory layout, another to its descriptions, another to its adjacent items, etc. this way it cheap to create a slightly different item, just change one of the pointers.
so maybe for any item an instruction would like to change, even if just a little metadata, it must create a new version of that item, which means a new collection of pointers.
snapshots: this instruction changed the entire state from looking like this to that, therefore the change made was this.
deltas: this instruction destroyed this and created this, therefore the new global state should look like this.
i think the latter is better because it does not assume a unique global state.

at any time, a state can be saved as a snapshot of a program, and all previously overwritten items need not be included or processed to reach that state.
in attempted conclusion, an instructions points to all items in the global state that it will read, and those it will write. it can write to any item by changing its layout, and additionally the option to change any metadata about that or any other item (but it cannot change any metadata without changing layout or some item).

i can probably get an idea how well this will work by using terminal as output (vertically stacked memory, items separated by new lines).

program structure:
an item: pointer to metadata about item describing both local and non local parts, size of any local bytes, pointers to any constituent items (non local). notice, does not include total item size, this way not recording redundant info.
an instruction, a type of item: important metadata includes pointers to reads and pointers to writes, and pointers to the new items that will replace the writes.

maybe instead of explicit metadata section, there should be a small fixed size context giver at the entry of every item that describes how it should be interpreted, like where its constituent parts are, and whether it is an instruction or not. this is also useful because users may which to attach all kinds of metadata to their items to keep better track of them, improve UX, etc.
the uncompiled program consists of an array of these variable-length items.

trying to choose, yet again, what platform to write this language for. my last idea is i would run it wrapped in C on my mac, and choose a platform later. maybe i can do this, but i'm tempted to know more about the environment in order to reliably run it, so i'm thinking about choosing xen, because people using this tool seriously will have to either run on a dedicated computer, or in the cloud, and running on hardware is hard so i can get help figuring that out later if it is worth the time. so for now, it should run in the cloud, and xen is more efficient than linux, though less portable. it is also more secure than linux. linux may still be better, but i should not choose it thinking people should execute it on their laptops. xen may also be simpler than linux.


a font 
can be anything that the user initially downloads, and then the dev sends SUCCINCT messages on how to combine them (in maybe a computationally intensive way) to render something. the point is most of the information is downloaded before hand, and small information can be sent to render. for example, a font could be a lot of symbols, each having parameters to be composed with other symbols. 

squares / unit (resolution)
units / page (size)
for now forget complexity of resolution and assume every square is a single unit, where a unit is the size of a single letter and assume preferred font size. everything else is based on this unit. so the only thing that changes is squares = units per page, that is the width and height of the display in squares (always assumed a rectangular shape).


fonts are tall and slim not square. 
a standard operation could be composing a small rect then scaling it by a factor
so if we use rectangles instead of square, we need to choose a good rectangle size. 
maybe the golden ratio, or maybe 2 to 1 so putting 2 together makes a square. 

(a + b) / a = a / b => ab + b^2 - a^2 = 0

so i suppose a font can have items for multiple ratios, the rectangle, two of them (maybe forms a square), etc. 

branch prediction, ie user action can be predicted, and changes for a set of likely user actions can be sent in advance. 

i suppose scaling shouldn't actually be done. instead font symbols should be designed to work for a certain number of units.

hexagons can be made from equilateral triangles. since our primary concern is letter that will need something more like a rectangle, maybe we should forget this for now. so for now lets keep it as simple as possible with rectangles.


have state encode minimal info. the is defined as the info necessary to render the current presentation of the app (what is sent to the world including user and api, along with their response handlers)
	state has knowledge of client's width, height, and other things about the ui, but state doesn't keep track of individual blocks (rectangles) unless it needs to. so it contains ui info but minimal info. thus the full output can be computed from it as a deriver.
then a second layer of logic derives the presentation from the state
then a third layer diffs the previous presentation from the next to produce the change
then a fourth layer performs the mutation

we need a programming model for deriving view from state. should it be a mapping of block to content, or content to blocks? the former would be have singature (i,j) => "font item", and the latter would have signature ("some part of ui") => [((i,j), "font item")]. the latter is more compicated and could have inconsistencies if it is not 'injective'. I like the former more and it could be understand by an external source that doesn't know of the parts of UI. so in addition to queries and posts, state's output could be such a function. actually state shouldn't output a function to remain pure, so thie view function should be a deriver. it appears we don't need a similar deriver for the network because network driver should be able to interpret. Well actually maybe a simple deriver that outpus complete requests and response handlers given the minimal state info would be helpful. So actually state doesn't output anything except itself. Drivers are responsible for delivering the view and network actions, even if they are just projection functions on part of the state. so there will be designated view and network deriver functions fo calculating the correct 'presentation' of the app to the world. later other aspects like 'sound' might be developed and each aspect as a corresponding driver responsible for showing the presentation to the word and reporting responses. now computing view: (i,j) -> fontcode for a given block is like a polynomial where even though only a single point is to be computed, the whole view logic must be traversed to evaluate it. how shoul this function be structured? 

what are the benefits of an externally executed client? devices have simple logic so are easy to make and implement. less band width used, presumably. but it seems client execution may be needed for quick enough responses. a standard would be needed for a language and its execution in this case. a benefit of local execution is not needing to compute and make and design a protocol around succint changes to be sent over the network. instead, the new output is just computed block by block with the existing one and whatever has changed is rerendered. in fact, maybe the whole thing is just rerendered on every state update. transformations don't make sense, because it is not preserving any work. instead its erasing and drawing as it would anyway. the DOM is a different story because it is not dead images but live nodes that contain a lot of info. in fact, using canvas for the web might be a good idea. for some reason I was imagining using the dom.

could the poly-ness of the view function be utilized for proofs? if anything, we'd have the property that two separate displays would yield two separate values when evaluated at the same point. thats obviously not the case.

so the whole app should have only pure functions. the state function, and the getters which can be pure and then transformed into memoized getters. what about modification? if these are pure functions they do not modify their inputs. could i invent a simple language for it now? it just involves parsing some syntax to express a pure function. of course make as general as possible. 

so derivers will be access during state updatesa dn also afterwards for presentations. the reason we can't just put derivers as part of state is we need state to be minimal, so it must be other functions (derivers) that return 
make a list of derivers, each one only deriving from those above it and state. think of state, though not a deriver, as being at the top.

the view function could be structure where it calls a all other functions with the same argument, each function for a certain part of the app, and each function decides if, given the state, that block is under its responsibility. if so the function returns a block, if not the function returns null.

suppose we now use the word drivers for the view and network and related functions. and we use the word extractors for the derivers because the words 'derivers' and 'drivers' sound the same. use 'model' for the shape of the state. use 'engine' for what we previously called drivers, the impure software that executes. use 'actions' for what the drivers send to the world and 'reactions' for the world sends back. this means replacing the word 'event' with 'reaction', but it can still be abbreviated 'e'. so drivers output actions. so we have 'model', 'state', 'update', 'drivers', 'actions', 'reactions', 'extractors', 'engine', 
event comes in. its fed to update with previous state. new state is made. the update function can access extractors with the previous state, and maybe values it in the process of computing. then the drivers (not exctractors cuz they don't need to be memoized cuz they are only called after a state update) drivers are fed the new state and compute new actions by accessing extractors with the new state. the extractors will form a DAG, with state at the top. maybe there should be a rule that state (old state for update and new state for drivers) be the only argument to extractors. seems to me most of the code should go in the extractors. should state be grouped by event or slice?


maybe instead of view trying to decide state in O(1) it could do it in time O(display size) by going recursively from the whole display to the smaller parts. first it looks it takes the whole display and state decide the primary, or first order lay out, by dividing into parts according to one of a few options. we could sacrifice an assumption that that display as a certain ratio and is in one of the two orientations. the choice of division must enforce that the dividing parts are of the same ratio or something related. then each of the parts would take their given size and orientation and state and decide how to divid, etc. the simplest division scheme has the ratio of 1.5:1 or the golden ratio. for 2:1, the 2 choices would be keep as is, split into 2 blocks. for 3:1 the choice would be the keep as is, split into a square and another 3:1 on either side, or split into 3, 3:1 blocks. That's 4 choices. For golden ratio, choices are same as with 3:1 exept no choice for splitting into 3. all of these cases means alternating between 2 differnet shapes when splitting. is there a shape that when split results in 2 of the same shapes? for 3 we know 3:1 works, but what about for 2? no not possible.
i actually don't think forcing a layout would be helpful, because in many cases, areas would have no role to play and would be left blank. well actually there is no requirement to divide in the first place, just the requirement that if you choose to divide you do it with a certain ratio such that the child components can have a certain ratio. actually this seems like a very sensible policy, this way 'components' can be made. however, some components may be monolithic, eg a text editor and choose not to divide at all. only those components composed of others are able to divide.
actually i don't want to return to a hierarchy of components. a motivation of this app was to avoid that go to a 'flat' single component. but it would still be helpful to pick a ratio so fonts can render is factors of this block too. 
i think the best option would be to have squares and have characters well suited for them with two stacked squares. fonts would probably not be per square then, but probably fonts would have items per certain ratios, and then output would give a font item, a reference point on where to place it, and then also a scale of the ratio. ratios could be expressed <width>:<height>, so a text would be 1:2 and scaled by 1. a square would be 2:1. default scale could be 1.


maybe we should rethink the mouse. terminal doesn't use one. do we need one? its usually associated with a graphical userface bu this app is focused on a text userface not graphical. well both mouse and keyboard are just ways to give input. mobile devides won't ever support keyboard so its better to think of them as touch. on a laptop a keyboard is more efficient and better input than a mouse. we should abstract away inputs. instead of saying 'click' we should say a 'block was selected' and instead of 'keypress' we should say 'use give you code xxxx (the letter)', so in the future these signals can be sent a number of ways. and each device will support different sigjnals. eg a mobile will support only block related input like selection. 

from now on lets use the words 'square' for the unit squares, and the word 'blocks' for factors of them. since we are avoiding a hierarchy there are only 'blocks of square' never 'blocks of blocks'. 


maybe think of vector space being groups of squares and scalars being the fonts. the orthonormal basis is the squares. the sum of two vectors (groups of squares) is the union of both groups. alternative basis could be made by dividing the grid into different groups. the zero vector would be no squares. actually this would be more restictive than a vector space. inverse vectors would be erasing squares. if the output is a vector we need vectors to not just be a rect, but a rect showing a font. a vector could be a group of squares with a font that gets drawn inside the group. even if the group has a different ratio as the font there could be a convention, eg that the (x,y) coordinates, that is the (left, bottom), are lined up. this probably wont' make a full vector space.
or we have could have scalars be integers that scale a font. and vectors are font items of certain scale. if vectors don't contain position we need a way to make a commutative addition operation. 
instead of font items with wholes, like a subscript, we could support downscaling, so take the main character scale it up then add the subscript as a regular square, then scale the whole thing down. this would reduce complexity a lot of both engine and app, but increase computation and rendering time for engine, yet it still wouldn't require complex graphics operations.
we'd like operations to combine two vectors left and right and also bottom and top. maybe a category would be more appropriate than a vector space. can think of a category as a collection of things called morphisms with certain properties. each morphism has a source and a target. for every thing that shows up as a source or target there must exist a morphism that has it as a source and target. for 2 arrows (morphisms) with chained signatures, a function must yield an arrow with the combined signature. this function must be associative, and when one of the arguments is an identity morphism the function must yield the other argument. 
i suppose instead of having left and right operations, there could be an orientation operation and a single concatenation operation. so right(X,Y) = concat(X,Y), and top(X,Y) = rot(concat(rot(X, 1), rot(Y, 1)), -1)
morphism composition could be concatenation. 
we could have an operation on a font consiting of a scaling factor followed by (1,i,-1,-i) for rotations. then we'd also need a single concatenation operation. 
another option that would allow for a commutative concatenation is to have not just rotation by translation. translation could also be expressed as expressing the new point of the origin. so we'd like a vector space where scalars can scale, rotate, and translate. this could be done with 3x3 transformation  matrices. 
actually handling the ratios of consecutive transformations is more complicated than i thought, because when combining characters of different sizes the ratios change. but then it would be that holes are still needed.
we could still allow scaling, but only no need for rotation or translation, and scaling down only allowed when it conforms to a proper factor. for example, to make a superscript, combine the superscript with 3 blanks on left top and right sides, then scale the whole thing to one half, this way the superscript now takes up the left bottom quarter of a character and can be place to the upper right of another character as a superscript. so what are valid downscaling values? it must divide both the width and the height. so the greatest is the gcd of both. upscaling can be any values.
anything can still be drawn in the limit of an infinite upscale. with scaling enabled, and being a major component, a view that outputs per block info will definitely not work. so a font item a drawing (maybe composed of others) with a ratio (its lowest form, no cancelling possible). so a font item can only be scaled up, not down. but when combined with others maybe after first scaling up, it can take a different ratio that can scale down even further. simplest example is adding blanks to an item until its double in size, then scaling down by 2.
in fact, maybe we could take the approach that 'everything is a font item' including the display. most of these are dynamically created. so every font item would be expressed the same, as a combination of other font items with scaling. assuming an identifier for every font item, how to express their combination into a larger one?
these dynamic font items can be designed so they can be form an amortization hierarchy. 

there's plan. use notation font_item(m,n) to take font_item with a certain ratio and stretch it to the ratio (m, n) which may not be in its lowest form. 

have a DAG of font items. each node has any number of parents. these nodes don't correspond to functions like for state, but may be created dynamically. it takes those font items stretches them and places them together to form a new rectangle of a certain ratio. this is the export. internally an export should be represented as a list of all constituent items, a scale for each one, and i guess the bottom left point for each. 
now for the notation of how to scale and combine multiple font items. for each item we give a new scale it should fit to (this new scale might have the same ratio or not, and maybe this should be made unambiguous). Then all of the scaled items are put together by composing one next to or on top of another. the latter could be done with two functions H (horizontal) and V (vertical) that take an arbitrary number of arguments and generate the correct object showing the relationship. one thing that's not explicit is a font item's ratio. knowing it requires going back to the function that generates it and looking at the output signature. hopefully this is not a problem. the system will make sure the output corresponds to the given signature in order to catch errors and hint as misuse of a font item. another way to help is giving two versions of scaling, one where the outputs are expected to be of different ratio and one for the same ratio, and if the given arguments are not of correct ratio an error is thrown. maybe SS for 'scale same' and ST for 'scale transform'.

font items are dynamic but they always come from some function. a function is called to produce one, and the system can check if the same function was called previously with the same parameters, such that the same font item would already exist. in other words a font item is defined by the function that creates it along with its parameters. for memoization we mostly care about path transformations, not colors. so maybe colors could remain an unset parameter where the result still allows you to se the color. the returned font item object could have unset fields for colors, eg color1 color2, etc. the simplest font item would have an inside color and an outside color. maybe we should only memoize scalings, both SS and ST. actually maybe we should memoize everything like the merging of two font items into one, which would include merging their color variables. 

catching clicks is hard now. take a block (font item) and give it an x,y coordinate and expect it to return the base block its on. if the block is a base bock it returns its own code. if its composed of other blocks it locates its constituent block that contains that x,y point, then forwards the request to that block. so how to we organize blocks to do this? its not possible to know all the blocks before hand so we cannot have a function for every block.

maybe use a virtual dom, where we return to the components method, but all components are pure. for now lets call them components, just like react. but unlike react components, a component here has to mother type and instead is defined by its children, how it scales them, and how it puts them together into a larger component. in fact, we should use the word 'block' instead of 'component' because although a block will be a function, it yields a rectangle so its a 'block' in that sense. like a component, a block (eg its children) can vary depending on its 'props' or parameters. so like components have instances, actually blocks are function that create block instances. 
so the 'virtual dom' or 'engine' calls the base block and it returns its description. of course if in cache it has the value of the block with the same params is uses that and stops. maybe we'd only have a one-layer cache where the same block instance (iblock) was used last time and now its just kept and not even redrawn. if the block is called with new params its recomputed. this involves looking at each of the children, obtaining each of them recursively, then scaling them and putting them together. maybe have block functions be memoized with their own cache that may be more than 1 level deep. with react comonents there's no such thing, and instead a new child is created from scratch for each component instance. since we now have a tree we can use jsx. a block could have the form
{
	block_name: {
		scale: [m,n],
		position: [x,y]
	},
	...
}
maybe the scale should be a property of the child, so you can call for a block child with your params indicating your desired dimensions. then different dimensions of the same block can be memoized within the block's function. this should be worth it because likely only a couple scale variations of each block will be needed. so basically the engine just looks at the children of a component, gets each of them necessary, recalculating if necessary, then then repainting those children necessary. this is the process of updating a child. jsx could probably work for this. so unilke state, the root doesn't return a large data structure cuz that would mean recalculating the whole thing. instead, the output is just the block functions themselves. the engine will navigate them when computing. certain components, eg the one computing a line of text, should have a much larger cache than others, depending on the variance of paramets and frequency with which its called. maybe these are optimization details we don't need to worry about for now, and a simple implementation could be no memoization at all and simply traversing the tree and completely recalculating and redrawing any children that have changed. 
another parameter (in addition to scale and others) from to a child should be about colors. that's simple, the child just has some color defintions and you plug them how you wish. likely this will require optimizations in the future, and maybe a change of api, eg using a theme context like a react context instead of passing colors directly. in fact, maybe this should already be supported where a block can set the 'inside' and 'outside' color and it will apply to all children, rather than going through each child constantly switching switching, eg when printing a line of text. 
now for events. when a click happens the engine can either calculate the base block itself, or better it seems, just return the xy position and then state can use that calculate via the block functions which child it lies it and the state need only calculate as deep as it wishes, whereas if calculation is left to the engine it must calculate the deepest possible and then there's also the question of how the engine would communicate this to the state. at this point we no longer need the concept of a square grid because it can go infinitely deep. but we shoud still have the conept that scaling units correspond to the standard unit, which should be the width of a visible english letter. 
maybe stretching unproportionally is not a good idea because it will be used not too often and will not look too good. a better idea could be parametric base blocks that draw the same pattern but according to a formula eg how tall you want it. the simplest parameters are width and height to keep the pattern consisten, this means width and height should be actual parameters of a block function, for the sake not only of scaling proportionally, but telling the child how to behave. but this brings up the problem of parent child communication. maybe the parent could give the child a few options, or maybe just two. or in other words the parent asks the child what it is able to do in a given space and the child tells the parent how it will appear so the parent knows how to arrange the other children. so for now suppose a width and height (in standard units) are given as params to each child. maybe every 

a single stream of blocks, each in terms of standard units. new lines can occure. a block can be composed of vertical components. if a block doesn't fit in a single line its width is compressed and height compressed proportionally, and the left sidebar shows its how many units it had to be compressed (an integer). the user can click on a block or navigate with keyboard using left side and arrow keys. user can select a block. or maybe no compression and instead the engine just shows part of the block and offers (ignorant to the app) a horizontal scrolling option. But the app needs to know where the line breaks are, eg to move the caret. maybe it could be sufficient to delegate the arrow keys to the engine, instead of the app receiving an 'up' command, the engine would intercept the key press and an intent to select a block and pass that info to the app. the app could always tell the engine to not intercept at certain periods. would this model, where the app is ignorant of width and height, be sufficient for other apps? how would a menu be displayed? every 'line', that is list of segment of the block stream containing no line breaks, must be formatted in a way ready for wrapping. so user feedback on mobile consists entirely of selecting blocks. output could be by components where each component outputs an array of children. the engine would just flatten all of them. maybe there could be minimum width of display guaranteed and a policy that every block must have width less than this or it will only be partially drawn. it could be enforced that all blocks on the same line have equal height.

maybe the best approach would be to stick with the philosphy that complex interfaces are harder to navigate and instead a complex interface should consist of a composition of a lot of simpler interfaces. so at any point in time the interface shows something simple, and a menu that may lead to other parts of the 'whole' interface. i think this is a better approach. so simply, if a child doesn't fit don't try to cramp it in, just show a link to it in a separate. but now keeping track of the tree of navigation is difficult. basically it looks like a routing issue. at any point, state can include what 'part' of the app the user is viewing, but then the menu would need to say what 'part' the the menu items shoud lead to. if the 'parts' can be expressed with near constant size this is ok. in this case the state implements a simple history. 

maybe the simplest model would be for the state to display all it has to show at once, and let the user navigate around that somehow with the help of the engine. so its a display of global state. user interacts anywhere and a whole new display is calculated, even if only part of it changes. so the user really is 'viewing' state, not just a part of state or as much of state as fits on the screen. in this case the view outputs whatever it wants and doesn't care about sizing. but of couse state size is finite so view will be finite. i'm thinking of prezi style. output could be a giant block and engine would help user navigate around those blocks with arrow keys etc and at any time the user can select block at arbitrary level of deepness, and engine would return the trace of that block. what about a text editor in this case? maybe this is where a standard ratio could help, eg a square. this would be for infinite zooming and every block, including those of math would need to have this ratio. the point is different devices will have different sizes so each will be able to view a different portion of the global view. so the question is how wide should a text editor portion of the view be?
suppose for a moment there is fixed size and ratio. then you layout text for that and when math comes you squeeze it in whereever you want, but it still occupies a rect. when selecting that rect you can view it alone. suppose it is not the same ratio as the display. its shrunk with the surrounding background on one side to the right proportion. if its a base font this is the end. if its composed of other blocks it can be decomposed further. at any time clicking or arrow keys can be used to navigate a level, and in and out arrows or buttons can be used to zoom in or out a level. thinking if this would work with text, i realize there must be a maximum on the number of children at a given level. one problem is when the state change, and the display with it, what would the next focused block be? maybe the same path number if it exists or its closest ancestor if it doesn't. or maybe let the engine set the active block or tree route. 

maybe right now it would be fun and productive enough to program math and technical diagrams. this would also give a better ida of how useful this block composition method is, and also give more time to think about if and how an app might best use it.
we'll use jsx. so there's a root 'view' that accepts the argument which in this case is source code, but first the code is compile to an object format. forget about colors for this. everything is black and white.
so a component has arguments for size and contents, then the parent positions it.

in an app it will be hard to build a code editor. for now we're not doing that.

Dec 17, 2018
building a math typestting for cavas as I said I'd do above still seems well possible though some small complications have come up like how to align a numerator and denominator on a grid when one cannot line up in the middle of the other. however that would only be a small part of the app. my goal was not a universal app framework but just this simple app for ideas as a bootstrapping phase to making ideas, which could include universal app frameworks. i'm interested in the area of UI for sure, and i'd like to give it more thought before going further. So now i'd really like a way to build this simple app without going too far. Last night i thought of one simple way to build to app, and importantly, I realized what may be a valuable principle for UI in the future, and that is the separation of 'viewing' and 'manipulation'. the command line does this well, but vim kind of breaks the line and browser UI's with forms and especially text editors very much break the line. the model i thought of that keeps them separate is having a file system as the basic model for the app, using a text editor to edit the files, and using the browser and a written problem to compile and render and present the file system in a suitable way, where file paths are used as roughts, and maybe a little input can be given (particular searches), but no manipulation of the content is allowed. To make the setup convenient, a local server could be setup to watch the files as they are manipulated and update the view instantly. the files system can maybe even be hosted externally. for a robust system, the server would'nt give you complete access to the file system but give you a copy of it, see what you change, then decide for itself if those changes are valid. if so it could implement them and then comute any necessary derived data which would still reside in the file system, but in a part not writabe by the user.
so righ now i'll archive this 'square UI' project and begin this new one in a new folder. 



the greeks and argumentation as it been for a long time is about winning someone over because ultimately decisions have been made in a democratic process. i would like to explore the possibility for arguing similar to the way it is done in science where nature, logic, and mathematical consistency determines whether the argument is valid. i like the idea of submitting something up, and by default having it take affect unless it is taken back down. i hate the idea of overshooting something because the deciding point is the average, and so by overshooting you're more likely to get your way. also in a situation where you want the other person to do something, you should think from his point of view what will be an input to him resulting in an output to you you desire. if a cop pulls you over and you start crying, he may feel very sad for you and want to not charge you, but acting in that way he will be doing something very inappropriate. instead you act in a way that gives him a good excuse to let you go. getting emotions and mood out of the context and doing formal argumentation is desirable. mood is huge, with guilt and all, and profiling, and reputation, in so much of everyday arguments. that's a different context. we'd like to strip it of all that since this is a formal context. past==blame, present==values, future==choices. a crucial idea i'd like to give weight to is the idea of argumentation between ideas, not people. what does this system offer that has not been easily offered previously in the context of argumentation? anonymity, trusted stats, a common resource of data. the idea of agendas fighting and not individuals is possible due to the anonymity implication. decisions are hard when there is uncertainty; about what the current state of something that is not easily measured or evident, an if this then what question concerning future implications that are not easily predictable, and about values like ethics about what should be priority. i think these three things are the three types of uncertainty that make concrete argument difficult. agree on goals, agree on how they are best achieved. it can be recursive where arguments and decision can be made concerning how decisions should be made. what should really be the deciding factor? it would be nice if it could be the opinion held my the majority of those who are well educated on the topic. decision are discrete in nature. it is like we do this or that. nature itself it not discrete but makes decision more continuously but there are tipping points. i'm thinking of args as like a centralized process where the combination and current state is central and from the periphery is the input of args and it is centrally determine what the outcome is and it is sent to the periphery. the system offers enormous use of computation and data analysis at a scale and in a trusted way unprecedented. what is the model, is it like agree on goals, then agree on strategies, or could this be generalized where goals are strategies and vice versa? call them items. items would be suggested, discussed, and agreed upon or discarded. once agreed upon a new context would unfold in which new items are suggested. i actually thought a lot about this in turkey. the model was present evidence, discuss options, and when something is brought up and someone says no, the two parties must resolve their own conflict. if conflicts cannot be resolved nothing happens. if a clear good idea is presented and it is denied with a stupid answer that won't give up it becomes evident the system should proceed with the request, so everyone makes the change, but there is no explicit change made, it is implicit. any system could have a current state, and that state could be described an sequence, or chain, or rules that have been passed just as the government can be described by the sequence of laws that have been passed. it would be interesting to explore git source control.

alienation
insecure
unstable -- crises of abundance, we produce more than we consume.

argumentation would be a recursive process of arguments. suppose you start with one argument. in order to conclude, the assumptions, definitions, and other elements must be identified, and those items themselves can form arguments, making the process recursive. argumentation would not just employ logic, but also probability, because we would not only seek correct arguments, but most plausible arguments.

whats true of the part is probably true of the whole, and vice versa
part to whole: generalization. whole to part: classification
fallacy of composition. fallacy of division.

f(x) = x
g(x) = x(x)
g(f(x)) = f(x)(f(x)) = f(x) = x

i've decided this idea is not very appropriate for fully encrypted programs.
the extreme example of an autonomous organism that spread across all machines, and the example of encrypted code that runs on someone else's machine, is perhaps possible but will require far more information hiding through type abstraction than I anticipated. combining appropriate patterns together with morphing those patterns over time (to make it autonomous) will make it all even more complicated. protecting against blind attacks (that is someone running the program alters the program and thus the outcome) such that they can be detected is necessary. this may be an idea worth exploring in the future for applicability to decentralized applications, but the idea does not have the simple nature i anticipated and therefore i will drop this idea in the regard I conceived it. i will continue instead with the three-tier system, in which i will have to deal with autonomous, self mutating cells anyway, but without the cryptography.

i just thought that although there are a finite (and low) number of practical type abstraction substitutions, since one expression is used for many things, replacing all instances of say $x.$y.x with True will replace instances of it that were not intended as True, and therefore be nonsensical. how could this be modeled? could it be such that the number of trials before meaningful substitution grows with respect to exponentiation rather than multiplication?
suppose N of one expression exist. suppose M of it mean exp1 and N-M of it mean exp2. then there would be ( N choose M) possibilities for the distribution of the two expressions. it is ideal if M is about half of N. assume a compiler can view the code and optimize for sufficient information hiding. how could this be used? this could be used to make personal avatars. there also is the idea that no one could know the secret key. instead the code would be passed around and compiled as it goes (suppose this is possible). but the code block would be restricted to standard input and output. every cell (including messages) could have its own secret key. think of this as each cell speaking a different language, yet two cells can communicate by each translating first to standard language. a mast could translate standard input to secret input. it would function like a public key for encryption. a public key for signatures could be a mask that takes a secret signature and outputs standard signature. if this could all work it would mean the three-tier cell system could function with all private messages and cells.

i'm thinking to write about my theory of cells. i will first, and many only, explore this theory in an environment of trusting, non attacking nodes.
a cell is an abstract object. it can send and deliver messages. at any point it in time a cell has a state. the only thing that influences the state is any incoming messages. given the sequence of incoming messages, one can derive the current state of the cell, as well as the sequence of outgoing messages. a cell can create other cell (such as its duplication), can self destruct. i will argue that a certain programming language can describe anything a cell can do. I will now go learn about different programming paradigms, functional programming and imperative programming.
it seems message authenticity is important. a cell will have to not just judge the content of the message but the sender of the message. the content cannot reveal the sender because all content of an account sender is public as part of the account. it could have a random solution, where a message is sent by an account by being signed by a random collection of nodes. in a trusting network, message authenticity is unneeded.

start from bottom.
assumptions. there are nodes that can send information to each other. they do not trust each other. the goal is to let an honest node be confident certain information is valid.
drawing line. suppose you are given information. do you verify the information by validating it yourself. or, do you trust that the information is correct by some other means. the former is unreasonable for too much data. the latter is therefore chosen.
...
multiple nodes much do a process.
nodes doing the same process cannot collude.
a node doing a process cannot know what other nodes are doing the same process.
nodes cannot choose what processes to do.
processes must be randomly assigned to nodes.
distribution of processes must be on basis of effort (work or cost).
- so its like whoever did this work does this process. it can't be that once assignment is made, you can look at what work is required for what process and then focus only on that work and get a much higher probability of doing the work that assigns you to that process. this can be prevented two ways. first, the assignment could be a one-way function from work to process, but not process to work. second, the work could be designed such that any work after the assignment doesn't count. how could work before the assignment be distinguished from work after the assignment? one way is that work would be publicly known, such as on the blockchain. an important question is how does a node that has done work, use that work to prove their rite to a process? work could be mapped to process, but then a node must prove the work. and the work cannot be publicly associated with a node or the condition on line 11 would not be met and collusion would occur.
- mapping work to process. mapping work to nodes. mapping processes to nodes. that is: work <-> process; process <-> nodes; nodes <-> work; this is a circle. work <-> process is for randomness must be public; process <-> nodes is for actual assignment must be private; nodes <-> work is for verifying assignment must be private; work could be tied not to nodes but to public keys that do not reveal the node or provide any way to contact the node. then it would be easy to secretly map work to nodes, and verify that the work was done by the correct node. what is the work? what is the mechanism for passing around processes and messages? how do processes get passed to nodes without revealing who the node is? i imagine this can be solved by a dynamic network mutation algorithm. it would be best if the work could be work necessary for the system. such as processing, but the two problems there are work must be easily verified correct, and this is a circle where the rite to process comes from previous processing, so what gives the right to process initially? maybe processing can indeed be work, but it cannot be the only work for the two problems above. work could also be storage of stuff, the only problem being nodes should have a choice between storage and processing without a requirement to do both as some nodes would be better suited for one or the other. storage work could be the initial work that can lead to processing work which can then be a work replacement. what about message passing? to prevent message forgery messages must be signed, even messages from public accounts to public accounts. this can be done by the message being signed by the node that processes the account when it sends the message, with the same public key as is used to verify the node's rite to that process in the first place. instead of passing messages directly to nodes, maybe all messages could first be placed in a blockchain and then accessed by all processors. there could exist a giant data structure capturing messages, processes, which can guide the routine of storage and processing. first imagine a message blockchain exists and is functional. maybe messages can pay a transaction fee in order to be included in the blockchain. storage is a matter of storing messages and processes. maybe there could be a process tree. does each block of messages go to single process? cells are divided and designed such that each has an appropriate balance of the variables: raw size of cell, average time to process message, number of messages awaiting processing. i think size can be independent of the other variables, and size must be in some range. i think the relevant combination is (average time for message processing)*(average number of messages to process). but even if the product is in an appropriate range, both of these variables must also have appropriate ranges. (average time for message processing) determines vertical division, and (average number of messages to process) determines horizontal division. so the number of messages processed by each cell per unit time is different across cells. the message tree should be organized by cell address. if this can be done then it is clear to the validators what messages they must process when assigned to a cell. the validity of any stored message is evident by a merkel proof of its presence in the blockchain. after each block of messages is processed the new state could be hashed and put in a tree so that a stored version of the new cell could be verified by the next node that processes the cell. suppose a node has processed half the messages for a cell and then the cell divides. when a cell divides a filter is created. when two cells fuse a filter is destroyed. maybe building the blockchains can be thought of as processing. processing a cell would mean searching for messages, the only difference that you don't have a blockchain to tell you what the messages are, you have to go find them. same for higher level blockchain validators.
i'm thinking too modularly with blocks and everything. i need to think more dynamically for such a large scale, like more with data flow reactive programming. imagine a system of data flow between nodes. the power of a node to manipulate data depends on how much data that node processes. the first concern that comes to mind is how to store large chunks of data when the paradigm is about constant flow of data. i guess nodes could store data just like they would for the more blockchain model and prove they are storing it in data flows and pass the data when requested in data flows. the advantage of this model is that it forces conceptually a decentralized concurrent architecture, whereas blockchain is an attempt to implement centralization in an environment of decentralization. streams could contain anything. think in terms of functional programming. data is no different than operation. everything can be described by lambda calculus. in lambda calculus there is:
<expression> := <name> | <function> | <application>
<name> := any sequence of characters
<function> := $<name>.<expression>
<application> := <expression><expression>
nodes could pass expressions in streams. to reflect on reactive programming, its where there are streams which are sequences of data. you can apply functions to streams, which always create a new stream, by mapping, filtering, or modifying the elements of the stream in some way, but the original stream is left unaltered.
i thought of implementing lambda calculus programs where the types are secret and thus you can execute a program without knowing the meaning while the types are kept secret. programs could convert between types. i will call lambda calculus notation 'symbols'. data could be stored as symbols. even hashing could be done by symbols, and the hash stored as symbols. importantly, you need not know the meaning of incoming programs to execute them. an un-meaningful program could be devised to prompt the executer to send messages as well. this means internet addresses must be yielded by a black box program. i think there could be a standard notation that any program could convert its output into, and any program could convert as input into its secret notation. in a system, messages are submitted to accounts, accounts change state, and can send messages out. is this concept of accounts and messages applicable for this scenario? we can still view everything in terms of cellular biology. can accounts exist in a data flow framework? information would need to navigate efficiently. throwing around programs of unknown meaning maybe this will be easier than before.

what about the external, autonomous computer question. the benefit of these computers is that they could hold and act upon private information. they could be trusted by all to hold and act responsibly with private data. they could also be used and trusted in certain contexts for immediate decision making where the blockchain is too slow. for these computers to function they must have random variables that no one else knows upon which to base encryption keys. suppose there exists such a computer, call it an auto. autos can change depending on their input, or which one type can be the blockchain. so we may suppose autos can perform arbitrary functions almost as if a human. an auto could use its own private info to set up another auto. the code that will go on the auto could be public like on the blockchain, and it could be public the format for which the first auto will send its random variable, or multiple, or private data, over to the new auto. this way it is clear, not only from the trustworthiness of the first auto, but the format presented by the first auto, that the first auto will honest set up the second auto without corrupted code. the public code should be analyzed to be uncorrupted regardless of what data the first auto will send it. the first auto will give the new auto a secret random variable that the new auto will use to make public keys so that it can receive data and create new random variables that no one, including the first auto, will know. the question then becomes, how is the first auto made. in fact, the first auto may only need the sole function of holding a secret key and receiving data with which it may generate new keys with which to set up each new auto. so how does the first auto get its secret key? maybe the first auto could just be setup by some trusted party. i suppose there would be a public code repo hosted on the blockchain or some auto, that would be the base code for each auto. this base code would be the model solely for how an auto modifies its actual functioning code, as it what constitutes a valid modifications of the code repo involving data on the blockchain or whatever. how close is an auto to a human with a personal computer. well just like a personal computer comes equipped with base code and adjusts itself according to input. for the personal computer the input is from the keyboard, mouse, and messages. for the auto it comes purely for messages, which could theoretically simulate keyboard and mouse signals. thus an auto has the same capability as a human with a personal computer. suppose a human with a pc wanted to setup a web server; the person would go go to websites and follow around links entering info such as financial info, and then receive info that gives exclusive access to web server control. just the same, an auto would use code to follow some api and enter info along with it, such as financial info the rest would follow similarly. but how would an auto provide financial info for dollars? well there would be a cell or an auto, just think of some application, for which you could trade digital currency for dollars. that is, you could point to a digital lump, call it ether, and the app would facilitate the exchange with someone else who points to a lump of dollars. i guess dealing with dollars would require a bank account for a cell or an auto. an auto with private info could setup of an exclusive access bank account, exchange ether for dollars, fill the bank account, and the pay the web service. can the code of an auto be reviewed, confirmed at any time, without having direct access to the auto? an auto could be told to show what code its running, but could it be trusted to show honestly? return again to the question of how an auto can be setup from scratch without an existing auto. this is where some confusion can arise as to what assumptions can be made when reasoning through this. first lets try the assumption that a computer exists on the network that nobody has access to, but the code that the computer runs is public. is there a way this computer, call it the central node, could develop a secret variable, that no other node on the network knows? we assume the only thing that can affect the central node is input from other nodes on the network. if any node can surround the central node and be aware of all messages that central node receives, the surrounding node has all the info the central node does, preventing the central node from creating any secret variables the surrounding node is not aware of. thus for the central node to create a secret variable, it must have access to a set of input that no other node can also knowably find access to. this may seem possible through the random network mutation process, where any node on the network does not know exactly who its neighbors nodes might contact. but the problem is in physical reality, a node, with proper capability, can definitely surround another node, and if that central node has no secret info like a private key then the surrounding node can have just as much info as the central node. so if physical surrounding can be prevented then maybe this could work. physical surrounding would be prevented by means of having the central node located in multiple, unpredictable, physical locations. but this requires that these different locations act as one node meaning they must be able to communicate with each other secretly meaning they their data is not all public contrary to our assumption. is it possible to initialize an auto from the blockchain, or only from another auto? the problem with autos is you don't have access to the full code. ideal would be to have a computer that runs public code (and evidently so) but may hold private data. this auto needs to have secret data that NOBODY knows, such as its private key. this means the auto cannot be surrounded. or it could mean the auto is setup by someone who gives it a private key and that person is the only one who knows the secret info. but if that person is not the one who surrounds it then that person does not have access to what the auto will receive that it will transform together with the secret to generate a new secret that neither the initiator nor the surrounding node knows. it must be that the surrounding node does not have access to the central node's code itself, only to the messages it receives. it must also be, which is more difficult, that the initiator loses access to the central node after initializing it with base code and a random variable. if the proper code is implemented this code could ensure that it grants special login access to a specified source that will provide the random variable. suppose the auto is setup so that it can accept the variable without granting the provider any control over the code, and once the variable is provided, perhaps from multiple sources, access is shut off completely, and then the node can take the variables provided, mix them in a random way, and create a public key with which to receive new messages with which to create new keys. if the initiator node could provide the surrounding node with the secret variable the two could collude and have full access to whatever the central node has access to. suppose an auto could be verified at any time what code it was running without revealing its private data. I think this ability is required anyway. In this case, the first node could be established with public code, verified. and then a public demonstration could happen whence a random variable is generated and handed to the first auto and from the method of generation and public display it is evident nobody, not even the generator knows exactly what it is. the random generation would be don probably by some physical process. i guess the initialization process would be just the same as an individual with a pc setting up a node, except this would be publicly done with evidence the computer that did it did not reveal the secret to anyone, and the source of the secret would have to come from a random physical process. how could this public demonstration occur? perhaps it could indeed by a grand public demonstration because it may only need to happen once. theoretically in a public square a person with a computer could set up a node. so practically this would occur by there being in a public square an embedded system and a physical process system such as a little radio active device with a particle counter or something. the embedded system would read the random variable from this counter, use the secret as it sets up the node, and then the system would erase the secret so it probably doesn't have to blow itself up or anything, and the system could be reused. it is essential that the system would have known and limited communications so it can't send the secret somewhere else before erasing it. so it must be known and confirmed that the system is set up to run the proper code. it must also be confirmed that the hardware of the system is properly designed, and that the software it put on the hardware. maybe this stuff could all be setup on the blockchain. the software would be easy, but the hardware and the physical demonstration would be more difficult, but i suppose the blockchain could indeed figure these things out and how they would take place, eg. this money is going to buy this hardware from this party and be shipped to this location where it will be combined with this software and then this party will do these actions at this location to run the initialization process. maybe its time to learn about computer systems.


the message tier, the state fingerprint tier, and the storage tier
suppose there is a mechanism for sending out messages (and getting responses) without any clear revealing of the sender or the receiver. this shouldn't be too difficult.

provided each tier functions, i will describe how the tiers relate and what the process of the system is involving the piers. then, i will describe how each tier works within the system of the other tiers. i will then have comprehensively described the system.

Assume the three tiers work. the three tiers function as follows.
message tier: a data structure that holds fingerprints of all messages.
state tier: a data structure that holds the fingerprints of all states after each group of messages is processed.
storage tier: a data structure that holds all messages, current states, and the data structures of the three tiers, each thing in storage called an item.

messages are grouped together organized by address and put into the message tier.
states are evaluated with a group of messages.
nodes do work, and thereby earn the rite to evaluate. work can be redeemed for rewards in addition to being redeemed for rite to further work, and the order of redemption does not matter. the first type of work is storage. suppose the storage tier functions such you can ask the network to store something, periodically check that the item is stored, retrieve the item at any time; those nodes who store the item can point to the storage data structure and provide proof of their storage work for anyone to see at any time after the work is done.
there exists a rv, that is a global random variable, that is random yet retrievable by anyone. this rv provides a mapping between work done and states to be evaluated. at anytime, a node can take the rv and the work the node has done, and derive what states the node has the rite to evaluate at that time. the node can then retrieve the state and the messages to evaluate, perform the evaluation, and then broadcast the result. the result is received, its fingerprint put into the state tier, its data put into the storage tier. this provides the mechanism for cell evaluation.

we now consider how each tier works. we would like to model each tier in terms of cells. let us begin with the message tier.
messages are broadcast. according to address, they are collected by certain nodes. when evaluating a state a node must query for the state, then query for the messages, and check for proof that they are the correct messages. similarly, a node building a level of the blockchain must query for the state regarding that level of the blockchain, query for the relevant messages, but not get proof they are correct for they have yet to be part of a blockchain. redirecting can be another task. a state that must be evaluated could be a redirection tree that queries for all messages that query for an account and redirects them to a different account. when a state is evaluated it may send out messages. the node evaluating the state has the rite to sign the messages, broadcast them immediately, and include them with the state output. after the first level of message collection, the message blocks are broadcast, and the next generation of nodes assigned to state evaluation for second level message compilation query for the first level blocks.
maybe there can be an acceptance of work record for each block, indicating which nodes are ready to do work for the next round. but a trouble is they should prove they have the rite to do work so that assignments are not distributed wrong. so maybe acceptance of work can be done after assignment, and a corrective assignment after that, but that would not help because acceptance of work would still require proof of rite of work.
a linked data structure is important so that the history cannot be altered. the present points to the past. what prevents one from generating an alternative history? to do this would require generating messages and evaluating cells. i could store a bunch of stuff, generate a bunch of messages, evaluating a bunch of cells pointing to my storage. in order to have a linking data structure when anything changes, everything higher level must change. if everything is changing at different times the top will be constantly changing at arbitrary frequency, which is unsustainable. for the top to change at finite frequency, everything must change in sync, so cells must be designed so they can be evaluated and collected in about equal time. this would mean each block, just like in bitcoin, would be giant tree of hashes with the leaves being the hashes of the new states.
proof of work needs to be fast. not only must it be proven that an evaluator did the prerequisite work, but that work has not yet been redeemed. this means not only is it necessary to have merkel proof that the work was done, but also proof through a julia tree of redeemed work that the work has not been redeemed. perhaps the evaluator can attach proof of work to the evaluation, and the collector can search for proof of absence of redemption, and at the same time say, if this work has not yet been redeemed, please add it to the tree.
some cells, like sub cells for storage, may not need to be evaluated during a cycle. thus in the message chain it will have a nil leaf, and in the state chain it will have either the same hash as in the previous cycle or a symbol of unchanged fingerprint. the messages in message chain cycle 9 will determine states for cycle 10, or maybe 11. what must be stored are the messages, the latest state of each cell, and the blockchains for messages and states. each new cycle means storing a new series or null of messages for each cell, a new state or null for each cell, and new data structures of hashes. the storage tier could have cycles just the same. storage is done in blocks. a block could be a cell, a collection of messages, or a part of a structure consisting of a certain number of vertices including leaves. storj uses merkel proofs and challenges and code erasure. storage needs a mechanism to decide what needs to be stored, who will store it, and how it is retrieved. supposing shards can be allocated for storage, how are checks performed? maybe shards can be passed around periodically and not held by the same node for too long. it could be public how many nodes hold a shard and when their time will be up. when time for a shard on a node is up, the shard can be queried for. a check can involve hashing the shard with the rv. just like states evaluated by chosen uncommunicative collection of nodes broadcast their evaluations without collusion, the nodes storing a shard broadcast their hashes, and they get put into the storage data structure. for each new cycle, a new block of messages must be stored for each cell, a new message structure, a new state structure, and any cells evaluated must be replaced by their new cell equivalents. the storage structure for a new cycle must show what is stored. everything must be broken in shards. every cycle certain blocks are checked, their hashes shown, the nodes paid. each tier has a willingness to pay chain. willingness to store offers are sent out to the network containing a pointer to previous work, which is a merkel proof inside the message structure, the state structure, or the storage structure. these offers are collected by a certain ordering, the first level collector checks the merkel proof and then must check that that work is unredeemed for work. should there be on giant redemption tree or one tree for each tier.
perhaps the mechanism for efficient navigation, and the need for confidence of correct chains, could provide an incentive to participate in relaying. to provide incentive for those storing to share what they have, maybe the sharing can be the mechanism for auditing. what a node finishes evaluating a cell, the node sends the new cell to storage, and sends the signed hash of the new cell to second level collectors, who first check that the signatures are only for those which the random variable together with the work map to, then look at the hashes they get, choose the one with the majority for each cell they are responsible for, take the hashes they choose, merkel them into a root hash, sign the root hash, and pass that on to third level collectors, who check the second level collectors are correct, choose the hashes that are the majority among second level collectors, etc, until the top level hashes are created, at which point, they are hashed with the previous top level block to extend the chain, and the broadcast as the new top level block, and as they are broadcast they should be unionized, but not turned into one, perhaps a node should only count a top level block declaration as valid if it is observed as a majority in a set above a certain cardinality of declarations from those authorized for them with the proper signatures. for the message chain, messages are broadcast, and first level nodes collect from a bunch of cells, group together the messages for a cell, and broadcast these groups to second level collectors, who collect for a certain number of cells, so for each cell they receive a few groups of messages, and they go through these groups picking out the messages that appear in the majority of them, making thus a new group of each cell, hashing the groups to make a merkel root for each cell, then passing these hashes onto third level collectors, and the process continues the same from there as the state chain. every cycle the message chain has a list of collection offers and a list of collection requests, and a random variable that must be known by anyone who will verify work. a chain could be built up of offers that consist of previous proof of work. maybe since it was an idea that only work from the previous round of work, that record of who is eligible.
each tier may have an additional tree called the balance tree. a new version of this tree is created for each cycle. this tree keeps a record of all work redeemed for messages or cell evaluation. at the beginning of each cycle, messages are broadcast containing a signed message that they would like to redeem so much from a previous work instance and they say what that work was (such as what cell evaluation, what level collection, at which cycle) so that it may be easily queried for to return a merkel proof of its existence and verification the work was done by the public key signed for. upon the first level message collector verifying the work referenced by the messages the collector must then check that they are unspent, and if unspent they must be spent. this means taking all messages with valid work, taking the work for each one, and querying the balance tree (a julia tree) for a merkel proof of the absence of the work being spent. suppose there is a rise in messages, there must be a rise in work, but the rise in work is required before the rise in messages, but that work will be targeted at less messages. maybe assigning work involves both the seeker pairing his previous work the random variable and sending that out asking for work. and others taking the content they have, pairing it out with the random variable, and sending it out marked for who its for.
an account can have a balance of money specified within the account. the evaluator can reference this balance as he evaluates, decrementing it as he processes. the balance can increase when a authentic message comes that says, you can increase your balance. the balance is also decremented by the evaluator when an account sends out a message. for outside accounts to send messages perhaps there can be a message cell to which messages can be sent for free.the message cell could keep a record of message balances and a message request sent to the message cell could reference a balance. the cell would then send out the message. how long would this take? the raw message would take a cycle to inter the message tree, then another block to be processed, another block to end up back in the message tree, then another block to processed by the intended recipient. for account sent messages on the other hand, as the account is processed it decides to send out a message and if the balance is sufficient the message is sent, arrives hopefully that block in the message chain, and then is processed next cycle by the recipient. when a node does work and can prove it, the node can send an authentic free message saying 'i earned this amount' and send it to perhaps a node payment cell, which would process the payment request by looking at the previous tree and see that indeed the requester did the work, and exactly how much work that node did. this whole payment system, and the system of accounts in general, relies heavily on the condition that messages are send from authentic senders. outside messages need not be authentically, because they can rely on the private key authentication method. but messages from accounts must be authentic. this means its must be checked that messages are sent only from the nodes who have the rite to evaluate a cell and the message is only sent if the majority of the nodes agree it should be sent, that is if the final state of evaluation includes the message to me sent. this means a nod evaluating a cell cannot successfully send out the message while evaluating. instead, the node should take every message the cell would like to send, set the message aside, and include the stack of requested sends together with the final evaluation -- perhaps the stack can exist within the cell to be more precise. next round the states are collected into a tree, the message are sent to be in the message tree, then next round they are evaluated. to speed things up, perhaps the verifying of an authentic message can occur half part in the state collection and half part in the message collection. when the states are collected, the second or so level of collectors could send out the message if its a majority, and that same round, those messages could be collected into by the message collectors who would verify that the majority of collectors of state sent out the message, and then the message would further go up into the message tree and be evaluated next round. this would mean one round the messages are sent, next round they are evaluated, and next round they are evaluated. actually even better when the nodes evaluate a cell they can send out the messages immediately and leave the verifying of the authenticity to the message collectors who check that a majority of the nodes who have the rite to send the message have indeed sent the message. this again means a full cycle to have the message sent and verified before it can be evaluated with confidence. when w worker node does work, perhaps it is not necessary for the node to send out a request for payment; instead, the request can be inferred and there can be a cell that every round request the previous trees and parses them updating the worker balances of not just the state evaluators but the collectors of the message tree, the state tree, the storers, and the collectors of the store tree. this implies anyone should have the option to request system operating such as previous message trees, messages, and state trees. even account should have this ability, such as the cell i just discussed that pays workers by parsing the trees. should accounts or nodes in general have to pay for the offering and relaying of this data? maybe voters could be incentivized to offer it because a request is indistinguishable from an audit. maybe nodes could be incentivized to relay because if they don't they will not be able to really participate and make money, like it could be a tit for tat system of relay perhaps. is there a way that outside messages could be sent more quickly? perhaps there need not be a message tree. instead, there could be the only messages sent out of accounts that count. outside messages would be collected by the evaluators of the message cell, the message cell would process them, and the message cell could create an ordered stack of messages that it says are valid within the evaluation period, and messages from internal accounts as they are evaluated would also be set aside in a stack. then all the cells are collected and made into a tree, then next round they are evaluated. but at this point the messages sent from cells exclusively would not be organized by cell address and therefore could not be immediately evaluated because the cell evaluator would not have immediate access to them as he did with the existence of a message chain. so maybe the collection process of states could involve not only determining if a sent message is a majority vote and therefore valid, but also relocating the messages from the cells that sent them to the cells that should received them; or maybe when the cells are evaluated the messages can be sent out immediately, so that they can be collected and validated for majority by the state collectors, and hopefully all messages that were sent to a cell arrive at that cell in the tree by the end of the collection. maybe at the beginning of every evaluation a cell could check to make sure all previous messages arrived in the tree as necessary and resend any messages that did not arrive; this would entail requesting the chain at the point of every cell a message was sent to and make sure that all of the messages the cell sent to that cell exist in the collection of messages sent to that cell.
cells would be evaluated and their messages immediately sent out to be begin being collected and sorted by the state collection crew even before the states are collected. the tree would function as a record of both the availability of work, which would be marked by the amount of evaluation and collection, and the need for work, which would be marked by the messages awaiting each cell. as far as storage, what must be stored now is all messages sent from all account, no longer the raw messages. the message are grouped into the tree at the cell they are for. the tree can contain more information than just simple hashes, such as the number of cells present, their estimated time of processing, how many messages await them, and this information could travel up the chain with the help of collectors, until the info reaches the top at which point it could become part of the random variable, so that any node could take the work previously done, combine it with the random variable, and know what work to request, and for authenticating, one could take the work done, combine it with the random variable, see what work is necessary, retrieve that work and make sure it is signed by the same signature as the current work. or actually better and i think this is what i have been planning for the whole time and just forgot, that verification of work is done by the worker pulling together a merkel proof, and the verifier hashing for the proof. so to verify, take the random variable, take the work, and see if its the same work as that proved by the merkel proof. storage now must store the messages which are connected to the leaves, and the tree. maybe the previous node responsible for each part could be responsible for storage as well as part of the job. this would mean nodes that process a cell should keep the cell and pass it on to the next evaluator set, and that collector nodes would each keep all the leaves at their vertex. it is evident to all nodes what the need is for storage: all trees and all messages, and the current states. how does one request merkel proof of work. not just the evaluators will need merkel proof but the collectors will as well. the evaluators depend on the collectors to give them merkel proof and not vice versa. I guess it could be done where the top sends out the root and the second to top layer collect them and sends them to the next layer down. actually taking the root from above and combining it with the roots you received should be part of the collection job that is necessary for payment. i should now think about how signatures verify work. so here. when evaluators and collectors pass up their work they sign it because that signature must be incorporated into the root of the tree, and they provide their address to be contacted directly (but they cannot be bribed because they have already done the work), and when the top level is reached the result is passed back down the chain (in an efficient way due to provided addresses) out of courtesy to provide each worker with a merkel proof of his work. then when a node does future work, the node provides the merkel proof with a signature to say, here is the work that is necessary for this further work, and i did it. all trees with their messages must be stored permanently. it is to the incentive of the collectors to each hold their merkel proofs through the next cycle so they can include it in their work when they send it out next cycle; during this period when they reassign themselves work, request for the work, do the work, and send it out. what about previous trees and message leaves, how are they stored permanently? the further back the history the longer each node should do storage and the more the storage should appear as an archive. binary trees are not exactly the best because extra hashes at a particular vertex may be less expensive than single hashes but at two vertices. for storers an audit consisting of retrieving and handing over the item instead of retrieving and hashing and handing over the hash may be better. cells must also be stored, especially those than do not receive many messages and are probably intended for storing data. storage should be charged for, and just like state evaluating this payment should work by the storer upon an audit also checking that the cell has enough cash to pay for another round of storage. an audit should be first decrementing the balance and getting paid then testing the balance for a next round. maybe the longer something is in storage the less it should cost because the less frequently audits are made. a trouble with long term storage is nodes may have the time to track each other. the previous round of storage should say whats available, and the previous round state should say whats needed. there will need to be collectors for storage. maybe if a cell does not receive messages for a certain number of rounds, or if its marked as archive data or something, it could go into long term storage. the previous state tree says which messages must be stored, the tree itself that must be stored, which cells may be new to archive (determined by either a cell being marked for it, or a cell completing a certain number round without being sent any messages), which cells may be returning from archive (determined by a message sent to, which necessarily means an evaluation is necessary even if its just a read, which means the storer must be switched). the record of who stores what must be mapped with the random variable to what must be stored. it is assumed everyone who stored previous round is willing to store again. how does a node that would like to specialize in long term storage declare that? the new items for storage must be assigned and old items of storage must be reassigned. every round some cells may receive zero messages, but they may still need processing so they should still be evaluated. but if a cell is only meant for storage it will not need processing and the evaluator will immediately realize this. in this case the cell need to be evaluated again until a timer goes off or it is sent a message. cells can have a sense of time, and for optimization instead of the evaluator calculating time each cell in its possession separately, the evaluator keeps a single record or time and any references to what should happen when certain times are reached, and then calls the relevant events within the cells. so actually a cell should be thought of as needing processing every time, just some of the time the processing is so minimal such as setting and waiting on a timer to act, or really nothing if the cell just holds static data, that the cell would be appropriate for long term storage. when a cell receives messages it must switch storers, but if a cell goes a cycle (maybe evaluating messages) but without being sent messages this is reflected in the new tree that is collected because the state there will have no messages pending for it to process. then the cell could be kept in storage because next round it need not be evaluated. after a round cell should be kept and then passed, and if they are not passed on to an evaluator because the cell does not need evaluation next round, and so the cell is put in storage. maybe those cells which have messages pending at the end of an evaluation, that is messages waiting for it to process when the tree is made, do not need to go into storage, but could just be passed to the next generation of evaluators because with their messages they will need to be processes; only those states without pending messages are collected by storage together with the messages and the tree.
i will try to outline the whole scheme. there are two tiers, state and storage. it is evident that the need for storage consists of all previous state trees with their messages, and the current state tree with messages. a cycle of the storage tree has leaves consisting of all state trees and messages up to the current cycle. the previous storage tree reveals the availability of storage. first level collectors collect the things that are stored, fingerprint them with the random variable, and pass the fingerprints up the collection tree to the top. the final tree will show what is stored and who has done the storage and the collection. the collection process for storage could be considered processing/networking work, there would be a need for that work and an availability of that work. maybe the collection work can be separated from the processing work to enable even more specialization. the tradeoff with specialization is separate mapping schemes and more potential for one to focus their resources in one area and dominate that specialty having potential for an attack. the storage trees are not stored, although it could probably be done without a circular problem. there is no need to keep previous storage trees as the work redemption and the money redemption of the storers and collectors are only available one round in the future. just as money redemption works for states, so it works for storage, where a cell requests the previous storage tree and parses it updating payment balances. when collecting is done, those who are holding older messages and state trees should not have to undergo collection so often, because those items are not as important and definitely not as needed. those nodes can be queried for their items but the script for storage collection will not allow for their audits to appear in every tree. collection has two questions: have you done the work, and were you assigned this work, meaning do you have proof of previous work and does the random variable map that previous work your current work. the trouble with long term storage is that proof of work must be verified not just from the previous storage tree but from a tree way in the past, which requires knowing the tree headers back a ways. the burden of holding past headers could be put on the long term storage holders, or maybe a cell could specialize in holding these headers. the question of double spending is whether the work from many cycles ago could be used in multiple more recent cycles. the principle of unique redemption must apply. that is, the long ago work must only be applicable for redemption one way. this one way could mean during one cycle. an audit occurs for a tree and its messages according to a sequence of such that size of tree increases slowly yet audits are frequent enough that trees and messages are available enough. whether this sequence involves addition, multiplication, or exponentiation I don't know, but a possibility (with addition) i see is the fibonacci sequence, or the fibonacci sequence with an addition constant of addition (but probably subtraction because the fibonacci grows so fast), Sn+1=Sn+Sn-1+c. do a sum of the series and plot against the number of terms added to see when so many generations have passed about how many audits will be needed for each cycle. every round it can be calculated what trees and messages are to undergo auditing. this calculation offers the demand for storage. could nodes specialize for what interval of storage they would like? suppose a node wants to specialize in long term storage. the mapping could be such that type of time storage you have done is mapped to that same type of time storage that you can do next. this means the prerequisites for a time type would take proportional time to the work itself. collection could be a different type of work. the storage record and state record would both represent the supply of collection by who collected last time and the demand for collection by what cells must be collected and an estimate of the messages they will receive based upon previous cycles, and the storage items that will undergo auditing in the next round which is fully deterministic. how will the items be stored, as in what order? each auditing period could have a tree structure where each tree being audited is arranged left to right, and then put together into a tree. each leaf, which is a tree with messages, would then be broken down into a second generation of leaves which would be the actual items being stored. how the state tree with its message leaves is broken into storable chunks is an open question. perhaps the size of chunks could lengthen as cycles pass. this would mean that when you are assigned storage, you might ask previous storage nodes for multiple chunks and combine them. it is now appropriate to discuss communication between nodes. communication between nodes occurs primarily when a node seeks something another node has, such as a state, a leaf of a state tree in order to process the state, a series of state fingerprints for state collection, messages for a state for state collection, stored items for storage work, storage fingerprints for storage audits. Suppose nodes in possession of the sought items are willing to give them if they are asked for them. The challenge is then passing communication through the network that answers a query. So the seeking node sends out a query, and the answer is returned if possible. the willingness for a node to give what it has is a little doubtful for long term storage nodes who do not want to pull out of their storage what they don't have to. for now i will ignore this incentive question. otherwise the seeking node has a strong incentive to receive, the supplying node has a willingness to give, and all the intermediary nodes must have an incentive to relay. how are messages to cells redirected? answering this means first answering a question that will also help decide how storage trees are structured: how do cells divide and merge?
cells should divide and merge in pairs of two. how should redirection work? redirection occurs whenever a cell is divided. what does the redirection can be called a filter. the filters are not cells as that would be inefficient. what is special about filters is that although they process messages the messages have no effect on the filter so the filter can be widely dispersed and the nodes that use it need not communicate. redirecting becomes a matter of dispersing filters when division occurs to redirect messages and removing those filters when merging occurs. this becomes a matter of communication and querying as before, such as a filter being sent out, or a kind of query being sent out but with no response expected requesting the removal of a filter. now we must consider querying. what motivates the intermediate nodes to relay and put effort into fulfilling queries? suppose that nodes are motivated to help. how would communication and deliveries be orchestrated? it is worth thinking in terms of graph theory now. must the network be dynamic? if it was static pathways could develop, but this could mean knowledge that could be detrimental could also develop. maybe filters could be used whereby what you seek can be described simply by a filter and what you provide could also be described by a filter, and filters could be compared. filters could be combined. like a filter could combine all filters coming from an edge, or combine all filters and send that filter out. items are what are sought and provided. an item must have a particular pattern in a filter. do to the randomization of mapping, the location of items is random within the network, so related items are not graphically related. for purposes of network searching, we can consider each item as unrelated to the rest, and so the identification of each item could be arbitrary and optimized for searching. whatever identification scheme is optimal for searching could be slightly modified and implemented just as other configuration details are. perhaps now would be an appropriate time to begin studying abstract algebra and the math needed for serious algorithm development. i could continue to write about, and clarify for myself, and so as not to forget, the schemes and concepts i have in mind, as i study math which would for a while be unrelated and not directly applicable to these schemes. i realized the community computer model could be combined with the blockchain model, where the computer could communicate with the outside world like a single private organization and yet it would base its behavior upon the blockchain which outside companies are not setup to do. the question is how would the computer be setup? it could be setup by another computer but how would that computer be setup, by the founder of the blockchain? maybe there is a way to get a fingerprint of the code on a computer. regarding how messages get around, suppose at the beginning of every round, every nodes has two sets of information, an item of information, and a set of addresses that can be contacted. every node also seeks one piece of information, an item held by another node. how can the item get from the node that has it to the node that wants it. each node could send out information to its contacts containing what information it has and what information it wants. then each node takes the information it received from each of its contacts, including new contacts, analysis and combines the information is some appropriate way, and sends out new information to each of its new contacts. the question becomes how is information best exchanged, for both contacts and items, and requests for items. if one node has the contact of another node the other node need not have the contact of the first node. also if each node has n contacts then when a node receives info from n contacts it could as many as receive n*n new contacts. but these are too many to have as active contacts because the average contacts capacity is n contacts. so either each either the sender node must decide which subset of its contacts to send to anther contact, or the receiver node must select a subset of its contacts to contact. each contact should be associated its desired item.
to continue the structured analysis in my notes app. instead of large items suppose its just messages of a single unique bit sequence that must go from one node to another. another assumption is that each node must run the same software thus using the same algorithm. to begin, each node could send its messages to all its contacts. that means that second round each node would have as many messages as previous contacts. pack of data are sent between nodes. the pack of data contain messages and addresses. the size of the messages is a variable and so is the size of the packet.
variables:
- rate at which a node can send and receive data.
- amount of data a node can store at a time.
- size of messages.
- size of addresses.
- size of data package a node can send or receive.
if a node could hold enough data and contact enough nodes, each node would contact every other node and within one round messages could be sent. but this is unrealistic. suppose a node is contacted by all its senders. they could send the messages they have and their set of addresses. for m neighbors on each node, the number of addresses the node would be sent is m*m. the messages needs to get from one node to another; therefore there must exist a chain of contact between them. the number of nodes in this chain should be minimized. this chain cannot be determined is less contacts than are contained in the chain. the chain must be sought for in random directions. each node must send out messages in random directions. the number of directions should be maximized. for m directions (nodes contactable at a time) the expected number of rounds necessary is x=Log[m,n/2]. a balance of m and x seems very possible for example x=8,m=40. now the question becomes what should nodes be sending to each other so that data doesn't build up on each connection, but info does. in order words, how you take multiple queries and combine them into a smaller number of queries that can be sent to all of a nodes contacts as a single data packet. these combined queries must be such that when they come in contact with they they look for the hit will be detected. if queries are messages the combined query no longer hold the full information of the messages, so even with messages intended to be sent one way this resolves to a query and response process. so to begin, each node need to send out the message, only a query. but if each query comes with an address, those addresses will all need to go into the combined query. let us explore the question of what form queries could take such that they are combinable. bloom filters are a possibility, but the sender of the query would have to create the filter without having access to the data. addresses would be exchanged such that packets follow unique paths, not redundant paths. how can a query be created without the data. maybe each piece of data could have an id which the sender can use to make the query and the holder can use to test the query. suppose these ids exist. bloom filters are then a possibility. like bloom filters, the ids could be used to make any kind of mark, and then marks could be combined. a type of this filter could be taking the id, turning it into a binary, splitting that binary sequence into chunks, doing modulo work on it. but thinking about the markings, the ideal scenario is prevent false positives. this means number of possible filters must be the same as the number of messages, or nodes. for possibilities for bloom filter are not bad, as a thousand length sequence means like a trillion squared possibilities. perhaps the id can intuitive define the filter marking. maybe the sequence can be interpreted as a number, and each id can be mapped to a number. but in this case when you combine numbers you end up with not a combination of two numbers but a sum of them which is another number. what about modulo, where two ids each have a number but they can be combined when their modulo is equal. if you can have a linear map that has an obvious target that ill be hit if and only it a target vector is input. it would be idea if the filter would map anything that doesn't go to the target, back to its position. this way, filters could be multiplied together into single filters. so is there a matrix that takes maps every vector to itself, except for a single vector which it maps to a special vector, such as the unit vector. what about an eigenvector? this is a vector that is simply stretched by a matrix. since ids can be thought of as numbers and a vector can be thought of as a number, linear maps could encompass all possibilities of mapping the number of an id to the number of a hit, because these maps are linear which is required because it must be that when you combine two maps they still offer the same functions. how are two filters combined? is it so that one filter acts first then the second?


first step is to remember where I was. the system is composed of cells that divide. cells receive messages and send messages. cells can merge again. does it need to be a tree structure? hopefully not. suppose there is a mechanism in place such that nodes can communicate and request and deliver any information they please, as it is all public, and no one knowns what someone else needs. there are 'rounds' of computation where each cell is evaluated together with all its messages. assignment of nodes to evaluation tasks is done by a random mapping, and multiple nodes do a single evaluation, and security rests on the protocol for information sharing to be such that nodes cannot seek each other out and collude, it is only the information itself they can seek. the system is designed for scalability. once a cell is evaluated it is passed up the tree. it looks like a summarized some stuff under cells/three-tiers.
a global rv does the random assignment of those with proof of work to cell evaluation. after a set of nodes evaluate a cell, they hash the new state of the cell, pass the hashes up to second level collectors, who compare the hashes, choose the majority, and pass it up another level, until it reaches the root level, where those at the root select the majority of hashes from below, and broadcast them. there is no official root node i don't think. everyone is responsible for selecting the majority of the root hashes. let me remember, a collector level 1 collects from hashes from multiple cells. oh ya, a level 1 collector is responsible for a certain number of cells. for each cell, the collector collects hashes from as many evaluations as possible and selects the majority hash among them. the collector then has a single hash for each cell it is responsible for. the collector then takes these hashes, arranges them into a merkel tree, and generates a root hash, then broadcasting this hash. level 2 collectors are responsible for a number of level one 'collections'. for each level 1 collection, the level 2 collector collects as many hashes from the corresponding level one collectors. for each collection, he selects the majority hash, hashes them together, then passes it up to level 3 collectors. if there is no majority at any stage, the relevant cells should be dropped for that round and 'backed up' for the next round where execution will be tried again. the messages are collected in a similar way. but at the bottom layer (where the cells are evaluated and their states hashed which are then passed up the chain) messages are collected for a number of cells, which results in a set of sets, the parent set elements corresponding to cells, and the child set elements corresponding to messages. they then pass these child sets up to the second level. second level collectors then compare the child sets for a cell and select the intersection of messages (or some majority selection). they then arrange the messages in a tree and hash them, at which point, each cell is assigned a hash the represents the summary of the messages sent to that cell in a given round. the hashes for the cell are then passed up to the 3rd level and handled from there on just the same as the state hashes for cell evaluation are handled from the 2nd level up. i think cell messages should be sent out while the cell is being evaluated. as they are sent out, perhaps they would only be relayed if nodes check they are sent by the correct party, and then when they arrive in the message collection process they could be just not just for being sent by the correct party, but that the majority of nodes evaluating that cell agree the message should be sent. a balance (aka payment) cell has the function of parsing the trees of the previous round, increasing the balance of those nodes that did work, and decreasing balance of anyone who sent an external message. while internal messages are relayed, they are checked to be authentic (sent from nodes with the rite to evaluate the cell). while external messages are relayed, they are checked to be sent from a party with sufficient balance for sending the message. how are cell's charged? this should also be done by the balance cell. as the balance cell parses the state tree, it decreases the balance of the cell. but this could be done instead by the evaluator of the cell by decreasing the balance within the cell; the cost of this is that what if the cell does not have enough balance for computation, and the computation stops? next it would have to receive a message increasing its balance, and the next message would have to identified as one, which means computation when it's not deserved, which could be abused. a cell holding its own balance is more efficient and the balance tree holding the balance is safer. maybe with sufficient balance, a cell's self contained balance is tolerated, but once the balance is below a certain level, it must reference the balance tree. this can be a protocol. this means the balance tree needs to know which cells have their own balance and which are dependent upon the balance tree. i need to think about storage. i also need to think about mishaps. i imagine both are doable. what this system is really hinging on is the networking scheme for exchanging material fast. I need to think about that.
For simplicity, consider a simpler scenario. There are a bunch of items. Each item has an ID, represented by a number. There are a bunch of nodes in a network. Each node holds a certain number of items. Each item is held by one or more nodes. Each node has an address, represented as a string of characters. Each node is looking for a certain number of items. Nodes can exchange anything, such as addresses or items. For simplicity, only one round of this should be considered now. The goal is to get every item to every node that desires it, as soon as possible. The caveat is that nodes should not know what other nodes have and what they desire.
Suppose this caveat didn't matter. How could the system work? Each node could broadcast to its current neighbors what items it has and what items it desires, and its address. Each node could then take the info from each of its neighbors consisting of {what i have,what i want,my address}, and compare them. how about a simpler example. suppose there are n nodes, and they each want to have an address of a another node. a node can hold m>=1 addresses at once. each node starts out with a set of addresses, that is a set of directed edges. the goal is to end up with each node holding a set of directed edges of which one points to the node that want to find, in as few steps as possible. if each node holds m addresses and each nodes sends to its neighbors its other m addresses (including its own, that of the sender node) each node will after the first step have m^2 addresses.
this is a crucial algorithm to develop, and it would be useful beyond this project. if one exists, great, if not i need to explore it. so i will begin to focus on this, and forget the system design itself for a little and hopefully next time it will be easier to recall. the way i think of it now it seems a solid system only hinging on this networking algorithm. to help remember, the number of rounds it takes a state or message to get to the top and be hashed with a root is logarithmic and depends on the number of collectors and their capacity, so more collectors and increased capacity would be good for a fast system, but a fast system may not be what this system is best suited for, maybe its better thought of as a slow secure system, and the fast insecure tasks can be left to the cloud.
i think about the networking protocol. suppose its done with fuzzy maps. each map (i envision a matrix but it may well be something else) can be matched with a subset of positive integers (representing the identities of a subset of items). maybe the map could be thought of as a function that for any positive integer returns a probability representing the probability that map was encoded with that input id in mind. nodes would send these maps to each other, particularly two types of these maps; one type would represent what a node has, the other type would represent what a node wants. to start, each node would create two maps, one for the items he has the other for the items he wants. the nodes send both their maps to all their neighbors. every node then looks for matches among the maps they received. if they find a match, they contact one or both of the corresponding parties and hook to two of them up. other wise, the node combines the 'have' maps together and the 'want' maps together and then sends these new maps to their neighbors. when matches are found, addresses are exchanged, but addresses may be exchanged on other occasions perhaps. to preserve privacy to prevent collusion the actual items could also be exchanged on certain occasions. the holder/sender doesn't know if the receiver needs it or is only asking on behalf of a neighbor. likewise, the receiver/wanter doesn't know whether the sender is the holder or is only offering on behalf of a neighbor. collusion occurs when a large enough subset of a group of nodes given the same task are able to communicate prior to or while doing the task. let me first consider the optimal way to transport items and consider second what threats it poses for collusion. so suppose u want to collude. you are assigned next round for something and you want to find others with the same task. so you pretend you have what you are assigned, and ... what happens when a match is found? the two nodes and connected but if the semi-holder is not the holder then he will have to hold the address of whoever held it for them. this means keeping a collection of addresses. and probably also keeping the collection of maps corresponding to the addresses. if not too many rounds of networking need to occur per round of the chain, this may be feasible. suppose every node keeps every address and every map. maybe a map could have attached to it a set of addresses such that upon receiving the map, any matches can be sent directly to the wanting addresses instead of the sender of the map that way the sender of the map need not hold on to it. this configuration is dangerous because of collusion, because a map that shows multiple nodes who are interested in the same thing. also, just the idea of a map encoding multiple numbers, how would that work? idk but i imagine something like a single row matrix mapping from a vector representing an address to a real number representing a probability. hopefully linear maps in general could do the job.
the only way to collude is to find those with the same task. the only way to do that is to pose as a holder of what u looking for. the only way to succeed with that is be an actual holder because u must occasionally deliver what you claim to hold. the only way to do that is ask for what you want to hold, but that means asking a round or so before you are assigned a task. this seems coherent. so at one end, when you say you provide something, you need to provide it every time; at the other end, when you say you provide something, you never need to provide it. when is an item actually sent? maybe design the network so that the bigger role you play in relaying, the more items you must hold at once, and the reward is that since the networking is tit for tat, you are more likely to receive those items you ask for more quickly, and thus you will have more time to process items, which means you can process more items. thus relaying should be an incentive, and your role should be proportional to how many items you process. so i think maps would be passed around with an attached address collection, both for hold and want maps. suppose maps of the same type can be combined and then the address collection would be the union of the two address collections. maybe if you find a partial match by multiplying a hold map with a want map, and the result is above a certain threshold, the the node finding the match should not hook up the two, but instead be a conduit for them, by requesting from a subset (maybe the full set) of holders to deliver what they claim they have, compare them against the holder map maybe hashing for integrity, and if anyone fails to deliver or what they deliver does not match the map, they are removed from the attached holder list. the want maps' attached addresses i think are final addresses, that is those of the wanters and i don't think this is a safety problem. well actually, another
at any given time a node has a set of hold maps and want maps with attached addresses. the node compares the maps, combines the maps, and sends maps to other maps by sending a map with attached addresses to each address in another map's address collection, and attaches the map for reference. it should be recursive process. for the base case, a node is sent a map that directly encodes, or almost encodes the item the node is looking for. the node then contacts subset of the attached addresses (which is hopefully a small set itself) and attaches the map with the node's attached address and sends it as a 'want' map to the holder nodes. actually, that is kinda recursive, so really its now that the base case starts. the holder nodes receive a direct (not just almost direct) want map with a single attached address and the holder node holds the item requested, and the holder node sends the item with maybe the map attached, to the wanter node. the recursive case is the actions a node takes in general with a set of holder maps and wanter maps. ok, for sake of vocabulary consistency, there are have maps (aka H maps) and want maps (aka W maps); each map consists of three parts: the tensor, the bag, and the type; where the tensor is the mathematical object encoding the IDs of items, the bag is the collection of addresses, and type is a binary variable of indicating if the map is a have map or a want map; a node 'multiplies' maps by multiplying their tensors; the 'product' of two maps is the product of the tensors. with a node given a set of maps, how does the node proceed? let us first cover high products of H and W maps. in this case, two possibilities are the H map could be sent to the W bag, or the W map could be sent to the H bag. if there are no high dual products (products of an H and W map) then it means redistributing the maps among the bags. i think its same to assume (and i see no reason why not) the addresses in each bag are equal. if you send something to a bag, you either choose the addresses randomly to send to or you send to the whole bag. for simplicity we will assume you send to the whole bag (but for efficiency, in certain cases it may be better to randomly select a subset of the bag). for now assume the only access to new addresses is through bags of maps you have been sent. so you have a set of maps with all low dual products. since you haven't found any matches (i.e. high dual products) you pass the maps on to other bags, but how do you choose to do this? for queries to spread quickly across the network, you must send W maps to multiple addresses which means duplicating queries and so the number of queries of the network grows exponentially which is unacceptable, thus queries must be combined into single queries that partially represent each query. maybe to help think about how maps should be combined and to whom they should be sent, i should consider what it really means to multiply two tensors which means developing the tensors more.
tensors: a tensor encodes a set of numbers. the product of two tensors must yield a tensor encoding the union of the ID's though with less precision. i suppose tensors should all be the same size (for convenience small tensors may have a representation that is smaller) but is just a short cut for the full representation. so how can a mathematical object encode multiple numbers. assume all ID's (numbers) too are of the same size, and think in binary. linear maps (matrices) come to mind, because they can be combined, but idk. how could a linear map represent multiple ID's. are the ID's for a map arbitrary? if linear maps are used they would be square because they must be commutative and two of them multiplied must result in one of the same size. maybe there should be a spectrum of IDs such that they have an order and can related to each other to different degrees. this way tensors can represent related numbers. well since ID's are just numbers this change only means the numbers are not arbitrarily assigned to items. efforts are made to group related ID's into the same tensor so that the tensor can best represent all contained ID's simultaneously. suppose this is done, then a node should look at his set of H maps and combine those together that are similar; likewise for W maps. who to send to the combined maps too then? if random it could be inefficient; if methodic it could be dangerous. an arbitrary methodic method could be send an H product to the W bags who coincide the highest with the H product; and likewise for H<=>W. actually this algorithm isn't so arbitrary but i suppose its the algorithm that must be used. without this algorithm we assume maps are only sent to closely related complement bags if above a certain threshold. why set such a threshold? would this algorithm be efficient? if ID's are distributed randomly i suppose this is no less random than any other algorithm. if related IDs can be encoded by the same tensor that means a tensor encode related numbers; related how? maybe by the Z^+ line, maybe by Z_n line, idk. the easiest way to relate two ID's is by their distance on the positive integer line. in this case a tensor could just a positive integer that is the average of the tensors it represents. but wait i wanted a tensor that would match more and more numbers the more numbers it contained. ma ybe the average integers could work but the tensor would also have a radius property which together with the average would represent a segment of the line that the tensor represents and anything within that segment the tensor would match. thus a tensor is a tuple of two positive integers. since these tensors are so small, and the maps will mostly be too, a lot of these maps could be passed around maybe within the same connections, and computation would be fast. so then the algorithm is, when u have a set of maps. partition the H maps into one closely related to each other and then multiply them, and likewise for W maps. how many partitions? in other words, how closely should maps relate to be in the same partition? this is a free variable and up to the node. once the products are made, maybe just send the H products to the closest W bags and vice versa which would cause denser products to be collected but the would be collected by narrower groups of nodes. but what a node wants is completely random from what it has and if it continues to route maps this way, from H bags to W bags, and vise versa there may be enough randomness. tasks are reassigned randomly based on previous work. its hard to predict how random this would be. the model is simple enough now that i should maybe simulate it. maybe the tensor could also have a density.
so current configuration is: ids are randomly distributed to items. a map has a tensor, a bag, and a type. the tensor i think is an integer tuple (left,right) on the item address line (Z^+). Multiplying two tensors is done as (l1,r1)*(l2,r2)=(min(l1,l2),max(r1,r2)). with a set of want maps, the nodes group together those want maps whose tensors are close on the line (the Z^+ address line); likewise for the have maps. how many to group together? could depend on a threshold diameter, or by the number of products you choose to send out, or by the weight (cardinality) of the bag. 


it seems crypto systems have been developed experimentally. an axiomatic methodical approach to development seems most appropriate.
axioms:
- everything is done in binary, because everything can be represented by binary.
- there is plain, cipher, and key.
- the cipher is the same length as the plain.
- there is a set of fundamental principles that a crypto system can offer, and no matter how complex the system it boils down to these fundamental principles. therefore simplicity is best.
- there are four attack types that must all be avoided. cipher only. cipher and plain. chosen plain (access to encryption). chosen cipher (access to decryption). cipher only is a sub case of cipher and plain, which is a sub case of chosen plain or chosen cipher. as we would like to prevent all four, we need only focus on chosen plain and chosen cipher.
- the security is entirely dependent upon keeping the key secret.
- length of key partly determines security and computation work involved.
- therefore assume length of key is known at least to within a few bits.
- assume the key is random.

since the plain and cipher are both binary, and the same length of bit strings, the relationship between then can be fully described by a series of binary operations. how does the key determine these binary operations? what if the key was a bit string, the longer the more secure. the key would be applied to the plaintext is some way to induce certain binary operations over the whole plain yielding a first level crypto. then the plain, the key, and the first level crypto would be combined in some way to yield the second level crypto. the key would offer information on how many levels there are total.
suppose the plain has length n. and key has length k<=n. suppose we restrict operations to {not,and} operations because this is functionally complete and with only two possibilities a binary number can represent one or the other. I don't think I have lost any generalization because the relationship between the plain and cipher text is arbitrary. the challenge then is how to map a random key to a relationship between the plain and cipher. the space of the key is smaller than the space of the functions, which means that if each key is to be one-to-one with a relationship, not all relationship will be possible. i was thinking a source of confusion could be not only which operations are chosen, but also how many times operations are performed. but this information of the number of rounds must be contained in the key. a bit can indicate whether or not a round is done, which means the difference between two relationships. if instead that bit was used as another gate it too would mean the difference between two relationships. so one meaning of the information is no better than the other. the goal is to randomly map the key space one-to-one with the function space. how big is the function space? the function either changes or does not change each bit of the plain, so for each bit there are two possibilities. therefore the function space is of the same size as the plain space and the cipher space. both of these spaces, the key space and function space, are binary spaces, just with different dimensions. each bit can be thought of as a dimension with two values.
a chosen plain attack is when an attacker can submit any plain, have it encrypted with the key, and then see the cipher. this means the attacker can see the function that the key maps to and it is his goal to recover the key. likewise, a chosen cipher attack is when the attacker can submit any cipher, have it decrypted with the key, and then see the plain. this means the attacker can see the function that the key maps to it is his goal to recover the key. the challenge can be reformulated as making it easy to map a key to a function but not a function to a key. the function that the attacker is one mapped to by a key. the function goes two ways as it interchanges plain and cipher. assume the attacker, due to the chosen attack assumption, has access to this function going both ways. so the goal is to create a random, one way mapping between the key and the function. a function can be represented by a bit string of length n. thus we are seeking a one way transformation between a bit string of length k and a bit string of length n. another way to think of this is mapping a number i<=2^k to a number j<=2^n.
to begin to see the challenge at hand consider a simple mapping, say take the key and repeat it until over and over until it is the length n, and the resulting string is the transformation. the problem here is knowing the length of the key, one could take the transformation to easily reveal the key. what i was thinking about is the key representing determining a set of binary operations. we want the only way to find a key to be testing the entire key space. suppose the key is of length 1 and a binary operation is done with that bit and each bit of the plain. the result is the cipher. the attacker could put in a plain of all ones, and assuming the operation is known, compare the cipher with the plain and determine what the bit must have been. but suppose the attacker does not know the operation. maybe the function can depend not just on the key but on the plain. suppose each operation is determined from previous operations and outputs, such as the previous output. this means for a given output, you know the next operation to be done, but you don't know the previous operation that was done to obtain the output. suppose from a given output there is an algorithm for determining the next operation. if you could recover the previous operation you might be able to recover the previous output, and there would be more possibilities for going wrong for each step back you take trying to return to the first output which was determined by the operation determined by the key. say for each output you count the number of ones and then do modulo the number of operations possible which determines the next operation. perhaps this would work if the number of operations are carefully chosen regarding the length of the key and the plain to cause as much confusion as possible. a question is could this process be reversed with the key, that is how would decryption work? to generalize, the first output is determined from any constants, the key, and the plain. the number of rounds is variable. the second to last round (which may be the input if there is only one round) is determined from any constants, the key, and the cipher. ok i'm gonna take a break from this for a little.

a function
input modifies function
  either by changing state of a balance in your possession
  or by

how to direct?
if I was at one point a parent then I will pass on
need quick but random way to locate a parent.
the number of transactions one makes should be the number of nodes in the network that know your path.

each is a child an has one or more parents.
the line of children should not be able to contact the line of parents.
i want to reach the parent.
suppose every node is honest. every node keeps their own balance. I earn, I raise my balance; I spend, I lower my balance.

 axioms of function.

can the network be divided into groups?
a group would a collection of nodes.
the network of groups would function like a network of regular nodes. A group could be arbitrarily large or small. This means a group must act as a single node.


what if we start from what we can imagine the system to look like. I imagine a bunch of nodes that change edges constantly -- a mutating network. Each node has a variety of choices to choose from regarding who to connect with. The current state of the node determines who to contact. As the nodes contact each other they almost always receive feedback. The feedback changes the state, helping to determine who next to contact. As the state progresses, the node builds up a navigation system, or a partial picture of the network. payments, balances, or parts of either travel among these nodes as they mutate, just as thoughts travel a neural network. everything is changing to fast for anyone to cheat. by the time one organizes to cheat a transaction, the transaction will already have been filled by other nodes. each node carries a collection of information and is constantly sharing and exchanging that information with other nodes as they mutate. so how does a transaction occur? A request for a transaction is launched into the system
if ones balance is many places within the system at once, and that balance need to change, how does it change in so many places so quickly? it cant. maybe nodes reduce their confidence of a balance overtime. maybe nodes have a list of balances and their confidence for each. The nodes share their balances and update their own accordingly. Why does one not lie about confidences? One doesn't know which balance is for who. confidences, must less balances wouldn't be able to change fast enough to reflect true balances.


suppose a child is able to contact the correspondent. but increasing the balance of the child is against the incentives of the parent. the free variable to make this possible, is how the parent is chosen. Suppose the parent is chosen such that increasing the child's balance means decreasing the parent's balance by the same amount. but then what if the parent decreases the child's balance, which may increase the parent's balance. if a mapping between child and parent can be established, what prevents children and parents from mapping themselves differently?

how many people keep a record of one's balance?
none and one can change one's balance.
too many and it takes too long to change the balance.
too few and one can team up with them to change the balance.

the closest I've gotten is with two possibilities:
1. nodes that trust each other because they are all governed by software that was written by the community in consensus. the problem is that if one node is hacked, the whole system comes down because other nodes will trust that node.
2. the correspondent idea, where the line of correspondents must be uncontactable from the line of buyers and sellers that way no bribery can happen. Correspondents and children would need to exchange information without knowing it. It seems possible, but I don't know of a function that could process so many different kinds of messages all the while remaining hiding what it is doing, that way preventing anyone from taking advantage because they don't know how. The problem is I can't think of an appropriate function, and also, there is the problem that if there is one correspondent and he goes down, who takes over? if there are multiple correspondent, I guess they could update each other when one of them gets a new balance, but they would have to trust each other. maybe this could be done if there was such a spectacular function and I could find it.

One variable, no matter what value, seems to present problems. That is, the number of nodes keeping track of a given node's balance. If it has value...
0: one can change one's balance at will because no one else knows about it.
few: one can find and contact these few nodes and bribe them.
many: bribery may be undoable, but it would take too long and too much confusion to update the balance for every node (preventing scalability).


how many nodes are involved in keeping track of your input, output, or balance?

0:
  what prevents you from changing your account?
  maybe there is some spectacular cryptography that prevents you from changing your own.
    if transactions are able to change your account, you are able to change your account.

few:
  what prevents you from bribing these few others to change your account.
  maybe there is some spectacular cryptography that prevents you from communicating with these nodes.
    if someone in the network can contact them, you can contact them.

many:
  why are there not too many nodes to contact regarding a single transaction, or too many nodes to process a single transaction?
  maybe there is some spectacular cryptography that lets many transactions be communicated and processed at once.
    it one transaction is invalid, it be identified and the culprit must be identified.

between few and many:
  At the point that there are many enough nodes that bribing is impractical, there will be enough nodes that communication is also impractical.
  Likewise, at the point that there are few enough nodes that communication is practical, bribery is also practical.


what are our assumptions?
goods are exchanged, and another medium (money) is exchanged for any of them.
what is exchanged is an indication of transfer of medium from one to another.
the transfer can go in arbitrary directions and be received as valid.
duplication of the medium is possible.
if duplicated medium is sent out in two different directions the action must be caught.

what about dropping the assumption of arbitrary directions.

take any group of nodes and consider them one node. the system should still work.

any group of nodes can be simulated by a collection of nodes with three edges each.


trust any message encrypted with a private key.
message authentication:
I know this message must be sent from whosever public key this is.
every node has a different key.
how do I know you, with this public key, are part of the network?
if I know the public key belongs to the network, then I know, when I send a message, only someone in the network will have the private key to decrypt it, also when I get a message, only someone in the network has the private key to encrypt it.
this software governance system seems too vulnerable. since someone can attack a server, nodes cannot necessarily trust each other, defeating the purpose.

if a system is distributed, nodes will be everywhere and of all kinds. any of them can be attacked, or have ill intentions. nodes cannot trust each other.

the reason bitcoin does proof of work is it prevents bad nodes from outpacing good nodes.

by what mechanism could a network ensure that the input of every subset of nodes is the same as the output?
I like the idea of limiting what I node can send to that way having a setup where output can be limited to input.

best system for micro-payments, continuous payments, etc.

assume network so large nodes can only communicate with tiny fraction of network.
what about a navigation system where its very carefully chosen which nodes communicate.
a rule is that nodes cannot rely on other nodes for the validity of information.

what if nodes do not carry information, but rather the information is the interaction or like interference pattens of the nodes sending each other not necessarily valid information.

can a network be modeled as a vector space? every communication could be a vector. how much of each dimension? in a vector space linear transformations can exist.

does the whole network have to know about each transaction? If not and duplicate money is send to two parts of the network that don't communicate, there is a problem. Suppose duplicate money cannot be sent to two arbitrary parts of the network.
suppose money only flows one way. a node can only send output to one other node, every other node knows to reject it.

what if money is encrypted, so one cannot tamper with the amount without changing its legitimacy. but one must tamper with amount if balance is to ever change with a transaction.

how about making the whole system encrypted, where each computer processes through encryption and is honest, because if the node owner tries to modify input or output, the encrypted text simply won't make sense to other nodes and they don't be able to process it. The system has a simple operation. Let others send you numbers. Add those numbers to your balance. Send any amount of that balance to other nodes and modify the balance accordingly. This operation is all numeric. It requires that a balance can be modified, and numbers can be sent without anyone knowing what they are. In fact, the computers do not know what they are processing. Everything is hidden. The only way to see things is to constantly try to pay oneself and see how much is able to be paid, reflecting one's balance. Even a person could process it but not know what they're processing. This is important because it does not require nodes to trust each other, or share keys or anything. What kind if encryption is necessary? Perhaps it can be thought of as translation. If the translation is just one for numbers, it must be that modifying the translation does not just modify the number but makes it invalid, like NaN. But the translation cannot be reversible. This brings to mind hashing. Hashing lets a number be disguised where nobody knows it and yet can compare another number to it and see if they're equal. In this translation, addition and subtraction must be done, and detection of negative numbers.
need a function that can, add a number to the balance, subtract a number from the balance. the balance must be represented by numbers of some kind, some indication of quantity. Arithmetic must be done with that quantity. The computer must do the arithmetic. If the computer knows the numbers you can know the numbers. If you know the numbers you can modify the numbers. If you can modify the numbers, you can modify the balance. If you can modify the balance, by trail and error you can figure out how to modify the numbers so as to increase the balance. so this wouldn't work.

blockchain can replace a server.
suppose servers can't be hacked, and they each have appropriate software. how would the system work? each node keeps track of a certain number of balances (and possibly histories). several nodes keep track of each balance. everyone knows what nodes hold their balance. when a transaction is to be made, Alice tells the nodes carrying her balance to transfer some to Bob, providing Bob's nodes which Bob provided to Alice. Alice's nodes make the exchange with Bob's nodes. Bob's nodes tell Bob about the exchange. Bob tells Alice whether or not the transaction was completed as he expected. Actually, Alice's node must contact each other to make sure every node agrees and is in sync. Likewise for Bob's nodes. If the nodes cannot contact each other, they can broadcast and hope the message ends up where it should. As broadcasting happens nodes can build up a partial map of the network to make broadcasting even more effective. What nodes hold what balances can be decided with optimization in mind.

what is a blockchain? a chain of blocks, each block pointing to the previous, where each block contains new info. each block can only be added to the chain if its info is compatible with the info of previous blocks.
the trouble with multiple block chains is that the chains must exchange info. but info is only valid in a block if it is consistent with past info. new info for a block passed from another block is not necessarily valid because the new info is validated only by the past info of the other block, not the past info of the original block.
multiple chains are not practical, because sharing between chains means sharing past info, so the chains will just grow and grow, duplicating each other's content. Or, they will not duplicate content or grow, but they will have to point to each other. Either way, validating a single transaction means following the source all the way back to the beginning. I don't like this about blockchains in general. It means as the network grows is size, it will take longer and longer to validate transactions. I think transaction validation time should be independent of network size.
the proof applies: multiple block chains choose the 'few' option. They avoid bribery due to the objective validity of the blockchain. But this introduces the problem of validating the blockchain which takes a lot of time and likely communication -- impractical.

the system must keep track of a lot. args, merits, money, subs, claims. does it do it with centralized or decentralized software?


last night, jun 3, thinking as i went to bed i thought of this:

there is a blockchain. people on the blockchain can make exchanges with each other. when Alice wants to pay Bob, she puts on the blockchain how much and something to identify Bob, and then the blockchain is setup to let Bob access some of the funds of Alice, or maybe all of them.

the number of partitions can only grow. to keep a partition from getting too large, users across the network can exchange partitions for their own benefit of reducing the number of blockchains holding their balance.
Bob and Alice both have partitions in two separate blockchains. they want to exchange part of their partitions. Bob sets up something on a blockchain saying I will pay Alice this amount if she pays me this amount of this other blockchain. Alice does or does not pay that amount of the other blockchain. Alice does or does not make a complaint on the blockchain that she paid as she should on the other blockchain but Bob has not paid her on the blockchain. The blockchain only intervenes if Alice makes this complain. The blockchain looks at the other blockchain, sees whether or not Alice has paid as she said she did. If she did, Bob is forced to pay Alice. If not, Bobs provisional, unpaid payment on the blockchain is ignored.
this pattern could be extended beyond two participants.

the proof of work could be to find a partition rearrangement. the rearrangements happens to prove the block. a side chain cannot be developed because the validity of each block depends upon an implemented rearrangement.
the first block to implement an arrangement should be the proper block. to discourage another block from taking place, as many other chains as practically possible, should permanently block any proposed arrangement (by the attacker) regarding the block number. This should reduce the likelihood the attacker will be able to fork the chain and implement a arrangement to validate each block in the forked chain. perhaps the arrangement could be unique where there is only one possible arrangement with certain restrictions (regarding the number of participants in the arrangement, the money in the arrangement, etc). If the arrangement was unique, then the first proposed block that found the arrangement and can prove it easily, will be the proper block. the attacker will be unable to use a different arrangement for different blocks in the forked chain.

in order to check a transaction for the bitcoin blockchain, you must quickly check that the input is greater than the output, that each input exists, and that the sender has valid access to each input. This means scanning the list of unspent transactions. (the full blockchain is kept in disk space, while the set of UTXO, unspent transaction outputs, are kept in RAM for validation).

we are all building our separate blockchains, but every block we hold hands and make sure we're all still together. the only way you can build a side chain is to build side chains for all the other chains, and hold hands at every block. but holding hands requires the signatures of the transactions involved in the shift.

we've found a substitution involving [n] chains! each of us [n] chains

i want to know about a tx that's a double spend.

PROOF
assumptions:
there are nodes. each node has a quantity called a balance. nodes can share portions of quantities. /*quantity can be created or destroyed.*/
each instance of sharing is called a transaction.
/*a valid transaction is sharing quantity that was not created.
an invalid transaction is sharing quantity that was created.*/

i want to show that in order for it to be known that an invalid transaction exists, all transactions must be known.
or, in order for it to be known that no transactions are invalid, all transactions must be known.
i want to show that all transactions must be known.

the _absence of an invalid transaction_ is known if and only if _all transactions are known_.
but the _absence of an invalid transaction_ is equivalent to the _negation of the existence of an invalid transaction_, which is equivalent to _all transactions being valid_.
thus _all transaction being valid_ is known if and only if _all transactions are known_.

assume all transactions must be known.
in order for a transaction to be known, it must be known by the network.
assume for a transaction to be known by the network, it must be known by one or more nodes. (instead of known by like the communication of the nodes).
which(how many nodes know any particular transaction):
0: the transaction is meaningless so this case can be disregarded
few:
many:
...
assume all nodes must know of any given transaction.
...
three assumptions:
1. all transactions must be known
2. there must be a history (order) to these transactions.
3. the history must be unique.
the combination of these three axioms implies a single list of transactions. i guess the best way to do this is a single chain of transactions.
how to create and share this history among a group of nodes? suppose a node can propose a transaction. this transaction if valid must make it onto the chain. suppose every node has a collection of transactions that might be added to the chain. which transaction is next? for uniqueness, only one tx can be added to the chain. suppose a single node must add the next transaction. nodes must be prevented from building their own chains, for uniqueness. (maybe the next transaction could be chosen such that only that transaction is a valid extension of the curve, providing uniqueness). so a node finds the next transaction and then broadcasts that transaction to other nodes. other nodes accept it or reject it and then build upon that next chain.


HE exists that is efficient, for example Microsoft's algorithm for neural networks. the biggest trouble i see is the difficulty of multiparty participation.
but even with the way for the multiparty i imagine and is the most obvious, there may be a quicker way. for the machine to send out a public key and have users use that single public key for their encryption means, sure, while computation can be done on all that data with HE, the result must be decrypted, which means someone must hold the decryption key and if its the machine that decrypts and decides who to send the results to, the raw unencrypted data will be available at some point and thus system is resorting to obfuscation rather than HE, and obfuscation is not reliable. so as i see it for now, the only possibility for multiparty without obfuscation is having each individual encrypt their data with their own key at the beginning (rather than the preferable configuration of everyone using the same encryption key of the machine). The most basic way to do this is for users A1,A2,..., they all encrypt their data with Ei(Ai) and they submit this to the system, along with their public key Ei. The system then applies all Ei's to all other user's data such that all data is then encrypted on the same scheme and is thus suitable for computation. how many encryptions must the system perform? for N users the system must apply (N-1) public keys to each of the N user's encrypted data. That means N(N-1)=N^2-N=O(N^2) running time for data preparation. A more efficient configuration for encryption would be merkel tree based. Suppose N is a power of 2, say N=2^j. suppose we make a binary tree out of these users for each node (not counting leaves) in this binary tree there would be two encryptions. how many nodes are there? well for the bottom layer above the N leaves there are N/2^1 nodes, and the layer above there are N/2^2 nodes, and this pattern continues for each layer until we reach the top layer at which the number of nodes is 1 and thus N/2^i=1 nodes, that is i=lg(N). Thus the total number of nodes written is latex is $\sum_1^{\lg N} \frac{N}{2^i}$ The number of encryptions required is thus 2 times this function. actually this formula turns out to be 2(n-1) which is of order O(n) while the other is of order O(n^2). This is not bad. so suppose the number of users in a computation is known before hand. every user submits his encrypted data along with his public key. the machine builds the merkel encryption key and ends. actually, at each layer more and more will need to be encrypted, so actually looking at the algorithm shows me that supposing that each user has an equal amount of data, then the number of encryptions that must be performed is nlgn. i need to think now about the decryption process. the decryption process could also be logarithmic. if user's can't be trusted maybe there could be someway of exchanging and comparing at different decryption stages. but more efficient for the whole system, i imagine something where each user has his own unikernal in the cloud handling his private data. the way unikernals would be created and maintained would guarantee that they would not be corrupted by their users, and thus they could be trusted. this way they can be trusted to cooperate, and each user can receive the output and apply their decryption key. but then user's would have to trust the cloud. this is a transition from trusting the cloud and trusting all apps that handle private data, to trusting the cloud and your own unikernal. is there any way a user's data could be encrypted even on their unikernal? if the data is to be decrypted only for the user without trusting others, the user must hold the decryption key. if the decryption key is not unique, then anyone, including those who could hack the cloud, would also have access to the key and could decrypt the data on the unikernal. if the key is unique, then the encryption is unique which means everyone is initially encrypting their data by a different scheme, which is not suitable for HE. by this logic, i think users will have to trust the cloud to hold their unencrypted data, and trust their unikernal to handle it too. a trouble with everyone having the same encryption key after agreeing on one, is that the decryption key must reside someone, and nowhere can be trusted, but wait, if unikernals can be trusted, then they can all hold the private key and only use if for decryption if they are supposed to. this depends heavily on unikernals being trusted, because should even just one person obtain access to a unikernal they could use that to decrypt every item of data in the computation. this is an example of putting all eggs in one basket at the benefit of more efficient encryption and decryption. with either way, the data need only be encrypted once. and then it can be stored elsewhere on the cloud, no need to store it on the unikernal, and arbitrary computations can be made on the encrypted data without re-encrypting. its the decrypting process that seems tedious for the more secure, merkel tree method. it seems every user will have to sequentially decrypt the output, which takes time O(n) and a lot of networking time and since its sequential it can't be parallelized as can the encryption phase. well if unikernals are trusted, then decryption can be merkelized, as from the bottom up, every two parties would share their decryption keys and pass them up the tree. which means, each unikernal would be sent decryption keys which are the combination of power of two keys, until the whole decryption key they get, which means there are O(lg(n)) decryption operations (combining decryption keys) to be performed instead of O(n).
I am now aware of the HEAT project and of Kristin Lauter at Microsoft research. how far should i investigate HE? i don't want to get into the details, rather i want to know the capabilities. i have a sense of them, and if i think broad enough about their capabilities maybe i don't need to do more research. maybe i can reach out to Lauter for advice. so like other cryptographic concepts, i need to consider HE modular as a possible component. what to do now? i have a book on machine learning waiting for me at home. it could offer insight into algorithms for the chain and the cloud. the chain can cheaply verify the cloud in its public computations. the cloud can use its privacy to help coordinate the chain. either the cloud or the chain can do homomorphic computation. if user's can trust a unikernal to manage their private keys and coordinate the use of their data, multiparty HE can be done on cloud or chain, with the unikernals being the sole point of attack. the unikernals must hold private data, that is not be readable, and must be free from unwanted modification, that is free from arbitrary modification. both can be satisfied if it resides in a trusted cloud. but with HE in general, the computations, although arbitrary, are limited because they are slow and probably more importantly, they can't do loops or jumps, though maybe some form of somewhat HE could do those things. could HE exist at all without the cloud? yes but it would require the cooperation of individuals in the decrypting phase, which is hard. my though is, the cloud should be trusted to a certain degree while the system is small, and it can help the system grow enough such that the system can eventually create its own cloud, which will be more trustworthy. until then, just as so many organizations already hold sensitive data and trust the cloud, so can the system and be just as secure as those systems. the world of HE and the world of the cloud are both things i have a slight grasp of, but which many others are experts and discussion could be heavy regarding them. i think my job is to just think about the big picture and get the discussion going. So in the beginning, the system would claim no more security than existing companies using the cloud, its just the openness and homogeneity of responsibility that would be unique. later the system could be built more secure to reliably handle more weight. so worrying about security of the cloud, and examining solutions like servers under the sea or in space is not a topic for now, but could be for the system once it is launched and gains momentum. so for now i will stop worrying about the security of the cloud, and just assume the cloud is trustworthy. in this case, is the chain still useful? it is at least necessary to explore options for the chain. i need to begin developing algorithms for how the chain and cloud would operate, and how HE could exist on them. also, how 'stages' upon which apps would be built, could be developed on top of the platforms (chain, cloud, HE). there could be multiple uses of the cloud, eg computing and storage. likewise, there could be multiple types of chains, eg computing and storage. i think first step is to think more about chains and solidify what exactly the situation is, what is know, what needs work. so maybe i should make a new folder for the chain, which would contain a free file, a rough paper draft, and a final paper draft. 

the caller lends, the callee borrows. the whole thing is called a loan. there are two types of loans, read loans and write loans.


i think a foundation for the language should be information and on top of that alphabets and on top of that data structures and operations, keywords, modules, linkage. at the base level there should be a modular structure with linkage.

what should an alphabet be. its a collection of things called letters and each one should be able to be referenced, which implies each needs information associated with them, which implies binary, but binary is the same as numbers so these letters should be numbered and reference by their number. alphabets should too be able to be referenced, implying they should be ordered. so its already looking like a hierarchy, or modular structure. like from a root choose from N alphabets and from an alphabet choose from M letters. the number of N determines the number of binary digits needed to identify the alphabet, and likewise for number M and letters. a binary sequence could correspond to a binary tree. and since N is represented by binary, it is necessarily a power of 2, same with M. suppose binary sequence b and d digits. the number of possible forms of b is n=2^d. log(n)=d. d can be any natural number, that is like 1, 2,... . suppose you take d and represent it in binary with as few digits, f, as possible. that is, if r is d rounded up to the next power of 2, then f=log(r). what was the point of this? it was how to represent information in context with a binary string.
ok i think i have something, let me go through this step by step. i have a binary sequence with d1 digits. the smallest number of digits it takes to represent d1 is d2. let f(d_n) yield d_{n+1} where d_{n+1} is equal to n such that 2^n <= d_n < 2^{n+1} .
this is about binary strings. a binary string has the regular expression {0,1}*, that is it is any sequence or zeros and ones or one or the other or neither, that is there exists the empty string. an id string says consider the next so many digits. if the id string s is empty it means consider the next zero digits. if s means consider the next d digits, where d is the binary string interpreted as a binary number plus one.

i am considering the idea of computation as contexts and bodies. forget about binary for a second, how valid is this generalization of contexts and bodies? code is a body, and the context is implied, as it is implied you will pass it through the correct compiler, the compiler will recognize what you've done, and then you'll execute it, and the result will be meaningful. math is the same, where what you write is the body and the context is your understanding of the symbols. should there be a distinction between context and body? reading about rust documentation make code seem all the more fitting. is a body a context? the body may well contain context information for a sub body or something. in fact, a body could be entirely a sub context. is a context a body? a context specifies information just as a body does, but that information implicitly applies to future information that way providing a context for those future bodies. so maybe there need not be a distinction. but type theory is dependent on objects and types. suppose i am able to have lists of binary strings and lists. thus i've got simple context free grammar. what can i develop out of this grammar? well actually i think all i have is lists of lists, which is more simplistic. maybe the empty list can be thought of as unit value, and the successor function can be the list containing only the previous value. lambda calculus is very repetitive and wasteful. hopefully through contexts this language could avoid that. contexts could even save the process of string inferring, by establishing for example that for the next 16 digits every 4 digits is an element of a four element array. in actual computing i think it is think type of context that at the base level is actually at work. so how can a context be defined, or more specifically what does a context do? it should modify the term (the body). there needs to be a super context for how to relate the context to the term. lets think abstractly outside of strings. this whole system is within the area of code. code is a string of terminals for an alphabet, and by the way that string is just information and thus can be represented by binary. with lexical analysis, syntax analysis, semantic analysis, and finally the input is transformed into some form of meaning, like with a parse tree. in compilation, the parse tree is then transformed into another form, the IR. the language should be developed with the goal being modularization, and what is best for the system, and implications follow.
binary computation theory. i suppose context and body should be the basis. nested contexts are important. the context for code could be a context free grammar. the context for the context free grammar could say the following is a context free grammar. contexts can come from human intuition as long as a computer is capable of computing an infinite number of things based on finite assumptions. i guess assumptions take the form of contexts. one such context could be the format of a CFG. perhaps contexts can be introduced whenever, that way the whole system must not rely on a base context. perhaps the default context, that is for an isolated string, is string-to-list. maybe by default the first element of the list could be an context identifier. how is actual manipulation done? suppose if it is evident what mutation should be done it can be done. it need not be like lambda calculus where the actual mutation is done by the representation itself and the two are non-separable. so the question becomes how is manipulation specified? manipulation itself takes the form of changing, adding, or removing digits. this manipulation can be considered with more convenient and abstract ideas such as moving a segment of digits somewhere else, or doing boolean operations on two segments of digits. suppose you have manipulation context and a body. a logic context could be a list with the first argument an operation type, the second arg a destination, the third arg the first operand, and the fourth arg the second operand. a selection number context could a sub context presented with an list and its job is select a certain element. in this sense contexts could be thought of as functions. so a selection context would be presented with two inputs, a list, and an index. just like a function can call on other functions, a context should be able to introduce new contexts not passed down to it. but at the same time the context in which the function is executed should matter, which is not necessarily the context of the caller. for example in regular code, i call a function and that function is somewhere else but the interpreted knows the meaning of that function and thus provides a context for it, even though its me passing the explicit args. perhaps it is appropriate to assume a parent context when thinking about contexts. that parent context can be arbitrary and i can imagine it exists and plan a context accordingly. suppose there is a context for manipulation that i can use in the selection function. if i was given an array where all the elements are of the same size, and i knew that size, i could count down the line and then extract the element at the index. just like a turing machine is limited to manipulation by reading, writing, and moving, and those functions are assumptions for the model that need not be performed by the model. likewise, i should be able to give some base assumptions on this model, like the ability to read down a series of digits by a certain number, that is to count digits, and the ability to read a digit and write a digit at a particular index. so for these base operations there should be base contexts, or imagine each operation is a function, there should something that identifies that function. i'm thinking about a binary tree where zero means take the right branch and one means take the left branch, thus for an identifier as many zeros are needed for padding to make log(n) digits are implied at the end of the identifier, this way an identifier can have less than log(n) digits. so a list of items can be set as the leaves of a tree and then be identified with an identifier. out of the operations of reading the current digit, writing the current digit, and moving left or right, and given a string of zeros, i can count. a string is finite. this means there is a start and end to a string. must strings be finite? we can't practically use the idea of infinity, but it could be that there an implied digits at the beginning or end of a string. theres also the idea of an empty string. out of read, write, and move, what other operations can i do. how about invert? actually counting relies on actions taken dependent on reading. if else also seems fundamental. maybe it would be best to visualize this machine as an infinite tape, an optimizations for the real work could be done, eg by dividing the tape. i suppose a context is like a state in a turing machine. each state could have a binary identifier. on this infinite tape there could be a start digit (position). moving could be a context, reading and writing could be a context. reading could be a sub context. in a context there could be a token for switching contexts, it could specify the context and the input to the context. input would be a binary string. hence we deal with finite strings, so may an initial infinite string is not necessary. there could be a standard library, or a standard context, that other contexts could reference. one function this context would provide is encoding and decoding a binary string into a list. so no longer are we restricted to read write and move, because context switches can mean jumps, as long instructions are explicit. i've realized we need a base context, or language, that can do anything the top level language can do, thus the base context must have more functionality than I imagined. the base machine that executes this base context could be me, or an appropriately configured computer program. In the base context i think its fair to have things such as arithmetic, storage, equality, jumping, basically the functionalities of a reduced instruction set like MIPS. The goal is to as quickly as possible introduce patterns as general as possible. i like the idea that everything be an algorithm. an algorithm would be a procedure. need there be a distinction between expressions and statements? an expression could be thought of as an algorithm that returns a value. a statement could be thought of as an algorithm that does not return a value. and a value is an algorithm that returns itself. an algorithm then i imagine could be a list of algorithms. but i guess a list isn't really an algorithm. what is a value, i guess a blob of raw information. so how would this base instruction set make it possible for any algorithm to exist? algorithms must be able to call each other. since everything i an algorithm, and everything can be reduced to a list of base instructions, an algorithm must be a list of base instructions. what i will now call instructions are base instructions. maybe suppose base instructions include a stack and heap, where push and pop are for the stack, while addresses are for the heap. i was thinking all about a tree of identifiers. as the number of things to be identified grows, the tree could grow without affecting the identities of existing things as their identifiers could have implied zeros following. for concurrency, the default could be execute something and keep going. if you want to emulate standard assign to returned value of a function you would set up the return handler, send a message, then halt. so inside an algorithm everything would reference another algorithm. really the algorithm is the return handler. even constants used in the algorithm would be an identifier that could be thought of as a reference to an algorithm. so this would be a very recursive paradigm where if an algorithm wants to add two constants and assign the result, it would mean referencing both constants (which we can assume would not return values as if the identifier is the value), setting them up as operands, then referencing the add algorithm passing the operands, halting, receiving the result of adding, doing the assignment, and continuing. this means modifying the handler to not do repetitive code but instead to jump straight to the assignment operation. maybe the handler could be a jump instruction, and that jump address could be modified for each call. this theory assumes different algorithms are present. maybe there should exist cell division. this could mean a create cell instruction and a delete cell instruction. these cells would be algorithms. there would be a send function, and maybe a receive instruction that would set the address of where to pass an incoming message, but i imagine a receive instruction is not necessary.
variables can be done with arrays. if variables can be declared in the order a, b, .. of the alphabet, they can likewise just be declared as positions in an array 1, 2.. . Arrays can be manipulated with the base manipulation of adding a new slot in last position, deleting the slot in last position, and rewriting the contents of a slot. how can variables be done with an array? have an value, call store, it returns an address. in a regular program that would be represented in a variable that is reusable throughout the scope. in our case instead the address would be placed in a certain index of an array. each index of the array represents a variable, while each element of the array represents the corresponding value. so i guess for a local storage you would just place a value in a certain index. just as a regular program makes sure to use the same letters for the same variable and keeps track of that letter, in this system the writer needs to keep track of indices. to use the variable once created, it means pointing to the index in a context where its value is then accessed. i was thinking of a file system kind of thing where parts of a structure can be pointed to and manipulated and i considered that these operations would be base operations but then i realized they can be implemented through function calls. so the idea of all procedures being a series of function calls still works, and some function calls being base function calls that in a sense make their function calls implicitly and externally. to make a function call the function is specified by a context, then arguments are specified, and probably this argument could be anything, like a two tuple with a type and a value. the callee dictates the terms of the call. like for a storage call, the callee will need to be able to access and mutate the storage array. there needs to be a way to point around the structure, just like a file system. there should be absolute paths and relative paths. a first bit a path can say which. if absolute its a matter of specifying a list of indices, which can be encoded as a list of numbers. if relative its a matter of specifying indices as well as the .. equivalent where you jump up a directory. so it would be a mix of indices and ups where an up would have a specific code like an empty element of the list. maybe there could be a pointer that moves around showing the current position of the processing. a path would move this pointer around. would all functions be like inlining? maybe parallelism could be expressed by having multiple pointers, one for each thread. what could move a pointer around? there could be a set of operations for a pointer like move to the next or previous element of the list, jump inside the current element, jump up to the parent of the current element. these would definitely, along with the concept of the pointer, be base ideas. but maybe if the encoding algorithm isn't base, then the base pointer operations could be shifting left or right by the bit. a homogenous procedure is one that appears the same for each caller. and i think inlining would work for all these functions. the other types of functions would be non-homogenous, so they cannot exist in multiple places as multiple instances without appropriate synchronization. so they should act together as a single instance. like a single server with multiple clients. so then i suppose there would be like a queue for the clients, just like on a web server. maybe base operations can be just bit operations of moving the cursor by bits, deleting and writing bits next to the pointer. pointers can be created and destroyed. there needs to be a mechanism for a pointer to jump around the structure. this means decoding both the structure and the path. let me confirm the structure and path protocols.
00
01 n
10 nn
11 0 nnn
11 100 0 nnnn
11 101 0 nnnnn
11 110 0 nnnnnn
11 111 0 nnnnnnn
11 111 1000 0 nnnnnnnn
11 111 1001 0 nnnnnnnnn

0 2
1 3
2 4
3 6
4 10
5 11
6 12
7 13
8 18
9 19

a path would be a list encoded as such with integers of one or more digits for indices and 0 digits for going up. i suppose lists should start with a zero index. so ../0/2../5 would have encoding below. .. /0 /2 .. /5
[00][01 0][10 10][00][11 0 101], or 00010101000110101

at anytime, the computation should be able to be paused and when taken back up all the info should be there to resume. a program could be written with only bitwise instructions that could navigate the structure, but this program would have to be recognized as such a program which requires the structure. reads need   to be a base operation.

0000
0001
0010
0011
0100
0101
0110
0111

just as the whole up project is based on a system where i the system maintains the system yet I need to build a base for it (even though that base may be changed). In the same way, this language needs a base that the rest can build upon. that building can re-express the base, and so one part of the language can then explain another part of the language, in a cyclic form. so suppose encoding and decoding the list format is a base operation. other base operations could be creating, deleting, or rewriting an item of a list. the base context would have different types. one type could be the list format. the values within a list could be of other types. a path could be zero or more bits specifying an empty or present integer to identify up or an element of a list. a path could also be a list of paths. a list is the format where the is a series of values. values can be bit strings of zero or greater length. these strings could be interpreted as integers, nil, or more lists. there should be types that determine how you interpret these strings. a path type would be a composite of list type and integer type. there could be boolean type. a context free grammar could specify this. a context. parameters. these form functions. i guess the base functions are the base operations of manipulating the structure. so certain functions should be recognized, as like oh, that is not a written function, but an external function that the evaluator needs to perform. maybe a function name can be specified by a bit string and an empty string means a base function. homogeneity is decided by the callee, that is should it treat each caller the same, or possibly differently. another way to ask this is, what is the lifetime of the callee? it is forever (inhomogenous) or per caller (homogenous). dependence is decided by the caller, that is should it create a new pointer (independent) or not (dependent)? the problem with the current model i have in mind is that it means inserting strings at different points in a list, but each list is identified by a precursor string of the number of digits in the list, but insertion changes the number of digits in the list, which means, changing the precursor, which is especially a problem because it means changing the precursor for the list that contains the current list, causing a recursive process such that all parent lists must be modified. this encoding together with modification of lists, is therefore inappropriate. i still like the idea of the encoding, and the idea of using lists and manipulating them. so suppose the list manipulation continues with the idea of functions, but the encoding of these lists must be different. the model i'm thinking about is composed of functions where each function calls other functions, and some are base functions. i suppose with binary one thing I was imagining is that strings could not just represent lists but could also represent numbers. the trouble is differentiating lists from numbers. with the previous encoding this would work having the default initial format a list, and then the first value of that list would identify a context with which to analyze the remaining elements of the list. this works as long as a list can contain not just lists but raw strings. suppose a new encoding for a list starts with a 0 ends with a 1. so you read a 0 and then you know you're viewing the first element of a list. maybe it could be another 0 if its another list, or a 1... but this interferes with empty lists so this encoding does not work. suppose there are two types of things, lists and numbers. every string is of one type or the other. every string begins with a bit that identifies if it is a list or a number. 0 for list, 1 for number. it seems a fundamental problem that differentiating items in a list requires that you know how long the item is which requires first encoding the length of the item, but since items are nested, if an item is changed the length of it and all parent items must be changed accordingly with is not viable. so I should develop the base before trying to develop a simplistic encoding for that base. because there are so many possible encodings and each one has certain properties. the base I want to develop also has certain properties i desire and obtaining those properties are more important than the properties of the base encoding. the base encoding will not actually be implemented, but is only for theoretical purposes and fallback. what are desired properties of the base encoding? there should be opportunity for parallelization and very high level abstraction. homogeneity should be addressed because different functions may behave differently and whether or not they are time dependent determines whether or not they can be copied. maybe i should think about the big picture first. functions get created and approved, and then new functions must use only existing functions and then they can get approved as well. there should be abstract syntax that makes writing the code easier, but with functions building on each other this may not be so necessary as high functions will already be very abstract. since different computer will be running the system their respective tasks should be well defined. to change the code running on a computer it will be necessary for code to run, and that code can be developed and approved within the abstract system, but actually running that code will mean handing it down to whatever software is interacting with the operating system, or hardware.
whats the difference between a computer in the cloud and something on the chain? its that a cell on the chain must send a message from multiple computers, whereas a cell from a trusted computer in the cloud need only send a message from one source, which is what the external world is expecting to interact with. just as programming languages are based upon three address code, which represents the physical architecture, maybe the language of this system should be based cells which represent the physical architecture of system in that computers are cells, whether in the cloud or on the ground (owned by consumers), and they send messages to each other. and inside a computer there are multiple processes running which send messages to each other. on the blockchain ground computers must run code, and the code they run should be thought of as cells, likewise for the cloud computers.
in OOM, there are multiple instances of higher level classes (appearing in lower level classes) and lower level classes can use multiple higher level classes.
in functions, there are multiple instances of lower functions (appearing in higher level functions) and higher level functions can use multiple lower level functions. every function consists only of other functions, nested inside it. each function has only one parameter, but that parameter can take any form, like a list of parameters. via their parameter, functions can receive messages. functions do not return values. maybe i should call these things cells, but they are still technically functions. cells can emit messages, or maybe message sending should consist of sending a message to a cell name, just like calling another function. if everything is a function that leads me to think of purely functional programming. i'm getting overwhelmed with functional programming, especially when i hear of all its disadvantages, like the idea that you can't do assignments with it. for the language i imagine assignments are easily done even if only functions are used. let me describe the language i imagine. every function is composed of other functions. to imagine this approximately but to purely, a function might analyze the argument and decide what functions to call as a result. in fact, this is what i would like the system to do. the question is what tools does the function have to work with, and what would be a sound theoretical basis for this? as far as tools, it would parse the arg and do pattern matching and conditionals. where is data stored? i suppose a function could offer you a service of storing your local variables. forget written language. the language itself should be about how to operate and develop on top of this system. the concern should be how to identify problems, divide them into sub problems, and how to solve sub problems. people need a way to collaborate with each others and a common vocabulary to do so. of course there are multiple layers to a problem and higher layers may require a different vocabulary than lower levels where actual computations are being made. all layers are based upon the same base language. thus there needs to be a mechanism for translating higher level vocabularies to lower level vocabularies. what about actual implementation onto computers? well the base of the language could be executed, even if by a human. and somewhere in the language it could be decided how the implementation should work, like how to take the base language and compile it to an existing executable language like machine code that is more efficient than executing the base language. i want a language that doesn't make assumptions about alphabets, like the english alphabet or special characters. all other alphabets should be able to be used just the same.
I need to think about bootstrapping compilers. I imagine I will use rust in some way. i can build a full language on top of rust. what i need to figure out is whether i should compile to rust or whether the language can be fully self hosting. before i read more, i need to think more. how does a language translate another language. there is source language (S), target language (T), and an implementation language or current language (M). The compilation process i imagine is the following. M takes input in form of S, builds tokens, a parse tree, and then translates that meaning into T. so the language C could be M. Suppose S is lisp, and T is machine code. so C takes a string of lisp and produces machine code. but producing machine code is not easy. suppose instead it produces more C code, which is then compiled to machine code. but the problem there is relying on the C compiler. another way. you build a compiler for S in M. what does that mean? that means i write M code that can take an S input and outputs, same M code. that means i can write some S code and have it run. then write an equivalent compiler but in S. then you have an executable compiler for S in S. then how to update the compiler? but as an article says, self hosting a language has first priority of expanding the language to have features, and only after that is done is the language used for its original purpose, application programming. first priority should be the language from the perspective of the programmer, not the compiler. i wouldn't mind relying on a portable system language that offers abstractions. the risk of this is that i miss certain opportunities, like a lower level language being better for certain things, like maybe putting together a functional language. but i think rust is comprehensive enough, just like C, that i won't be missing anything.
list processing. LISP. what about primitives. so the basic format is the list, but its not so pure that everything is a list. there could be some primitive list types, where the first element sets the context for the rest of the list. there could be one for defining a function, making an anonymous function, defining a variable, a conditional, logic operations and or not, and arithmetic operations. there would also be primitive types, which i imagine would be numbers, characters, and booleans. if the first element in a list does not match any of the primitive list types, then it this could be the last type which would be function application, i think. i am not certain this would work, and there is uncertainty about scope still for example, if the full behavior of those primitive types is not explained.
a new function can be introduced. a function can be modified. i suppose every function can have a cost. primitive functions can have assigned cost dependent upon hardware. higher function have costs as the sum of their lower level costs. the goal is to lower the cost of functions, whether that is done by rearranging the lower level calls within higher level functions, or by developing new hardware for the primitive functions, or by developing hardware for higher functions. so lowering the cost of a function lowers the cost of all function that use that function. sum together the all the improvements and call that the reward. the goal in optimization is to fix functions that maximize reward. reward can also be weighted by how often functions are called, but ultimately the calling of the top level function occurs by hardware, which can be thought of as a function (like me typing a command and pressing enter), thus this weighting may not be needed and simply adding together the cost of all functions that apply the otimized function can calculate the reward.

substitution model
free and bound variables
primitive types
tail recursive: procedure recursive but not process recursive
data abstraction: separate use and representation of data
coercion


for a new procedure, decide what it does, make sure no other procedure already does it, then implement it.
ways to encode characters. unicode, huffman, my idea.
my idea is like a huffman tree but its a full binary tree with known depth. new characters can be added to be added to the tree without modifying the codes for existing characters, but the depth of the tree may need to be changed, that is incremented (thus doubling the possible number of characters) if the tree is full and new characters are pending.
ways to encode strings. unicode with utf-8. huffman just listing the codes in a sequence. but for my idea its more complicated, but it could be done similar to utf-8, which is similar to my original binary idea of expressing the length of a code with a prefix.
in every day life, optimization could be thought of as removing middle men in business.
to simulate system that changes state, and wish to do it through symbols, must have mechanism for reassigning symbols.
detecting sameness means taking two objects thought to be the same, changing one, and seeing if the other changes. detecting change means taking an object at two different points at time and seeing if the second instance is not the same as the first instance. thus detecting sameness requires detecting change and vice versa.
i like lisp a lot. i'm beginning to think languages maybe should be independent of their implementation, that way a universal language can be made that can have all kinds of different implementations for different specialties. why do i want to have a single language? let me back up and establish goals. i want programs to be reused as often as possible. I want the logic of the system to be minimal, so that it is as understandable as possible. I want it to be very modular. like i want to separate high level logic such as policies from low level logic as to how policies are implemented. different layers will be manipulating different kinds of objects and have different concerns. will they be working with different languages? to answer this I need to clarify what a language is exactly. a language supports different types of syntax which perform different functions, yet it is still just one language. different layers too will support different syntax for different functions. what is important about a language is that different types of syntax can be reduced to the same type of syntax. suppose all languages run on the same instruction set, then all languages are really just one language. i realize this is a big field with a world of tradeoffs. if i'm looking to design something universal i should not study this expecting to find a perfect solution. i'm just trying to get an overview of the field so that i can implement something general enough to not miss any opportunities. there are all kinds of data structures and optimizations that can be made to better implement any language. suppose i take a random language. if this language is turing complete i imagine it could be used to interpret or compile any source language to any target language, however inefficiently. suppose everyone operated in this language. if one wants to program in another language they could develop it and use this language to compile that language to this language. or if they wanted to execute on a different machine they could use this language to translate that language to the target machine. not every language need be build and maintained in the base language, because anyone who uses it needs to learn it. i think the system should be based around a number of languages, without any of them being a primary language. there would actually be a hierarchy of languages. there would also be a collection of instruction sets. this means no universal language but instead an ecosystem like the current one we have where there are multiple high languages and multiple instruction sets and never along the hierarchy do two languages necessarily collide. i need to really get it in my head that compilers and interpreters are just programs. everything is a program. i'm imaging two primary languages. a systems language like rust for implementing applications, and a higher level language for for application logic. i suppose everything, including language changes, is described first in the higher language, and then implemented in the system language. the interpreter or compiler is what actually defines the language, even if it is documented somewhere else. why not everyone use their own language? they wouldn't be able to easily communicate. they wouldn't be able to use each other's code because the two languages have different meanings and do not share an interface. so different languages can exist and both be useful as long as they share an interface. perhaps this could be through the operating system or something, like writing to files and then executing files taking the written files as input. multiple languages also means more learning though. it would be nice if a single language could settle the whole base layer, like OS's virtual machines, system programming languages, embedded systems, etc. and i think rust could be a good fit for this with its emphasis of speed, concurrency, and safety. i was getting more comfortable with the idea of a base language of rust for all system and high performance computation, and then developing arbitrary languages on top of rust, including rust itself. but then i realized that all code will either be running on a vm on a consumer computer or in the cloud on a hypervisor like xen. maybe i should take cloud infrastructure into account when thinking about what languages would have the best performance. i'm now looking at unikernals. just as a compiler can have a back door, a cloud system might be able to do the same. suppose a comp runs encrypted. it cannot be modified intentionally, and modifying it unintentionally has high risk of touching data such that the system will realize its been modified, and will either not be able to function, or signal an error, in either case the responses from the comp will likely signal the comp is down, and appropriate action will then be taken by other comps.
since no existing language is perfect i suspect i should just pick a good language and start using it right away to modify the system and improve the language. right now i'm looking at ocaml for mirageOS. I like the idea of unikernals because then the system is more modular with lots of small components rather than a whole operating system (which was not designed for this system) that is monolithic and hard to modify. i'm thinking about the security of the cloud which is never guaranteed. not only is it necessary code cannot be changed but that all the data in the cloud can't be read because much of it may be private data. there is a possibility of encrypting the data and encrypting the computing process or control process such that one can watch the computer do all its execution but yet not be able to find the decryption mechanism and therefore be unable to decrypt if not locate the private data. i imagine error correcting codes could be involved, and just like a back door for a compiler, such that if something is modified by the wrong source that should not be modified, and error occurs. the system should be designed so that there are cells that accept signals, and they can accept any signal so there is no wrong signal, but if the cell is modified in any other way than a signal than the error correcting code catches it. i imagine such modification prevention could occur, but it may be unnecessary because what is more important and should be first priority is preventing the reading of private data, which i think really translates into preventing the locating of private data and the locating of the decryption key. since there is a lot of data and data will follow similar control flow there is a lot of opportunity to locate private data and so i will assume that is already done. then the prevention is a matter of preventing location of the decryption key. i think this is equivalent to the task of encrypting code. how do you encrypt code yet still let it run. this brings me to the law i established earlier this summer is developing the system, well actually i established it in turkey. that is if a person (or company such as AWS) owns a computer that is executing software that person has full control over the software and we need to develop the software such that the person could be executing it, in other words that the person is the computer. encrypting code would then be like giving a person instructions to execute such that they can execute them yet not know what they are executing. with type abstraction and low level operations perhaps this is possible but i imagine with statistical analysis patterns in computation could reveal what computations correspond to what tasks revealing private info. this analysis would be particularly dangerous if the person could feed the system whatever signals he wants as much as necessary, and this seems unpreventable because it may well be that in certain circumstances such a multitude of signals is actually needed, and it cannot be differentiated whether those signals are initiated by the person or someone else because the person could have outside connections. for now i think encrypting code and thus preventing reading of private data should not be sought but rather we should trust the cloud to do it for now, just as many companies already trust the cloud to hold private data -- even hippa compliant data. this way the system would not be guaranteed secure as AWS or the gov could get access to private data, and even modify the data, but they system should be trust just as much and no less than other companies with data in the public cloud. if the cloud turns out to be very secure, most of the system would then reside in the cloud and private data could be analyzed. if the cloud is not secure it could be used minimally. if code encryption could occur it would just as applicable to the chain as to the cloud. suppose private data is not accounted for but only the correct handling of public data. the chain could work, or a cloud system that catches ill modifications could work, and the question of the better solution i think is just which is cheaper. private data is a different question and can either be handled by trusting the cloud or by developing code encryption. maybe a public system without private data could become powerful enough to begin ownership and management of data centers such that the cloud can be trusted through trust of the public system. this would be the necessary solution to handling private data if it turns out code encryption is impossible or just too expensive. actually i think there are two concerns at hand: trusting a system not to modify data, and trusting a system not to read private data. the first concern is what the chain or the cloud with correction codes could provide. the second concern is what the cloud with code encryption could provide on chain or on cloud, or what a trusted cloud could provide. for now we will trust the chain for the first concern and trust the cloud for the second concern, and by trusting the cloud to to read our data i think its appropriate for now tot trust the cloud not to modify our data either so we also trust the cloud for our first concern. it would be interesting to think about ways to develop a trustable cloud should the public system decide to do so so as not to need to trust a third party like AWS. such a cloud would require computers that can detect tampering and can hide their location. does this still mean data centers, or what about things in space that could avoid attack or a bunch of drones flying around. if our own data center existed how would it run, what stack would it have all the way up? it could have different types of hardware, and protocols for software running on them to communicate. a VM environment may still be necessary to abstract eg the address space. on the stack there is no single layer at which everything converges and then diverges again, like a single IR. that would have the shape X, which would be great but its not possible to do everything that way. the worst would be the shape (). I imagine the real shape will be more like )(. i need to develop a system of modular thinking. about computers. there is no perfect language at any level, hardware, IR, or high level. i imagine hardware will only get more complex to become more efficient by directly handling special cases, especially in the area of scientific computing. last night i realized what the system needs. as i have been wondering while researching languages, where exactly will these languages unify? i had imagined some kind of modular system where modules could refer to each other but how that would happen when they're all in different languages i didn't know. now i think i realize at the base level there will need to be one language that will unify them all. previously i didn't think this was possible because no language is perfect. but now that i asked myself what exactly that language would need to do, i see its role would be so limited that there could be a single language that plays the role and i believe in fact a unifying language is necessary. this language would occupy no particular position along the spectrum from low level to high level, but would be everywhere. the purpose of the language is to describe what each module does, really what each program does. there would be a directory of existing programs that have been approved. this directory and the approval process would exist on top of approved programs. a new program can be added by passing the approval process. an existing program can be modified by passing the approval process. an existing program can be removed by passing the approval process. all programs are subject to change, including the program that describes the approval process. so this language is programming language independent. its a protocol language. it could describe not just software but i suppose hardware as well. i will call the things it describes now as modules, not programs. a module says what it is, what it does, and how to use it, so it like gives directions. like one may be a program that says what arguments to provide and what return value it provides. or it could just give directions, like describe a piece of hardware and how to get it and use it. so it could like have a low level piece of software and describe how to get it to run on hardware. a module could describe a full language like rust. instead of an explicit inheritance structure with submodules i instead imagine a collection of modules that form a graph with the edges describing relationships. the directory would be the context for modules, and each module would be a sequence of unicode characters, probably encoded like UTF-8. since modules are obtained over the network computers can access and parse modules and thus modules can manipulate other modules. since modules describe how modules are created there is a chicken and egg problem. but theoretically i've been developing modules in my head and those modules can describe how the first physical modules will be created, like on paper, and those will describe how the digital ones will be created. the languages i choose to start with in developing modules depends on what they would be used for which depends on the design of the applications. a language is a type of module. a module is something that describes some meaning in some context, and has a purpose whether it just reveals information or whether it performs a task, the former is like having a return value, the latter is like sending a message, and either way it yields information. so a module is something that takes information, contains information, and yields information. even if one of those three types of information is non existent it can be still thought of as information, like nil, the information being that there is no information. i'm thinking all kinds of programming paradigms will be needed for the system. i realize quick prototyping should be easy with languages, but syntax easy to write on a whiteboard should not be a priority. on whiteboards there would be other more terse ideas but not actual code. every paradigm has its meaning and this meaning can be expressed with simple syntax. and further meaning can be built on top. the challenge would be implementing those base levels which would be kinda high level things like loops and iteration and functional programming and data structures like lists, tuples, trees, arrays, and math stuff too. is it appropriate to use an existing language lie rust, haskell, or ocaml? i suppose one could do that by introducing a module that says use this language to implementing these base types and implement this language this way. building on any base concepts requires a syntax as i have the assumption that code is written as a sequence of characters. with binary encoding of characters the default context to interpret in would be something like utf-8. and the default meaning would be all the meaning we already have infused in human languages. as we are assuming this prior meaning interface available we can use this meaning to describe this default context of characters. Since Ocaml has a syntax and is able to mix and match multiple paradigms maybe Ocaml is way to go. but maybe different paradigms will be different enough and used by different communities such that different syntax is not bad. and oh, with code being a sequence of characters, lists are not a bad syntax for any paradigm, and any syntax can be converted to list syntax. suppose rust is a replacement for C. another language could be a basic replacement for application programming, like swift. another language could be used for scientific applications. i think it is appropriate to have a new language wherever there is a community to use it. a person shouldn't have to switch between many languages in their daily routine. they should be able to work with just one language. this is akin to people having a particular area that they work for, whether in scientific applications, general applications, or systems software. i think those are the three primary areas of effort in this system and each area has sub areas which perhaps could have sublanguages. since this system will depend heavily on distributed computing with a lot of networking and stream processing, for all kinds of applications, what language is best for developing those types of applications? in a sense it seems any language would do, but people say functional programming is best for that kind of stuff. i really think rust should be used at the base level for programming computers. so i'd say a big question now is what language is best for writing distributed applications? another question is suppose i find such a language, how is it implemented in the cloud? if with a unikernal, that means using either an existing unikernal language or writing a unikernal with the language. but Ocaml may be good for applications as it can be imperative yet also OO and best of all for distributed computing functional. since mirageos has already managed to use Ocaml for a full computer maybe ocaml could be used for now even at the system's level and given its supposed numerical performance, even for scientific applications. maybe julia would be best fo the scientific computing area. i'm beginning to think i should just begin working on mirage OS with Ocaml for basic operation, and when it comes to system programming or technical computation a unikernal could be developed for those things using languages like rust and julia. or maybe some of those computations could be sources to the chain where they could run on a computer with a rust program. actually the solution would probably to use another existing unikernal, but one for like c++ such as includeOS. i suppose mirage would be sufficient to test whether the system is possible and its functionality, together with functionality for chain in maybe another language, see whether other developers are interested and if so get them involved, from which other resources can be developed such as unikernals for other languages and they could help develop better software stacks. until then, Ocaml will do, especially if it is fast enough that it can crunch numbers for RSA, and if not another unikernal like includeOS could be made to handle batches of just such operations. ClickOS is also interesting. an intersting question is how this project will grow. over time, what will be the ratio of system functionality with committed developers? i'm hoping developers will commit sooner than latter with only as much functionality existing as mirage can provide. includeOS seems even more minimal than mirageOS, but a functional language may be desirable. i hope a mirageOS with Ocaml like system would be permanent for the system and I can imagine it would be for the general application layer, though i imagine improvements on both mirage and ocaml would eventually surface. i thus think its not a bad idea to begin development of the system on top mirage. Ocaml has good potential to support domain specific languages. i was thinking unikernal instances can be specialized and therefore there is a need to recompile each one for optimization when any part of it is modified. also, when a connection is made, the distributed cloud could handle it, and just as a request for a web server goes through middleware, data can flow through unikernal instances in this distributed cloud system. also i was thinking about encryption and that analysis may be able to be done on encrypted data. 

the blockchain community seems eager to develop applications on the blockchain targeted industries in their existing configuration. new ideas within existing industries is usually what occurs when new technology is introduced. but these new ideas are short of ambition and do not see the full potential of the new technologies. The blockchain community seems to be undergoing this process. the community envisions patching up existing industries with blockchain technology. The primary ways the community is still stuck in the pass is in regard to:
existing configuration of organizations, existing laws and justice systems, existing financial institutions, and governing institutions. i believe all these institutions were configured for the past and now with new technology they can be completely replaced. the blockchain community, like ethereum, seems focused (as they claim to be) on digital contracts (agreements between people). as they see, these contracts are applicable to a lot of situations in current society, to patch up some industries. i want to go beyond contracts. though the system has no controller, the individual contracts upon which all operation happens have controllers. so i see ethereum as only solving half the problem. 

we need to change, and we have the opportunity to change.



I take the two system of mechanical physics and economics. I know that there is a concept of velocity in physics. Reading the word 'velocity' in an economic context I realize a similar concept exists in economics. In order to establish the equivalence of these two concepts, I pick definitions for the economic context. Velocity for econ is the rate at which money flows between people, while for phys its the rate at which mass moves across distance. For phys, the object that has velocity is an amount of mass, and its velocity is measured as change in distance per unit time. For econ, the object that has velocity is an amount of money, and its velocity is measured as the number of transactions per unit time. This velocity can be reframed as the number of people the dollar passes through per unit time, or moreover, the change in people per unit time. Thus the appropriate definitions for the equivalence of velocity as I imagined it are the following.
mass -- money (kilograms -- dollars)
distance -- people (meters -- people)
time -- time (seconds -- seconds)

Given a set of definitions we will now test see if the relationship of these definitions holds for other concepts beyond that of velocity. A good place to start would be to test the validity of newton's three laws of motion within the context of economics and see if three equivalent laws exist for economics.

Newton's first law: the law of inertia.
Every object in uniform motions remains in such uniform motion unless acted on by a force. This means an object with a certain mass and a certain velocity will continue to have that mass and velocity unless a force is applied. In other words, momentum is constant with no force present, and momentum changes when a force is applied.
For economics, this means an amount of money with a certain dollar amount and a certain rate of being passed between people will continue to have that dollar amount and rate of exchange as long as no force is applied. Momentum is constant if and only if there is no force present. This makes sense. If you are an individual with a certain income and certain expenses to pay, you will continue to be paid and continue to pay as long as nothing changes your income or purchasing habits. If you hold every dollar for the same amount of time, that is you spend at the same rate you earn, momentum is unchanged. If you begin to hold dollars for longer than usual before spending or shorter than usual before spending, you are changing the momentum of the money and thus a force is present that causes you to act in this way. Newton's first law seems plausible for economics. Let's move to the second law.

Newton's second law: the law of acceleration.
The force acting on an object is equal to the change in momentum of the object. As explained above for the first law, this makes sense for economics.

Newton's third law: the law of action and reaction.
For every action there is an equal and opposite reaction. When object A exerts a force on object B, object B simultaneously exerts an equal and opposite force on object A. Establishing this law for economics is more difficult.
First we must find a force, that is something that causes you to spend faster than you earn or vice versa. Suppose you, a single individual, has a strong feeling that prices will rise in the future, and this feeling is unique to you. You will spend more than you earn in order to concert your wealth from dollars into physical assets while prices are relatively low. A force is present that is increasing the momentum of the money in your possession. Newton's third law states that whatever is causing this increase in momentum must experience a equal decrease in momentum. Follow the supply chain, and each individual in the chain will have an increase in money coming in (due to your extra spending) and a immediately an increase in their spending to cover your costs. Follow the supply chain until it returns to you. You are offered more, but you refuse. Thus the economy as a whole experienced an increase in income equivalent to your increase in spending, but zero increase in spending due to your zero increase in income. The economy reacted to your action with an equal and opposite change in momentum.
This action reaction theorem holds for a graph with one edge, two edges, and now the induction step. By adding an edge, it just means the spender and the earner have a new path for money to flow on.
_thinking_
suppose everyone at once decreases their spending by half. everyone's income will simultaneously decreases by half. does it take a force to organize this synchrony? now there is the same amount of money in the economy, but each dollar is moving half as fast. the momentum has decreased by half. for the whole system to decrease in momentum means an external force must be applied and whatever exerts that force must experience an increase in momentum by two. when one person decreases spending, there is no reason everyone else cannot simultaneously decrease spending. it would, however, required infinite precision. everyone could organize the change as continuous, which would decrease the momentum continuously. the paradox is that the individuals are not changing momentum but the system as a whole is changing momentum. so what is the external force and what reaction is that force receiving? perhaps we do not need to know, just as we do not need to know what is causing the expansion of the universe to know it is happening.
_done_
so anyone in the network may experience an equal and opposite change in momentum. If not, then the momentum of the whole system changes.
so if an increase goes around indefinitely it is like an wave of momentum going through an infinite line of blocks. suppose as the first wave is passing through the network, another wave is released. suppose more and more waves are released. suppose the time interval between these wave releases approaches zero. it then appears the entire system has a new momentum, when in fact there is just a bunch of indeterminate actions whose final reaction will occur at time infinity. it is the circular nature of an economy that makes a new system momentum possible.  multiple people can initiate the acceleration but whoever does it first will experience a loss of money in possession.
so I think newton's third law holds, even if the reaction comes at time infinity.
or wait, an external reaction could be one that causes money to accelerate independent of the behavior of the rest of the economy, that is independent of money accelerating elsewhere in the economy. if such a force is applied it is appropriate that the entire system changes momentum. an interval force is a behavior of the network that causes money to accelerate. that behavior is causing an equal and opposite acceleration, thus the behavior must be the acceleration of money.
the only way the third law would really work is if when an external force causes one to accelerate money, those who experience the acceleration know its an external force and therefore simply pass on the accelerated money, but if the force is an interval force than those who experience it must absorb the acceleration of money.

nature never changes its mind. maybe this whole physiconomics thing would work if people never change their mind. but it doesn't seem to work that way.

umm, so it looks like the three laws hold. presumably this means other laws should hold.

let us explore conservation of momentum. momentum of the system is constant with no internal forces or when a force is applied shortly. there is an action and reaction and total momentum does not change. but due to the circular nature, if the force is applied for long enough, it can appear the momentum of the system has changed. ... so i don't know, is it conserved? construct an equivalent physical system and see if its conserved for that.

what about conservation of energy?

a force could be a change in interest rates.
KE could be the money you spend minus the money you make.
PE could be the money you make minus the money you spend.
KE + PE = 0
for a firm borrowing PE could be negative, and KE would be positive.
but in physics, KE and PE are differently dependent. KE is about mass and velocity, while PE is about position and mass. but in this model, KE and PE are both dependent on the same two variables.

KE is already defined. PE must be such that its sum with KE is constant in the absence of a force. the shorter time money is kept (the greater its KE), the less potential it has for investment at the current interest rate (the lesser its PE).
just like with gravity, when money has a lot of potential energy (it has created a lot of other money by investment) that money is eager to be spend, and KE will be great when it is released.
since money will pass through a person in finite time interval (not too long or too short), KE will never be zero or infinity and PE will never be zero (b/c money will always have some time interval for investment) and never be infinity (b/c the time for investment is finite). KE and PE are always positive, and so is their sum, E.
if a force of interest rate change is applied. say interest rates increase. people will borrow more, so velocity and thus KE will increase. people will invest less, so PE will decrease. I need to check and see if KE + PE really is constant.
the trouble is the units don't seem to work out. maybe think about this when studying macroeconomics, comparing forces of interest rate changes vs forces of inflation.


meters == people
seconds == seconds
kilograms == dollars

what allows an analogy to hold? two systems must have equivalent definitions such that relationships within one system hold in the other. if it can be proven that the two systems have all the same relationships, then discovering a new relationship in one system will necessarily yield an equivalent relationship with equivalent properties in the other system. how can it be proven that all relationships, and not just some, hold? It must be shown that for every relationship in one system, an equivalent relationship for the other system will exist.

another approach is to take two systems and find similar properties, and then seek definitions that definitions that describe the relationships.

Given the above attempt at definition equivalents, let us explore how far the relationships hold. the first definition that will be tested is conservation of energy. This means that for an isolated system, the mass times the square of the velocity stays constant, equivalently the dollars times the square of people per seconds stays constant.

to take the second approach with the property of conservation of energy, the property and the definitions are already set for the first system, the mechanical system. The relevant definitions are mass, distance, and time. Now the property must be described for the economic system. For a simple, isolated economy with no change in money supply, and no technological breakthroughs, no change in population, and no change in consumption habits, the amount of money per value is constant. The amount of value consumed per unit person is also constant. As such, the amount of dollars per unit person is also constant. In such an economy the same amount of value is purchased per unit time per unit person. I only know to describe this as exchanges per unit time being constant. Now economic definitions must be sought so that the relationship between definitions for the mechanical system (described by the units for energy) hold for the definitions chosen for the economic system. The relationship for the mechanical system is kg m^2/s^2. Thus economic definitions, x, y, and z, must be chosen so that x y^2/z^2 is constant.
when conservation of momentum holds, so does conservation of energy. but conservation of energy can hold during a conservative force, when conservation of momentum does not hold. so when an economic force is applied, and money changes velocity, does conservation of economic energy hold?
does conservation of energy hold? given the current definitions, is there a quantity that is constant with unit $p^2/s^2 that is the sum of two parts, 1/2$p^2/s^2, and another part that is dependent upon the dollar, the conservative force, and the person? for this to occur, we must find the quantity PE that is inversely proportional to KE such that the sum of the two is constant. Lets us follow a single dollar. As KE increases, velocity is increasing. Remember that a force is defined as that which accelerates a dollar at one person per second squared. If the dollar amount is fixed to a single dollar, and a force is determined, then PE depends solely upon which person has the dollar. As the dollar accelerates faster, what quantity decreases with the correct units? suppose the value exchanged per second does not change. then if suddenly more dollars are exchanged per second then value per dollar has decreased. thus value per dollar is inversely proportional as needed, but for units to work, the unit of value must be determined and judged for plausibility. 


costs must be shared.
a loan is where labor and assets (both people's time) are being used towards a project that does not generate value immediately. if the project is to have sinks it must have sources. those sources are sinks of those who would like to contribute. perhaps the reward for contribution could be first to be served. likely these contribution sources come from sinks with excess sources.

insurance on one side, subscription on the other.

trouble with money is that it can be kept, while a stream must be spent.
if a stream must not be spent immediately, it presents the same problem as money in that it can be kept, as one could send it back and forth between two owned nodes.

suppose streams must be 'sunk' simultaneously as they are received. suppose one has unneeded sources. The sources could further contribute to existing sinks, such that cost are shared for that sink more (eg more healthcare coverage), or the sources could contribute to a new sink. Directing out-streams is a matter of balancing premiums and subscriptions for personal needs, and contributing to development for potential value of the future.
suppose the system of streams exists. can the system coexist with the system of money? would it be problematic if streams and money were interchangeable? if they were, one could indirectly keep a stream. if streams and money are not interchangeable, in order to spend with streams one would have to earn with streams and vice versa. what if one would like to spend but is unable to earn, streams? what if one can only earn streams, but needs to spend where streams are unaccepted? the two must be interchangeable. the more things are transacted with streams, the more valuable streams become.

maybe keeping streams would not be problematic. if streams are used for premiums maybe the incentive can be to spend as soon as possible. maybe providers can be set up to only provide if certain continuity in premiums is met by the receiver.

any problems with interchangeable streams?
streams could be kept. they could be turned into money or physical assets, and not redirected an excess streams in order to share costs.
if the two are interchangeable, perhaps it is appropriate to view streams simply as money.

Let me split this topic into two sections.
First, what would the system look like if everything was in streams, money nonexistent?
streams are continuously directed. The idea is that each node provides something and has certain needs. For a person, needs might include, healthcare, food, housing, entertainment, education, transportation, resources to do work. Each of these needs is met by the existence of a different out-stream. Two nodes and the stream connecting them is all given the name 'relation'. Every relation places somewhere on the spectrum between premium model and subscription model. The premium model is where the need of the source is unpredictable. The sink receives premiums and like an insurance company, covers the costs when they arise. An example of a relation for the premium model is healthcare. In premium model is where the need of the source is predictable. The sink receives subscriptions payments and provides the services when needed by the source. An example of a relation for the subscription model is food. In fact, every need is unpredictable to some degree, and yet predictable to some degree.
A need does not consist of only a single relation. For example, when one needs food, there is a chain of relations. The first relation is from the receiver a food manager node that distributes the streams across many sector of food, eg breakfast. the breakfast node receives some part of the stream and will distribute the stream to where every the receiver chooses to eat breakfast that morning. The breakfast node could pay a continuous subscription to breakfast providers proportional to the number of people that eat there in the network of the breakfast node.

is it possible to make an autonomous insurance company for certain services where the receiver can be verified by the provider to indeed have a need? and the network can verify that the provider is indeed meeting needs that can be met?

Second, how can system of money be converted to a system of streams that would lead to the system described in section one?

currently, transactions in have such a discrete nature.

it seems the system of streams would work effectively. the necessity is that money really flows like streams, even if not continuously but high frequency discretely. the amount of input must be the amount of output. how can this be done discretely?
the direction of streams is determined by the communication of nodes. The communication of nodes is discrete. therefore the direction of streams is determined discretely.


I've realized the best way to do this whole thing is the following:
Use a cryptocurrency. it will have the side effect that people can keep money.
encourage subscription and premium based trading, saying it is what is needed for our future.
gradually move into a sharing economy where the currency becomes more of an accounting statistics tool than what actually carries value, because hopefully with cost sharing value will be distributed more equally.
along the way, supplant bureaucracy with open decisions, where systems are autonomous. with the technology that is coming, open systems with open decision making is just as important as cost sharing. otherwise, many people wouldn't be able to offer their help. how to distribute income to those working on a project? in the end system it doesn't matter what one contributes, cost is spread all around and everyone is covered regardless of the work they do. in short term, when cost is not equally spread, and currency has more value, some complex algorithm, decided by open decision making, will have to determine how income is distributed to contributors.
there are two fundamental themes that go hand in hand. subscription, and openness, because each helps spread the groth of the other. the currency will facilitate these.


how to do a cryptocurrency?
it must work where you take any collection of nodes and consider them one node. what if you send out a token that you would like to pay someone. after a certain time the payer and receiver will hear from the network if the payment went through.
the idea that any payment can be meshed with any other and the resulting sum of money can be divided into other payments. how to find duplicates? suppose two identical tokens were sent out. those tokens need to spread exponentially. meaning every node that receives a part of it should mesh it with other payments and then divide them out. if a payment is to remain a certain size, it means the mesh of payments must have size of sum of payments and then that sum must be divided out. when combining it must be that each payment is completely distinct and one does not contain any substance of another. it they do and they are combined and then divided, the payments must be seen invalid.
once the duplicate tokens spread exponentially they will meet somewhere. when they meet the node can send back, one of your sources has a problem. suppose somewhere is a way to backtrack it to the source. they payment is then invalid. if there is no alarm in a certain amount of time, the receiver of the payment can know decide that the payment is valid, and pass that payment onwards. could the receiver pass the payment onwards immediately, even if not yet known valid?
the system must be designed for lots of small payments that way it is friendly to micro-payments, subscriptions and premiums. what about the application of linear transformation? can a payment network be represented by a matrix, that is a linear map? since any group of nodes can be thought of as one (the whole network could just be one) it may be best to forget about nodes, and only about payments.. payments are coming from different places and going to different places. never think of one place as only that place as one place could be many plcae (one node could be many nodes). but you do know that places do not change. if a bad payment comes from some place, feel confident to track it back to that same place.


the tags should describe the essence, and are really the title of a post tags i think should be
separate from the content, that is the content should not need to reference any tags. tags should be
intentionally chosen to reflect the essense of theconcepts to help connect ideas, rather than chosen
to fully capture the concept at the cost of lost flexibility and overlap with other concepts.
given the separation of tags and conent, maybe tags can remain mutable while content is immutable.
if content is immutable, we definitely want chains of the content for signifying edits. 
if tags are mutable, they can be renamed, as well as added and removed from already published content. this could enable users to organize published content that is immutable. For example, a certain tag could automatically be attached to the head of every chain.
if the platform becomes distributed, each user would have their own choice of tags, and would share this choice when sharing the content with others. there could be concensus mechanisms for helping communities develop consistent tags, eg to decide whether a tag should be remained, and eg to group multiple tags together as signifying the same concept. 
to have chains, a post needs to be able to show (by means other than tags) if it extends something and and if so which post it extends. i suppose in a distributed environment this could be done by hashing. The tags would not be part of the hashing because they are not immutable. 
there should be a name for these posts. really they are 'thoughts' and 'ideas' but those names are too general. maybe 'mark' or 'notion', and I will use both below but I think the latter is better. 
to better support typing tags should be just lowercase letters with spaces, maybe approspophe's 


capabilities for the app:
create new mark
view a mark
	for now no need to edit tags on a mark (though they will be mutable in the future)
view all marks for a particular tag search
	i was going to support clicking on a tag to search it, but one may likely want to search multiple tags at once, and clicking on multiple tags at once doesn't seem to easy. either no clicking and all tag searching requiring typing every tag. or, clicking on a tag copies it to a special place so when you go to the search place they appear there and you can select them for search. i suppose they could form a queue so no deletion of tags in the search area is not necessary, but maybe such capability would be helpful.
	i suppose a search need not be too complex, even involving an OR or NOT, and AND may be enough.
	actually for now lets make search as simple as possible. maybe we don't even need AND to start and searching for a single tag is enough. in this case, just click on that tag. after this we can decide how to improve searching.
viewing all tags
	for now no need to rename a tag


so the possible states are: viewing a result list corresponding to a particular tag, viewing a particular mark, creating a mark. the first state can be defined by the searched tag, the second state can be defined by the id of the mark, and the third state can be defined by nothing. using these definitions we can implement a history. maybe the app could be slip into two parts, searching/viewing and creating. the history would only exist on the searching/viewing part (this part would later support tag mutation). instead of back and foward buttons, maybe a history could be implemented as a scrollable (or swipable) list that shows you past and future. the history would include 3 things: viewing all tags, searching tag <id>, and viewing mark <id>. the creator side of the app would just let you constantly modify a mark in the making, and publish it when you like. i suppose searching for a tag can only happen by clicking on it, whether it appears in another mark or on the list of all tags.

how about this: divide app into 3 parts. on left is for viewing all tags, which will later handle renaming of tags. in the middle is searching/viewing, where the view will later support tag modification for a mark. the middle will have an option to see the history which will go to a list of 'search on tag <id>' and 'view mark <id>'. clicking on any tag, in any part of the app, and thus searching that tag, will modify this history. clicking on a mark link will load that mark in the middle and also modify the history. on the right is where marks are created. maybe the left part can include an 'any' tag, enabling a search for all marks. each part can have a symbol, the left 'list', the middle a 'book', the right a 'pen'. 
maybe the app can be title 'notion ink' and the posts can be called 'marks' and the tags can be called 'notions'.

we should remember that tags are really to be treated carefully because they are the important idea that ties together ideas. in fact, a tag is basically a shortname of a concept itself. so maybe doing a complex search of tags (ie concepts) is not not necessary if we should be thinking at most about one concept at a time. mage we shouldn't use the word 'tag' but should use another name to reflect its purpose, like 'tie', but maybe we don't need to name them at all. 


now how should we style the creation interface? first consider mobile. on mobile i suppose the whole box can be one big input that can be clicked on and modified and when exited or 'submitted' it becomes rendered and maybe automatically saved to the cloud at that point. once rendered user needs means to return to editing mode. maybe clicking on it should be enough but scrolling wouldn't affect it, and i guess at the top it would say 'click to edit'. actually the input will need to be multi-line so at the bottom clicking 'return' shouldn't exit it but instead go to a new line. i suppose we'll need a menu like the notion.so app has. ours will pop up, idk where, when the user enters the input mode, and clicking on it will simply exit the input mode and also render it.

to make full use of mutable tags (meta tags) we'll switch the searching model to searching for all queries in the order of the tags listed on the list page. reording tags is possible by dragging, and clicking a tag (on the tag list and elsewhere) puts it at the top of the list and then performs the paginated search.

now for the web creation style interface. it could identify blocks by content separated by a blank line and render blocks unactive (without the cursor) and show unrendered in editing mode the active block (with the cursor). there could be a popup on the right of the active block to exit the block. re-entering would be done by clicking on a block. it would be preferable not to use texarea or content-editable but instead build a customizable and simple interface. copying text from the app then pasting in or outside the app can be done but text copied outside the app cannot be pasted, which I think is Ok. 


EventTarget
	() addEventListener
	() removeEventListener
	() dispatchEvent
	E click
	E keydown
	etc

EventTarget -> Node:
	RO childNodes
	RO firstChild
	RO lastChild
	RO parentNode (and parentElement)
	RO previousSibling
	RO nextSibling
	RW textContent
	()appendChild
	()removeChild
	()replaceChild
	()cloneNode
	()contains
	()hasChildNodes
	()insertBefore

EventTaget -> Node -> Document:
	RW body
	() createElement
	() createTextNode


EventTaget -> Node -> Element:
	RW classList
	RO children
	() scrollHeight
	() clientWidth
	() scrollTop
	() scrollLeft


EventTaget -> Node -> Element -> HTMLElement:
	RW draggable
	() clientHeight
	() clientWidth



worst case problems
SVP: shortest vector problem; search
SVP_{gamma}: approximate shortest vector problem: search
GapSVP_{gamma}: apprixmate shorted vector problem: decision
CVP: closest vector problem; search
GapCVP: decision
SIVP: shortest independent vectors problem; search
SIVP_{gamma}: approximate shortest independent vectors problem; search
BDD_{gamma}: bounded-distance decoding problem; search

two main average-case problems (reductions from the worst case)
SIS: short integers solution; search
LWE: learning with errors; search

SIS compresses input, LWE expands input
we are interested in SIS

SIS problem is parameterized by n, m, q, b.
n is security parameter
m is input length; usually m >> n
q is integer modulus; q = poly(n)
b is infinite norm bound; b << q

SIS can be made to a hash function
collision resistant
universal (shows how random it is dependent on the distribution of instances)
good randomness extractor (idk)


an n-dimensional lattice is a subset of R^n such that
	its an additive subgroup
	its discrete: every lattice point is isolated in a neighborhood
usually defined as all integer combinations of independent basis vectors

SVP: given a basis for a lattice, find a non-zero vector of minimum distance.
SVP_{gamma}: given a basis for a lattice, find a non-zero vector of norm with at most a gamma (a function of the lattice dimension) factor more than the minimum distance.
GapSVP_{gamma}: given a basis for a lattice, determine whether the minimum distance is less than or equal to 1, or greater than gamma (a function of the lattice dimension). When gamma is subexponential this is conjectured to be hard. 
SIVP_{gamma}: given a basis for a lattice, find a set of n linearly independent lattice vectors where each has norm at most a gamma (a function of n) factor more than the n'th successive minima. 
BDD_{gamma}: given a basis for a lattice and a target with the promise that the target is less than distance \lambda_1(L)/(2\gamma(n)). The larger gamma the easier the problem. 


lattice
lower bound, maximal, least-upper-bound, join, disjunction
upper bound, minimal, greatest-lower-bound, meet, conjunction


basis: set of linearly independent vectors
dimension: length of basis vectors
rank: size of basis vector set
full-rank: rank = dimension
If rank < dimension then we can just change the dimension to the subspace spanned by the rank. Thus we can always consider full-rank lattices.
If rank > dimension then the basis is not linearly independent

A unimodular matrix is a square integer matrix with determinant +-1. such a matrix \Z^{n*n} permutes \Z^{n}. Thus one can multiply the domain by such a matrix before multiply the lattice basis. By associativity, this is equivalent to multiplying the lattice basis by the unimodular matrix to obtain a new basis. Thus a basis is not unique. 

A fundamental domain is any region containing exactly one representative of each coset member of R^n/L. Most commonly used is the parallelepiped centered at the origin. 

dual lattice: all point in \R^n with integer valued inner product with all lattice vectors.
a basis for a dual lattice is any power of the the inverse of the original basis


a ^ log(b) = b * e ^ a 

a^b = e^{b log(a)}

g^n = h^m

m = n log_{h}(g)

predetermine a sequence of generators, and pregenerate powers of 2.
then make a circuit that intakes a bitstring and the number it should represent.
apply the bitstring to the generator powers, a linear combination, and also apply the bistring to the raw powers of two and compare the result to the claimed number.

p=2
1
2,4,3,1
3,4,2,1
4,1

1,2,3,0
2,0
3,2,1,0
0

p=3
1
2,4,1
3,2,6,4,5,1
4,2,1
5,4,6,2,3,1
6,1

6 * (1 - 1/2)(1 - 1/3) = 6 * 1/2 * 2/3 = 2

2p+1 is safe prime
multiplicative group has 2p elements
since ordere of any element must divide 2p, it must divide either 2 or p, or be 2p, and since both are primes, the order must be either 2 or p, or 2p.
the order of any subgroup must divide 2p, so again, subgroups must have order either 2 or p. but there is a unique subgroup for each such order, so this group is composed of one subgroup of order 2 and one of order p, and also the 2p group (in the case p=2, we just have a single subgroup) (in the case of p=3, we have subgroups order 3 and order 2).

2p * 2p = 4 * p^2 mod 2p+1

consider the subgroup of order p. this is our group of interest. our generators will be in this group. and our exponents can be in the range [1,p], which has the same size but not the same elements as our group of interest. 

we need a bijective efficiently computable mapping between them. suppose we have one. then we'd have our field as Zp, but then our field would not support many elements of the desired group, which is twice as large.

suppose we use the whole group (Z_{2p})^* with exponents in range [1, 2p] or [0,2p-1], which in both cases include all the elements in the field Z_{2p+1} which has 2p+1 elements, except 1 which is 0 or 2p respectively. So either 0 or 2p must be forbidden as an exponent, maybe we allow them both but justify the overlap not hurting security.

i'm realizing we may be better with elliptic curves, because i only know how to do exponentiation in the multiplicative group by dissassembling into bits, but now im realizing for a single element there are thousands of bits, rather than hundreds as for an elliptic key, and each of those will need their own keys. for an elliptic curve we don't need to exponentiate, only multiply and confirm inverses. but the problem with elliptic curves is efficiently hashing from a curve point to the field, as necessary in sumcheck. now i'm wondering if it would be better to use a method like QSP that does not involve sumcheck.

let me see if I can remember. we have some witness poly that encodes all the data and we have another dividor poly that is trivially constructed and is the trivial poly with a certain set of roots, those roots for which the data poly should have. then the prover computes the quotient poly h, commits to that and the data poly and the verifier queries both at a single random point, and also manually evaluates the the dividor poly (what i'll call the zero poly) at the same point, then multiplies to see that data(r) = h(r)*zero(r). Now to remember how the data poly looks like. I think its mulivariate. suppose we want to represent an arithmetic circuit. suppose we have a vector of the trace. then our constraints are of the form that inputs and outputs match as they should. there is a constraint for each gate. suppose we have a trace poly. using gates i suppose the constraint poly could be 3-variate, and we'd require it to have a root for every gate. or since there's two gate types there could be only linear gates so 4-variate. then evalutation at random point would mean reducing evaluation from 4 points to one. now we'd like our trace polys to be multilinear. so it may make more sense to use 4*n variate polys where log2(n) is length of trace vector so n is degree of the trace poly. 
but i think the problem with multivariate is its hard to divide.
and the problem with univaraite is it doesn't reduce.
before trying to solve either of these problems, first see if we can use multivarate for trace while univariate for rest.


one thing we know is multiplying out multilinear polys creates exponential terms which is infeasible to handle. 


r^{a*2^2 + b*2^1 + c*2^0} = r^{a*4} * r^{b*2} * r^{c*1} = (r^4)^a * (r^2)^b * (r^1)^c = (r^a)^4 * (r^b)^2 * (r^c)^1

suppose we have univerate poly of degree d, that is d+1 terms which suppose is a power of 2, ie 2^v. suppose we wish to evaluate at r. suppose we replace term j from 0 to d with
\prod_{i=0 to v-1} (r^{ith bit for j})^{2^i}


simply set the ith variable from 0 to v-1 to be r^(2^i). so in order our variables are x0=r^1, x1=r^2, x2=r^4, x3=r^8, ...

a 1
b 2
c 4
ab 1 + 2 = 3
ac 1 + 4 = 5
bc 2 + 4 = 6
abc 1 + 2 + 4 = 7

so now when asked to evaluate a poly at r, we can instead evaluate a multilinear poly. 

suppose we're reducing evalutation from two points r,s to one.
xi = r^{2^i}
yi = s^{2^i}
zi = xi*t + yi*(1 - t) = r^{2^i}*t + s^{2^i}*(1 - t)


so my hope is that verification consists of evaluate a basic format of a constraint poly, which involes evaluating the trace poly together with an in-out poly, and the zero poly and the divisor poly. the trace and divisor are reduced to single points, then they can be reduced to one poly. 

but we still have the trouble of a one way function from the elements of the field to the field. if we put it through an elliptic curve, we need a uniform mapping from the curve to the field. 
https://eprint.iacr.org/2014/595.pdf uses subset sum over the additive group, which will require that our input is in bitstring format or only log(field size) length. 


H(a) + H(b) = H(a + b)


H(a + H(b + c)) + H(a' + H(b' + c')) = H(a + H(b + c) + a' + H(b' + c')) = H((a + a') + H((b + b') + (c + c')))


H(H(a) + H(b))




m log(2d+1) > m log(2d) > log(p)

m > log(p/2d)

lets have (2d + 1)^m/2 = p so m/2 log(2d + 1) = log(p) so m = 2*log(p)/log(2d+1)

17^6

so the bijective map could be take the value in range p, and find its representation in base 2d+1, and to represent the number one should need log(p)/log(2d+1) digits. compute each such digit then offset it around 0. this is a bijective mapping.
(2d+1)^{log(p)/log(2d+1)} = e^{log(p)/log(2d+1) * log(2d+1)} = p^1 = p
to verify, one is given the final reps, un-offsets them, multiplies them by radixes, and sums. 



H(a,b) + H(a',b') = \sum ai*zi + \sum bj*zj + \sum a'i*zi + \sum b'j*zj = \sum (ai + a'i)zi + \sum (bj + b'j)zj = H(a + a', b + b')

r1 = H(x1 . ^x2)
r2 = H(r1 . x2^)
s1 = H(y1 . ^y2)
s2 = H(s1 . y2^)

r2 + s2 = H(r1 . x2^) + H(s1 . y2^) = 



we have contraint poly, in and out polys, trace poly, zero poly, and divisor poly. 


(z - a)(z - a^2)

f(r) = z1
g(r) = z2

f(t) = 

p = p1, p2
p1*r + p2

use powers of a generator to make other generators for commitment. then use FFT to 

g^a h^b = g^a' h^b'

g^{a-a'} h^{b-b'} = 1

g^{a} (g^2)^{b} = g^{a'} (g^2)^{b'}
g^{a-a'} (g^2)^{b - b'} = 1
g^{a - a'} g^{2b - 2b'} = 1
g^{a - a' + 2b - 2b'} = 1
g^{}
g^a g^{2b} = g^{a + 2b}

so can't do that. generators must be chosen privately by verifier. this acutally means a setup that I wasn't aware of. 

ri*(Gi*(1 - t) + Hi*t)
r1*G1 + r2*G2
r1*H1 + r2*H2

r1*(G1*(1 - t) + H1*t) + r2*(G2*(1 - t) + H2*t)
=
t*((r1*H1 + r2*H2) - (r1*G1 + r2*G2)) + (r1*G1 + r2*G2)

cant do this either. 

have a problem. i think discrete log must be implemented in a prime size group. 

use multiplicative group with generator for whole group, but make sure blinder is trustworthy published and not a residue. 

could also do discrete log over binary field


we'd actually like our native field as small as possible for efficiency. the security without composition only relies on the poly lemma. 80 bits or so is fine. but discrete log on elliptic curve, at least the one by NIST, has field size no less than 192 bits. if 192 is not too large for the native field, we can natively support curve arithmetic. but if its signficantly more efficient to do like 80 bit native field, then we do discrete log using long arithmetic. would it really be possible to natively support curve arithmetic without doing any spreading (ie bit separation and reconstruction)? 
the size of the field for a curve should be twice the security parameter. 

but if our construction requires owfs and we use the subset sum, then our input must be deconstructed. 
do we need owfs at all? if we use sumcheck yes. if we reduce the same poly at two points to one then yes. otherwise no. oh well i guess applications will crucially rely on them with merkle trees.

but consider reducing a poly to one point without owf. the special case is we are evaluating at rg^i for i=0,1,2,etc. consider just 0 and 1 for now. we could do dot product of powers of g with the poly vector then we have transformed the problem to evaluating two polys at the same point r. notice r is particular to each proof, but g is not so the vector of g powers can be made before, maybe even speeding up proof of dot product which I don't know yet. suppose we have the vector of powers of g. then to get the vector for higher powers of g, we just add the vector to itself, or exponentiate it.
0,1,2,3,4
0,2,4,6,8
0,3,6,9,12
g^2^0,g^2^1,g^2^2,g^2^3,g^2^4
g^3^0,g^3^1,g^3^2,g^3^3,g^3^4
maybe we could reduce even further by only requiring proofs to keep track of the poly at the point r and leaving the poly at rg^i for i>0 as a derivative. but then the proof can't evalute the formula so some reduction of two formulas to one would be needed. the formulas are already viewed as polys having an unknown and this means reducing satisfiability for two unknown to one, which is the same as reducing eval at two points to one. but we may be able to take advantage of formula structure to avoid the standard procedure which requires owf. 
if we reduce two points to one it takes this form: r*t + r*g*(1-t) = r(t + g(1-t))



since dlp is secure in the whole multiplicative group we could revert to using that as the exponential and the owf. but the field size is still costly. 



0, 				 a1*x^2, b2*x^2, 0, 				<a1,b2>*x^2
a1, 			 a2, 		 b1, 		 b2, 				<a,b>
a2*x^{-2}, 0, 		 0,  		 b1*x^{-2}, <a2,b1>*x^{-2}
a1 + a2*x^{-2}, a1*x^2 + a2, b2*x^2 + b1, b2 + b1*x^{-2}, <a1,b2>*x^2 + <a,b> + <a2,b1>*x^{-2}

a' = a1*x + a2*x^{-1}
b' = b1*x^{-1} + b2*x

a'*x^{-1}, a'^x, b'*x, b'*x^{-1}, <a',b'> 
	= <a1*x + a2*x^{-1}, b1*x^{-1} + b2*x> 
	= <a1*x,b1*x^{-1}> + <a1*x,b2*x> + <a2*x^{-1},b1*x^{-1}> + <a2*x^{-1},b2*x>
	= <a1,b1> + <a1,b2>*x^2 + <a2,b1>*x^{-2} + <a2,b2>
	= <a1,b2>*x^2 + <a,b> + <a2,b1>*x^{-2}


multilinear interpolation on binary intputs is easy


now attention should go towards optimizing circuit size and prover time. we could define the minimal problem of the identity function where one receives a bit and returns it, or some other simple problem. then we optimize for the smallest circuit that can verify this computation together with verifying other proofs. so we don't need to distinguish between prover and verifier times, but just circuit size. we should probably set the minimal problem as a variable with its inputs, outputs, wiring, and trace. 

now to optimize we'll need to decide what computations verifying circuits are dominated by. this means choosing our model of proof and if requiring owfs, which ones. actually we assume applications are packed with owfs. so we optimize heavily for that anyway. we need to pick one. we also need to optimize for the discrete log and figure out how that will be performed. 

whether we do long arithmetic or native arithmetic for the discrete log depends on how large our field has to be anyway. that in turn is determined by the owf we choose. so first choose an owf. the only other restriction on field size is the poly lemma, which doesn't require a large field size. only discrete log and hash have externel security requirement, so depending on what we choose we determine minimal field size. 
if we do long arithmetic, characteristic 2 may be helpful. 



m = 2nlog(q)


nlog(q) < 2nlog(q) < q/{2n^4}

4nlog(q) = q/n^4


for subset sum i realize the matrix may need to be 


the subset sum i'd like is having 2n random elements each of size 2^n mapping 2n bits to an element of size 2^n. 
oh wait, subset SUM in characteristic 2 is then xor which is feasible. what about subset PRODUCT? 
another problem with characteristic 2 is the discrete log group will not have prime order. 


when we homomorphically add vectors we need the arithmetic to stay in the field. this means there should be f possible exponents where f is the size of our field. oh, this isn't a problem because most safe groups for discrete log, eg one of order (p - 1)/2 is prime when p is a safe prime. so we need to match the discrete group size itself (not its host group) to the size of our native field. does this elliminate possibilities?
well apart from this i realize discrete log requires integers as exponents, actually natural numbers, so characteristic 2 isn't an option unless we somehow translate it. a field modulo a prime then seems necessary for the native field. well we need the homomorphic property to be xor of the bits. if we translated the bits to integers the arithmetic wouldn't work, and if we kept them as bits and committed each one with a discrete log then we would need the discrete log group to have order 2 which of course is not a secure group. so DLP woulnd't work for characteristic 2, though if we could find an xor homomorphic commitment it would work but i don't know of one. 
we can't use the multiplicative group modulo p because that has order p-1 instead of p. so if we use a multiplicative group it must be a prime subgroup of some larger multiplicative group.
now for elliptic curves, the group of the curve, whether the whole curve or a subgroup must have order of the field. seems unlikely to find one where the subgroup size is the same as the field size, though in NIST they are multiplicatively close but we need exact matching. and one where the whole curve is the group and is the same as the field size is vunerable to smart's attack. so we'd work over some curve in some non-native field.
so it seems we will work in a non-native field, though with an elliptic curve its possible that field could be even smaller. unless we can get some structure between native field and that of log, we'll need to do long arithmetic, so we want to minimize discrete log computations. we simply want to receive commitments to polys as non-native elements, and multiply (or add for elliptic curves) them in the non-native field iteratively for calculating powers and non-iteratively for just combining two commitments. that's it, no more tricks with DLP. 

but regarding characteristic 2 we have Yao's XOR lemma. 
F(x1,...,xn) = XOR_i f(xi)
F(x1,...,xn) xor F(y1,...,yn) = XOR_i f(xi) xor XOR_i f(yi) = XOR_i f(xi) xor f(yi) ?=? XOR_i f(xi xor yi) = F(x1 xor y1,...,xn xor yn)
so i think if we find a weak owf f that is homomorphic we have a strong owf F that is homomorphic.
oh actually the regular lemma is for a f being a predicate, meaning the output is a single bit. actually we'd like the output of f to be the same size as input so it could be a permutation. 


need field size to match dlp group size. thus dlp group size must be power of a prime. 
we need to optimize for the collision resistent hash functions for merkle trees. these must have output compatible as input, and ideally map 2 to 1, ie compression factor 1/2. if we use elliptic curves we do discrete log with two or more exponentiations. so input size is log(group_size), output size is 2*log(field_size). and the two must be equal so group_size = field_size^2, and this seems unlikely. I suppose the problem could be mitigated by having the input size be 2*log(field_size) and its committed by a 2 combination of generators. then we just need group_size > field_size, which I think is possible. and actually if done over a prime field, we could avoid all bit spreading, as the output would be immediately interpretable as exponents for input. need to analyze group vs field size to prevent collisions. 

for discrete log we can pre-compute powers of the bases for efficient hashing. but with the vector commitment the base to multiply will depend on the vector. sumcheck will require hashing, as will the application in general, but sumcheck requires uniform output distribution. if we use discrete log for hashing, probably most efficient method is to use elliptic curve over the native field. remember the field is then a curve over another field. the only cost maybe unnecessary is the large native field necessary for the elliptic curve. 

consider the commitment. its curve size is the size of the native field. there is a bijection easily computable from the field to the curve by exponentiating a base point. but the prover must submit the curve point because the prover can't compute the bijection the other way. 

it may be costly to use the fiat transform. we need an efficient collision resistant with uniform output. what if we add the two points of output of a curve point. probably still not uniform. 
if its too hard we have to go without sumcheck and instead poly div.  


prover and verifier nodes could engage in own interactive proof for prover to convince verifier that evaluated at random point gives certain result. this reduces bandwidth and avoids sending whole witness, even if in zk. 
the prover should not have to keep a giant witness around waiting for the time to prove. so when receiving a proof a prover verifies it, and waits until using that proof to generate his own proof and interact with another party at which point the witness is generated. so after verifying one just keeps the commitment, having been convinced its valid, and will use it in its own proof. 


suppose we find a curve with group order same as field size, requiring then that cofactor > 1 to avoid smarts attack. this means we can map from the field easily to the group via a generator. maybe this would satisfy all compatibility issues. the native field would be that of the curve. all discrete log operations would be performed on the discrete log group, having same size as the field. this includes the commitment, and hash functions. the fiat problem still unsolved without a uniform mapping from the curve to the field. 
due to Hasse's bound, this is not possible. prove this in the paper and later show how its the (missing) proof for the 1-cycle in the paper on elliptic curve cycles. 


suppose we have a cycle of elliptic curves. call the two fields Fp and Fq. So #E(Fp) = q, #E(Fq) = p. Cofactors are not possible. Suppose we have a proof over Fp, and we commit the coefficients in E(Fq) because this admits p distinct powers. Now its the verifiers turn, who is implemented in Fq. Thus the verifier can accept the commitment. The verifier can have a bit representation of p, and it can choose a random bit string of this length and multiply the commitment as such, then add the commitment to another. 
now what about hash functions for application. suppose we're in Fp. we have a hash output from the q-order curve consisting of two elements of Fp. Then these become exponents to a curve of order p. So every layer of the merkle tree shifts circuits. 
so a proof in Fq starts when verfying proofs made in Fp and when its job is to compute discrete log hashes when the base is in q and the exponents as bit vectors of log(p) size which is the input. We're gonna want exponents in bit form anyway and have our bases hard coded.
now p and q should both be large enough for security, but they don't need to be primes, only powers of primes. 

still have problem of fiat transform. if we use the discrete log we need a mapping from two elements of Fq which form a point on a curve of order p to Fp. Note that for every x there are exactly two y (sometimes maybe one). now twice the number of x points + 1 is then the whole curve size which is size p. (actually not sure about that cuz then every curve would have order 2n+1). 
note that dlp already does compression. so maybe to commit p values, we should do them in a group of half the size of p. then the output of the curve would be two elements of size p/2, summing lengths to p. but is this secure? consider committing a single value. so suppose g has order p/2, an integer. suppose we commit n, an integer between 0 and p-1, g^n. suppose we find m in 0 and p-1 such that g^n=g^m. then g^{n-m} = 1 so one just needs to find n and m that subtract to p/2. 
we have the ability to give a lexicographic order to the curve points by comparing two curve points. but we need a way to immediately calculate the index. 
our best option may be the subset sum problem with input the log rep of the x coordinate together with a bit for the y coordinate, thus basically no compression. this works because even though we ignore most of the data of y, we still have a surjective mapping from the points to bitstrings of log the curve order. so DLP does the compression, and subset sum does the randomization. a verifier in Q will have to verify that a bitstring corresponds to a set of Fq elements. but remember, we want to avoid any bispreading 
even if we just add or multiply x and y (which is arguably appropriate, as y reshuffles x) we still end up with an element of q when we need one of p. as far as proving, say, addition of x and y, we need to show there are few other curve elements that will add the same, and that they have a random distribution, such that even if one knows what they are, one must solve the discrete log to access them. 
y^2 = x^3 + ax + b
y'^2 = x'^3 + ax' + b
f(x,y) = f(x',y')
we want to choose f such that f(x,y) = f(x',y') implies the two form the same point but we want f to have a simple form.
well consider f(x,y) = x + y
suppose we have point X,Y. Then consider Z = X + Y. Then consider the line through the modular table of the field that passes through all points x,y such that x + y = Z. how many of these points will be on the curve? note this line will always be diagonal 45 degrees from top left to bottom right but it will overflow as needed. 
x + y = x' + y' = c
(c - x)^2 = c^2 - 2cx + x^2 = x^3 + ax + b => x^3 - x^2 + (a + 2c)x + (b - c^2) = 0
(c - x')^2 = c^2 - 2cx' + x'^2 = x'^3 + ax' + b => x'^3 - x'^2 + (a + 2c)x' + (b - c^2) = 0
implies x and x' are roots of cubic poly p(x) = x^3 - x^2 + (a + 2c)x + (b - c^2)
which can have at most 3 roots. so for any c we have a poly p, and at most 2 additional pairs of x and y can satisfy it. That is few enough. we now need to show they have no structured relationship between them so knowing access to one doesn't admit access to others. we'd like a reduction from finding a collision to the discrete log for another arbitrary value.
first, as far as uniformity, we say there are at most 3 collissions even possible. so there are p points on the field Fq and for each Fq there's at most 3 points, so the image is at least p/3 elements of Fq. 
another possibility is just selecting the x value because then there are at most 2 collisions. then we need to make sure the inverse is easy to access. But this seems easy. If g^n = c then g^{-n} = c^{-1} so just take the inverse of the exponent. 

regarding sumcheck, prover of circuit in p sends a commitments to coefficients. verifier in q accepts those commitments, generate random p values from them, and then must add them and first multiply some by powers of that random p value. 

we can have as much verificaiton off thread as possible as long as the implication is collapsable, like tail associative. we'll need different parts of verification, but maybe they can all take the form of a poly commitment evaluated at a random point having a certain value (whether multi or univariate). these can always be collapsed. two different points for the same poly can be reduced to one point on that poly. two polys on the same point can be reduced to one poly on that point. thuse any set of polys at any points can be reduces to one poly at one point. but what is the merging process? for two polys one point, suppose we have the polys over p. given the commitments in q, we need to generate a random point in p then multiply one commitment by that, then add the commitments. this would probably be done in q arithmetic with p represented in binary when is then used to multiply a (un-preset) commitment (such multiplication requires a bit representation of the multiplier anyway I think). the output is a new commitment to a p poly expressed in q. for the two points one poly we would treat the polys as multilinear over field Fp. commitments given in Fq. points to evaluate at given as binary in p. small poly t given 


what if we make the native language of the circle the additive group of the elliptic curve? so its only a group, no distinction of addition and multiplication. suppose the circuit is over the curve group order p (which is over Fq). values are curve points in q, but for any point generated one should keep track of the base and the index, a scalar in Fp. 

suppose we consider an elliptic curve as a vector space over a finite field. then we can have linear operators. for a prime order curve, the dimension of the space is 1 because scaling any point spans all other points. then any linear map is just a single scalar. 
could think of elliptic curve as function from p to q assuming we add outputs. we could then compose dual or cyclic functions. then we could have a function space. 

if we had a curve for a native language we'd need a way to represent the points as a poly. if we keep track of each point's index, we could just do polys over the field. so if curve has order p, its over field Fq so we commit indices in Fq (and thus curve points) via the p order curve. so the circuit can accept commitments to its own type of values and alternating proofs may not be necessary. 
Now to what degree could just an additive circuit merge conditions? For two polys one point, it accepts two inputs but it appears it must have knolwedge of the field to do multiplication.

seems such additive circuits would have much restriction anyway and performing application ops other than hashes via curve computations could be costly.


s_j(0) + s_j(1) = s_{j+1}(r_{j+1})
s_{j-1}(0) + s_{j-1}(1) = s_j(r_j)

a*s_j(0) + a*s_j(1) = a*s_{j+1}(r_{j+1})
b*s_{j-1}(0) + b*s_{j-1}(1) = b*s_j(r_j)

(a*s_j + b*s_{j-1})(0) + (a*s_j + b*s_{j-1})(1) = a*s_{j+1}(r_{j+1}) + b*s_j(r_j)

so take all polys s_i and multiply each by random scalar then add them. then evaluate result at 0 and 1, sum and compare to right side which doesn't have such a convenient merging. 


for poly, use subgroups for small domains


if poly constraint method, verifier receives poly commitments. the verifier sends back random values for each. prover commits to random linear combination of them over the zero polys. verifier then evalutes both sides of equation at random point derived from commitment of h.

reducing two poitns to one. poly in p, commitments in q. calculate evaluation points in p, receive claims in p. receive t poly in p. evaluate t and 0 and compare with one claim, and t at 1 and compare with the other claim. hash t and get random p and evaluate t at that p and get new claim for poly at that p. 
suppose we do this in the context of p. receive commitment as bits. evaluation points and claims in p, as well as t. poly composed with t is given without commitment as p values. its evaluated at 0 and 1 and compared with claims all in p. then p values that make up t must be hashed to a new random p value (idk how). t evaluated at this random p value and new claim is generated about the original commitment. notice unlike for the case of two polys one point, the commitment doesn't even need to be seen. 


if we perform them we need to perform operations like reducing two to one in the native field.
suppose we have an exponentiation map as like a one way hash function with compression possible, from each field to the other. this assumes we have found a way to safely compress the two output components to one field element. this exponentiation map must be performed in the field of the output. 
another one way function, but without compression, that could transfer between the fields is the subset sum. this would also best be performed in the output field. 
so our outputs will be generated in their native field, while inputs will be expressed as bitstrings in the other field. 
consider reducing two points on one poly. suppose the field is p. almost everything should be done in field p but upon receiving the intermediate poly its coefficients must be hashed into a field element for p, which means a round trip through q. 
i'd like to consider the possibility of avoiding round trips and only single trips by having a proof in one field produce a random value for another proof in the other field. for each poly, the bit forms would be sent to one poly for hashing, while the elements would be sent to the other field for evaluation. or maybe they'd be partially commited first. the trouble is whatever way its done there must be redundancy of data between the polys, so there must be a consistency check between witnesses, which are in different fields. in the case of sumcheck, maybe the top statement doesnt just refer to data in its own witness, but can refer to values in the witness for the other and if using constants not variables for this referencing it should not matter that the other witness poly is in another field. i think the interdependence wouldn't need to affect soundness because we'd require that both proofs are valid, so manipulting one will hurt the other, and manipulating both for cancellation of harm maybe we can show is infeasble. but how to resolve the interdependence at the end if both witnesses rely on each other? 


one worry is we won't be able to efficiently do curve multiplication. scalar is input as bit vector. suppose all powers of 2 of generator is preset as constant. then for each 1 in the bitstring we want to add those generators. we'll multiply each power by corresponding bit, using some reprentation for the identity. my worry, which I suppose isn't too bad is we then perform curve addition for all those elements (when really we could ignore all the identities). 
note that doubly a point is actually a different calculation that adding two different points.


maybe we can take a commitment of p as elements of q and in the field q use the two output elements of q to produce a random element of p expressed as a bitstring. then no sharing is necessary between fields, and this bitstring could immediately be used to exponentiate the commitment. but this would actually only partly some only some cases. for two polys to one it would allow for efficient exponentiation. but then recalculating claims would need to be done with long arithmetic. I still think the interdependence is better. 

to commit, be in base field. to manipulate committed values, be in base field. 

what if have a hash function, however complex, with the property that the statement
H(a) = b and H(a') = b'
can be efficiently reduced to one like H(a + a') = b + b'. this means the implication goes the opposite way as a usual homomorphism. then we may not need to perform any of the hashes inside the circuit, just like we don't perform poly evaluation in the circuit. instead we only include the claims and we merge them as we go. does DLP have the property desired?
suppose the claims are made aG = A, bG = B and at least one is false
then a random r is chosen, and we infer the claim (a*r + b)G = r(aG) + bG = rA + B
if only one claim is wrong, eg aG = A' != A then (a*r + b)G = r(aG) + bG = rA' + B !== rA + B
suppose both claims are wrong, aG = A' !== A and bG = B' !== B
then (a*r + b)G = rA + B
=> r(aG) + bG = rA + B
=> rA' + B' = rA + B
=> r(A' - A) + (B' - B) = ID
since r is determined after A,A',B,B' this only holds with small probability.
however, using this still requires a single computing rA + B which includes a multiplication and can be considered a crypto operation (while a*r + b is not considered crypto) so we can't fully avoid crypto operations. 
this is similar to the poly merging reduction but we are not given the commitments. but now we have the new cost of generating a random r. we want to avoid this. can be merge this too? suppose the base is Q and the exponents are in Fp. Given the claims, in p, we need a random p value. this brings us back to the original problem.

a1*Q1 + ... + an*Qn = A, b1*Q1 + ... + bn*Qn = B. then choose r, and have (a1*r + b1)*Q1 + ... + (an*r + bn)*Qn = r*A + B
but we choose r to be based upon both A and B, which belong to a different field than r. Disregarding how the interdependence works, suppose we switch to the other field and we end up with something like r*Q_{n+1} = R. then we leave with ((avec*r + bvec) concat r)*Qvec = r*A + B + R. 

The basic idea is to merge n hashes we need a n-1 hashes to generate n-1 random numbers to multiply by the n-1 of the n original claims to induce enough randomness for a new valid claim. The costs cancel out, because n merging to one, with the addition of another n-1 hashes reduces n to (n-1)+1 = n. Suppose we start with 2 hashes but they are multivalued of length n, so they already have compression. We take the 2 commits and generate a random number via an additional hash, then reduce the whole thing to one multivalued hash of length n+1. The hope is that the the additional claim frees us from having to do any hashing ourselves. Suppose we work in the same field (rather than complementary fields). 

start with
a1*Q1 + ... + an*Qn = A, b1*Q1 + ... + bn*Qn = B
c1*P1 + ... + cn*Pn = C, d1*P1 + ... + dn*Pn = D

then generate random values as
Rq = C[1]*Q_{n+1} + C[2]*Q_{n+2} + D[1]*Q_{n+3} + D[2]*Q_{n+4}
Rp = A[1]*P_{n+1} + A[2]*P_{n+2} + B[1]*P_{n+3} + B[2]*P_{n+4}

then merge them as
(Rp[1]*a1 + Rp[2]*b1)*Q1 + ... + (Rp[1]*an + Rp[2]*bn)*Qn
= Rp[1]*A + Rp[2]*B
(Rq[1]*c1 + Rq[2]*d1)*P1 + ... + (Rq[1]*cn + Rq[2]*dn)*Pn
= Rq[1]*C + Rq[2]*D

finally also merge with new claim about random values
Nq =
(Rp[1]*a1 + Rp[2]*b1)*Q1 + ... + (Rp[1]*an + Rp[2]*bn)*Qn
+ C[1]*Q_{n+1} + C[2]*Q_{n+2} + D[1]*Q_{n+3} + D[2]*Q_{n+4}
= Rp[1]*C + Rp[2]*D + Rq
Np =
(Rq[1]*c1 + Rq[2]*d1)*P1 + ... + (Rq[1]*cn + Rq[2]*dn)*Pn
+ A[1]*P_{n+1} + A[2]*P_{n+2} + B[1]*P_{n+3} + B[2]*P_{n+4}
= Rq[1]*A + Rq[2]*B + Rp

so verifier is given the vectors
[a1,...,an], [b1,...,bn], [c1,...,cn], [d1,...,dn] along with A,B,C,D asserting the initial claim.
then given Rq, Rp along with their claimed form with respect to C,D and A,B.
verifier can then calculate new claims left side as
	[...(Rp[1]*ai + Rp[2]*bi)..., C[1], C[2], D[1], D[2]] and [...(Rq[1]*ci + Rq[2]*di)..., A[1], A[2], B[1], B[2]]
verifier also calculates the new claims right side as
	Rp[1]*C + Rp[2]*D + Rq and Rq[1]*A + Rq[2]*B + Rp
so now we have two new claims each of size 4 more than the previous. we reduced 4 claims to 2. 
now think about which computations are performed in which field
consider circuit in Fp. it receives [a1,...,an], [b1,...,bn], C, D, Rp. With these values it can calculate the new claims left side. But now I realize calculating the right side (of the other claim) requires two exponentiations, thus not reducing the cost from just calculating the random values directly.

suppose we calculate each hash function, regardless the compression, at unit cost. Is there any way to merge them and reduce cost in this case?
H(a1,...,an) = a
H(b1,...,bn) = b
H(r*a1 + b1,...,r*an + bn) = r*a + b
H(a,b) = r
on first thought it seems this is not possible. 

notice that in vector commitment opening, the prover must reproduce the outputs for each input, before multiplying them all together to get the commitment value. however, given claimed outputs one cannot bind them to the commitment. 

continuing the above which seems impossible, what we need to do is extend further making use of compression in another way
reduce H(ai) = a, H(bi) = b, H(ci) = c, H(di) = d, etc
to H(a,b,c,d) = r, H(r^3*ai + r^2*bi + r*ci + di) = r^3*a + r^2*b + r*c + d
if we have two polys, univariate, and we evaluate both at the same random point, they will most likely evaluate differently. 
so breaking this without break the DLP requires that r^3*ai' + r^2*bi' + r*ci' + di' = r^3*ai + r^2*bi + r*ci + di which means two polys at a random point coincide for all i.
so I am exploring this in the case that the verifier knows all these values, but is just not willing to compute H. Note this works even if the initial sequences are not of the same length as the shorter ones can be padded with 0s. the reduction reduces from any number of arbitrarily long hashes to two hashes, where the first is of length the number of hashes and the second is of length the longest hash. 
could it also be used when the verifier does not know the initial sequences and only has the commitments? Verifier could calculate right side of second output, but would need to hold on to all of first output. so it doesn't fully translate where output state is analogous to input state. but it could be used to reduce evaluation of multiple polys at the same point to one poly at the same point. what about reducing evaluation of one poly at two points to the poly at just one point? I think this also works.

now i remember, the above was an idealized version, where output and input were both same field of scalars. but now we have to adjust to our case, where input is scalar in one field, while output is elliptic curve point in another field. actually this prevents us from doing what we planned, because the elliptic points may not be able to function as coefficients of a poly. the only possibility is that we use complementary fields and justify reducing elliptic curve outputs to one field element. otherwise we would need to find an H that is additively homomorphic, allows for compression, and operates over the same field, which need to be prime. H(a + b) = H(a) + H(b). 
but from below we see reducing output components to one (eg using addition) won't work homomorphically
H(ai) = (a1 + a2)
H(bi) = (b1 + b2)
H(ai + bi) = (a .+ b)1 + (a .+ b)2 != (a1 + b1) + (a2 + b2)
because (a .+ b)1 != a1 + b1, that is, addition is not just component wise.

so it seems the verifier needs to perform the exponentiations, so reduction may no longer be advantageous. so we return to how to efficiently compute them with interaction between the fields. just computing r values (with respect to a standard basis) seems more convenient than reducing (then performing exponentiation with respect to a non-standard basis). 

one possibility is to have a homomorphic hash function from Fp to Fp^2 with output operation point addition, by composing the two elliptic curves in complementary fields, with the cost that each DLP requires hashing twice. but evern this wouldn't prevent the verifier from needing to compute logs. also its probably not homomorphic cuz it goes through two different fields, but I'm not positive. 

if we implement with our current best option which is communicating between the circuits and performing hashing in them, we will be doing a lot of hashing, and for every exponentiation we'll need to do log(F) point additions (assuming hardcoded powers) which is like 200 of them, but at least each one only require elements and not bit spreading. we'd assume each addition is of two different points, and indeed this should be the case for exponentiation, but when multivalue hashing its an assumption and if two are the same (with negligable probability) we can have an escape hatch for the prover to generate a different proof by some free variable. (but if it happens with probably less than someone cheating we could maybe just ignore it, and this would mean negligible completness). 

review subset sum. it is a suitable owf from one field to the other, performed in the field of the output. but it is not homomorphic, and doesn't compress. it could be a final conversion from an elliptic curve output in one field to an element in the opposite field, closing the loop. it could take the binary version of the x value and a single bit corresponding to the lexicographic look of y. the other option for conversion to a single element from a curve point is adding the components, but this does not close the loop. 
if we could get the subset sum to compress enough we might be able to avoid the dlp for generating random elements and just (maybe recursively) use the subset sum without need to transfer between fields. 

we may be able to remove the need for fiat shamir using a setup where a random point of evaluation is encoded in the exponents and hidden from all. for sumcheck, suppose for a multilinear poly a random point is chosen and all possible terms are encoded in exponents from a single base. prover sends coefs to each poly. verifier can evaluate at 0 and 1 and sum results, but evaluating at the random point I realize requires like a bilinear map. however, maybe the evaluation with bilinear maps could happen offthread. 

what if use QAPs but without pairings. proof is commitment to the divisor poly and the transcript. verifying the proof means evaluating at a random evaluation of the encoder polys. given two proofs evaluation happens at different points. can we reduce to one point efficiently? suppose we treat all encode polys as multilinear. remember we can evaluate a univariate at any point by evaluating multilinear at particular corresponding point. so all encoder polys are to be evaluated at same multilinear point for each proof. verifier creates line between this points. then prover sends linear (or higher if reducing more than 2 proofs at the same time) polys for all each encode poly, for each proof. these are too many for verifier to handle but for now suppose its ok. verifier evaluates at 0 and 1 checking each series corresponds to set vector of random points that upon evaluation of transcript satisfies proof (again too much for V to do). then v chooses random point on line, generates new random point and how is left with condition that both proofs are valid at that same point (whereas previously they were on different points). 

maybe we could have a single constraint for each value in the transcript, in particular the computation that yields its value. this would leave with a STARK type construction. 

what if encrypt evaluation point, prover computes encrypted evaluation at that point. then given two polys and two claims verifier can just add the polys and add the claims to get a new claim (probably no need for random exponentiation because evaluation point is unknown). addition of curve points is thus still required but we have relieved ourselves of the need to generate a random element. i think offthread proofs are still doable in zk using the log dot product argument. this doesn't allow for multiplications without pairings. 

really all we need for fiat shamir for sumcheck is a function that take coefficients of a low degree poly and maps to a single element such that it is infeasible to find a pre-image that maps to one of its roots. at least I think we can reduce security to this. 


in coefficient form, there are (p-1)p^d polys of degree d. the p-1 is for non-zero leading coefficient. the p^d is for the ordered d coefs for other d monomials. in root form, there are (p-1)((p + d - 1) choose d) polys of degree d. 
(p + d - 1) choose d = (p - 1 + d)! / ( d! (p - 1)!) = (\prod_{i, 0, d-1} (p - 1 + d - i)) / d! = (\prod_{i, 1, d} (p + d - i)) / d!
no finite field is algebraically closed, so there are always more polys in coef form than in root form, and using the calculations above we can calculate the ratio. prover will always send polys in coefs form because some (the irreducable ones) are not representable in root form. we still care about root form, however, because we'd like to calculate tightly how likely it is to map to a root. for now though, don't worry about tightness and just consider the each degree d poly has at most d roots. 
we want a function H from d+1 elements of Fp to one element of Fp. 
suppose for any element of Fp there are I polys of degree <= d that have the element as a root. 
clearly H does compression. suppose H is invertible, that is given any output one can compute all inputs that lead to that output. this means H is not collision or 2nd-preimage resistant. consider final round of sumcheck. tv is fixed, while prover needs to choose a tv' such that H(tv') = r and tv'(r) = tv(r) or r is a root of tv'-tv. To do this, prover leaves tv' as a variable, calculates the degree d poly t = tv'-tv which has coefficients containing variables. prover replaces the unknown in the poly t with H(tv') in variable form and expands t to obtain a multivariate poly in the variables of tv'. then the prover find a root for this poly and set tv' to this root. no step in this process could be infeasible for the prover except expanding t with H(tv') and finding a root. what must be the nature of H if this is to be infeasible?


https://crypto.stackexchange.com/questions/65975/snark-friendly-one-way-compression-functions?rq=1
https://eprint.iacr.org/2018/1162.pdf


Edwards curve
Twisted Edwards curve


for poly commitments we will need to use DLP on a curve.
for hash trees for application I think DLP on a curve will work.
for fiat-shamir I think we just need owf to a field element. this is the hardest to decide. DLP on a curve would amortize our already-heavy use of it, but we need it to map to a single field element. hmm, well since collision resistance is not necessary for fiat-shamir we could just use the x component or maybe add the components. 


i imagine there is no way to reduce oracle queries. if there were could do oracles offline. do we need a homomorphic hash to do reduction? suppose like Mimc the hash is a univariate poly of super large degree. if it was multilinear of low degree we could reduce evaluation from two points to one point. but i don't know of such a construction and also don't know of one security enough with a useful homorphic property (some exist but its not a helpful homomorphism). 


what about doing constraints without multiplication? we would make heavy use of constants, like for comparisons. so all our constraints would be in the form of linear combination equations (so we allow multiplication by constants).
I'm thinking of a modification of QAPs. we'd still have encoder polys. note that if we have more than 1 term they can reduce to one, so assume just one term. it would the dot product of the transcript with the encoder polys. we'd have something like 
f1 . K(x) = m1*z(x)
f2 . K(x) = m2*z(x)
where K(x) = [k1(x),...,kn(x)] is the vector of encoder polys. n is the size of the transcript, and the degree of these polys is the same as the zero poly and equal to the number of constraints (that is why the divisor poly is just a constant).
now we can add these two proofs to get
(f1 + f2) . K(x) = (m1 + m2)*z(x)
of course we have completness, that if the first two are valid then their sum is valid. what about soundness? if only one is incorrect soundness holds, but if both are wrong they can cancel each other. after given f1,f2,m1,m2, one must choose random r and instead yield
(r*f1 + f2) . K(x) = (r*m1 + m2)*z(x)
now f * K(r) for some r can be interactively computed using the log-comm dot product argument, but the verifier must compute K(r) as well as the right size m*z(r) by itself, but this is all fine because it is done offthread.
now while the proving system seems ideal here, with prover work linear, this model of computation may be limited in several ways.
first, we don't just want a linear combination of elements. also we want to include constants. for this we includ an extra encoder poly c(x) that encodes constants not multiplied by any transcript element. degrees remain as they are.
f1 . K(x) + c(x) = m1*z(x)
f2 . K(x) + c(x) = m2*z(x)
(r*f1 + f2) . K(x) + 2c(x) = (r*m1 + m2)z(x)
=> 2^{-1}(r*f1 + f2) . K(x) + c(x) = 2^{-1}(r*m1 + m2)z(x)
actually I think we can reduce an aribtrary number of them at a time still with a single fiat-shamir computation to get r (using powers of r). 
now of course our application requires multiplication, so if this can't be done with linear constraints this method won't work. at best it would require long arithmetic. since this method requires fiat-shamir anyway (though only one), and only really benefits the prover, it may not be worth much. we still need to do quite a bit of multiplication for elliptic curves which we still need because of poly commitment long arithmetic would be costly.

but what if we define the transcript the be vectors to be elements of some abstract group, and linear combinations to be multiples. notice the field size doesn't matter compared to the group size, but coefs for inverses depend on group size. now we still use the root method, were constraints are of the form a linear combination of the transcript plus a constant must equate to the identity. the left side is really a linear map (with a shift). the transcript is a list of vector elements, and the encoder polys are a matrix serving as a linear map. I don't think there's a way to do the right side. anyway, i realize merging proof would require operations in the field not just the group. 


so at the moment our state is that we use sumcheck type proof and we are only lacking an appropriate fiat-shamir transformation, or justifying that dlp is good enough and in what such way.
if we choose the x for hash output then we have a collision but that is I think submitting the negation of the exponents which means the negated poly, but the negated poly has the same roots as the original poly so this collision is harmless. 


to reduce the cost of exponentiations maybe use something like "Addition Chain Heuristics". I think the details and optimization can be deferred to prover strategy. we take a small 'window size', eg 6 bits so 64 elements and pre-compute them all. then instead of adding n pre-computed powers of 2 for a bitstring input of length n, we add 

2^m * 

43210
2^4*6

maybe we could do all hashing both for hash tree and for fiat-shamir with a fixed base always of the same length, probably 2 or 3 or 4. this would be the radix of the tree, and for fiat-shamir transforms with more elements we could iterately apply it because we don't need homomorphism. we only need homomorphism on the transcript commitment, and that also requires many more bases but all that happens off-thread. with this all hashing would be done with the exact same pre-computed base, so hopefully we could introduce some optimizations. suppose our tree is binary. then from what I was think we need to hash both the components, so at every branch we have 4 hashes to compute. i think 4 is also convenient for GKR style (if we use a single sumcheck on subject poly consisting of at most 3 multiplying multilinear polys with the same variable, so each message is at most a cubic unvariate poly consisting of 4 elements to be hashed). but we will still need some interation for certain cases like reducing 2 points of evaluation to one, where poly degree will depend on length of transcript and be much more than 4. but i think its safe to assume a fixed base of length, say, 4. what optimizations can we apply to hashing? we can assume many such hashes are taking place. can we amortize them in any way?
oh but there one important instance of DLP without a fixed basis, and that is when reducing two polys to one. remeber we can reduce eg 4 of them by computing r^3w + r^2x + ry + z = r(r^2w + rx + y) + z = r(r(rw + x) + y) + z which means exponentiating by r 3 times. this is cheaper than binary merging because only one r must be generated and maybe the repeating of the exponent r can be useful. for a fixed exponent, one finds the shortest addition chain and uses it multiple times for exponentiation (it is provers motivation to make addition chain short, verifier only needs to add it up to verify it comuputes correct r). 

we need to design a proof system with complementary fields such that the randomness from a reduction is influenced by those things it reduces. so eg in sumcheck taking two polys and generating randomness with them applied to the other poly is unfortunately not secure because one of them can be committed after its randomenss is generated. so if we use dlp for randomness then we take commitment of p values giving us q value, then we need convert this back to a p value. we can do that by passing it again through the dlp or maybe intepreting q as p. but whatever circuit does the verification must have access to the initial p values and to the r value and be confident that r is the correct output. if the circuit is given the q value and it then computes the second iteration of dlp, how does it know the q value is correct? It must verify whatever proof did the q calculation. 
suppose we have a p proof and we have a q circuit and a p circuit both verifiying. which circuit plays which roles? 
q circuit: takes p elements and commits them into q values; takes the transcript commitment together with its hash (no have) and reduces them to a new commitment (point of eval and claimed result no have).
p circuit: takes p values and checks them with respect the output of their commitments (no have); take the transcript commitment and hashes it. ... nasty

the fields may have to communicate with each other purely through bitstrings. we cannot really take advantage of asymmetry between the fields and their circuits and what those circuits are expected to perform because both circuits have the (large) minimum requirement of helping verify circuits in the other field. but what are some small asymmetries we can take advantage of?
the larger field can accept elements of the smaller field, but inverses will be interpreted differently. they can only be interpretted correctly if viewed as natural numbers, which seems fine for exponents. but if we use dlp, which are are, the exponents must be submitted in 'spread' (not necessarily bit-spread) form anyway, so submitting as whole elements doesn't seem to optimize much. 

what if we take the simplest but maybe least efficient way of simply alternating, no parallelism, and all values are submitted from one proof to the next in bit form, and no values 'skip' a proof (which would enable them to be submitted in native form). so we start with a proof in p, transcript commitment in q, all other parts of the proof submitted to the next q circuit in bit form. the q circuit hashes the bit submitted values, generating random q values. q circuit also receives in bit form the claimed values for transcript. q circuit hashes these and combines this with hash of transcript must just adding the points. now q circuit can represent new commitment as old commitments multiplied with hash of the random q value generated. it can also just pass the claims in bit form the way it received them, but it can't multiply them without knowing the hash of the random q value (and it can't perform arithmetic in that field anyway). similarly, it can hash the other p value it receives but it can't complete the verification so it passes these values on in bitform but adds their q value hash. no when the q circuit passes to the next p circuit, it bit outputs must become element inputs, and its element outputs must become bit inputs. now the way this transfer happens depends on how one is to read the claim of a proof. ideally one can read the elements of the claim in any format. suppose this is the case. then we just pass the values as said, and the p circuit converts the bit submitted values to p values (hashing again or something) and then completes the verification. but how does the p circuit know the bit values are correct? it doesn't, but since it will begin the verification process of the q circuit itself, it can reduce validity to that claim anyway. actually we're not done. the r for merging two polys to one must be returned to yet a new q circuit in bit form in order to reduce compute the new commitment. then we're done. 

the above is too long, takes 3 verifier levels. we need to use parallelism, and before we get into the details we need to comfirm how circuits read each other and we can force agreement for a value when it is submitted to parallel circuits (on different fields) in different formats (one native and one bit). suppose we only use one layer of parallel circuits and they do everything needed and all we need to do is make sure about agreement of their values. maybe we could delegate this to a new proof that computes on both of their transcripts. or maybe special transcripts are made that hold the values to be compared for agreement. 

i just noticed we may require non-zk with the prover sending full trascript to the verifier. this is because if prover is to argue zk he has knowledge of openening he needs access to the transcript. if this is indeed the case maybe the whole scheme is better framed as a single scalable proof. well maybe it can still be zk if the final transcript is the same and maybe it can be a 'padding' proof at the end. or maybe just generate a random mask poly then reduce that and the real poly to one, proofs still trust it, and its blocked so can be sent to next prover zk.

fixed base
https://documents.uow.edu.au/~thomaspl/pdf/NegPlaRob19.pdf
fixed exponent
http://delivery.acm.org/10.1145/2840000/2835046/p89-dwivedi.pdf?ip=198.11.30.230&id=2835046&acc=ACTIVE%20SERVICE&key=B63ACEF81C6334F5%2E1FC7ACB276C876CF%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&__acm__=1566362109_2c9e246d3a2f07e121723d659008f284

how does one proof take other proofs as inputs? its just presented with those proofs as data that gets encoded in the transcript. but i'm wondering if the transcript of a proof includes its claim. theoretically yes, its part of the satisfying assignment. but we also want easy access to it, whereas we don't have easy access to transcript without a full evaluation. even encoding claim as an accessible part of the transcript, the viewer must evaluate it to verify if. but i suppose the reading circuit need not perform that part of the verification. so we can consider the statement of each proof to consist partly of bit values and partly of elements in its native language. i think its safe to say all native elements go to the native verifier proof and all the bit values go to the other field. by 'statement' I think we just mean output, not input. input gets encoded in the transcript in any way. so to separate the output from the rest of the transcript we'll need an extra bit. i think we'd need this bit anyway to direct constraints on or away from the output versus the statement. we should actually probably refer to them separately, as the statement and the transcript/trace. the terms of the statement and the transcript can be standardized so the proof accepting it doesn't need to evaluate it but can use it for reduction, just as done with the trace. 
the final verifying circuit, whatever field, will need to take native values from one circuit and bit values from another and do a final match between them to make sure they are the same. 

we need to decide exactly how random values will be generated. i think we agree, as will already be the case for two to one polys, the first stage will be DLP the natural direction, but we are left to find out how to translate it back to the native field. the reason we should probably do dlp for raw element hashing is simply the need for compression. 

i'm thinking about the compression method of adding or multiplying together outputs. because then finding a collision is almost as hard as solving a single instance I think. this might be applicable for hash trees. and if collision resistance I imagine it would be one-way. in general xor'ing outputs of owfs is not one way. can we prove its owf for addition and multiplication for certain owfs though? we know it is for DLP. what about, say, subset sum? suppose we add the outputs. finding collisions here seems easy. given, say, 2 bitstring inputs we consider any bit position where the inputs differ and we flip them, so the corresponding entry still remains in the final sum once. but if used for fiat-shamir maybe collisions are ok and we just require owf. inverting requires finding 2 bitstring inputs. hmm, i guess solving in this case is equivalent to solving in a single instance but with input not restricted to 0,1 but rather to 0,1,2. in general m instances on [0,1] and adding up results in equivalent to 1 instance on [0,1,2,...,m]. since our hashing is of a small number of values, like 5, we may be able to have m=5 and still make this owf, and even collision resistant, using the works seen. suppose this worked. we could take p values and generate a random p value from them all in the same circuit, but note that we still would need bit-spreading, however maybe we could reduce the bitspreading by a constant factor by not submitting bits but by submitting the small norm integers for the problem. our advantage is that we can take advantage of the small compression available in this method, whereas for a tree hash we need arbitrary compression. if this works we can leave open the question of application hashing to using an arbitrary collision resistant owf. 


at the minimum we use DLP for reducing two polys to one. this means taking their commitments and generating a random number from them and then exponentiating. this forces us to exponentiate with a non-fixed base and at best we can exponentiate multiple with the same (but not pregenerated) random r. suppose poly is in p, so commitment is in q and exponentiation will take place in q circuit. given q values one must generate random p values. I'm thinking we could do this in a p-circuit using subset sum with density exactly one (due to nearly identical field sizes). which is probably faster than DLP, which would take place in circuit p as well. since there is no compression we'd probably generate a random r for each poly being reduced and I think this is still safe. But now i'm thinking this won't be much more costly then using DLP as both require a single round in a p circuit with the same abount of bitspreading. it will only be useful if we can also use subset sum for fiat-shamir but that requires compression. 
i think for our parameters no-algo can work in poly time, ie for regular modular subset sum when density is a constant > 1. so if not poly time we can expect exp time and I think this measurement is with respect to the number of elements we sum, so an exp gets us time around our field size which is already huge. but note that inputs will not be random. i suspect this will work for our random generations, but again remember that bit-spreading is still necessary. we may still end up just doing regular subset sum which requires more random constant elements than if we repeat it over and over. if we compress by allowing small norm inputs we need to verify they are indeed of small norm, which may require lot of constraints, whereas if we use bits the constraints are simple but we have many more gates. 
maybe we could even establish security for hash trees.
if we can show hardness is exp in log of our field size (ie our field size) then we don't need to worry about concrete params because our field size is already large enough. 

papers
http://users.monash.edu.au/~rste/subsum.pdf
file:///Users/josephjohnston/Downloads/TR96-007.pdf
https://cseweb.ucsd.edu/~daniele/papers/Cyclic.pdf
https://web.eecs.umich.edu/~cpeikert/pubs/cyclic-crh.pdf
others
https://arxiv.org/pdf/1807.04825.pdf


https://eprint.iacr.org/2018/383.pdf
https://csidh.isogeny.org
what do we need for homomorphic commitment?
gX hY
g'X h'H



what if we don't even require homomorphism in the commitments, but push evaluating the poly on the coomitments to another proof that will open the commitments and evaluate. then out task is reduced to verifying that proof, but we can make a commitment to the poly of that proof. thus in total we have reduced evaluating one commitment to evaluating a single commitment. this would only require any form of compressive polynomial commitment. so the statement of the proof would be that a list of commits and evaluation points result in a list of outputs. oh, but I suppose the problem is that the transcript for the proof must include the transcript for all input transcripts so the total transcript will group linearly, rather than remain constant. so this idea isn't worth much.


in our protocol for two polys to one the point r and the claimed values v1,v2 are generated before the random linear scalar is. 
m1(r) = v1
m2(r) = v2
make univariate poly with unknown as scalar. 

i was thinking about making it quantum proof using a non-homomorphic commitment like tree to all evaluation of several polys. then reducing manual evaluation of them (which covers low degree testing) to manual evaluation of a random linear combination of them (with random coefficients actually powers of one random value). a benefit is smaller field size, and all arithmetic done in that field. one problem is commitment via trees seems to require a univariate poly, whereas for GKR we're looking at multilinear. 
note that univariate poly f can be represented as, wherein g and h only need to be committed to on fields of half the size. theoretically this means rather than committing one poly f on a large domain we commit two polys g and h on a field of half the size which constitutes the same number of commitment points (or a generalization like g1(x^3) + x*g2(x^3) + x^2*g3(x^3)). but we can exploit the latter because we will evaluate at a single point so we can place both evaluation of g and h in the same leaf. 
below we see recursively breaking it up is equivalent to just generalizing the top layer like we did above.
f(x) = g(x^2) + x*h(x^2)
g(y) = g1(y^2) + y*g(y^2)
h(y) = h1(y^2) + y*h(y^2)
f(x) = (g1(x^4) + x^2*g(x^4)) + x*(h1(x^4) + x^2*h(x^4))
	= g1(x^4) + x*h1(x^4) + x^2*g(x^4) + x^3*h(x^4)
so deciding how much to break up depends on layers prover performance and verifier layers to hash tradeoffs. so in univariate case a poly commitment can be made with breaking up into k pieces by making a hash tree and evaluating each of the k sub-polys over domain of original size divided by k and storing all evaluations together in the same leaf.
suppose we make the field actually very small, in particular suppose transcript size is 2^N, then we choose domain size 2^D such that 2^N/2^D = 2^{-s} such that v*s = 64 so we perform v verifications. eg s = 4, v = 16, so 2^D = 2^N * 2^s = 2^{N + 4} = 2^N * 16. so we perform v=16 queries to achieve soundness 2^{-v*s}. But now the domain is super small and so are field elements, almost as small as the transcript size. subset sum will not work in this case, but now we can use the vector version of subset sum that generates multiple values and we will use these values for multiple queries. we might even go to the extreme where v = 64 and s = 1. 

but wait, what exactly would I do supposing we could easily commit and open polys on a random point? using hash trees we would not know the encoded poly is indeed a poly, ie of low degree. using multilinear polys, we'd get to the end using usuall process and as usual reduce to evaluating a lot of polys at a single identical random point. then we generate random random coefficient(s) and ask for commitment to the linear combination. now via our assumption of reduction (like induction) we can assume the linear combination commitment indeed encodes precisely (not just close) some poly of appropriate degree. now our task is to compare the linear combination to the polys in the previous commitments. remember we can treat these commitments as oracles to real polys, not just hash trees of arbitrary values, under the condition that these polys are of arbitrarily high degree. now we want to polynomial identity test the linear combination commitment with what it should be via the previous polys. we can test at random points via the oracles. we use the zippel lemma, but we must assume the left side (previous polys) to be of maximum degree which corresponds to their size, so we must make this sufficiently smaller than the domain size. if not sufficiently small we just query multiple times. 

could require grinding with nonce for fiat-shamir. i think we can justify eg for 32 bits of grinding we can reduce number of queries above from v=16 to v=8

even if we adopted used univariates in our proofs applying the previous wouldn't suffice for a complete proof system because univariates cannot reduce two points to one unless we first convert to multilinear and recovering from that is the problem we aim to solve below anyway.

consider what happens when we swap two variables in a mutlilinear poly. which terms are affected? any terms that contain both or neither of those variables are not affected. terms that contain one xor the other are affected. suppose we group the terms into these four categories, those containing both x and y, those only containing x, those only containing y, then those containing neither. then we factor out the variables x and y so we end up with a poly of the form a*xy + b*x + c*y + d where a,b,c,d are multilinear polys in the other variables.
this b*x + c*y will turn into b*y + c*x, and adding these together we get
(b + c)(x + y)
subtracting bottom from top we get
b*x + c*y - b*y - c*x = b*(x - y) + c*(y - x) = (b - c)(x - y) which is also the result for subtracting the whole polys.
subtracting top from bottom we get
b*y + c*x - b*x - c*y = b*(y - x) + c*(x - y) = (b - c)(y - x) which is also the result for subtracting the whole polys.

suppose we consider the permutations under which the polynomial is invariant. doese these form a group? if its closed then yes, it forms a group. and indeed I think its closed. 

i realized we can use sumcheck to evaluate a multilinear with the coefficients committed via a univariate, but the subject in the sumcheck would be of high degree, so the prover would need to commit to all the intermediate univariates. while unfortunately the commitment of one poly is needed to generate the randomness necessary for the next poly so they must all be committed separately and we are unable to merge the commitments. 

what if we actually commit to a mutltivariate poly, even just a bivariate, but over a small domain, so that size raised to the variacy matches our original intention. this may have possibility but I stop here due to the problem beow.

wait, i don't know if I understood the soundness of what I step up before. if the commitment to a univariate is over domain size 2^D, prover could commit to a poly of degree 2^D so the zippel lemma will be identity comparing a degree d^N poly to one of degree 2^D over field of size 2^D. this yields an upper bound of error of max(2^D,2^N)/2^D = 1, so there is no soundness at all. the higher degree could interpolate the lower degree almost every except a few places, so it could be high degree but still pass the test. this means we would need some more sophisticated analysis like 'closeness' to low degree which I don't want to get into. I think this ends this possibility of a fully post quantum scheme in one field and we are left to continue with a homomorphic commitment of which discrete log is the only one I know. 


now before pursuing the previous by inspiration of univariates, I was considering isogeny crypto. but for several reasons I don't think that will work. Before that I was analyzing using sumcheck. before that I was considering the minimum requirements for proof data flow to comply with captibility between the fields. we need to do exponentiation with non-fixed base and exponent, but maybe everything else can be done with subset-sum hashing. I will complete my investigation of the security of subset sum if it seems desirable. even in the best case of using it we still need communication between circuits. given a p-circuit, its proof consists of a lot of p values and one pari of q values corresponding to the commitment. all the verification can be done in another p-circuit, including the generation of the random exponent, so this verification circuit will yield a conditional acceptance in the form of q values in bitstring format together with a random exponent, and the claim that at a certain point the polynomial (once exponentiated) will return a certain value. So in total the condition consists of q values in bit format, and 3 p values. a q-circuit will have to perform the exponentiation. this q-circuit will do this for multiple proofs, then add the points together to compress them to a commitment for randomlized linear combination. regarding p-proofs, I think a q-circuit will only perform this role and no more. it accepts q points and p elements in bitformat and exponentiates them and adds them together, and returns the answer, not knowing anything else about the p-proofs. 
but to use the information from any proof, the using proof must condition on the proof being used as correct. thus there must be a succinct way to state such a condition while also knowing the claim that proof makes and being able to relate that to the representation of the condition. 
the difference between proofs with the same logic is just the statement and the transcript. So for a proof in p, it could be represented in un-verified form as the transcript commitment which is a pair of q values, together with the p values that make up the statement (these p values are coefficients). I suppose it should also consist of an identifier for the code. if we use GKR we could have the statement be the output of the circuit rather than an extra input, but either way we still treat the statment as plain p values. since its a complete claim given these values it doesn't matter how they are presented. a circuit that receives them can pass them on to the next asserting its validity is dependent on the validity of that one. realize this representation can be reduced even further by hashing these values to a single one. however, in order to make use of the claim one must unwrap the hash enough to read the relevant part of the statement. 


NIST elliptic curves: https://nvlpubs.nist.gov/nistpubs/FIPS/NIST.FIPS.186-4.pdf

could we do QAPS but prevent the circuit from doing full verification (ie pairings computation) and instead do that part offthread. Suppose we are able to reduce verification of two proofs to one onthread. then yes, this will work. but reducing verification is troubled by the multiplication of polys, ie a sum of two transcripts is not another valid transcript.
the multiplication looks like this
(f . K1)*(f . K2) = (\sum_i f_i * K1_i)*(\sum_i f_i * K2_i) = \sum_{i,j} f_i*f_j*K1_i*K2_j
so we could pregenerate not just K1 and K2 but their outer product. so each encode power, now of form K1_i*K2_j would now be twice the degree as before. more importantly, the sequence would now be square in length. this would probably be too costly. I thought the linear combination of two outer products would be another valid outer product but that is not the case and there seems to be no way to combine transcripts to yeild another valid transcript, ie reducing verification of two proofs to one.


lattice based zk: https://eprint.iacr.org/2018/560.pdf
this paper claims we can achieve almost arbitrary compression


trying to think about how use the highly compressionable lattice based, limited homorphic commitment schemes for homomorphic poly commitment. the coefficients of the poly must be reduced to small norm. 
the sumcheck can be used to verify that one knows an opening for a commitment (vector of not-necessarily small enough norm). at the end the verifer would need to evaluate the constants vector at a random place and the input vector (the same) random place. so for this the input vector would be expressed as (bit access-oriented) coefficients in a multilinear poly. but we'd also like a protocol to verify the input is appropriate (or small enough norm). to do this we could construct the degree d poly consisting of the d roots 0,...,d-1 that appropriate values for the vector elements (infinite norm assumed). Then we use sumcheck to iterate over the poly with supposedly small norm and for each step we plug in the value to the small univariate poly and the result should be zero, and to make sure all add up to zero (instead of two cancelling each other) we multiply term by a random linear combination similar to how we plan to do for the general circuit. 

suppose we have some highly compressive collision free hash function that accepts elements of small norm, eg bits, and outputs something in some format. we require that it is computable by deterministically iterating over the bits in a uniform way a fixed number of times and combining them with constants. if adding is not the primary way of concatenation we can use intermediate polys to store values as long as those values remain of small norm, because we will need a commitment for each of them. in fact, using the sumcheck like this, each time computing new intermediate values stored in new polys, in a general construction we could use, somewhat similar to GKR. of course its assumed all polys at the end will be reduced to one for evaluation, and that all polys are multilinear.
now given a hash output, which is really a commitment to a vector of d-values (by d-values I mean values of small norm at most d). we can interpret it equivalently as a commitment to a multilinear poly with these values as coefs. so this poly commitment is dependent on the norm of the coefs, so this doesn't work for an arbitrary poly. 
we may now use such polys in non-interactive versions of sumcheck protocols where the commitment to the poly is given in the beginning, and at the end the verifier opens the commitment to evaluate. in fact, we could interpret the commitment as one to an arbitrary poly of coefs determined by the value hashed.
really what we care about is reducing evaluation of two polys to one poly. the way I've been thinking to do this requires the ability to compute the commitment to the addition of two polys from their respective commitments, or rather to verify it. In the commitment case above I'm interpreting the d-values and radix scalars, so adding two actual polys is not equivalent to adding the d-values, but adding them as a number base system with carrying. I think this should be possible using the sumcheck where we use commitments to the two input d-polys (polys encoding the d-values) and the output d-poly. oh, but the problem is at the end we must evaluate all three polys, including the input two, at a random place and that is exactly the problem we started with. this is not the case for DLP homomorphism because we can compute the commitment for the output poly directly from the input commitments, whereas here we rely not on the input commitments themselves but on the polys they represent. 
however, given a purported valid commitment, we can use sumcheck over that d-poly to verify that it is indeed a valid commitment, at the end of which we must only evaluate that d-poly. so if two commitments are claimed to add to another valid one, the verifier can add the commitments then check that it is indeed valid. 
if we have two valid commits, we divide them by 2, then we add them we are guaranteed to get another valid commit. Interpreted as polys, dividing the commit by 2 is equivalent to dividing the coefficients by 2, generating a new commit. so we have effectively generated from two commits to polys, a commit to another poly, in particular for f and g we generate poly (f + g)/2. now we'd like to reduce evaluating f and g at the same random point to evaluating some variant of (f + g)/2 at a random point. this is tricky because before we would scale by a random value r but now that would not yield a valid commitment. also we need to clarify what we mean by /2. we need integer vectors so we can't just divide by 2 cuz we might get rationals. hmm, trouble.
suppose we don't use the larger field of constants as our native field, but instead use the d-value range as our native field. call the smaller native field p and the larger one q. so p < q. we might be able to do what we want, but we would need to do something like long arithmetic when working with the q value commitments. suppose we're given two commitments (q values in bit form). suppose we multiply a commitment (a q value) by a p value, then reduce modulo p. 
(q1*p1 + q2*p2)*r mod p = q1*(p1*r) + q2*(p2*r) mod p = q1*(p1*r) mod p + q2*(p2*r) mod p = (q1 mod p)(p1*r mod p) + (q2 mod p)(p2*r mod p)
we end up with a p value, not a q value of course.
what if we work over q as before, but the p values are all even. this way we can divide two commits by 2, get two valid commits with odd values, then add those two commits together to get another valid commit with even values again. Now we need to randomize. (Maybe extend to not just 2 but arbitrary multiples). But we stil dont know what it means to divide by 2. intuitively we know what it means by natural numbers. but how do we perform this. suppose k is the inverse of 2.
(n*2)*k = n*(2*k) = n*1 = n
so really we just multiply by that inverse. we can generalize to multipels of m (even is multiples of 2). 
(n*m)*m^{-1} = n, n=0,1,...,d. maybe we choose m such that m*d covers most of the field. 
we can change m as we go. if we multiply by an arbitrary r, assuming no overflow, then our new m' is our old m multiplied by r. actually if overflow occurs I think we are still ok. suppose our d value is n. we start with scale m, then we scale by r, then we adjust our new scale to m*r and our new inverter to (m*r)^{-1}. we still get ((n*m)*r)*(m*r)^{-1} = n. so it appears the 'scale', (m, r, etc) can be anything in the field that is invertible, ie not zero. so whenever we have a poly we associate with it a scale relative to d, and we know that multiplying by the inverse of that scale should yield coefficients all in the range [0,d]. 

what if we multiply by p or p-1 or p-2? does it shuffle them? multiplying the numbers [1,...,d] by p gives all 0s. Multiplying by p-1 returns [p,p-1,...,p-(d-1)]. 

hmm, i'm wondering if I can establish that the hardness is not due to small numbers, but only having a few choices for the numbers. If this is the case, we can multiply d-values by any constant and the result is still a valid commitment so we don't have to worry about getting the nubmers back in a base form. Oh, but even if this works, when we add polys they must be have the same scale or else the numbers are not limited to ensure hardness. and if they are on the same scale they won't randomize. I don't think we can randomize by scaling. What about randomization by shifting? Again, when we add polys they must be in the same 'state'. 

and actually the above method of combining I don't think will work because it only works like once, and I've explored variants as well. eg the original was to take 2 evens, divide them by 2, then add them, but their sum might not be even. If we restrict the numbers such that the result will be even, then the result after that may not be even, etc.

maybe we can take two valid commits, add them, and assert that some standardized derivative of that sum as a poly should act a certain way. but the sum could have any value in [0,2d] if valid has values in [0,d]. oh, actually we can't just assert the derivative, we need to compute the commit to it. 

our intended construction was to unquestionably correctly compute the commitment to the sum from the commitments. but i'm wondering if that is necessary. we would compute some result poly that can be evaluated instead of the original polys. the verifier would be holding the commitmens to all 3 polys then the verification/proof proceedure using sumcheck starts and at the end the verifier must evaluate all 3 polys at a random place. we reduce eval the sub polys to the same point. then we need to randomize. but the problem is we will need to compute the randomized poly, but computing that will require using the sub polys and so at the end we'll need to evaluate the sub polys at a new point. if we didn't need randomization, we could compute the formal sum, then reduce evaluating the sub polys to evaluating their formal sum. the idea is if the computation was wrong, then the evaluation of the formal sum is wrong. but the problem is the prover knows we're just evaluating the formal sum, so at the end when we ask for the eval of the sub poly, the prover can give answers that are wrong for each but their sum is right for the formal sum. the only advantage the verifier has is that all three poly commitments, as constants, are given from the beginning. what if we just do polynomial identity testing. given those commits the verifier generates random points for evaluation but the problem is verifier either need to computer answers on sub polys manually, or needs to ask prover for answers, but then randomize the new poly rather than just the formal sum.

suppose we do sumcheck to compute formal sum commitment, which prover provides from the beginning. we go through the process with prover sending back messages as it likes that are probably not correct but pass the verifiers tests. then at the end the verifier is to evaluate the two subpolys. now keep in mind that while the prover send messages of his choice the verifier hashed them into messages the prover has no control over. so at the end the verifier is to evaluate the two subpolys at two random places uncontrolled by the prover. the verifier asks the prover for the values at those points. both the prover and the verifier know the line that connects these two points. the prover sends the evaluation of both on this one, implicitly specifying the answers to the random points. the prove strategically chooses these t-polys. verifier sums the two t-polys to form what the new commitment should be when evaluated on that line. for identity test verifier chooses random point on line and manually evaluates t-polys and reduces test to evaluating commitment poly at random point. if the t-poly sums is incorrect, the latter identity test will probably fail. So the prover needs to choose the t-polys such that their sum is correct. it also needs to choose the t-polys so that one evaluated at 0 gives one value and the other evaluated at 1 gives another value such that these two values satisfy an equation. oops i thought this method might give something but looks like prover can pass no problem.

our advantages are:
can compute anything on data encoded in polys using sumcheck at the cost that at the end we evaluate the data polys at random points.
can reduce evaluation of a poly at 2 points to one point without touching the poly.
can reduce evaluation of two polys at a point to evaluation of one poly at that point, as long as we can multiply one of the polys by a constant which requires touching it maybe homomorphically through its commitment.

suppose we make commitments to d values and interpret them as commitments to polys in d space, maybe d is a prime so they are polys in a field. when we add two commitments they may overflow d, but we ask the prover and have the verifier interpret a commitment with overflown coefs as modulo d such that they did not overflow. now we can multiply a commitment by any d value, and despite overflow, when interpreted modulo d the result is like a poly in the field d multiplied by an element in the field d. on the outside, the prover starts with commitments to poly1 and poly2 in field d. upon adding them the vectors together modulo d the prover obtains a commitment to (poly1 + poly2) in field 2 because indeed, adding two polys in field d just means adding their coefficients modulo d. so we have abandoned the idea of interpreting the d value polys as radix based representations for ambient space polys. instead our polys are in the semi-subfield d. maybe we don't need to have d a prime. maybe we can just view these as polys in the ambient space but with limited coefficients. i'm worried the addition modulo d will get in the way. using the polys in sumcheck to access data seems not to be a problem. at the end we must evaluate at a random point in the ambient space. the soundness is then with respect to the whole field size, not just d. so our primary worry now is that there's no clear relationship between polys f, g, and (f + g)_d = (f + g) \mod d. 
note that (f + g)_d(r) \mod d = (f + g)(r) \mod d. 

or what if our native space is the d-space (eg d could even just be 2), and we doing long arithmetic adding commitments in the ambient space.

actually I may have glossed over incompatibilities, eg the added commitments doesn't give a new commitment, at least not directly. Let me start again. when we add two valid commitmens F and G we get
F + G = (F + G)_d + [0,1]*d
where (F + G)_d is the modulo poly, and [0,1]*d is the overflow vector, because when adding two numbers in the range [0,d-1] the max possible is 2*d - 2 < 2*d so at most 1 overflow can occur. The above i'm only showing the vectors. now suppose we plug them into the function by multiplying by matrix S.
S(F + G) = S(F + G)_d + S[0,1]*d
the left side is what the verifier will compute given SF and SG. Now the prover could supply the two commitments on the right side and the verifier could check they add to the left side, but this does not ensure the provided commitments are valid. 
Suppose the prover provides 'S[0,1] (supposed S[0,1]) and verifier computes 'S(F + G)_d = S(F + G) - d*'S[0,1]. Suppose 'S[0,1] is wrong. Then 'S(F + G)_d is wrong and can either contain coefs < 0 (some large inverse number) or >= d. Suppose, audaciously, that we can prove that if 'S(F + G)_d (or rather 'S[0,1]) is wrong then the prover doesn't know an opening for 'S(F + G)_d. in particular the prover doesn't know a valid opening, so he will be caught on offthread, not onthread. if this works, this puts us back where we were before I had doubts. It puts in the position that given two valid commitments to d-polys the verifier (now with help from the prover) can compute the valid sum of those polys modulo d.


One question is, given valid openings F and G for F' and G', is it feasible for one to find a valid opening H for H' = F' + G'? Let us suppose that F + G is an invalid opening, that is F + G = (F + G)_d + [0,1]*d for [0,1] != [0,...,0]
F' = SF
G' = SG
H' = (F' + G') = S(F + G) = S((F + G)_d + [0,1]*d) = S(F + G)_d + d*S[0,1]
suppose indeed one computes valid H such that H' = SH
SH = S(F + G)_d + d*S[0,1]
only probabilistic, but split d*[0,1] into two parts P1 + P2 = d*[0,1] and maybe the split can be done such that 
H - P1 and (F + G)_d + P2 are valid openings. Then one has
SH - SP1 = S(F + G)_d + SP2 => S(H - P1) = S((F + G)_d + P2) => collision => infeasible
maybe this argument could be improved if we choose a d below the security level, so that in fact values of d or a little above still break the security assumption.

now remember that when we're talking about the hash function our setting can shift from modular arithmetic to integer, where inverses are natural integer negatives. 


i'm still considering subset sum for hashing because I think it can do a little compression and thats what we need while at the same time we need to output a single field element, whereas the output of the above function, despite the large compression, is about the security parameter field elements. before I return to figuring out the parameters and whether compression like 4 to 1 is ok, I'll first settle on whether to use elliptic curves or lattices. 



note that we can shift a commit by adding to it the sum of all constants, raising each element in the commit by 1. 

is there a way that given the addition of two commits, ie with values [0,2(d-1)], the verfier can compute modulo d by itself? 
suppose the ambient space was not a field so the constants were q for some, say, even value. then suppose d = 2. we could scale by q/2, so any 2 value gets mapped to q = 0 \mod q. any 1 element gets mapped to q/2. the result could then be multiplied by (q/2)^{-1} to renormalize. unfortunatively this inverse doesn't exist. even so it would still be helpful to renormalize to two values 0,q/2 rather than 3 values 0,1,2, but this is not possible since q will need to be prime.



how hard would it be to take the long-arithmetic approach where our native field is Fd for prime d. for the constants we could represent them in base d as a vector of the scalars. now a commit would be a vector (really a vector of vectors). would this work? I don't think so, because we need the vectors to add up modulo q, not d. only way seems to be preventing overflow d by making the number system large enough that they can all add up with no overflow. (btw, note that if we do this d will need to be small, otherwise q will be too huge). 


maybe we derive a different but similar problem than SIS from SVP. 

I think we can get concrete parameters by fastest SVP algo which approximates to a factor of (1.012)^n or something. So must choose n such that approximating this amount is not feasible. also need an estimate on the time it takes to do this.

Lattices are infinite, so we need finite representation. 
Lambda(upsilon, q) is the lattice of all integer sequences h_1,...,h_m such that \sum_i h_i*u_i = 0 \mod q for upsilon = <u_1,...,u_m>, u_i \in \Z^n. That is, a lattice point is a vector h \in \Z^m such that h is orthogonal to the set upsilon (not orthogonal to every element of the set). q and m will only be functions of n. 

maybe condense the commitment from n elements to 1 element by interpreting the n elements as coefficients of a poly (uni or multi). for a fixed random point, evaluate the poly and the result is the commitment. the same homomorphic property applies. finding a collision means finding not finding the same poly but finding a poly that evaluates the same on that random point. by zippel if commit1 != commit2 then commit1(r) = commit2(r) with probability degree/field_size, but actually this is not entirely so because r determined before the polys, whereas zippel assumes the polys are set before r is selected. ideally the analysis can be that the prover can only sample polys at random (by the hash function) and that for a random poly, its evaluation at random point r being any particular (not random) value has negliglbe probability. since r is drawn at random then multiple polys are drawn at random, we could instead forget the order and just ask, I think, what is the probability that for random polys p_1,...,p_n and random r that any of them evaluate to 0. I think this can be via zippel lemma 1 - (1 - degree/field_size)^n.

another possibility is to intepret the output as a point of evaluation of a degree one linear poly f with some random coefficients. this is also homomorphic as f(commit1 + commit2) = f(commit1) + f(commit2). but here we must rely on analysis different from zippel because the difference is the evaluation point not the poly. but analysis may still be good.






Our generalized SIS problem can be given a finite abelian group G, a set of m random elements from G and a target t, find an integer vector x of length m below some bound B relative to some norm N, such that the linear combination of v and the random elements equals the identity element. another parameter is the order of G. parameters must satisfy certain conditions for this to be owf. This can be turned into collision resistant hash function.

Oh, the lattices considered are subgroups of \R^n but this could be a subgroup of \Q^n or \Z^n.

I'm thinking we would need to use \R or \Q if we are going to invert it, because while an integer matrix has an inverse it may not be integer valued, but rather rational valued. I know rational inverses are rational because only standard field ops are performed, no ops like roots that could result in irrational values. 

Any finite abelian group is the direct product of a unique series of prime-power cyclic groups. the primes are those that divide the order of the group. 

I think a module is a vector space except over a ring rather than a field.
While lattice problems can be extended to have scalars (using modules) other than integers I think we'll stick with integers because it corresonds naturally to the underlying group. Also, I'm hoping to keep the proof field simple as having prime order, rather than an extension field.


Suppose we have a lattice (integer lattice) over \Q. We consider the approximate SVP problem. Given basis A our problem is to find non-zero integer vector x such that Ax = s is bounded in some norm N by some bound B. Note that s will be in \Q.
s = Ax
now A is a basis for the lattice and thus for the \Q space, so its invertible
A^{-1}s = x
An equivalent problem is finding \Q vector s of appropriate norm such that x is an integer vector. 

Suppose one can find an s then computes the greatest common demominator d and factors it out then brings it to the side with x getting
A^{-1}s' = d*x
where s' is integer valued and may no longer satisfy the bound. 

Suppose we try to make this compressive where A^{-1} has many more columns than rows. In this case A^{-1} is not actually an inverse so lets call it A'. To support the reduction we need to multiply the second equation and have it yield the first equation so we need AA' = I. So if A' is to be 'horizontal' then A is to be 'vertical'. So suppose we concat multiple lattice instances of A on top of each other (also increasing hardness). Can we find a right inverse A' for A?





lets figure out, which we should anyway, what we can do with subset sum.
now remember we're only considering subset sum for fiat-shamir, so we don't care if collision are possible, just that its owf and random looking and compressing. 

its possible we could use a general version of SIS hash where the group is our additive field, but it would have to be of size q^n = {n^{c*n}} which is way too big if n is our security parameter. 

subset sum with density about 1 are the hardest. I am wondering if they can have a constant factor difference, but assume for now density is precisely 1.
several sources cite density as n/log_2(q)
what is the security parameter? I think its m = log_2(q)

one source says
if m = c*n then it can be reduced to SVP. 
if m = c*log(n) then it can be reduced to poly time dynamic programming. 
I think m = log(q) is our security parameter
so we must have c1*log(n) < m < c2*n, or rather w(polylog(n)) ~ m ~ o(n)
this suggests something like m = n^2 is hard, but notice density n/m = n is far from 1.

another source says
m = O(n^2) can be attacked by SVP
m = O(log(n)) even O(log(n)^2) can be attacked by dynamic programming (referring to previous source)
this work attacks when m = n^e for some e < 1. or rather n = m^d for d > 1.
it solves this in time 2^O(n^e/log(n))
so having n a power of d is gone is not safe

what I'd like is just n = c*m for a small constant c > 1. 
then the algo above works with
m = (mc)^e => log(m) = e log(mc)
e = log(m)/log(cm) = 1/(log(c)/log(m) + 1)
and exp time is (cm)^{log(m)/log(cm)} / log_k(cm)
taking natural log we get exp time (log(m)/log(cm))*log(cm) - log(log_k(cm) = log(m) - log(log_k(cm))
exponentiating again we get exp time m/log_k(cm)
so our security param is no longer m but now has a log factor on bottom depending on c
even with the benefits of c=1 and natural log, for security 80 we need like m = 500, which is worse than elliptic curve numbers (but maybes still worth post quantum security). keep in mind how such a large field blows up the gate number since we need to do bit-spreading for subset sum. 

but note that just as we are taking our constants into account, we must take the running time constants of their algo into account.
also regarding constants, we shouldn't account for quantum algorithm until they are developed and set our constants only to account for current algorithms. 
if a large field is really too costly in terms of gates, we could use SIS hash and generate multiple hash values and I think pass them to a random multivariate, degree 1 poly, ie take a fixed random linear combination of them (remember collisions are ok) or use some other way of merging them while preserving randomenss. 



note we can use UOWHF for application where instead of regular hash tree, the hash function is selected at random from a family based on what is being hashed, ie we're just extending the hash, but this requires PSF to go from hash content to hash params. 


we assume that exp time algos will always be the best available for our problem distribution. but as algos get better we may have to change our security parameter. 



Given a bit commitment, we can perform any bit operation on it
for 0 -> a and 1 -> b on x perform (1 - x)a + xb
the only non-trivial such operation is negation

Now we'd like to consider what we can do on two different commits.
suppose on the first we send 0 -> a1 and 1 -> b1, and on the second we send 0 -> a2 and 1 -> b2, then we add them.
in general, the most we can do on two n commits is multiply each by a scalar then add another constant commit, eg affine.
so given two commits, suppose we adjust each one arbitrarily as we like, then we add them.

for a given boolean operation we need to find a matrix M of the form
[
	s1 s2 0  0
	s3 0  0  s4
	0  s5 s6 0
	0  s7 0  s8
]
such that for input vector [a1,a2,b1,b2] we get a desired boolean output.
I don't think any nontrivial operation is possible. for example, AND below has no solution
AND 
a1 + a2 = 0
a1 + b2 = 0
b1 + a2 = 0
b1 + b2 = 1
--
actually the only seemingly non-trivial operations are basically selecting one of the 4 bits, eg to select a1
b1 = b2 = a2 = q/2
a1 = q/2 + 1
--
a1 + a2 = 1
a1 + b2 = 1
b1 + a2 = 0
b1 + b2 = 0
--
but in fact this is trivial as its just projecting onto the first vector, cancelling the second.


suppose we switch modulus between proofs. so a commit ends in some state such that when switched to a particular new modulus reduces to a partiuclar 0,1 in it. this means a 1 should be an intended in position 1, while an intended 0 should be in a position congruent to the new modulus q'.


in the case of d-values, we need to modulo-out the d*{0,1} side-effect commitment. suppose our modulus from the beginning has d as a divisor. d*k = q. then we multiply by k. the side-effect commitment goes away, but now the new commitment is multiplied by k (and k has no inverse). getting rid of it means switching modulus to q' such that gcd(k,q') = 1.
suppose d = 2. need gcd(q/2,q') = 1.
	try q' = q - 2 (to remain even). gcd(q/2, q - 2) = gcd(q/2, (q - 2) - q/2) = gcd(q/2, q/2 - 2) = gcd(q/2 - (q/2 - 2), q/2 - 2) = gcd(2, q/2 - 2). now suppose q/2 is odd. then q/2 - 2*n is odd so we continue with gcd(2, q/2 - 2*n) until q/2 - 2*n = 1, then gcd(2,1) = 1. so this works iff q/2 is odd. but unfortunately it terminates next round. q'/2 = (q - 2)/2 = q/2 - 1 = odd - odd = even.
try for arbitrary d. need gcd(q/d,q') = 1. if we are to repeat this for same d, then we will need d to divide q', and gcd(q'/d,q'') = 1. 
	...d*gcd(q/d,q') = gcd(q,d*q')
	...d*gcd(q'/d,q'') = gcd(q',d*q'')
I now realize we could do d=2 with q' = q +- 4. gcd(q/2, q+4) = gcd(q/2, q+4 - q/2) = gcd(q/2, q+4 - q) = gcd(q/2, 4) = gcd(q/2-4, 4) = gcd(q/2-4*n, 4) = ... gcd(odd, 4). 4 won't divide odd, and 2 won't either, so only 1 will. then q'/2 = (q + 4)/2 = q/2 + 2 = odd + even = odd. So we can repeat. this allows us to do XOR. 


i imagine modulus switching could give a solution to doing operations on the commitment without help from prover (though we haven't ruled out help from prover). but we assume every circuit will have a fixed modulus, so switching modulus at all means switching modulus between proofs. this brings us back to the concept of switching between fields/rings aka environments (envs). we need to carefully analyze what comptabilities must exist between the prover and verifier environments.

suppose we're using constraint-based sumcheck. in this case we have a transcript poly and code polys all consisting of elements from some env, variables, and the constants 0 and 1. we want to prove the composition poly evaluates to 0 over a set of all combinations of 0 and 1. To do this we treat the evaluations as coefs of a multilinear poly evaluated at a random point using sumcheck. the random values chosen for this must be in the verifier env.

for unique poly testing, verification is best done in a field p, and indeed I think this can be done while still allowing computations with variable modulus. We would like to allow a custom modulus q for each constraint. Our transcript is in env q. So for a given constraint suppose we want to confirm that v1*v2 = v3 \mod q. This holds iff v1*v2 = m*q + v3 with 0 <= m <= q-2. If we were in env q then we'd reduce modulo 0 and get mq + v3 - v3 = mq = 0. But we are in env prime p. So if we subtract we get mq. We want to make sure this quantity is of the form mq for known q and unknown 0 <= m <= q-2. So we consider the range 0 <= mq <= q(q - 2), and only (q - 1) values within this range are valid out of (q(q - 2) + 1). This is roughly 1/q of the values. But if we reduce this sparse range modulo p it results in a dense range. One possibility that I don't think allows cheating is having an additional side-transcript with the integers in [0,q-2] to specify the various m values. so we'd do
(v1*v2 - v3)q^{-1} - m

suppose we figure out modulus switching. it would have to operate on whole commitments at a time and do so component wise. 

we will not be able to treat commited values as field coefs and perform regular poly techniques simply because the coefs will be from too small a field so for soundness we'd have to apply the techniques too many times. 


think about base change (base as in the constants used for commitment). upon choosing a random point at which to evaluate the poly, one can infer the new base. supposing we have two polys to evaluate at the same point, the problem of evaluating them is reduced to making sure they have a commitment opening for the new base. note that computing the new base only involes the old base (a standard), and the random point (little data), so the verifier can compute a commitment to the new base. 
now we'd like to reduce verifying two openings to verifying one opening. suppose we can do modular addition on the commitments. we'd like to prove that one knows an opening to the sum iff one knows openings to both commits summed.
i realize this won't work if we use large random evaluations since the commit to the new basis will involve those with exceed the bound.
but i'm thinking about how to efficiently increase soundness if we reduce the random values to d-values. 
if we do multilinear poly then a single point of evaluation should have sufficient soundness (the larger the commit the greater the soundness). but the new basis depends on the point of evaluation. 


our main trouble now is how to randomize the addition of two polys so they don't cancel. the simplest way, though costly with DLP and seemingly impossible with SIS, is to multiply one by a large random value first. is there any other way. if we leave the random value as a variable we could build up a multilinear poly but it would not be constant in size. if we multiply by a random d-value in our d-value field the problem is the soundness is only 1/d.
what if we ask for multiple random points what both polys evaluate to. Then we generate a random scalar and scale as normal. Then I think if we test each of the n asked points we get soundness 1/d^n despite only the single small random scalar. At this point we have many claims on the new poly. Then we could go through and reduce all these evaluations to one. oh but reducing two to one also has the same weak soundness. So the latter step doesn't help. However, I think it can still be used to reduce evaluating two polys at two independent sets of points to a single set of points. Well actually maybe not, cuz every application would involve testing the line-join at multiple points, and each line would be different so the number of points would not be constant. eg consider two polys with tw points to test each. we'd make two lines, and for each we must test 2 points, so the number of points doubles to 4. from this I think I realize for the first time how important it is that the field size is large so that we don't need to perform multiple tests. 
well actually suppose for each line we only did one evaluation. by itself thats not enough soundness but keep in mind we're doing multiple lines and they all must pass. so if we have two polys each with n points, suppose we connect each with another to make n lines (maybe some convention about how to match them), then for each line we evaluate once, ending up with n common points to evaluate these two polys. thus we have successfully reduced two independent sets to one set with soundness I think 1/d^n. 
But one problem I'm now worrying about is that we can only have multilinear polys of size at most 2^d (if univariate then only size d). One possible way around this is extending to general multilinear polys, were allowing individual degree at most m allows size (1 + m)^d. This increases the total degree of the poly. The limit would be something like m = d. we need to make sure the zippel lemma holds for this when... lol zippel says soundness is degree/d so if we want this < 1 we need d' = degree < d. so how many points can we encode with degree d'? for allowing all monomials we could do multilinear with 2^{d'}, but that would be too small. only something like d'=20 would be reasonable but in that case we have d like 30 or 40. for v variables of degree at most d' it can encode (d'+1)^v points if we allow all monomials. but obviously v is bounded by d because otherwise at least 2 variables will always take the same value. Does this mean we can validly encode (d' + 1)^d points using d variables of degree at most d' and the zippel lemma still holds with d'/d? If so, this would be sufficient. 

try to encode large cyclic group via small cyclic groups. we saw the direct product doesn't work. what about a subgroup of linear maps where the group operation is composition? Oh, but we would need the group to be abelian so most linear groups wouldn't work. Memember we want a finite abelian group, which is isomorphic to product of prime powered groups. What about an elliptic curves? remeber they have about the same order as the field. If we use direct products we need to make sure they are coprime. To reduce our field size we could still encode the q as a direct product of all prime cyclic groups up until q is large enough. I think largest cyclic group will be no larger than 100. If we can implement the appropriate arithmetic then our field only needs to have size < 100. Maybe there are other representations of finite abelian groups via small cyclic groups. Eg, the multiplicative group of an extension field. it has size p^m - 1 for some prime p and power m. and apparently this group is always cyclic! only trouble is implementing the poly modulation, but actually this may be easier than what we tried before. of course p would be d. then we just need a way to compute an iso between modulo q and this representation of it. now modulus switching in this context could be tough. it means changing the order of the group, which might be done by changing the irreducible poly, but this may be feasible.


I should aim for a more theoretical result. Constant size proof for NP. doubly efficient. hopefully can say enough by subset sum that don't need random oracle model. based on falsifiable assumptions. but this result would be impossible. i think the catch is the soundness deteriorates with length so if we want certain soundness it can only prove certain size statements, so size is actually not constant. but in practice the soundness shouldn't deteriorate. 



lets make sure we can implement modulus switcing on the general group problem in order to do addition modulo d. note if we can do addition modulo d we can do multiplication modulo d by repeated addition.
suppose n1 and n2 are two scalars in [0,d-1].
n1*g + n2*g = (n1 + n2)*g = (n1 + n2)_d*g + d*{0,1}*g.
we want to shave off the right term. this means we want the d'th power of g to be congruent to 0. But that is not the case already because the power of g can be anything up to the order of G which is much larger than d. So we need to operate on the value.
suppose we scale by s such that s*d = #G, such that the right term disappears. but then the left term becomes s*(n1 + n2)_d*g. we will need to switch to a modulus such that s is coprime with the new modulus. then we multiply by s^{-1} to get s^{-1}*s*(n1 + n2)_d*g.
By modulus we actually mean group order, so what must the new group order be such that s*d = #G and gcd(s, #G') = 1. Before we had d=2, and we chose #G' = #G +- 4. To understand this pattern suppose we try d=3. Then s = #G/3. gcd(#G/3, #G') = 

we need to make sure modulus switching is ok in terms of the constants. just because s^{-1} doesn't exist doesn't I think mean it can't be performed via an affine function. after multiplying by s we have put our d-values into a subgroup of size d. i don't think there is any way to map them out except by changing the group (ie modulus switching). So what's the problem with changing the modulus? 



first lets try the simpler technique of relying on the prover to provide the overflow commit.
well clearly the prover could provide anything such that after subtraction the result is a valid commit. thus just accepting commit from the prover is not safe. what if we could force the prover only submit a 0,1 commit. Suppose what should be a 0 is a 1. overflow should not have occured but the prover said it did. this results subtracting d from a value < d which results in underflow. likewise an opposite error results in overflow. now the verifier holds a commit calculated from an invalid opening, and we'd like to show the prover doesn't know a valid opening. suppose we set up the equation, transfer the underflow to the other side so now both side have at most overflow, no underflow. further, the overflow is at most to 2d-1 on both sides. now since overflow and underflow can't occur at the same place, both sides of the equation are different. if we choose 2d-1 to be in our bound then we have found a collision. 
but how would we verify the commit to be 0 and 1? we could process it with a sumcheck at the cost of having to evaluate the poly at the end. note that evaluating the poly doesn't assume it contains only 0 and 1 coefs so our argument isn't circular. but this by itself is not helpful because it just generates another poly to evaluate. every reduction of evaluating two polys to one requires (more than) one addition, so if each addition introduces a new poly we achieve nothing. is there some way to batch? could the prover encode multiple overflow polys as one? I think this works for doing multiplication in a single round where the commit it can actually be any valid commit, but each addition requires its own overflow commit. so batching would need to happen at the verification level. given a set of commit one wants to verify that for each one there exists a set of 0s and 1s such that the dot products with the constants result in these commits. well if there exists a valid commit then it can be split up into at most d different subcommits each consisting of 0s and 1s. but the problem is we don't just need to verify the sum of the commits but each commit for itself, and knowing a sum opening doesn't imply knowing the individual openings. 
with modulus switching we could multiply by q/2 to set everything to q/2 or 1, but then we'd need to change modulus.


note that for poly commitments finding collisions is not enough to cheat. a prover must find a collision that evaluates at a random point to a particular value. we can think of this as finding satisfying a dot product with the random value monomials. note that these random monomials are given after the first poly is determined, so it not collision resistance but 2nd pre-image resistance. we could add an additional bit of security for this if the monomials were uniformly distributed but unfortunately they are only pairwise independent. but remember we will be evaluating multiple points, so these would add multiple security bits. this is relevant to universal hash functions but the the hash and what defines a collision isn't fully defined until the random points are chosen which occurs after the initial poly commit. 


what if prover just gives normalized commit, and verifier multiplies by s and compares with unnormalized commit? i suppose this won't work because the prover can give a value that is not normalized but it will still scale to be the same. the s values 0,d,2d,3d,...,(s-1)d all map to 0 when multiplied by s. so the prover can give the correct answer, or any d-offset of it and all will pass. Now whatever the prover submits, it will be interpreted as the final commit for which the prover is expected to known an opening. So suppose the prover knows of an opening x such that
answer = S(F + G)_d + n*d = Sx
n*d = S(x - (F + G)_d)
now n*d for all n is a set of s numbers that are fixed from the beginning, even before S is chosen. now it may not even be that there exists such an opening for these numbers. well actually there are s such values and the domain size is s*d so with probability as high as 1/d something will hash to this, so the prover only needs to try like d values before it works.
i don't think modulus switching will work because the entries affect the translation of constants between the moduli. 


Consider decomposing the addition H' into the two parts that correspond to indices that overflow (R,r) and those that don't (L,l).
F' = SF
G' = SG
H' = (F' + G') = S(F + G) = SlL + Sr(R + d*[1]) = SlL + SrR + Sd[0,1]
what we want to compute i s SlL + SrR
suppose prover provides S_r[1], that is just the sum of all constants at indices that overflow. if this is provided correctly, the verifier can multiply it by d, subtract it from the sum, then get the right answer. but this is a 01 commit which we'd have to verify as such, and we already saw (on iphone notes) that that comes out at net 0. 

if the prover submits the claimed correct answer and the verifier can confirm via manually opening it that an answer is correct, then we can replace the manual opening with sumcheck. 


we need a batch way to verify openings. 



what about making for a single proof composed of a lot of small proofs an amortized evaluation or opening at the end. 

W = AY rxn
Z = SC + Y 
AZ = ASC + AY = TC + W

Z = SC 
AZ = ASC = TC


v is length of commitment. l is how many commits to prove. 
n is about security parameter, which is about size of regular commit (except is 01).
challenge with be of size l*n
response will be of size v*n
so this only amortizes when the number to prove (l) begins to exceed the security parameter, eg > 80.

in our general model, the only foreign data send from offthread to onthread are commitments, and only data sent from onthread to offthread are challenges. we distringuish between foreign data and derived data. foreign data is assume to have high entropy. 
this is our model as we can prove its safe given certain conditions satisfied, and we can describe the two implementations using DLP and lattices. model can also be independent of computation model mostly, but we might depend a large amount on being able to reduce evaluating a poly at two points to evaluating at one point, but just like reducing two polys to one we discovered to be a special case, so may this be. so multilinears and related computation models may be special cases, eg sumcheck. 
can describe as oriented towards one long indefinitely long computation, as opposed to a circuit or set of constraints. even in distributed environment it can be considered one large ongoing computation. 
proof size between parties will be the 'base size'. 
call them sagaproofs. but how are these different from streaming, where space compleity is important. 
present independnetly the theme that both models share, eg polynomial based
abstract model can have zk as last step between parties
note how sagas can be forked and also joined. they form a DAG. 
use the word 'recursive' justified by how we use prepare to use an answer before we know what it is. 


note that we could reduce polys to a standard size. then we have more polys but of smaller size, so amortization savings goes up but commitment savings in compression goes down. 

GKR model can be thought of as splitting the witness into layers so only the top layer has the original data and only the top layer needs to be evaluated. would there be any faster way to give a source map of derived values back to the original data other than layering through the intermediate values? 
in general note the relationship of witness size to depth. depending on the uniformity and depth of the computation I think there is a linear relationship to show. many models, like the univariate ones, are at one end of the spectrum with everything in the witness, because they are made to take advantage of particular ways of processing large witnesses, eg pairings and low degree testing. 


I thought at the most basic level we need a way to reduce eval of 2 polys to eval of 1, but I think there is a more basic level. we need a way to amortize commits so we don't have to manually open them. like we operate on them homomorphically or we batch their openings. we don't need the definition to be reducing to a new commit because as long as we can reduce to manually processing less data, we can then further reduce to processing this new data via a new commit. 
we need a way to reduce manually processing a lot of data to only touching some data. 


still to decide how best to represent data with SIS. but assume we use the amortization technique. assuming we use poly based proof, we need to reduce evaluation of polys to openings. one way is changing the basis so an opening for a commit under the new basis corresponds to an evaluation of the polys at a particular random point. another possibility is using the committed values to build up larger coefficients and always identifying the commit with the corresponding poly. but how to reduce this to openings? idk
another possibility is using modulo arithmetic, but here the eval points would be in a small field so we'd do multiple points. to do the arithmetic we would need to verify openings to valid side-effect commits (we'd combine the 01 commits for polys into regular commits). the extractor needing a double bound I think is ok because we already need that for the amortization proof. 


for crypto require all algos including attack time poly in security param (?), and advantage negligible in security param. 

a function family has one input, a key, that defines the family, and turns it into a regular function. 



SIS concerns a large finite additive group. given uniformly random elements find a 'short', non-trivial, integer combination (positive and negative and identity powers) that sum to the identity. 
usually parameterized by n,q,\beta,m. 
can be thought of as SVP problem on q-arty m-dimentional lattice.


i think application computation would benefit greatly from a small field, especially to encode a lot of 0s and 1s. this probably means using the modular technique is more efficient. for fiat-shamir, we split the few coefs into bits and get something out of density 1 (make field large enough that there are enough bits for security). then we pass it in again (cuz of density 1), get something out and get about as much out as the amount of coefs that went in (density 1). these coefs are our multiple evaluation points. 
for sumcheck I think we just evaluate on random polys with as many coefs as we want iterations. we don't even need to evaluate the polys themselves, it just replacing a random point with a random poly. I can prove this works I think. Though it gives prover more work, poly should be multilinear to reduce degree. in this case the number of random points to evaluate will be a power of 2. suppose we have base soundness 2. then say we want to go to 2^128 so we evaluate at a poly with 128 coefs. that is, a log(128) = 7 variable multilinear poly.
suppose we want field size 32. then for soundness 1/2 we want poly degree max 16, which is enough for transcript size. now for total soundness 1/2^128 we need to evaluate at a poly with 128 coefs, that is a log(128) = 7 degree multilinear poly. 
we can try to make the model map-reduce such that we always have degree 2 in the sumcheck. 
now since over a small field we would need to replace the random combination of constraints with polys. this increases the degree of sumcheck polys. we can avoid the need to do this if we adopt a fully map-reduce style, were a sumcheck doesn't check a series of constraints but rather computes the value of a derived poly at a random point. at the end of a proof the verifier first wants the raw data about the claim, and wants to convert this claim into the evaluation of one or more polys at random points. if the claim was a single value (maybe possible, eg the hash of the real claim) then it could be the output of a sumcheck. but suppose we have a claim consisting of multiple values. well we could suppose that singletons can come from reductions, or groups from mappings. for a singleton use the sumcehck. for a group, we are given the resulting poly. we'd like a sumcheck that results in a poly rather than a single value. we could do this by doing a sumcheck over polys rather than values. or maybe best is that groups of elements come from transcripts, so for every transcript we reduce all the points of evaluation of that transcript to a single point for it. remember we can reduce multiple points at once with a higher degree connecting poly. 

maybe we use the constraint method with the basis change method over large field, and the map-reduce method for the modular arithmetic over small field. 

evaluate on poly
lattice params are hard to select, depend on hermite factor, and larger params seem necessary in practice than theory


reduce c, see 8

carbonine

you have an 2 sets of inputs into this hash that give this output iff you have a set of inputs that give this related output in this related hash


ab+c
000 -> 0
001 -> 1
010 -> 0
011 -> 1
100 -> 0
101 -> 1
110 -> 1
111 -> 1


we can compute anything with data, so we can compute any variant of a commit as long as we are yet to evaluate the data in the commit. 



use the placement of elements in a large hash in order to encode the information. have an encoding such that two valid encodings can be added
ask for a new valid commit. use the data from that to backwards compute the verifier computed invalid commit and see if they match. 

what about sumcheck with a single data poly at a single spot, such that verifier can infer the necessary eval result. suppose final poly from prover is incorrect. its supposed to equal the subject poly with a single variable. if its wrong, it will be spotted when identity testing. what's different here is, depending on the random point the verifier chooses for identity testing, the expected output of the data poly will be inferred without the help of the prover. I am wondering if this may be a substitute to randomizing a claim before adding, and simply adding. oh I think cheating can be done since after the exepcted evaluation points are determined the prover knows the sum poly that will be evaluated, and can split that sum into any two polys that respectively satisfy the expected evaluations. then the rest will go fine for the prover.
we are hitting the information theoretical ceiling and I don't think we can get much further without non-trivial crypto assumptions. 


since we have to use the commit values, and they represent sums, it seems the only appropriate manipulation of them still pertaining to commits is linear combinations of them. 


since we can do manipulations of a commit, suppose we do a random suffling of one. given the new commit of the supposed suffle, the verifier can make sure it's correct, and reduce evaluating the original vector to evaluating the shuffled vector. again, we're pusing the info theoretic bound. 

i'm wondering why its easy to calculate new commit with DLP but not with SIS. maybe the amount of buildup before amortization makes sense is a measure of complexity. maybe this is cuz SIS is NP-complete, like subset sum. if two could be reduced probabilistically to one then verifying two NP statements could be reduced to verifying one of the same size. what our amortization says is that we can only reduce a collection NP statements to a smaller collection when the original collection exceeds some threshold. 
notice how our use of subset sum (SIS) means we are reducing any NP satement probabilistically to subset sum. but subset sum is in NP so it can in turn be reduced to other NP complete problems. note how DLP is not NP-complete like subset sum and that's probably why two instances are easily reduced to another. 



maybe provers only need to keep track of two polys to merged. in small field one they will be randomized many times then added, and it is all these commits and all the modular-side effect commits the verfier will hold. naturall we'll have about as many randomizations as the security parameter so after this they will be amortized and so the threashold will be for two polys. so between parties we only need to pass two polys (instead of as many as the security parameter). actually its only the side-effect polys that will be amortized until the fixed constants. but these side-effect polys can be obtained from the two original polys by the prover so they need not be transported. 



an important concept is saga proofs is the entropy of foreign data. we know we can do transformations on the data at no cost. but 'merging' or 'getting rid of' data while preserving soundness is what's hard. we are looking to see if different data satisfy a property with respect to the commits. we want to reduce this to testing the property of less than the sum of both. so we combine the commits but the data could be made so that neither have the property, while the combination of them does. thus the combination of them must be randomized in order to make the data independent from each other. well now I'm wondering if we could achieve independence in another way, where we take many commits and the randomization is which we combine with which. 
in the case of polys, we're considering the property of evaluation at a particular point. or rather whether an opening exists. i want to say that if instances are independent then with high probability, if either is invalid then their sum is invalid, but that is not true for 01 entries because 1+1=2 (valid) is about as likely as 0+2=2 (invalid). but if the sum of the two are valid, I think we can say the two inputs are not too far off. we ask for a renormalized commit, and then use it to check that it represents in 1-1 the merged commits. now this new commit contains the same about of information as the merged commit, but less info than the original sum of both data sizes. 

for maximum information in a single vector using euclidean norm, we could allow any euclidean norm including negative values. and note that since beta < q and the square of additive inverses is a it should be, we can add them add them up after squaring, and keeping track of overflow all along we verify it satisfies the bound. but adding won't be ok because at the extreme two vectors could be nearly inverses of each other and both way too big, but their sum will be small. 

another way is we encode d values but 2(d-1) is still safe. then we check if the result has values less than or equal to 2(d-1). for 01 vectors, the only way to cheat is to encode 2 values and hope the line up with 0 values. maybe we could have 2 values be benign for the verifier. if a 2 value is encoded, the prover may fail, but if not, it will simply be translated into the wrong value for computation.

if we just use 01 coordinates we are vector is always pointing at a side of the hypercube. 

remember verifier can negate (or generally shift) a vector without prover help.


suppose we use the method from the paper and the parameters work out. how do we do zk? suppose the last proof has finished. one way would be to put an additional proof at the end that transforms (no merging) each commit to evaluate into a related poly to evaluate with same soundness but zk, probably via masking poly, or maybe via random generator. this would be a general method for any system. the other possibility of doing the masking offthread seems to depend more on the commitment scheme. 
for blinding the polys I think can just generate pseudo random polys, figure out the eval of that poly at the random point, add that to the expected eval answer to get the new answer. somehow must compute the commit to the added polys. 

maybe have number base system where a number can be represented in bits or up to 2, and adding bit representations of two numbers results in the 2 representation of the sum. if the 2 rep is easily computable from the bit rep, then one can be given two bit reps, add the commits, ask for the bit rep of the sum, then compute on it to verify it represents the same number as the 2 rep of the committed sum. 


we will suppose the zi are positive, so that upon subtraction we get positive and negative but still with bound B instead of 2B. this is ok because this is only the theoretical reduction that uses negative values. assuming positive values shouldn't have cost on the reduction because if we assume prover can do it we can use that ability in the reduction just like the other. 


SWIFFT paper says subset sum from 1024 to 512 bits is intractiable.
note I think to compress output of subset sum we could add up bits


make amortization l equal to the field size so when multiplying by challenge, responses are already in correct poly form, and thus commitments to them need not be sent to verifier, beause verifier calculates them by challenge and old commits. then the verifier doesn't need to calculate using this data, only to check that is a valid opening. but we started with needing a valid opening to bit vectors but now we'll need valid opening to field vectors. but I think we can just reduce field vectors to bit vectors with the amortization still making sense.




I'd like prove my own amortization method. without zk. suppose the commits are bit vectors. 
the idea is we need randomness to operate on the commits, but only operations we can do on commits that result in almost-valid commits are addition of commits. therefore to have enough randomness we need enough commits and we will randomly decide which commits to add together. we will add them together in groups, maybe a commit appearing in multiple groups, such that the number of total commits is as few as possible for sufficient soundness.
suppose we have l commits and we make k groups completely at random. we can do this with a random challenge matrix C of dimensions l*k. 
ASC = TC
suppose A, S, and T are fixed. we'd like to say that if prover P has noticible probability e of passing the test with probability over C and its own randomness, then there is a ppt extractor that can use P to extract semi-preimges for all T. 
consider two random combinations and suppose we subtract one from the other. any commits that appear in both will disappear.
suppose we have two combinations that differ only in a single bit. and suppose there are valid z',z'' that open them. then subtraction gives the right side just t_i for differing bit i, and thus z'-z'' will give a semi-preimge to t_i. So a goal could be to obtain solutions z_i' and z_i'' for any two challenge vectors that differ only in bit i. the way the other paper does it is extractor E samples challenges until it finds one, as its exected in do in time 1/e. then it samples only the i'th challenge again until it aborts or finds one for which it succeeds that is different than the first i'th challenge. 

just consider a single challenge c. to pass all l challenges with noticible probability e, it must pass an individual challenge with probability e^{1/l} > e. what is probability we can sample two successful challenges that differ in a single bit, in particular bit i? well it takes expected 1/e^{1/l} time to generate a successful challenge. 
consider the space of all possibilities for success which consists of all possible challenges and all possible randomness. at least e^{1/l} of this instances are successful instances. we can put this space in matrix form with two columns, one for each possible bit i. an entry is filled if the instance is successful. we'd like to know how many rows have 1 in both columns. then we want to know the probability the extractor can sample one of these rows. 
the matrix contains e^{1/l} filled entries. 
suppose there is k random bits. there are 2^{k + n-1} rows and 2 columns. we say a row is heavy if at least a e^{1/l}/2 fraction of the entries are filled, ie it contains >= 2 * e^{1/l}/2 filled entries. now an e^{1/l} fraction of all entries in the matrix are filled. we have the lemma that at least half the filled entries in the entire matrix appear in heavy rows. thus e^{1/l}/2 of the filled entries are in rows containing 2 * e^{1/l}/2 filled entries. suppose we want heavy rows to contain at least two filled entries. 


try again
supposedly out of all possibilities of challenges and prover randomness, an e fraction of possibilities are success.
e > 2^{-s}
if we are going to use the lemma, we need the number of 1s in a heavy row to be at least 2. if there are j columns then
j e/2 = 2 => e = 4/j => 2^{-s} = 4/j => -s = log(4)-log(j) => log(j) = 2 + s => j = 2^{2 + s}
the strategy they use is to find any 1 in expected time 1/e. then to search for another 1 in the same row. if we set the abort time appropriately, then if the row is not heavy (impossible to find another 1 in the row), we will abort, while if the row is heavy (has at least one other 1) then we will probably find it before we abort. 
there are e 1s in the whole matrix and at least e/2 half them appear in heavy rows. this means that when we find a solution in a row, with probability 1/2 we are in a heavy row. 
how efficiently can one find a second 1 if the row is heavy? there are j samples in the row, >= 2 of them are 1s. but one of those 1s is the solution already find. so only >= 1 of the samples will be a valid solution. so there are j = 2^{2 + s} samples, and only >= 1 of them is valid. if the prover were to sample the columns of the rows directly, the probability of sampling the valid one is (1/4)2^{-s} < (1/4)e so this wouldn't help. 
but if there are only 2 valid samples as we assumed above then j e/2 = 2 => e = 4 / 2^{2 + s}  = 2^{-s} which is worst case. instead we need to keep in mind that e can be much more than 2^{-s}. so there are not just at least 2, but at least j e/2 > 2^{2 + s} 2^{-s}/2 = 2 valid samples, but we don't substitue for e, but instead keep 2^{2 + s} e/2 = e2^{s+1}. thus after subtracting the already-used solution, there are e2^{s+1}-1 valid solutions remaining. from 2^{2+s} total samples, we have probability (e2^{s+1}-1)/2^{2+s} = e/2 - 1/2^{2+s} probability of landing the second success if we sample uniformly (manually using the prover randomness). so if we iterate v times we fail with probability (1 - e/2 + 1/2^{2+s})^v. choose v and simplify such that this becomes negligible. this is the only constraint needed on v if v will be fixed (maybe a function of s), because if the row is not heavy (which only happens with probability 1/2) it only matters that don't execute too many times, and a fixed number is ok, since we're not expecting a solution anyway.
so we land on a heavy row, we'll succeed with negligible error. if we land on a not-heavy row, we'll abort in reasonable time. both of these scenarios happen with equal probability 1/2. thus if we execute s times we'll fail to hit a heavy row with probability 2^{-s}. 
now we've said all this without saying how this matrix is formed, what the rows and columns are. a bit of a complication is that the matrix represents all possibilities, which accounts for all possible randomizations of P. but upon an implementation P will have fixed randomness. oh, but I suppose we use P only to find the first solution, so its used as a subroutine over and over (I assume we have unlimited randomness to invoke it so many times, giving fresh randomness each time). what we have so far is a general method of using an ability to cheat to find two 1s in the same row to yield a solution. maybe we could extend to finding more than two 1s in a row. 
we could reduce the number of columns at the cost of requiring a larger gap between e and 2^{-s} but i don't think that's considered safe.
does it matter what ratio the dimensions of the matrix have, as long as the columns satisfies the lower bound? no matter how many rows, more than half the 1s will appear in heavy rows, so P will succeed with probability 1/2. No matter how many columns, at least e/2 fraction will be 1s, but will the amount beyond e/2 depend on the number of columns? the absolute amount matters but I don't think the fraction matters.
note that we could say the same about heavy-columns, and landing in an entry located in both a heavy row and a heavy column then happens with probability at least 1/4. 

if we're going to solve SIS without granting slack, we need to find a set of challenges that satisfy an additive equation, eg c1 + c2 = c3. but the problem is nothing prevents us from finding solutions that add to 0. but there are enough solutions that don't add to zero that we could maybe probabilistically sample until we find some.
suppose we start with 
c1 = ab
then we find c2 = Ab and c3 = aB
aB + Ab = (a + A)(B + b) = AB + ab
so it appears we'd need to find AB too and thus a square of solutions, not just a triangle. 

But note that if we used the triangle we would find a solution for AB = aB + Ab - ab which is closer to random than the initial commit, so this would be almost as legetimate as ISIS. how random is AB? suppose a separate party could distinguish it from uniform. then ...

if we want j*e/2 at least h then j e/2 = h => j = 2h/e = 2h2^s = 2^{s + 1 + log(h)} so the number of columns we need only grows logarithmically with the number of 1s in each row. the fraction of e/2 remains the same but now there is a higher lower bound in each row. so there are >= h in each row and we have a probability of e/2 - u*2^j of hitting one having already hit u of them. for any constant u we could do this, even if u is a function of s. 
so suppose we can hit heavy rows and columns and extract a constant number of hits from each. what could we do with this ability? this doesn't help us find a square because we can't claim anything about the distribution of the 1s within the matrix. keep in mind the density of 1s doesn't grow. but also note that we are free to permute the matrix columns and rows. for example, suppose we permute such that all heavy rows and columns appear at the top left corner of the matrix. then this submatrix contains half the 1s of the entire matrix. now assuming nothing about the distribution of 1s in the submatrix, maybe we could make a pigeon hole argument about how many squares there are. But remember finding a square is not enough as it may yield a 0 answer.
suppose we could lower bound the number of SIS solutions for our parameter choice. 


just thoughts about multilinear maps for multiplying commits.
f(a1*x1 + a2*x2, b1*y1 + b2*y2) = a1*f(x1, b1*y1 + b2*y2) + a2*f(x2, b1*y1 + b2*y2)
= a1*(b1*f(x1, y1) + b2*f(x1, y2)) + a2*(b1*f(x2, y1) + b2*f(x2, y2))
= a1*b1*f(x1, y1) + a1*b2*f(x1, y2) + a2*b1*f(x2, y1) + a2*b2*f(x2, y2)
f(a*x, b*y) = a*f(x, b*y) = a*b*f(x, y)
what if we make f such that it vanishes on the n^2-n mixed monomials. or even take the special case that we plug in the same value to both inputs of f, so it vanishes on (n^2-n)/2 = n(n-1)/2 values f(xi, xj) i!=j. we'd want it to output 1 when xi=xj. oh but of course we can't do this cuz f will only have 4 interpolation points if its of 2 variables and multilinear.
the number of interpolation points we can do with f variables is 2^v. in general if we do degree d we can interpolate (d+1)^v points. 
we need to take advantage of how multivariate polys can have many more roots than their degree, and we need to figure out what pattern these roots can have. we want roots of the form xi^d1*xj^d2 for i!=j. if we consider bivariate we think of it as a uni poly in x with coefficients as uni polys in y of same degree. actually we should think of it as a uni poly in x but in root form \prod_i (x - coef_i). the coefficients are uni polys in y. so every y poly is a map from the y value to an x value such that f will vanish on that x and y.
the total max number of roots is d^v. the total number of monomials is n^{d*v} I think. 
for bivariate, total roots is d^2. for monomials, total is n^{2d}. so the min number of terms we can leave non-zero is n^{2d} - d^2.
so the basic problem is the number of monomials is exponential in the degree but the number of roots is polynomial in the degree, so the roots can't keep up with the monomials. 
of course composing polys just results in another so that won't help.
actually the about count of roots is the number of custom roots. but i'm looking for patterns. for every value of y there is a set of x roots so that f vanishes, namely the x roots are the outputs of the y polys on that value of y. 
now the problem is that our monomials are composed of constants which have random relationships, so there is no pattern to them. 


notice prover who has knowledge of the small coefs can quickly compute the modulo of two commits by adding the commits, adding the coef vectors, seeing which overflow, then subtracting the relevant constants. Actually prover will just add together the relvant constants and give to the verifier as a commit to the side-effect poly. this, however is still linear, but will probably only take about half the work. for the challenge the prover can compute z quickly, but will probably need to re-represent it as a commit to a 01 vector, unless we want to use it as a data poly which requires more multiplications. 


consider the univariate sumcheck from aurora. prover sends commits to f and h, maybe g. then checking f sums to zero over coset can simplify to making sure a coef of h is zero, which can be done by adding q times the releant constant to the commit. then verifier must evaluate each for identity testing. maybe we could reduce identity testing for multiple groups by making a random linear combination. the addition cost is computing polys squares to explicitly compute g, h. but the benefit is simpler fiat-shamir. maybe just take the commit itself (consisting of multiple q values) and use those q values as random coefs, and construct as many identity tests as these q values (make it the security parameter). reducing identity tests requires addition and multilication so we will support those in the same way. in this case I think we'll require prover resonses to challenge to be encoded as 01 so they stay in the amortization box. 
actually checking the low degree seems like it requires multiplication by q, not addition. hmm, could we still make this work? maybe the verifier expects the coef to be zero, then adds d-1 to it to make it the maximum allowed coef. actually this wouldn't work cuz not matter how much verifier adds, prover an make it to after addition its a valid coef. but verifier will need to check they have valid openings, which is not required in the other model (which only requires 01 openings). another problem I'm realizing now is that we want a small field if we are to do modular arithemtic on commits, but univariates will require large field. so we need to rethink if this simple univariate 'sumcheck' can be helpful. I suppose it might be appropriate for the elliptic curve model. again the cost is quasilinearity, but the benefit is less fiat-shamir which in turn might minimize the number of times a proof verification gets tossed between the fields.
prover submits poly commits. as before, we must take this commits and generate a random value in the other field (can be done with subset sum). then we exponentiate and add. it seems this could indeed simplify fiat-shamir, where we no longer need to consider the intermediate sumcheck values, nor reducing two points to one, both of which require compressions, whereas reducing two polys to one does not. as for verifying g is of low degree, I realize we could just enforce the highest constant used, which will be preserved because we'll add linear combinations of g polys. field size is huge anyway, so it suits this. a proof in field q has commits in field p, and first goes as bits into field q where the subset sum will be used to compute a random q value. then the commit in regular form and q value in bit form will go to field p where exponentiation and addition will happen. that seems enough so thats only 2 tosses, whereas if we were in one field we'd do 1 toss. 
remember this didn't work for QAPs because it would need to be linear. so here too, we will need to actually evaluate at a random point, that is get a claim for a point. now unfortunately to reduce two polys to one we will need to reduce two points to one after all. this still saves the sumcheck sum fiat-shamir. so we will need compressive subset sum. but I think with such a large field size we may indeed be able to compress enough, we will need to compress 2log(transcript_size) elements to like 3 or so. oh also remember QAP didn't work because the polys aren't transcript polys but a different kind. the stark model, however, would work. we still need to figure out how to best represent constraints via vanishing points of a uni poly on a coset. of course the uni poly will be composed of others including the transcript. oh, and our ability to map-reduce won't exist and we can only execute one sumcheck, because the transcript poly will have to be evaluated at multiple pionts, and if we reduce those points to one point we shift into multivariate polys which can't be shifted back to univariates. one round makes sense too because we can't use the univariate sumcheck to evaluate the transcript because the output must be zero, so its suited for constraint checking anyway. as a result our transcript will not just contain original data but the whole trace.
now as for how to encode the circuit in the uni sumcheck. note we can use multiple virtual polys and as long as we evaluate them at the same place it can reduce to evaluating a large one at one one place. what aurora does is basically QAP structure, and uses sumcheck to check the linear relations. 
oh one important observation is the univariate sumcheck relies on summing over cosets, but we are in a large prime field for the ECC version so the additive group has no subgroups and thus no cosets. so I don't think we can do QAP/aurora structure unless we find a way to test the linear relations in a prime field. the other possibility for univariates is stark structure, but I see no benefit to that, and the more non-uniform the computation the more random elements we must select and the more poly reductions we must make. so I think this means we return to GKR style for ECC. but maybe for simpicity due to the large field, ie to avoid doing many sumchecks, we do the constraints based one instead of map-reduce. this means not following the idea of only commiting to initial data. but this can be a benefit to show how other ways, though not optimal, also work. the decision depends on optimising and how difficult it is to do map reduce verification in a circuit. 

and remember in order to have field compatibility we're planning on something like the multiplicative extension field group for the constants, and doing this together with the sublinear knowledge protocol requires that the matrices remain associative. this is indeed the case. 

note that reduce two polys at two points to one point can be done in one step by the prover sending the line connecting the points and the verifier using this to generate both the random point on the line and the random scalar. this is because previously we were going to use the output of the response on the random point to select the scalar, but the output is determined by the random input to the response which is determined by the response, so the scalar might as well also be determined by the response. oh, but note there will be two responses, one for each poly. both must together determine the common random input, but both can also determine the (single) random scalar. 

maybe we can justify properties of subset sum by SIS with n=1 and large q. 

regarding turning multivariates back into univariates, the whole idea of replacing two old variables with a connecting expression is flawed because the expression will be evaluated in one place and since its a function it will only yield one value, whereas we need it to yield two values.


Libra
zk is basically the insight that we only need as much blockage as evaluations the verifier sees. 


maybe do like lambda calculus, where our gates don't take two inputs but only one input and combine it with something already there. so for eg multiplication, we first compute the vector containing all left-inputs to the table. this is just a copy operation. we could even have it multiplying a constant vector. whats important is that they multiply component wise. then to get the final values we multiply this resulting component-wise by the initial vector on its right-inputs. in this form our wiring predicates will be multiplying the initial vector in both cases, and in both cases will only be with 2*s inputs, not 3*s. in fact the constants vector could be outside the sum. we could also have an added constants vector.

can we do recursive programming on a vector? we could indeed make new values from existing values, but at the end we'd have to evaluate at a random point which is itself, so circular. but maybe we could do something like have a single very long poly and evaluate at a random point partially. the goal would be depth-invariant verifier time but I think the only way to do that is by copying. 
another goal worth changing for is convenient code representation and organization. library function support would be great. of course variable length data, maybe using commits as pointers, would be great too.
but of course recursion is still desireable and I don't think regular circuits can do it, and we will rely on our code organization to make it possible.

what if a prover was implemented in a proof? then if we could verify the proof its contained it, we could know that the prover didn't cheat in proving its own proof. 

for zk I think we use mask poly and reduce to both data poly and mask poly evaluations, but then we can combine them into a single poly and that is the poly sent to the next prover. now do we actually have to send a poly? i think so, because otherwise not just the prover but the proof would need to be convinced with a sublinear argument. maybe the overehead of sending will be the same for ecc and lattice because ecc is only one poly but its huge, about the size of security parameter, whereas lattice is as many small polys as the security parameter. well actually for ecc we might be able to make the poly of small coefs. 


for SIS we can find regular solution to SIS in q. then we translate to a solution in the interval [-q/2,q/2]. then we move the negatives to another side, and get a collision with positive infinity norm q/2. thus we need positive infinity norm < q/2. 

well it looks like highly compressive infinity norms won't work. we can still use infinty norms with a hash tree, or we use beta norm and compressive using position information. but the question is how to reduce claims about evaluations to claims about opening. the only valid idea I had was modulo using infinity norm, and using opening to prove valid over-flow vectors. 
how else could we manipulate commits only relying on valid openings, not considering them as polys?

i think we should define sagas as reducing a 'claim about the external data', and merging these claims. its not enough to just say 'the claim exists' and I think its maybe more than enough to say 'the data exists and it has a pattern' cuz we say for overflow vectors we only need existence. So in general its just a claim. Now it happens that with a commitment scheme we will also need the prover to have knowledge about the data, so the claim about the data should include the specific condition that the prover has knowledge of that data. 
in the poly case, our claim is the prover has knowlege of a vector of coefficients satisfying the commit, they they evaluate as a poly at a particular place to a particular value. for lattices we can reduce this claim to the claim about knowledge of 01 commits. 

suppose we can do params for SIS hash so we get enough compression with bits. what can we do with that? the previous idea of transforming the base then testing for an opening doesn't apply like I thought because the opening would consist of the random powers which are large.
in fact, in order to apply the amortization we need it to be hard for about infinity form of more than the security size. suppose we work out the parameters for this, which I think should work asymptotically. then we might as well have all our basic commits with this norm which will be high enough for our purposes to use for modulo. so I think we can construct a proof system that works asymptotically. 


for SIS commits, what if we do a transformation of an input to one under a different base, perhaps even newly randomly generated by the verifier. then the verifier is only left to verify under the new base. the verifier takes two commits, each from a different base, and we'd like to join them to be under a new common base. suppose we just add them. then the new common base could be the addition of the two previous bases, which remains random. this new format might help us reduce some of the data. but now i'm struggling as to how we'd do that. maybe continually transform this combined data into a form such that the verifier can efficiently reduce the data by itself. 


i think we just go ahead with the impractical but asymptotically meaningful lattice construction. 

we need a proof of the root-relation-intractability of subset sum or the SIS hash. i think we can reduce the problem to finding inputs for subset sum, high density (not one way necessarily), such that it yields a root to the poly described by the inputs. in reality the poly will be another related poly, but first lets try this. for now lets consider the case of sumcheck where we're considering a low-degree univariate poly. for simplicity lets express our poly in factored form, so the inputs are scalars, some of which are roots. then the goal is to find inputs such that the output is one of the inputs (a root). consider the further simplified problem that all inputs are roots, and output can be any input. maybe even consider the further simplified density 1 problem of finding an input that yields itself as output. notice the representation of input and output is different. what's important about this relation is that its a permutation (identity considering the relationship of the representations). if we can prove for a permutation, lets try to prove for a non-injective relation but one where every target has about the same number of sources. actually we should consider this first because the case for density 1 likely has no solution. 
suppose with noticible probability under the choice of constants and coins, one can find the identity relation as above for k inputs in binary form (subset sum). use this to solve subset sum. I think we'll need to do this by constructing our own constants related to the relation and the instance of subset sum we are given, and hand that new problem over to the subroutine.
consider the equation with the constants and the input in bit form on one side, and the powers 2 and the same bits on the other side which are equal to the answer. then consider the constants for the input that will be the output. subtract the powers of 2 from those constants, yielding new random constants, but still multiplied by the same bits. then a solution to the resulting subset sum (target 0) solves our problem. but for a particular subset sum we may need more randomness, more options, to make sure the noticible probability has opportunity to show. above we only have as many options to change the problem as we have there are inputs (depending on which input is the output), though I suppose we could multiply by random values, but then ratios remains the same. we need more instances that we can take advantage of, and they should be uniformly distributed. lets just go straight to the final goal of unknown poly relation, because that may offer opportunities not seen here with the idenitity relation. 
so there is the invisible poly, then there is the submitted poly, and the relation is the submitted, input poly to the roots of the difference of the two polys. so really the relation is determined by the invisible poly. 
maybe we can replace the powers of 2 with any constants that map the input (a source) to one of its targets. then again we subtract one side from the other, factor out the common inputs, and we have solved the problem for target 0. notice we can do this for any coefs, not just bits. the condition is that for random constants and the set of coefs, its hard to solve the instance. 
the problem is we need the relation to exist before the constants. or else maybe the prover could choose the initial poly such that the relation has a certain relationship to the constants. like the prover could just plug in a poly to the function, see what comes out, then commit to any poly that agrees with the input poly at the point that comes out. maybe we should plug the commit into the hash as well. this might be equivalent to a choosing a random target from the commit. 
or what if we use the commits to pseudo-randomly generate the constants, such that the hash is chosen after the relation is established. the constants would not be totally random, but the ability to solve the instance for pseduo-random constants and not random constants in noticable time would contradict the indistinguishability of the pseduo random number generator. so this argument works as long as we can prove we have a pseduo random generator, and also that if the constants were truly constant and chosen after the commit then the relation is intractable. 



sublinear zk argument paper
p: prime for field of circuit
N: number of gates
\lambda: security parameter
n: number of \Z_p elements in one commit.
q: ambient space for commits.

commit to N values in \Z_p. total size of commits and proof must be sublinear in N. 
hiding matrix is of size r * 2r\log_p(q)
commitment is t = Ar + Bs
so we commit from n elements of p, n\log_2(p) bits, to r\log_2(q) bits. 
set n = poly(r)
communication complexity (total size of commits and proofs) is O(\sqrt{N\lamda\log(N)}).
do this by creating N/n commits, each committing to n values, so total values committed is N/n * n = N. (I think n will be \sqrt{N}).
make a matrix of the \Z_p values (they have 3N total but only N distinct), making a commit for each row (eg N/n rows, each of length n). 
in terms of element of \Z_p, the space for committing the N values will be N/n * r\log_p(q)
they say we can set \log_2(q), r = O(\log_2(N))
they set n = \sqrt{...}

negligible means about 0
overwhelming means about 1
[N] means {0,...,N-1}
use rings \Z or \Z[X]/(X^d + 1), d power of 2
consider norm of poly rings elements as norm of coefs
use quotient ring R_q = R/qR for odd q
norm of element in quotient ring will be norm of unique representative in [-(q-1)/2,(q-1)/2].
operator norm of matrices defined as s1(A). 
they say we can reduce columns of random hiding matrix to make it computationally hiding based on LWE

make sure binding euclidean bound \beta satisfies \beta < q (can justify ignoring this), \log_2(\beta) < 2\sqrt{r\log_2(q)\log_2(\delta)}. they set \beta < N^2p^2, p < N

two sets of constraints: multiplications, linear (additions and multiplications by constants)
embed constraints into poly equation over GF(p^{2k})
then argue for satisfiability of equation
also construct product argument of commits for k commits over GF(p^{2k})

first try embedding into \Z_p then over GF(p^{2k})
N = nm multiplication gates, labelled from 1 to N
inputs and output connected to multiplication gates
mult gates arranged into 3 m*n matrices, A,B,C for left, right, and output wires respectively.
do linear constraints too keep track of wire duplication
also add linear constraints for additions and constant multiplications
somehow transform the latter linear constraints to the form of the former linear constraints so they in terms of entries in A,B,C. 
want to prove entry wise product A.B = C as well as the linear constraints on them. 
can standardize expression of the linear constraints. U such constraints.
in total N + U constraints.

reduce the N+U equations to two poly equations in Y
distinct equations will be embedded into distinct powers of Y
suppose Y is (Y,...,Y^m)
Y(A.B) = YC, so each power of Y multiplies a row. that is, \sum_i Y^i(a_i.b_i) = \sum_i Y^ic_i
now (a.b)Y' = (a.Y')b so it turns into
\sum_i (a_i.b_i)Y'Y^i = \sum_i c_iY^iY' =>
\sum_i a_i(b_i.Y')Y^i = \sum_i c_iY^iY'
(don't exactly know what Y' is)
do similar for linear constraints, making them values of coefs of poly in Y
so we end up with two poly equations. note they are univariate.
now reduce testing these poly equations to testing related polys for certain coefs being 0.



for sumcheck, the prover sends the univariate, verifier hashes it to r, then next round suppose prover sends correct answer, such that verifier is comparing correct poly at r with different, submitted poly. 
suppose we use regular hash like SHA. why not first round, prover chooses poly that on 0 and 1 sums to incorrect answer. prover hashes it and sees what the evaluation point will be, then commits to a different poly that agrees with it at r. i think we just need the commit to go into the initial hash along with the submitted poly. 
doing this means the relation is not predetermined (while the hash may be). or like said before we could use the commit to generate the hash so the relation is pre-determined but the hash is not. 
think about how expressive our map from inputs to a target can be via an inner product with constants. first notice we can map input bits to powers of 2 then add the results to convert them to natural form. then we can multiply each of these numbers in natural form by a constant then add them. what we can't do is multiply them together. so basically we want to linearly map a source to a target. of course this was easy for the identity relation. 
imagine a matrix equation Ax = b where x is the hash, b is the vector of targets, and the rows of A are sources. if we knew the relation, A,b, then we could solve for x, though the height of A and b would vary. 
for a hash, consider the subset of relation instances that hold with respect to that hash. note that each source can hold with respect to at most one target since the hash is a function. note that the function is almost-surjective, and almost-uniform. also note that the relation is about k to k for small k. almost certainly I think there will be a solution. We want to say that such solutions will be intractable to find. Maybe we can say there are few enough solutions and they will be distributed uniformly, such that solving one is like picking a random target and finding not just any pre-image for it, but a source, so its at least as hard as inverting the hash. the difference is mutiple targets can be chosen, but what we need to prove is that only a few targets will yield solutions. It seems subset-sum or the SIS hash is not relevant to the analysis and we simply frame it in terms of functions and relations with certain densities and structures, one-wayness, etc. Then we invoke that subset sum and/or SIS have certain properties so they satisfy the analysis. 

does it make sense to accumulate hashes in the fiat transform? while it may be that won't need to, I think we should, as it captures better the idea that each execution the randomness is different. i realize we would need to hash the commit on every hash on every round anyway, so it just makes sense for the commit to start a chain hash, with the inputs to every hash being the submitted value together with the output of the previous hash.
Note that considering the commit as a subset sum hash, we are making a hash tree. 

what structure does the relation solve? if the relation was two polys as sources and their points of agreement as targets maybe we could give some structure. But we are planning on the chain hash, so maybe we can justify thinking of it as a relation from just one poly to a point of agreement with another hidden poly, which will determine the hash. the 'last hash' (which is connected to the commit which is connected to the relation) can 'determine' the hash in two ways. one is it generates the random constants. another is we hash it with pre-deterined constants, but bringing it to the other side, its like it randomizes the target. for the abstract framework we don't want to consider these details, and I think we can safely say the relation is chosen first, then a random hash is determined from it. since we hash a poly representation to points of agreement with another (invisible) poly, maybe we can take advantage of some of this structure to describe the relation. 

where else will we use the transform besides the sumcheck? another place is reducing from two points to one. line is implicit. prover sends line poly, verifier hashes it to choose point on line. then we do identity test. this is again instance of prover submitting a poly that will be used for an identity test, so its exactly the same as for sumcheck. there is also the case of two points on two polys to one point, where prover sends two polys, verifier hashes both to choose single point of evaluation for two instances of identity checking. i think this is equavalent to two of the regular instances given before, but now the relation is with respect to two polys cuz they are both hashed together. 
the other instance we use the transform is from two polys at one point to one poly at that point. this is the random linear combination test, which is equivalent to a degree 1 univariate identity test where as usual one poly is invisible and the other is submitted. so I think we can focus on this general case.

suppose we have a fixed, invisible poly. when a submitted poly changes, how does that affect the point at which the two polys agree?
if the two polys share a common root, they will agree there with answer 0. suppose consider the difference poly, factor out all terms (x - ai) where ai is a common root, such that we are left with two polys of the same degree. take the difference poly and consider its roots. 
for simplicity suppose our poly is quadratic so we can use the quadratic formula.
roots are (-b \pm \sqrt{b^2 - 4ac}) / 2a where the variables are the differences of the coefficients. so now we have a mapping from representations of the two polys to the points where they agree. but actually our inputs to the mapping will only be one poly, the other one fixed. what we wanted to do was make this a linear map. this won't work. 

what is the prover's noticeable probability over? I think the relation is fixed, and the probability is over the hash and the coins? This is analogous to the lattice case where S is fixed, and randomness is over challenges and coins. So we can use the noticeable probability to find several solutions. How can these solutions for a fixed relation help us solve a problem? given the problem, we need to relate it to a relation, then generate 'challenge' hashes for the prover. our relation should be of the poly agreement form. 

our goal is to reduce correlation intractability to pseduo random generation. 

they do intractability first assume intractability against the relation of decryption. then the make it more general by taking the input, homomorphically computing the relation output, then passing that through the original hash. then if one can hash to a relation, which is equal to the decrypted input, then one has managed to find an input to the original hash that hashes to its decrypted value, which we assumed was not tractible for the original hash. now, the exact relation is unknown, so they can't actually evaluate it with the exact circuit, but rather use a dummy-circuit. the justification is a dummy circuit results in a dummy ciphertext that should be indistinguishable from the real ciphertext. if it worked for the dummy then they would be distinguishable. 
if we follow the same pattern, with out original hash being intractable against the linear relations, then we need to transform the input to something (analogous to encryption) such that a linear transformation of that something yields targets for the original input.
for input x compute transform f(x). Suppose target(s) r'(x) can be expressed as a linear function of f(x). H'(x) = H(f(x)) = r'(x) => f(x) and r'(x) are an instance of a linear relation breaking H. now we don't know r'(x) and we might not even need to know the linear function from f(x) to r'(x) if we can design f such that a linear function always exists. 
we are putting a transform on the input before it goes into H. could we also put a post-transform on it? the goal is to reduce finding a relation of H' to finding a relation on H. for general H with certain intractability, what restrictions must we put on f and g to ensure finding a relation R' instance on H' implies finding an relation R instance on H?

let R and R' be the relevant relations.
if (x R' H'(x)) = (x R' g(H(f(x)))) (that is H' is broken) then we need (y R H(y)) for some y (that is H is broken). 
--
suppose g is identity. x R' H'(x) => x R' H(f(x)).
f(x) R (x R' _)
--
suppose f is identity. x R' H'(x) => x R' g(H(x)).
x R g^{-1}(x R' _)
--
x R' H'(x) => x R' g(H(f(x)))
f(x) R g^{-1}(x R' _)
--
notice on the right hand side we have sets, so we mean for each element its related to the left side. 


gosh, I had dreams about this last night, and they kept waking me up in a frightful way. I was thinking how there always exists a linear transformation from inputs to arbitrary points, like the points of agreement. but i was worried the linear transform would need to be fixed for all inputs. Is that true? Suppose, given two polys for input, one calculates their points of agreement on the side, then find a (possibly different) linear function from the inputs to each of those points. this could be trivial is what I dreamed about, where you simply take a non-zero input and multiply it such that it results in the desired output point, multiplying all other inputs by 0. then one has the equation with right side as inputs multiplying constants, left side as inputs multiplying some chosen constants (most of them zero) such that it equals a target for the input. then take the difference of the sides, and we have solved the hash for target 0.
oh yes, the trouble is the linear function must be fixed for the reduction to work. this is because the prover has noticable probability of finding an instance, but we don't know which instance, and the linear function depends on the instance. thus we must find a linear function that works for many instances to give the prover opportunity to all those instances. we could choose multiple linear functions, each working for a separate set of instances. so how many instances can a single linear function cover? I think very few. 

a similar approach would be the same constants on both sides of the equation, but different inputs, such that upon subtraction one has solved SIS. this is harder, however, because prover must describe desired output by a valid input (different from main input) on the constants, which means finding pre-image for desired output. this is not convenient.

what if we mix both, where the left side has different constants and also input. the constants would be fixed, powers of 2. since these are fixed and not random, the constants for the solved problem are fixed and remain random. an instance would be found, then the input to the left side would be the bit representation of that instance. oh, but then subtraction doesn't work because neither constants nor inputs can be factored out. 


well suppose we were given the two polys as input. we could calculate the roots of their difference (idk how efficiently), ie their points of agreement, then pass those points through the hash. 
we could pass to the hash anything (f) such that each point of agreement can be described by a linear transformation of what we pass. but this doesn't seem much help because because whatever f creates, it could just as well append that linear function at the end of its computation anyway. 
what about post-transformation (g)? the condition is for any point of agreement, the pre-images of that under g must be a linear transform of the input. Not sure how to use this, and I think using f may be sufficient if the current idea holds.
notice that we don't have the need for a dummy-relation due to not knowing the exact relation. given the inputs, we can compute the exact relation. 
now our transformation (f) need not have an explicit form. we could say that given inputs it finds their points of agreement. now how we will do this is have the prover compute them on its own offthread, present them, then we will verify them, and also verify none are lacking, and then we will have the info to compute the non-explicit transform along with the explicit original hash resulting in the final hash. 

maybe we could also 'conditionalize' the fiat hashes. prover gives answer, verifier later batches them giving them to another proof and then verifying that proof. but that proof itself requires fiat hashes, but fewer than it will verify, thus compressing them. these proofs may provide side-data that the verifier doesn't want to evaluate so it goes into the amortization box. note this is only worth doing if there is side-data. 

so it seems only lacking is way to verify a set as being the complete set of roots of poly. maybe one way is the prover doesn't give roots but rather factored form (roots implicit), then verifier can do identity check (I suppose manually to avoid generating random points). but we must still verify that a term is irreducible. may be easier just to provide roots, test that each is a root, and make sure none are missing.

now a little tricky may be to form the hashes such that we can calculate the exact relation. actually I may have been mistaken that we can always do this. just like they chose a dummy relation, maybe we chould choose a dummy-invisible poly. they justify saying ciphertexts should be indistinguishable, as to which belongs to the real relation versus the dummy-relation. but we're not encrypting, but computing in the open, so one may distinguish so that cannot be justification for dummy-invisible poly. 

can we reduce hashing to agreement points with a particular poly to doing so for random polys? suppose we take two random polys that sum to the particular poly. one must declare weights on the decomposition, eg 1/2 each. then find points of agreement with both with the weights. then one has found a point of agreement for the whole.
well given a poly equation and we want to find points of agreement, suppose we add a random poly to both sides, then this preserves the points of agreement. but it may not yield randomness. for better randomness, maybe we also add or subtract parts based on the original equation. 

maybe take the approach of reducing linear relations (eg identity) to other relations. then we need only transform to another relation. 

maybe we can use that our hash is a universal hash function. so we imagine the constants are generated after the relation is determined. its this that should make it work against aribtrary sparse relations. 

consider arithemtic circuit that computes a target from a source. and suppose the target is also computed from the source via the hash. Notice that arithmetic circit is multivariate polynomial, and hash is also one but of degree 1. note the hash is fixed, and so is the circuit (though implicit). challenge is to find point of agreement (input) for the two polynomials. in the case that the circuit is also a linear poly, we know this is like solving SIS. in general we assume the inputs to the polys must be small compare to the coefficients. 
I'm wondering now if the related problem of finding short input to a multivariate poly (eg multilinear) that evaluates to 0 is hard, because we know its hard for degree 1 (SIS). even if this was hard, this doesn't completely translate because the circuit will be of high degree than the hash, so subtraction won't render a completely random instance, cuz only the terms of degree 1 will be randomized. 
actually calculating roots might not be possible by a pure arithmetic circuit, and if it was it would be very high degree.

in the distinguishability approach, the dummy is indistinguishable from the real (and thus so is anything computed from it). 

why not just take a dummy, accept purported points of agreement, verify them, then encrypt them so they are indistinguishable from the real ones. then pass them through a decryption intractable hash.  

suppose we generalize the identity relation into cycles, eg a two cycle is H(H(x)) = x. this also is intractable. but how to reduce it to finding a relation? maybe compare to finding poly cycles, ie treat the roots of a poly as coefs of a new poly, then find roots again. now there are always <= roots than coefs. 
the task of solving original sum problem is reduced to finding short cycles. think of a directed graph, with a node for each number. there is an edge from A to B if the binary rep of A hashes to B. Since hashing is a function, each node has only one-outgoing edge, but maybe several incoming edges, and if not then there are some nodes with no incoming edge. this means wherever you start, you will end up in a cycle.
suppose we generalize and allow compression. then there is an edge from A to B if A can be anywhere in an input that hashes to B. there would be a lot of outgoing edges. to have less, only allow a few bits b of compression. then there are about 2^b outgoing edges. 

try to reduce from particular invisible-poly to random invisible-poly. 
we have to be able to say more about what it means structurally to find a solution for our 'points of agreement' relation. the identity relation is clear with structure, so thats why it allows us to reduce the original problem to it. but we don't know much about structure between 2 poly representations and their points of agreement. one reason is there is no simple algorithm for calculating those points (ie calculating roots). this seems the limiting point necessary for any instantiation at the moment. 


perhaps best for us now is to suggest a hash, and show how other more expensive ones, even the intractable proven ones, can be computed via pushing to another proof and putting the side-effect data in the amortization box. specify when this would be worth it. we also need to show that appropriate compression is possible. 


maybe probabilistically check fiat transforms by random linear combination, only works if hash is linear transform. reall what this means is just reducing it, and never having to evaluate directly.


for constants, maybe prover computing in additive form, then mapping to multiplicative form for verifier is faster than computing in multiplicative form. could compare costs, and show algo for isomorphism. given an element of q its easy to identity if by its index with respect to generator 1. thus I think we just need to have a generator for the multiplicative group and be able to calculate the index with respect to it for any element. oh no, I think this means discrete log over the multiplicative group. so the isomorphism is easily computed from q to the field, but not vice versa. So suppose prover computes commit in q, then maps to field. keeping track of the q values, as long as any new value is derived from those already had, the prover never needs to compute in the opposite direction. adding commits is fine, as is computing overflow polys. what might be a problem is the challenge. given the challenge as plain binary, prover can compute new commits in q, then again map them so it seems no problem. the question then whether it would be worth it. I think so because we know multiplications in general take more time than addition, but we should compare exact costs. Note the generator for the multiplicative group can be pre-computed and chosen for optimization. 
http://www.lix.polytechnique.fr/~ionica/IonicaAfricacrypt.pdf


lets figure out the parameters for lattice setting
we must choose an infinity norm \alpha that implies a euclidean norm \beta. thus \sqrt{m\alpha^2} <= \beta. we will choose equality. thus we have \beta = \alpha\sqrt{m}
\alpha is what we want binding. we will use \alpha/2 for our d-values. 
I suppose both n and q are mostly free, but we want enough compression between n\log(q) and \alpha/2*m. Note these are our four parameters. now we must choose \alpha/2 as the field size such that it is at least twice the degree of the multilinear polys, which is log(m), thus \alpha/2 >= 2\log(m), for simplicity now and later we choose equality. now we can choose to express in only m or \alpha, and I think we should pick \alpha because it will appear in more places. then m = 2^{\alpha/4}. now \beta = \alpha*2^{\alpha/8}.
it would be helpful to establish a relationship now between n and q, reduce to only one variable, then leave open the relationship between that and \alpha.
well the constraint for lattice reduction relates \alpha to n\log(q) so that imposes no restriction on the relationship of n and q. actually the combinatorial attack also only relates to n\log(q). 
But we must have \beta < q. but I think this inequality can be tight and need not involve n.
so I think we choose q > \beta, which then fixes q relative to \alpha. then we're only left with n. thus I think we can have a free relationship between our only two parameters n and d (we should analyze wrt naming d rather than alpha). 


maybe use the word 'tail recusion' to better capture the 'folding' and differentiate it from regular recursion. its tail recursives in the sense that proof A verifying proof B doesn't know whether proof B is valid, yet the answer to proof A gets folded with the answer to proof B. We view the recursion as vertical with the top layer being the top proof, ie A on top of B, ie A calling B.
Sagaproofs: scalable proofs via tail recursion 


Lets try to really figure out the LWE relation intractable hash and see if we can simplify if
The relation must be fixed. this is done by considering the commit. in our cases, the relation consists of visible poly (for now consider one), invisible poly (also consider one), and points of agreement between them. so its a ternary relation. 
We consider the 'real' versus 'fake' cases. we only see the visible poly. we refer to real or fake based on what we don't see, that is the invisible poly and thus the points of agreement that depend on it (as well as depend on the visible poly). That is, we can only see one corner of the ternary relation.
Upon any instance, we assume the invisible poly corner of the relation is fixed. this leaves free one to find a visible poly and points of agreement that complete the relation.
Graphically let us assign the top left corner ('left') to 'visible', the top right corner ('right') to 'invisible', and the bottom corner ('bottom') to 'points of agreement'.
The goal of the adversary is to find a left that hashes to a bottom such that they complete the relation for the fixed right.
We design the hash such that the left (input) interacts with the right and bottom to produce a bottom (output). Now we also have the right encryped, or more general represented in an unrecognizable way but such that it can still be interacted with.
Now we are perhaps giving too much structure. we are assuming the left is input, the right is encrypted, and the bottom is output, but maybe there could be different organization. we are not assuming anything about how one is to arrive at a bottom given a left and a right. one could explicitly compute one. or, a bottom could be proposed and then validated. 
The real right will be unknown, so we use a fake right, that is again encrypted. We assume the two encryptions are indistinguishable. Now when computing, or verifying, bottoms, they will be with respect to the fake right not the real right. The goal of the adversary, however, remains to hash to a real bottom. Thus while it be remain instractible to hash to a fake bottom, it may be feasible to hash to a real bottom. By contradiction, suppose the adversary has noticeable probability of doing this. The given an encrypted right, the adversary can try to hash to a real bottom, and he will succeed with noticeable probability if and only if the encrypted right is real, thus yielding an ability to distinguish.
I think the only way to make sure the relation is fixed, via the invisible right, at the time of hashing, is to include the commit in the input and treat it as a parameter to selecting a hash from a family. Thus if the relation changes, the commit changes, and so does the hash. Thus the hash cannot be determined before the relation because the hash itself is unknown without a relation.
I still don't understand exactly how the encrypted data must be used. Satisfying the condition of intractability with respect to the real data doesn't require the data to be encrypted. I suppose not just encrypted data but the entire scenarios must be indistinguishable. Thus if data is unencrypted, it must be the same data in both the real and fake cases. We cannot include the real data in the fake case because we don't know what it is. We also cannot include the fake data in the real case meaningfully because it is irrelevant to the real case. Thus it seems any data that varies between the real and fake cases must be encrypted.
Suppose we make an encryption scheme such that decryption is a linear function of a secret vector. Then hashing from an encryption to a decryption means solving SIS. This means we must submit encrypted roots as input. How are we to compute the encrypted roots? If they are also submitted as input, we need a way to validate them and reject them, that is abort the hash, if they're incorrect. But since they're encrypted we can't read the internal state and decide whether to abort.
We need a way such that in the real case, if an unencrypted target value is hashed to, then we found a solution to SIS. I think this means the target value must be the output of the hash, rather than having the output as, say, 0 and then adding the unencrypted target value (which we don't know). Thus we need a target output to yield an SIS solution. As said, this can be done if the targets are fixed linear functions of the inputs. 
Unfortunately there is no straightforward way to compute roots.
Notice that even if we validated the roots and aborted, they would not pass for the fake case. This suggests we don't want aborts. But without aborts, the adversary could submit incorrect roots in the real case. Then we would end up with the wrong encrypted roots, which may pass through the hash to the decrypted targets without violation. 
Though it won't resolve our problem, we are able to efficiently compute the number of roots (but not the roots themselves). 


I realize like I did before, two important things, and now I realize how to put them together. The first thing is we can reduce an evaluation of some data to an evaluation of a one-to-one data transformation. The second is that we can only 'shed' entropy by direct computations by the verifier. Why not take a 01 vector specifying values in powers of 2. Then add the two polys to shed information, and the result is an expression of the addition, except that we have not performed the carrying yet. Then is a one-to-one transform to the binary form of that number ... actually there's not as there are multiple ways to carry and end up at the same value. But we've already shed most of the entropy so maybe we can make it work. 

Consider an n-digit number with radix d. There are d^n possibilities. Upon addition of two there are (2(d-1)+1)^m = (2d-1)^n possibilities. Suppose we add the numbers without moduluation. I think we can do this for partiuclar radix bases and particular number of digits. But we will still need to eventually do modulation and lose more entropy.

But remember, we can allow supplementary data from the prover as long as the total cost is sublinear. And recall that we can use this data in anyway we want. The verifier manually reduced from d^{2n} to (2d-1)^n. If we want to compute modulo, we need to reduce from (2d-1)^n to d^n. Comparing these ratios it appears we were able to jump a bigger entropy gap manually, and so we will only need sublinear auxiliary info from the prover to jump the second smaller gap.

In fact, could we just keep dropping our entropy by this addition, together with transformations, with not need for auxilieary info?


multiplication is the challenge. it drop no entropy but we can't rely on prover. 
remember we have small field, so many multiplications will be needed. maybe we can amortize them.
we must review our strategy for handling polys over a small field. we will execute the sumcheck multiple times for each, at the end needing to evaluate each poly at a number of places. beginning the sumcheck we have the answers for each sumcheck all in agreement, and the sumcheck says if the answer is incorrect then with a certain probability the final evaluation will be incorrect. if the answer is incorrect, then all answers are incorrect, so with high probability at least one of the evaluations will be incorrect. oh, i was hoping they would all be wrong. with only one wrong, we must spread that error around. Consider a wrong evaluation. Then when connecting it with another poly through a line, the returned line poly will be wrong if it agrees with the wrong evaluation. It seems spreading keeping the error spread is difficult.

Suppose we use the small field, but we do evaluating on a large field, like an extension field. Then sumcheck is not a problem, and reducing from two points to one is also not a problem. But reducing from two polys to one we will not be able to simply multiply the commit by an extension element. We can do two equivalent things. One is select an extension element and multiply the commit by each coef separately and consider it a decomposed scalar by the extension element. Or we just select multiple random elements and multiply each commit by it, and I think soundness is the same. Now we need to see whether we can maintain this without blowup. Notice that multiply constant by extension field element is equivalent to just multiply component wise by the constant. We can regard the group of commits as referring to a single poly with extension field coefs. So given new polys with only scalar elements we first reduce it to evaluation at a single point (probably extension), then we connect the two extension points to arrive at a single point (note we can do all this using virtual polys). Now we have the task of evaluating the old poly with extension coefs and the new poly with scalar coefs at a single extension point. We can't multiply the extension poly by an extension element because that's more complicated, but luckily the new poly has only scalar coefs so like we did before we generate a new commit for each component of the extension element multiplying the scalar coefs. Now we have two groups of commits, and our task is to add them as extension elements. I think this can be done in pairs with regular addition using overflow polys. Using this technique, we can always keep our poly commmits to only 2, like with DLP. But we must still figure out how to manage the overflow polys. 

Gosh, now I'm worried the regular SIS amortization doesn't actually compress. 
We start out with l bit commits of length n, so entropy (2^n)^l
Then we end up with s commits of length n to values in l, so entropy (l^n)^s. 
The former equals (2^n)^l. The latter equals (2^n)^{log(l)s}.
Our s is fixed, and our amorization will be l/(log(l)s) = factor => l/log(l) = s*factor. So two half information we set factor = 2, and for security s = 128, we need l/log(l) = 256 => l is about 3000. 
So actually it does compress, but for a large commit q and thus commit size, 3000 commits to boolean vectors is a huge overhead. 
One option may be to have prover return z values in l form rather than bit form, thus submitting s commits rather than log(l)s commits. Then instead of putting them back in the amortization box we treat them and polys and reduce their opening to their evaluation at a point and merge them with others polys. 

A technique we want to use in general is to compress as much as possible to avoid many commits and thus large circuit size. its more efficient to have a single large commit (even with much larger q), such that the commit is smaller, but still gives 'virtural' access to the same data.

Suppose we compress the boolean overflow polys. This means instead of many commits each to boolean polys we have one big commit to field size elements. For the challenge we usually compute random linear combinations of the commits. But in this case we don't have access to the individual commits. Another possibility is not to receive a compressed commit to the boolean vectors themselves, but rather a commitment to the commitments. Then ...

A problem now is that our infinity norm must be as large as l! 

A best option may be to start our hardness assumption in term of the not-yet-well-studied infinity norm. Our commits use bits. Then we choose each challenge limited to k 1's such that the z's should have infinity norm at most k. Hopefully this only comes at the expense of reasonably more challenges. 

In a binary extension field suppose we have a bit poly. Then we want to multiply by an extension element. Following the approach outlined previously, we have two tasks. One is to multiply a bit poly by an extension element. The other is to add two extension polys. The latter can be done simply by xor. The former we have to think about. What is special is we don't need to multiply extension elements, only multiply by bits or add extensions. 

I will generalize the idea. Suppose all goes well using an arithmetic circuit over p with sumcheck changed to k dimensional extension field. We ask that the polys providing data be encoded with bit coefficients. When it comes time to evaluate one of these polys, we multiply it by a random extension element. To do this, we duplicate the commit k times forming a row vector. We can visualize the data of the commits forming columns of a matrix. Then we multiply the commits component wise by the extension components which are p values. We can now visualize each row of the matrix as a coefficient of the poly with extension elements. We have thus randomized the poly without help from the prover (under the strong assumption it was in appropriate bit form to start). Then we will add it to another poly with extension elements by adding the commits component wise modulo p. How we do this is still undetermined. Note that characteristic 2 may be optimal. 

We want error in the beginning to persist until the end. One kind of error is invalid commits. If the prover doesn't have an opening for a commit, we want to make sure he doesn't have an opening for any derivative of it. In our model, a opening means a bit vector. The first derivative of a commit is multiplication by a p value. 
Suppose a prover does not know a bit opening for a commit. In particular suppose some values are non boolean. Upon multiplying the commit by a p value, the result will not a vector of 0 and that p value but will contain other values (though they may still be p values). 

I'm worried the auxiliary data necessary for addition will not be sublinear in a bit vector, which is the amount of data it accounts for. 

Note that we can use normalized result of addition as data, but it drops some entropy.
Consider adding two vectors modulo p. There is p^{2n} data. Upon addition there is (2p-1)^n data. Now our target data is p^n. This means our remaining data to drop is (p/(2p-1))^n data. We want to drop as much data as possible by addition, and as little as possible by overflow. In total we must drop p^{2n}/p^n = p data. Well the first gap is p^2/(2p-1) and is about p/2, so linear in p. Our second gap is (2p-1)/p = 2 - 1/p is basically constant. So the bigger p is, the greater the percentage we can drop by addition. 

Lets work with this and see what works, considering our plan about as a subcase that may or may not work.
We will consider addition of polys, disregarding whether the two polys are different or not, or what level of correlation they have.
We want to consider a valid way to add polys such that if the input is valid then the output is valid and correct, and if the input is invalid then the output is invalid. For a moment we disregard the entropy costs of doing this, and only focus on completeness and soundness. Our approach has been to take the commits, add them, then consider information provided by the prover to compute modulo. For completeness, if commits are valid and the overflow data is correct then result is correct. For soundness, it could be either that the commits are not valid or the overflow data is not correct. ...

I'm worried we have the law that any data given by the prover must be directly manipulated by the verifier, and if the verifier asks for more data from the prover it must be a request with sufficient randomization. For example, the following exchange would not satisfy this law. Verifier has two commits to add. Verifier adds them and asks prover for the overflow commit. This request has no randomization. It would only be randomized if there is enough randomization as to which commits the verifier will add. 

Before we made the assumption that we start out with valid commits. Then we said if a boolean overflow poly is given the if its wrong the result will be an invalid commit. We knew this leaves the need to verify the poly is boolean and we were planning to use the challenges for that. But what we didn't realize would be so limiting is the initial assumption that the commits are valid to begin with. Our commits will come from the prover and there is no reason to believe they should be valid. Thus it could be the case that the initial commits are invalid, but the addition is valid and thus so is anything else that follows. 

We need a method to immediately randomize the commits of a prover. With DLP its exponentiation. 
One possibility is like the challenges where we wait until we have enough commits and then we randomize by adding some of them, and perhaps asking for the overflow polys. 

Suppose that figured out some way to randomize with only additions modulo p, and that each one only required an bit overflow poly. Upon our randomization we would be able to reduce a certain number to another number, at the cost of a certain amount of new information. It only becomes practical when the amount saved, ie entropy dropped, outweighs the cost, ie entropy added. I think we can establish a threshhold. This is the same situation as for the 'heavy' challenge. 
What we have on our side is the high entropy drop in addition, and the low entropy drop for overflow. 
So at this point I think we should rethink the 'heavy' challenge. To be clear, we will accept a number of commits from the prover that may be flawed, and by our random challenge we expect the flaws to persist. Now different from the regular heavy challenge, we don't just want use this as a proof of knowledge of opening, but also as a reduction, where there is a meaningful polynomial relationship between the initial commits and the randomized ones derived by the verifier. 

I think the general scheme will be that we have a collection of commits, each claimed to be a valid commit along with a claimed evaluation value at a particular point. We can reduce all evaluation points to the same. Then we choose random linear combinations (how many and how many in each combination subject to analysis) of the polys, and reason about what their addition evaluations should be. Then we perform the addition, asking the prover for the relevant overflow information, which should be sublinear in the total. We can assume this data is correctly formated because that can be checked, but what we can't assume is that the data is correct. We would like to show that if the data is incorrect then the result after adjusting for modulo will be incorrect, in which case it the error will probably continue to persist. I think this is indeed the case because given an addition of two p polys, one will only end up with a poly in the p range if correct modulation is done. 
Another approach is that an 'over p/2' poly is provided for each commit before any randomization. This extra data is sublinear and is the 'cost'. Then after randomization the verifier can manually compute modulation. But with only the 'over p/2' information, the verifier won't be able to compute exact modulo but only approximately and only for a limited size linear combination. Therefore I think the previous approach is better. 

Keep in mind we may like to keep the idea of randomizing a single poly, but allowing the random linear combinations to include scalars other than 0 and 1, like 2, and maybe even all scalars. Maybe our scalar vectors could be limited to a certain norm like euclidean such that each new poly only requires a certain number of overflow polys. 

Remember the general proof technique for the heavy challenge. Previous analysis on line 1091. There is a matrix of instances. It has 0 and 1 entries depending on whether than instance was a success. We assume there are eps > 2^{-s} fraction of filled entries. Thus we can invoke the subroutine such that in expected time 1/eps we find a solution. Then we manually (without the subroutine) search for a second solution in the row. If it's a heavy row we will probably find one. If its not a heavy row, we will abort. Enough of the time we will land in heavy rows such that we should eventually find a solution. Once we find two solutions in the same row, we can use them to solve the problem. 

I suppose our scenario should take the same general form as the heavy one.
A: n*m
S: m*l
T: n*l
C: l*j
Z: m*j

Uh oh, suppose upon addition before modulation, infinity norm exceeds bound. Then prover could find a collision maybe, and send the overflow poly for that one, and then verifier ends up with a valid poly but one that is incorrect. Thus we must make the theshold high enough, and limit the number of additions performed before a modulation. Even this might not be safe. Suppose prover knows all that you will add together, finds a collision, computes the non-boolean modulo poly, splits it into parts, and each round sends a part. Then in the end the verifier ends up the same, with a valid but incorrect commit. Thus we need the threshold to be high enough to account for the number of additions we'll do before re-randomizing. Note that we can add a fixed number, like 2 or 3, do modulo, then re-randomize and repeat, which will only cost more in terms of overflow polys. 

So regarding the above we won't have C as a random boolean matrix like the heavy challenge. Instead we'd like to limit the 1-norm of each column of C to, say, B. Then each vector of Z should have infinity norm at most B*p. The overflow poly will have entries in [0,B-1]. 

Regarding the high amortization cost, maybe the network can be layed out so that commits are gathered a few passes ahead, then challenges are sent back and parties worth together to generate the new polys. This means a sublinear number of polys must be sent in total, but it requires more 'liveness' to the network and enough trust among the community. 

Actually the method considered last night of bit vectors and extension element would work, because we could check the initial commits as being bit vectors. then we only need some overflow commits for the additions. In this case our challenges wouldn't be over p vectors but just bit vectors. 

I think it is sufficient to consider a single 'row'. We want to show that if some elements in this row are outside the valid space then a random linear combination of them will probably be outside some corresponding space. 
We want this to be a statistical argument, not one of hardness.
I actually think the randomization doesn't require an amortization threshold, that's only required to make amortization worth it. 
We will our random scalar vectors are bounded by a 1-norm B. 
We may be able to get better soundness if our random scalar space is larger than our coef space. In particular, we may be best if our coef space is bits. 
We want to know the probability of soudness error, that is
\sum_i c_i*r_i <= \sum_i b*r_i = b*\sum_i r_i
when max_i c_i > b.
It depends on the distribution of the c_i. We need to relate the probability of different tests, because there is a lot of conditionality. That is, passing one test gives information about the distribution. 

Suppose we can end up showing that for a certain number of random tests the error will persist. We then wanted to say that if the wrong overflow vector is provided, the error will also persist. I think this part has no soundness error, and its not randomized anyway. So really the request for the overflow vectors by the verifier is a randomized request, a function of the random linear combination. We assume the overflow poly is correctly formatted, because we will verify that. 

Given C and Z, can one solve for an S such that SC = Z? Since j < l, maybe we can using some kind of optimization. If this is the case, then given a Z that satisfies the challenge equation, one can obtain an S such that SC satisfies it. Of course we are assuming different norms on Z and S, and solving for S must take this into account. The contrapositive is that without knowing a valid S one cannot have a valid Z, because if one did have a valid Z one could solve for a valid S. But actually this is not sufficient because need not only ASC = TC but AS = T and C probably has no right inverse because j < l.
Suppose we put a certain restriction on C such that we can recover particular columns of S and T that fully satisfy As = t. In other words, while we can't ask for a right-inverse to C, maybe we can ask for something else. Remember we can write the right side TC as \sum_i t_i*c_i^t. Similarly we can write the left side SC as \sum_i s_i*c_i^t.  

Using the adversary as a subroutine is one approach. In this one we assume nothing about S and only assume adversary is able to find Z, and we use that ability to recover an S as best we can. The other approach is the statistical one where assume the adversary knows an S such that AS = T (because a trivial one is easy to compute), and we show that if S exceeds some bound then so will Z for random C with high probability, which contrapositively states that if adversary knows a Z that satisfies a bound then with high probability adversary also knows S that satisfies a bound. But the proof techniques seem very different. 

We have been assuming S is fixed, but the prover is free to choose S after receiving C. In the end, the prover must find Z with two conditions: first that it satisfies a bound, second that AZ = TC. 

Suppose we show as considered before that given a Z that satisfies its bound and AZ = TC, one can solve for an S that satisfies it bound and Z = SC. Thus passing the test implies finding an S that satisfies its bound and ASC = TC. We would like to show that feasibly finding such an S implies AS = T.
Cheating means that for random C over fixed A and T, one finds valid S (meaning an S that satisfies its bound) such that AS-T != 0 but (AS-T)C = 0. 
Somehow we need to take advantage of the bound on S, but S multiplies A yielding nothing special relative to T. 
Actually S need not satisfy a bound. The prover passes if and only if he can find an S such that SC satisfies a bound and ASC = TC. 


We should probably conditionalize on the invisible knowledge of the prover regarding A and T, before C is given. I think we should set some threshold on the knowledge. If the knowledge exceeds the threshold then we reduce to Z = SC for valid S with high probability, or finding some collision. If the knowledge does not meet the threshold then we reduce to finding some having more knowledge after C than before which makes no sense. 
The main challenge is that the target and the bound for input changes with C. 

One theshold is how many S vectors prover knows prior to C such that they all satisfy some bound, though not necessarily the proper S bound. Since we assume intractability of collisions for bound Z, we can assume if the bound is not above Z then prover knows at most one vector for each target. 
Suppose prover has knolwedge of s vectors all satisfying Z bound, call this matrix Z', prior to C, such that AZ' = T. Now suppose after C, prover is able to find Z'' satisfying Z bound such that AZ'' = TC. 
	Now suppose we conditionalize on whether Z'C satisfies the Z bound.
	Suppose Z'C satisfies the Z bound. We will need to show this has small statistical probability.
	Suppose Z'C does not satisfy the Z bound, thus Z'' != Z'C. Suppose we set the binding high enough that a challenge multipling a Z bounded vector remains collision resistant. Then Z'C and Z'' yield a collision.
Suppose there is a t in T such that prover has no knowledge of a z satisfying the Z bound such that Az = t. We must show it is infeasible for prover to find Z' satisfying Z bound.
	The indented following I don't think is correct, but I keep it just in case.
		Well actually this is not the case. What we can argue however, is that prover cannot find response satisfying Z bound for any challenge that involves t in its linear combination. This means we will have to modify the argument so that every challenge includes at last one target for which prover has no knowledge of a pre-image. Since we want't to prevent against even a single instance of a target without a pre-image, we either must include all targets in all challenges, which wouldn't be achieve anything sublinear, or we must adjust the argument for the above scenario. Maybe our overall condition will have to be variable rather than fixed on the bound Z. Proceed as we would.
		Now for any challenge c with witness z and target t, suppose prover has knowledge of pre-images z_i' for targets t_i' and does not have knowledge for targets t_i''. 
		A*z = \sum_i' A*z_i' + (A*z - \sum_i' A*z_i') = \sum_i' t_i'*c_i'^t + \sum_i'' t_i''*c_i''^t
		=> A*z - \sum_i' A*z_i' = \sum_i'' t_i''*c_i''^t
		From the subtraction I think we can reduce this to proving that if prover doesn't know any pre-images for a set of targets, then prover won't be able to find pre-images with same bound for a random linear combination of them. I think we can use a variant of the heavy argument for this.
	I will work on this part below starting line 1550.

In the final part of the heavy argument, suppose we look for an index of the two different challenges that are the same. Then upon subtraction we don't get t but rather 0, yielding us a collision of not 2B but B. Now we have to bound the probability the two challenges are different eveywhere, ie have no index with common values. Consider the binary case. If we add rather than subtract then on the left we get the sum of the two witnesses, and on the right we get the matrix with all columns as t. 
Maybe we could just take one index of the challenge being focused on and group it with the other challenges in the heavy matrix, forcing it to be the same. We could repeat try for every index since there are only as many as the challenge length. 


Consider the possibility of doing binary addition with overflow polys that are sublinear so we don't need the challenge scheme. Binary is the only case when the overflow poly carries 1.5 info, and for any other base it carries much closer to 2. The idea is that even though upon any addition any number of carries may happen, if we add in a tree format where we use the results of what we have already added, then we know that in total the amount of information about carrying is sublinear. But one problem I see is a lack of randomization. Suppose we do as before where we wait for a large enough batch then we randomly pair. Remember we can't add successively, so I suppose we only do one round of random pairing. In return we request the carry info for all pairs, and I think we can expect this info to be sublinear in the information lost. Maybe we can request a single bit for each commit on whether its proper interpretation should be the negation of the commit or not. Then provers can always commit with no more than half the bits filled. Now it takes n+1 bits to encode 2^n data. Now at most n/2 bits of two added commits should overflow. We are unaware of their positions, however. And accounting for that information makes the whole thing worhtless.

Disregarding randomization, and only considering information capacity, we could add together 2 d-value vectors, get a (2d-1)-value vector, then do a transformation of it to a d-value vector of length between n and 2n, thus saving information. Take the more general case of add k of them together. Then we start out with d^{kn} information. Upon addition we have 
(k(d-1)+1)^n, which we'd like to convert back to d^m for some m. What is the relationship between m and d and k?
(k(d-1)+1)^n = d^m => m = n*log(k(d-1)+1)/log(d). The larger k and d are the greater the total savings. 

Suppose we gather enough polys such that we have sufficient randomization to add them and get soundness regarding poly evaluations. I suppose we have the same challenge in this scenario as the previous. We can imagine the prover directly sending us the results, but we don't know how to extract valid initial commits from valid linear combination results. 

In fact, is it a falsifiable assumption to say the prover cannot cheat? Maybe not, and just like other falsifiable assumptions like KOE we can still partially accept it. Falsifiable means I give you a problem and assume you can't solve it, but you can give me data, ie a witness, to prove that you did solve it, eg SVP, DLP. Here the problem is, I give you random A. You choose T. I give random C. Then you must find Z below bound such that AZ = TC. Now the question is whether you can do this without knowing S below bound. You could pass the test easily by choosing any S below bound and then choosing responding with T and Z accordingly. But suppose you don't know a bounded S for the T. Can you still find a Z? 


Consider taking the limit in the batch size. then a random linear combination, even just a adding two together, gives an almost-random target. Prover is unable to find Z with bound for a random target. 

Regarding randomization, which vectors get added must be randomized, but the transformation need not be. 

I'm thinking a new method for randomization would be to generate random vectors then perform addition with that and one given by prover. This, however, will cost information. 


Revisiting relation intractable functions, suppose we try 'composability' towards resistance against any relation expressed by a 'series' or 'compositions' of linear functions. What we mean by composability can vary. 
Suppose we take the output of the hash, transform it in some standard way, then feed it back into the hash. I would like to consider any circuit with each gate a linear transformation, but for now just consider a single gate at every layer.
We want to make the statement that if at every layer, the prover can hash to some intermediate value such that at the end a valid target is achieved, then the prover can break SIS. 
For the case of a single layer, if prover can hash to a target then prove can also pass hash through linear transform, subtract sides and find that the input is a solution to SIS for the related constants of the originals minus the transform constants. The security reasoning is the following. The relation, though unknown, is fixed relative to the hash. For a fixed relation, we consider the noticeable probability over the chosen hash and adversary coins that the adversary manages to break it. Suppose we are given an SIS basis and we'd like to use adversary subroutine to break it. We are free to try multiple relations. For each relation, we compute the random linear combination, add it to our constants, then give that hash instance together with the relation for the adversary to break. Upon receiving an answer, the answer should serve as one for the original SIS. But we give the adversary all linear transforms corresponding to all targets. 
Note the target could be any injective function of the output of the hash, still yielding a break. Mapping the output to binary form for input is just one example, in particular a permutation. 
Note that while our hash may not be injective, it behaves as such because collisions are intractable. 
Now consider two layers. Unfortunately, it could be the prover achieves the final output with different intermediate values than the circuit. These intermediate values would be unknown to the outside and could not be used in the reduction.
Even if we could prove the circuit we have is a lower bound in complexity, prover could find a negligible fraction of instances that map with fewer steps (or in our case with the same number of steps, but with different intermediate values). 

Note that hashing to a relation means locating a single-hash pre-image for it, inverting the injective function, then finding a single-hash pre-image for that, etc, until finding a pre-image for the top hash, which must also satisfy the relation. 


What we need for the heavy challenge is to have two columns the same. Then we have
AZ1 = TC1, AZ2 = TC2 where C1 and C2 share a common column j. Then A*z1_j = T*c_j = A*z2_j. If z1_j != z2_j then we have a collision. Oh but its the exact same challenge so with high probability z1_j = z2_j. 
Consider the regular challenge again. All rows are the same except row i. Suppose the two rows have an index j that share a common value. Then column j is the same, and if z1_j != z_2j then we have a collision. So assume for all common j, z1_j = z2_j. Remove all such columns, left with a challenge matrix where rows i differ in all indices. Then we must subtract, to get
AZ1 - AZ2 = t*c'^t where c'^t is a row that contains no zeros. 
Thus every every column j we have A(z1_j - z2_j) = t*k for some constant k!=0. Now the regular argument ends here, assuming there is a k=1, then using z1_j-z2_j as the solution. I'm wondering if we can use the different columns to do better.
Note that none of the columns in Z1 and Z2 are the same. Now k must be in the range [-(d-1),d-1]\0 if the challenge entries are in the range [0,d-1]. If we have at least 2d-1 remaining columns then at least one constant will appear twice. But subtracting could yield a 0 pre-image. similarly if there is a constant c and also its negative -c then adding might also yield a 0 pre-image. 

Again, we want to prove that if there is target t for which prover has no knowledge of a z with the Z bound such that Az = t, then prover will be unable to respond with Z' with all columns bounded by Z bound such that AZ' = TC. In particular, the prover will be unable to find appropriate columns for Z' corresponding to challenge columns that involve t. I think we can thus reduce this to the problem of finding these particular columns. 
We will need to conditionalize based on what other targets the random linear combination contains, and whether the prover has knowledge of pre-images for those targets. From now on consider the Z bound as the only relevant bound, though I guess we will have some kind of bound on the challenge, I think 1-norm. 

Suppose we set up system of equations with C as coefficient matrix, Z' as right side, and Z as unknown to solve for. ZC = Z'. with dimensions we have (m*l)(l*j) = (m*j). Consider each row of Z as a separate unknown vector. Each column of C serves as a linear combination of these unknown fectors, and the right sides of each equation is the corresponding row of Z'. Thus for each system we have a vector of l unknowns, and j equations. With j<l the system is underdetermined. We would like to solve for an appropriate Z such that ZC = Z' and AZ = T. 


Think about soundness for adding poly evaluations. Think of zippel lemma. Our polynomial coefficients consist of a subset of the targets. The degree is 1 (maybe changing this would help as it decreases randomness necessary). So for each target in the subset there is a random point coefficient. We must select the coefficients uniformly and independently from a subset. This means our challenges might not always have the same 1-norm, but we can calculate the 1-norm (maybe this can be improved). Suppose our subset is [0,d-1]. Then the probability two different polys evaluate the same is 1/d. Suppose we must repeat the test t times to be negligible. If the`re is only a single wrong evaluation, then we must then that wrong evaluation must appear in t tests. We think there will be at least as many challenges as the security parameter s. With 1/d^t = 1/2^s we have t = s/log(d). A simple possibility is to have all targets are the subset, thus choosing a completely random [0,d-1] matrix. I'm confused because if two coefs are different but they both get multiplied by 0 they disappear from the equation, so can we really count this? Well by the lemma if our poly is really all targets then it indeed seems valid by the lemma. For the case d=2 we would do the random 01 matrix and the number of challenges must only be s, so the regular heavy challenge would work. 
This covers the soundness of poly evaluation. What remains is preventing the prover from not knowing a commit for some poly but then knowing a commit for their addition. If there was no randomization, prover could easily create invalid commits that sum to a valid commit. Therefore we must show that our randomization is sufficient not just for evaluation soundness but also for commitment soundness. Remember we don't need to worry about modulo because that is a separate issue that we may not even need to do soundness analysis for. 

How about the analysis regarding the probability of a vector above bound S multiplied by a random linear combination not exceeding bound Z. Well maybe consider the 'closest' vector that does indeed satisfy bound S, with distance measured by how many evaluations differ on the challenge space. We want to test via random evaluations whether this distance is greater than 0. Let's use schwartz zippel to figure out how these must differ. All we take into account for zippel is that the two polys are different, not minding how different or in what ways. We want one of our random choices to yield different answers. Whereas above our polys for zippel were the evaluations of the polys, now our polys are the committed vectors themselves. In particular, each poly will be a row of the commitment matrix. We will again interpret them as degree 1 and draw our random samples from [0,d-1], so I think we'll get the same soundness as above. An important point I mentioned that justifies this approach is the valid poly has coefs that are less than or equal to the committed poly.
But of course we don't have access to the 'closest' poly. We only have access to the evaluations of the real poly. We know a bound of each evaluation for the closest poly, and our only way of detection is to see if an evaluation of the real poly exceeds this bound. 
I realize this method is flawed because we can't compare against a fixed virtual poly, even if the closest poly. The evaluations may differ yet the evaluation of the real one does not exceed the bound, and in fact could be less than the evaluation of the closest poly. Instead we need to ask whether there exists any poly in space of bound S that agrees with the real poly on all the particular challenges issued. Thus this virtual poly, if it exists, is not determined until all challenges have been issued, so we are far from comparing against a fixed poly. 

Can we do analysis and conditional reasoning on the commited vector by how the few random evaluations relate? It an under-determined system of equations. The degrees of freedom is l-j. Maybe we can relate this to an l-j variate linear function, ie a hyperplane in dimension l-j. 

I think we can make the following lemma. Given a set of l vectors satisfying targets all 'close' to bound B, given a set of random challenges j < l, one has negligible probability of producing j satisfying vectors all bounded by B. We need to clarify the 'close' condition more, the concept regarding 'maintaining' the bound.
But for sake of contradiction, suppose there is noticeable probability of maintaining the bound for one round. Then one can repeat at least a constant (if not log or linear or poly) number of times and with noticeable probability the bound will be maintained. But within a constant (or maybe need more) number of these transformations of the targets, we will arrive at one uniformly distributed (via the randomness of the challenges). Thus the adversary is supplying a vector satisfying the same bound for a random instance, which is considered intractable.
Complications include not just the 'closeness' issue, but also that the challenges for which the adversary succeeds and thus proceeds upon may not be uniformly distributed, thus affecting the distribution of the final target. Note each round there are fewer columns. 

I realized another way to randomize it to take advantage of every commit being a matrix with multiple rows. We could randomize the constants by replacing each row of the previous set with a random linear combination of them. This means every constant gets replaced by a random linear combination of the other constants in the same column. If the prover is honest, he will have commited the same valid vector to all rows. Then a random linear combination of rows will yield a commit for which the same vector is a witness. Presumably, we would generate just as many rows as before to keep the same commitment security, the only use being the randomization, whereas for the randomization of commits the result is sublinear. Also note that the number of rows involved in the combination, or the range of the coefficients used, have no impact as they do for the randomization of commits. But note that if q is composite we may not want to multiply such that it sends constants into cosets, but either multiply by constants c such that gcd(c,q)=1, or simply add. 
Now how might this randomization be used? Note that when adding two commits they must have the same randomization of constants. If prover commits same vector to all rows this randomization will have no effect, even if the vector is invalid. Thus it seems this may only be useful for catching the prover committing different vectors to different rows. For constants A and B that have different value x and y commited, upon their addition the prover will be forced to solve for z such that
Ax + By = (A + B)z
and due to the randomization, z may well not be a valid value, even if x and y are.
I suppose we should speak of the above as columns instead of rows when visualizing it in committed form.

Yet another way to randomize would be linear combinations of rows, equivalent to replacing each coefficient of a poly with a random linear combination of other coefficients. This may replace the need for many commits before randomization can take place. But either we need a meaningful way to translate this transform to a polynomial evaluation statement, or we need to reduce our polynomial statement to statements just about the existence of an opening. 

It may actually be more efficient for some networks to batch a lot together because in total I think it requires less circuit work, and that may outweight the extra cost of communication and trust. the bandwidth would be about the same. 

Back to the regular random linear combination of commits.
I should be able to establish some inequality about probabilities of exceeding thresholds. 

We were thinking its important that a vector meet a precise infinity bound. But is that really necessary? Remember we just need to be sure that the prover cannot change initial commits, and cannot use the randomization process to obtain a commit other than the proper randomization of the initial commits. So suppose we can be sure the prover knows commits (unique via collision resistance) for each target. Then we need to make sure the only witness for the challenge the prover knows is the random linear combination of those initial witnesses the prover already knows. Well the regular heavy challenge can solve the first part, with bound 2B. Then suppose prover can find witness Z with bound B for challenge, and that Z is not the corresponding linear combination of the prover's already known witnesses for the targets. Show this is infeasible.
Note that the linear combination of the initial witnesses may not yield a valid witness Z, because it might be as large as 2B and then with low probability satisfy the B bound. If it did, then we could say if the prover provides that for a Z witness then there is no problem, while if the prover provides another Z then we have a collision. But we cannot say this.
Also, if we could say the Z witness is the linear combination of some witnesses for the targets, those witnesses must be the original ones or else we have collisions. But unfortunately, it could be the prover finds a Z witness not a linear combination of target witnesses. 
Here is an ugly solution. Make the constants so big that there is collision resistance even for a random linear combination of 2B values. Then the extracted vectors bounded by 2B can be multiplied by the challenge and yield a solution to the challenge. Then as argued above, either the prover used the same ones and there is no problem, or the prover used different ones and thus found a collision for the challenge under the new bound. This can be what we do if we can't find a way to improve these constants.

I'm confused how the heavy challenge is not sensitive to the format/density of the challenge matrix. Maybe we will need to compensate for the smaller space of each challenge with more challenges. To the extreme, suppose the challenge is 01 but with only a single 1 for each challenge. Intuitively, this won't work, but where does it fail for the heavy challenge? Each column of C has a single 1. Thus there may well be rows that are all 0. There are j challenges, and each challenge has l possibilities. But we need to enumerate not the number of column possibilities but the number of row possibilties. The number of row possibilities is precisely 2^j when its completely random. So our first challenge to use the heavy test is to construct a challenge matrix with enough structure that the row space is clear. 

Suppose we build a matrix by selecting a column at random, then selecting k rows at random and filling those entries. We then randomly select one of the remaining columns, and again select k random rows. We do this k times. Then we continue selecting random columns, but now to prevent more than k entries in each row, only certain rows are available for selection. I think something like this would be appropriate. Perhpas this is equivalent to filling the top left k*k submatrix with entries then randomly suffling both rows and columns. The latter is more elegant and the parameters, like the filling of the entries, and the size of the submatrix, are straightforward. Also the amount of randomness is easily calculated. 
BTW, is unshuffling a matrix an easy problem? 
I shuffling the columns is equivalent to multiplying on the right by a permutation matrix, and shuffling the rows by multiplying on the left by a permutation matrix. But matrix multiplication is associative so it doesn't matter which happens first, and since permutation matrices form a group, it also doesn't matter how many different permutations are applied.
Actually these formulations are not the same. 
If we want challenge vectors all with the same 1-norm, it doesn't need to be the case that each column have the same number of entries. Thus we could go through each row i, selecting the the k columns i through i+(k-1), modulo the column number. We end up with a matrix where each row has the same number of entries, while some columns may have more than others. Now we have to decide how to fill the entries. If we ensure that all columns have the same 1-norm then the row space is not easily enumerable. Maybe we select row values at random making the row space enumerable, at the cost of different challenges having different 1-norm which yields different Z bounds for each response vector, yet these bounds are still computable by the verifier. I think it would be better to have a fixed 1-norm, and deal with more complicated enumeration of rows and columns for the heavy matrix.

If the lowest witnesses the prover knows for the targets is Z bound, ie they are all higher than Z, then we may use an argument suggested before to say that with negligible probability the prover can obtain an witness for a random challenge also satisfying the Z bound. What's most important about this theorem is it does not assume how the prover finds the witness, like it doesn't assume its a random linear combination of the other witnesses. 
This means prover can only likely pass if knows witnesses to targets below Z. This is better than the 2Z bound. 

Suppose we set up a system of equations SC = Z, with S unknown and independent from C. I think we won't get far until we assume a lower bound for S, because of course if S is very small, eg containing many zeros, yet exceeding the infinite norm, it will pass with high probability. I suppose Holder's inequality is useful, saying SC bounded above by the p norm of S and the q norm of C when 1/p + 1/q = 1. But here we are actually considering the rows of S, not the columns which are the value vectors themselves. 
Note that if l columns of S each have p-norms n1,...,nj then concatenating them in any way yields p-norm n1+...+nj. Now one way of concatenating them is by rows, so we can say that adding the p-norms of all rows equals the same. In other words, the sum of p-norms of columns of a matrix equals the sum of p-norms of rows of a matrix, which are both subcases of the p-form considering every matrix element as a coordinate. I think this is the case for all p-norms except infinity. 
We can take the holder inequality and say that the p-norm of the S row is bounded below by the response (a 1-norm) divided by the q norm of the challenge, both of which we have access to. Thus we can explicitly calculate a lower bound for the p norm of each row of S (actually just the entries involved in the challenge). For example let p = infinity and q = 1. Then the response divided by the 1-norm of the challenge is a lower bound for the max element of the S elements. This isn't too helpful, but maybe it would be more helpful to invoke for p other than 1 and and infinity. For example, for p=2 we can find calculate a lower bound for the euclidean norm of the S elements. Maybe given all challenges for that row we can estimate the lower bound for the whole row, then the sum of these lowerbounds is a lower bound for the sum of the euclidean norms of all S columns. 

Above we are trying to make sure prover has knowledge of S bound commits so that they can be used as a linear combination for the Z witness, then argue that if the prover used another one for Z then there's a collision on bound Z. But this is just one means to an end. Our end is not to make sure prover has knowledge of S bound commits but just commits low enough for collision resistance, eg Z bound. But our end also is ensuring that the Z witness is the linear combination of these commits. So another means would be to show that its infeasible to give a witness Z that is not some linear combination of known vectors, disregarding the bound on these known vectors. Then show that with high probability of the challenge, for a linear combination of these vectors to be a valid Z, they must be below a certain bound, and that this bound is collision resistant. Therefore the Z witness is a unique linear combination of vectors known by the prover. 

Now for the first task above, that is to show Z witness must be linear combination of existing witnesses, even if these existing witnesses don't satisfy a bound (eg the Z bound). We suppose Z is obtained by some function of the targets and witnesses for them (may not be valid), and that this function is not a linear function. Perhaps we can formulate that this function is a circuit, representable by a multivariate polynomial. To be precise, we assume there is a probabilistic, uniform algorithm representable by a circuit that accepts randomness, the targets, any witnesses for them S', the challenge, and the original matrix A and outputs a valid witness Z, and that the circuit does not output SC where S is a subset of S' containing an instance of a witness for each target. 
I dont know how to continue.

For the second task above, that is to show if a valid Z witness is a linear combination of other witnesses, they must all satisfy the Z bound. 

Maybe we should just put together as many lemma as powerful as possible, independently, then later consider how they can compliment each other.
One lemma should be the one about the infeasiability of 'maintaining' the bound. if all commits are greater than or equal to Z bound then its infeasible to find in anyway answers to a challenge that also meets the Z bound. The benefit here is we don't assume how the adversary find the answers, but the drawback is no commit can be below the Z bound. 
Another lemma should use holder's inequality. It should consider the 1-norm returned by all challenges, add them together to get a 1-norm for the whole row combination, divide by the q norm of the challenge, then assert the p norm of the row is lower bounded by this. add up all these bounds then assert the sum of the p norms of all commit vectors is lower bounded by this. we then compare this with the valid upper bound for the sum of all commit vectors. this may be most helpful with p=q=2. the translation from rows to columns doesn't work for p=infinity, q=1. For p=1, q=infinity we are wasting our knowledge of the challenge unless we set all challenge entries equal to the same, which means no randomization. Note that just as for the euclidean norm we can also calculate an upper bound on other p norms for an appropriate vector, then calculate using corresponding q on the challenges. I think taking advantage of this lemma means performing these extra checks for each norm. Actually we can also do checks before we translate from rows to columns by only checking the rows. we bound what each row must be for any norm, then compare with our calculations for the challenges of that row. In fact, this is the original check for p=infinity, q=1 (the case that doesn't work for translating to columns). I'm not quite sure whether adding up the answers for a row then calculating or calculating separately for each answer is better, maybe both. See if one implies the other. Oh wow, I think Holder's generalized inequality holds for 'norms' in (0,1) too. All checks the verifier makes will probably be of this holder form. 
Another lemma should be the heavy extraction for our challenge format.

Luckily I think the only thing that changes without our security reduction is our security parameters, including the number of challenges, and we can hopefully keep the basic scheme of randomization. But the particular checks the verifier performs may vary, though I think they will have similar form. 

To recollect the entire merging scheme, the verifier is to take the responses, check them, then if they pass use the coefs and evaluate at a particular point and compare with a claim. But we want to translate the data back into a commit form suitable for re-merging. So maybe the prover submits new commits for each response (the verifier already holding a commit for each), but these commits claim to hold the same data but in a renormalized form. Since this is a one-to-one transformation, the verifier only needs to process the data in the new commit. Now the verifier can perform the checks regarding either via the old data, then checking the old data with the new data, or do a direct translation and checking from the new data, and I think the choice depends on gate count optimization. 
Unfortunately, I realized, to transfer from the old to the new commit, one must open the old commit, and this must be done in the circuit. This means taking the new data, translating it to the coef form of the old commit, then computing the commit and checking it against the target. But computing the commit inside the circuit means explicitly computing it (rather than using the isomorphism) and it will take a lot of gates. This computation is analogous to DLP exponentiation, which is costly, especially with all the polynomial modulations. One, still costly, solution is to make q prime and make it the field size for the circuit. Then computing the commit becomes trivial. Another solution is to try the overflow poly. But this costs more information and may not maintain sublinearity. Also worth noting we plan on hashes and fiat transforms to be based on the same kind of explicit computations, so maybe we should focus on optimizing it or amortizing it. But amortizing means merging old commits but those commits commits have already reached the threshold. Another possibility is we delegate the commit computation to its own circuit which takes place in q, which means verifying this q circuit in the main field of much smaller characteristic. but the conversion is simple enough, basically linear, it might be able to be done with a simple proof system. 
Anyway, suppose we renormalize. Instead of trying a clever way to relate the decomposed poly to the old poly, we explicitly compute the coefs of the old data and evaluate it at the relevant point. Then we are left to treat the new data as its own poly, with no ties to the old poly. This requires a long explicit computation, and it means computing the same coefs as if we are opening the commit, execpt we are not opening it so we don't need to exponentiate.

Proof system for commit conversion. We want to accept two commits, and old and a new, and verify the data in the new maps in a certain way to the data of the old. It should be an isomorphism. I thnk we may have to split the new data into multiple commits because it is large, but for now ignore that and assume a single new commit contains all the data. We don't access the data in the old commit and only access the new data. 
We take each value in the new data and scale it by some constant, add together segments, then multiply each by one of the constants, then add the whole thing together. This is a single linear function of the new data. The proof statement then doesn't even need the old commit and it just says that the data tied to the new commit, when reassembled, yields a particular commit (which should match the old commit). Thus the statement really just consists of two q values, the new commit and the supposed old commit). So supposed we have a fixed linear function. 
c(x)*(q(0)*d(x + 0) + ... + q(n)*d(x + n))
probably a single sumchek should work, and the q data sent by the prover would be translated to the multiplicative group in the main circuit. 



What is the sagaproofs model?
Use the concept of on and off-thread. 
offthread computation sends meaningful messages to onthread computation, which sends back randomness.
complexity-preserving
integrity, not privacy focused
practical, rather than theoretical approach
regular recursion of proofs is where one proof computes its statement by using another proof as a subroutine, like regular function recursion. tail recursion is where a proof only computes its role but leaves the answer open ended by delegating to a condition on other proof answers. this is like functional tail recursion where control is passed entirely to the delegated proof and doesn't need further assistance upon return from the parent proof. Technically, tail recursion is when a tail is called recursively, and a tail call is when a subroutine is called at the end of the main routine. 



What about zero knowledge?
Can we add a final layer that does nothing, not even merging, but mask the previous layers? In this case it may not need extra data, so it doesn't add to the payload. How can we mask a poly? If we keep the same point of evaluation, the last layer can generate a random poly that is zero at this point. 
I suppose all polys should be zk, especially in lattice setting where they will travel between parties. Maybe each transcript poly is accompanied by a commit to a random poly. We execute on them as in Libra, and at the end we must evaluate both. This means more merging is required for the same amount of meaningful data. I think we can also make commits zk by adding an extra constant. I think this is both necessary and sufficient. Upon adding the commits all info will be gone so revealing is then ok. Oh, but a problem is that for the sake of randomization we cannot deterministically add the transcript and its mask. But we also cannot use the gaussian noise for zk because we need the random linear combination to be a meaningul poly combination, not one with noise or else the poly evaluation assertions won't hold. If we must commit real data, and the opening of those commits must be added to that of other parties, then zk seems unlikely. Only way I see is that each party merges all its data, which can include masks, and then send the randomzied merges to the next party. But this requires large cost for each party, only worth it if computation is very large for each party. 
for DLP we can deterministically add the transcript and mask due to our ability to randomize one. 

can we specify how to do zk for the general case? maybe we can try the last layer masking method. The data for the last layer could be a supposed isomorphic randomization of the real data, but the randomness that determines the isomorphism would have to be hidden. But through the verification of the last layer we would need blockers.
ZK is not such a problem for the protocols that don't do tail recursion because the full evaluation by hashing or interactive proof or whatever is completed, whereas with tail recursion the data will get passed on.
The libra Zk requires the verifier to evaluate by oracle access the mask polys. But as said, with tail recursion we evaluate polys directly. So if the verifier is to evaluate the mask without looking at it, then it must be randomized with other ones.
I suppose we could just some general mask polys that can mask any other poly. these polys would be used in the randomization, so only their combination with others would be sent. We would need to prove appropriate hiding, which depends on the randomization, ensuring at least one random poly appears in each combination, and that a poly doesn't appear in too many combinations to leak info. Soundness holds because all poly commits are determined before we choose random evaluation point.

So suppose we hide all commits, and use appropriate masking polys as in libra, and also have some generic masking polys that will appear in any combination of polys one sends to another party. I think this is the general technique we can formulate for the polynomial method, including DLP. we can describe this in the general poly construction section.



How to prove the heavy challenge for our matrix format? Ideally we'd like all columns (challenges) to have the same 1-norm, and all rows to have the same too. Also we'd like all challenges to involve about the same number of commits, and each commit to appear in about the same number of challenges. One way to achieve this is with the number of rows (l) a multiple of the number of columns (j). There are multiple ways to then fill the entries to achieve same numbers in each row and each column, eg diagonals, checkerboard. Then we shuffle the columns and the rows, ie multiply on each side by permutation matrices. It remains to choose the values for the entries.
Best for security might be that each row contains a high variance of entries, maybe even one of each value, and there are probably no more than j values. Such a design would also let us (almost) enumerate the rows. So maybe we could formulate that each row contains exactly one of each value in a set like [1,d-1]. Not quite sure about columns. 

Precisely what requirements must our challenge matrix meet to work for the heavy argument?
We have our heavy matrix H. It contains 1 in entries for success. We assume an eps fraction of the entries are filled. We would like the row space to describe the coins and all rows of the challenge matrix C except row i. We would like the column space to describe row i of C. 

eps >= 2^{-s}
j*eps/2 >= k => j >= 2k/eps
j >= 2k*2^s >= 2k/eps

a problem with the heavy challenge in our format is that with non boolean entries, subtracting two rows may not yield a vector containing a one but rather a nontrivial divisor of q, and thus we must divide both sides by that constant, but such a divisor doesn't exist. this leaves us with a Z witness for a scalad target. Hmm, but remember one thing driving up our constants is that we take a linear combination of the extracted witnesses. Well maybe that linear combination can be precisely the one we find in this extraction process, solving both problems at once. To review, the context is that the prover found a valid Z witness that is not a linear combination of known targets. Well we extracted targets in the same collision resistant space, so we know the prover has knowledge of these targets. So supposedly, despite knowing these targets, the prover gave a Z witness other than this linear combination, thus yielding a collision in the Z binding space. So our task is to take the extracted target witnesses and multiply them by the same challenge to obtain a valid Z witness. So can we do this linear combination? Well each target must appear in multiple combinations, each with a different constant. Well if our challenge rows contain one of each scalar in a random order and spacing, then subtracting two of them should yield a vector with entries in the range [-c,c]. Consider breaking a constant up into parts. Doing this would exacerbate a potential issue alread not addressed and that is knowing an extracted witness of a scaled target does not imply knowing a witness for the target itself. Do we need to have that knowledge? Well we thought we did, but maybe its infeasible to have knowledge for witnesses for the same target at multiple scales without having knowledge of a witness for the target itself. 

how does holder's inequality help us in any proof of security? 
maybe we should just prove security for the case of a fully random 01 challenge matrix, yielding large constants, and then suggest the other possibilities that may yield smaller constants, by using more structured challenges with more variability in the entries, and making more general use of holders inequality. we shouldn't guess the most efficient challenge matrix anyway, and leave it as an open question. 
Whatever challenge matrix we choose, recall what role it must serve. One role is in the context discussed above, where we want to know the prover has knowledge of target witnesses who's linear combination is the Z witness. The other context is poly evaluation soundness, where we proved security thinking of the evaluations as coefs of a poly, then each of the j challenges corresponding to evaluation on a random point in the challenge space. For 01 this this is {0,1}^l, giving soundness 1/(2^{l*j}), when we only need 1/2^j assume j > security_parameter.
Anyway, we specify the protocol which will use the general holder inequality and what must be prover, then we leave as a separate issue which we approach separately what distribution is best for C.


what proofs will we show in this paper?
proofs for the definition
proofs for the general poly construction
proofs the EEC scheme satisfies the definition and the poly construction
profs the lattice scheme satisfies the definition and the poly construction
maybe proofs about application schemes, like network topology


can we generalize the schwartz zippel lemma where we choose uniformly from a different set for each variable? The univariate case is simple. Suppose it holds for case n-1. Now show it holds for case n. Take the new variable x_n and a poly with that as the variable and the other polys as the coefs. This univariate poly is of degree at most d_n.

What about even more general where we pick the n random points not independently but from a joint distribution. The motivation is that our distribution of interest is the challenge space where we don't want the coordinates independent. In fact, our poly evaluation soundness relies on the zippel lemma for this so if we can't rid the need for independence then we may be forced to have our challenges with coordinates independently. In particular, we would like to choose uniformly from the distribution of F^n where the vector is bounded by some p norm bound. The usual case is when p=infinity, because in this case each coordinate can be selected independently. We measure the norm in Z not in the field F, and we use the standard representation by cyclic groups and their polynomials for extension fields. For now just consider the prime field case, and picture the selection domains as a subset of the n-dimension domain in Z. In fact, just consider the case of n=2 for now. So with the infinity norm we pick from a square. With a 2-norm we pick from a circle, and with a 1-norm we pick from a diamond, the lower the p the smaller the smaller the space. Intuitively, if I imagine a 2 variate poly over this space then it seems the exact shape of the space doesn't matter. The entire domain is a square, but can I select randomly from a subset that it not a square? I don't think our proof can use induction on the dimension since the dimensions are no longer independent. Here our variable selections are dependent. One way of uniform selection is uniformly selecting the first variable that meets the bound, then restricting the remaining variables. 
(x1^p + ... + xn^p)^{1/p} <= B
x1^p + ... + xn^p <= B^p
x2^p + ... + xn^p <= B^p - x1^p
So we can just change the bound. This implies we can actually use induction.

Any more general distribution?

Organizing the challenges by column is suited for the poly eval soundness, and bounding the combinations all the same. Organizing challenges by row is suited for the heavy argument, and intuitively makes a certain amount of sense for soundness in that every commit should receive equal attention.

Hmm, I guess I didn't think of this, but even though polys in their initial form are best bounded by infinity norm, so they can best represent data, once added and only considered in the amortization box, we could forget about their infinity bound and only focus on euclidean bound. This would work for the sublinear proofs of knowledge of opening. By the triangle inequality we would need to support 2\beta for a bound. But remember our response is treated as data again and processed, so actually data doesn't stay in the amortization box but rather follows a loop. Thus to close the loop we need a way to reduce evaluating a virtual poly at a random point to evaluating its representation in euclidean space at some related point, still viewed as a poly. we should still view it as a poly because I don't know another way to take advantage of the additive homomorphism. 

What if we just did all commits with respect to the euclidean bound, then do an isomorphism to the poly. The justification is that indeed we are making more efficient use of our constants n and q.
A beta (radius) bound gives possibilities of the volume of an m-ball, while if we use infinity b with beta = b\sqrt{m} then we have information b^m = (beta/\sqrt{m})^m. 
Turns out we are comparing the expressions pi^{m/2}/gamma(m/2 + 1) for the euclidean space and 1/\sqrt{m}^m for the infinity space. 
Well using the full euclidean encodes far more information. 
Before our plans was to commits to polys in infinity bound, add them, then receive a new infinity commit and compute an isomorphism from the new commit to the sum of the original commits. Suppose instead we have an isomorphism between infinity and euclidean polys. Prover chooses infinity poly then computes euclidean and commits to it. Proof is executed with respect to virtual infinity poly. Then before (rather than after like the previous scheme) the addition of commits, we compute an isomorphism from the commit to the virtual poly, perhaps in a different field. This means the euclidean poly is our new data and with that data we compute the coefficients of the virtual poly, multiply by the random point(s) and thus evaluate the virtual poly as needed. Then the problem reduces to evaluating the euclidean poly at a random point. This is where we add together euclidean poly commits. Oh, but we will still have to do an isomorphism after, like the previous scheme, where the prover gives new commit to a renormalized version of the data in the computed sum. Maybe we can do this all in one field. I already touched on this before I think, but prover can give the new data multiple formats. One option is in a new virtual poly, forming a data cycle. Then the task is then to compute the isomorphism to the euclidean version (opposite direction as computed before), then evaluate that euclidean poly at the random point. The other option is present the new data as a euclidean poly but one of appropriate bound. Then task is to compute a (different) isomorphism from the smaller norm larger dimension poly to the larger norm smaller dimension euclidean poly. Then evaluate the latter at the random point.
It seems using the euclidean version will same some data but have basically the same complexity for a protocol, actually a little more complex.


I think a continuous map from the euclidean to the infinity for the same volume is
scalar*Norm2(x)/NormI(x)*x where scalar is a constant to make the volumes the same
In general to map from a p-sphere with volume vp to a q-sphere with volume vq take the point in p space, multiply by its p-norm, then divide by its q-norm, then scale by volume. 

I think it will take substantially less gate count to do checks in the infinite rather than the euclidean setting. Also, the infinity problem is thought to possibly be harder than for euclidean, but its not yet well understood. 

wondering again if we should only stick with the 'original data' concept. I think we can classify by a 'rate' at which the entropy can be reduced, ie a ratio of how much data is dropped compared to the original amount (maybe use the word 'molding'). Probably better to consider the range [0,1] than [1,infinity]. For DLP we can do ratio 1/2. Ratio 1 means all entropy is dropped, which means fully verifying. In coding theory, the rate is k/n if there is n total information, and k of it is useful. Thus these rates are in the range [0,1]. Rate 0 is useless for codes in both, so I think the analogy is useful, as while they use the words 'code rate' we could use something like 'mult/molting rate'. Another reason to use the word molting is it suggests something periodic. 
I think we need to compare the molting rate with the ratio of the amount of redundancy in the transcript. Rather we could consider 'code' rate, which is basically regular code rate, ie how much of the total is useful. For example, in the poly div construction, I think code rate is well below 1 due to h. So code rates are in [0,1]. Lower code rates and lower moting rates are worse. Suppose we have code rate r, and molting rate m. We need to calculate the threshold that serves as an upper bound, above which r and m will be small enough that data will blow up. 
For a second suppose r = 1. Suppose we have x total information. Then after k molt cycles, regardless how the data is split (sequential or in tree form) we will have x*m^k data left. Well its not so simple because, remember, we need a certain amount of information before molting can take place. So x must exceed this theshold. Actually I realize the rate is not constant but may depend on the amount of data amortized. 
Oh I think the threshold is when we start with original amount of data x, the we have x*1/r data to evaluate, then after molting we have x*1/r*m data left that we must consider as having full entropy. If this exceeds x then we have added extra costs and saved nothing.



oh gosh, the computation model I've been imagining, map reduce, I think has a lot of redundant data in the transcript. But at the same time I know intuitively it should still work despite this low code rate. Perhaps this is a good indication that the code rate doesn't really affect molting. I think this is because molting allows us to keep the amount of data to process at a constant, regardless how big that constant is and how much redundancy we will need to process. 
I wish I could remove the redundancy necessary by allowing for agreement tests somehow. Maybe there is no way to get around this except recursion itself, in that we want to process arbitrary depth computation with processing only dependent on the width. The redundancy in our regular model of computation is proportional to the depth, so does not achieve this. 
Maybe we can describe an equation that holds between the code rate and the number of rounds. It seems this tradeoff translates to one between the communication cost between parties (more amortization) and the gate count (more rounds). 

Remember the most crucial verication costs are fiat transforms. For efficient fiat transforms we want to use subset sum/SIS, but in this context the input and output are in different formats. we definitely want to avoid checking prover provided data for correctness or manually decomposing it, eg operating in field for the constants of the hash, and checking the input is in the input range, or instead manually decomposing the input to the appropriate format. We want input and output in the same field, and I see the best way to do this as operating in the input field, and using the multiplicative groups for finite fields for the constants. The price paid though is that hashing is no longer scalar multiplication but instead scalar exponentiation. 

What if instead of the poly transformation we do a fully evaluated snark using hashing commits. Suppose our hash is the SIS type. Can we amortize the hash openings? I don't think we'd save any cost. And I think the cost of opening all of them manually may well exceed the amortization costs, but the benefit is due to full evaluation communication cost is much lower. 

I have the chance to prove secure an abstract definition of tailed recursive proofs.  

The hologram paper talks about 'universal simulation' and saying its concretely inefficient, and that holographic proofs are better. By this they're talking about proof systems that are uniform where the verifier simulates the basic steps that are applied over and over, eg in STARKs. 



for correlation intractability with SIS consider the non-compressive version first. suppose take the input and pass through a fixed affine transform to obtain each coeff of the difference poly. Then suppose we use plain auxiliary data, the roots, to output all the roots (verify them). Rather, actually instead of the roots we would output the diff poly in factored form, which includes roots. Now how to we achieve compression? I was thinking we reduce the problem of mapping poly to roots, to problem of mapping roots to roots. So first we do the non-compressive transform from the input poly to the difference poly in factored form, which is non-compressive. Then we map the factored form to output, and if output contains a root it will match one of the inputs, satisfying the identity relation. Lets think in 3 steps. First take input and map to diff poly. Then take diff poly and map to factored form. Then take factored form and map to output. Only the last step is compressive, but we know since its a linear relation that is ok. The first two are non-compressive.

Consider if just the first step was the full hash, so a successful break is mapping input poly to correct diff poly. We have a fixed mapping from the input to each diff coef. Then a break means break all the induced hashes. So actually a break means breaking the hash (one of the those induced). Thus if the prover does not break the first hash then we assume all coefs it computes are incorrect. 

I think our basic problem is we still can't prevent the prover from taking an alternative path. Prover could map to a totally wrong diff poly, thus not breaking the first hash. Then correctly map to its factored form. Then map to a real root that is not an element in its factored form, thus not breaking the last hash. In other words, inability to break the first hash and inability to break the second hash does not imply inability to break them together.

I thought it would work, with the non-compression of the hash giving us control over the input to the last hash. But what I didn't realize is the invisiblity of the first step takes away this control.

Note that in sumcheck for GKR we plan to send quadratic polys. By the quadratic formula roots are explicitly computable, but require sqare root and division. If we could compose one intractable for this relation I think that suffices for GKR, though not the other fiat transforms. So for a quadratic we have 3 coefs. Hmm, don't know how to implement this.

Basically we have the ability to do any invisible permutation intractable hash. And we can also do a compressive hash for linear relations. But we don't know how to combine the two. 

So I think we leave this for now.



to review, for lattices we haven't yet chosen a distribution for the challenge matrix. we were thinking of proving the zippel lemma for p-norm distributions. This would be for the context that our challenges are chosen uniformly within the p norm space. But if we choose our challenge matrix in such a way, eg with respect to rows, that challenges are not uniformly distributed in some p norm space, then we may have more trouble proving poly soundness for the randomization. 
Remember how crucial it is to work in a native field. To do this I think we want to take the normalized poly and since it is above the field size, we want to do modulo on it before doing the final evaluation. We will still open the summed commit before modulo.
But I realize checking p-norms is costly, and generating that distribution is as well. Only uniform distributions are easy to generate, and the larger the range check the more costly (bit checks are minimal).
Maybe a random 01 matrix would serve us for our current best solution. It suffices for poly soundness in randomization exactly, with the regular zippel lemma. 


We don't need a field. I think an integral domain is sufficient. With zero divisors a term may be zero when it does not contain a zero, so zippel won't hold. Like integers or rationals, even complex rationals. I'm thinking of this in the context to avoid modulo to make euclidean analysis more convenient. We still want finiteness for precision, and we want to avoid blow-ups. 
before we were limited by fields, but I think integral domains can have many more forms, like analytic functions. 


Suppose we limit the 1-norm of each challenge to the field size d, such that the result of each challenge should be bounded by (d-1)^2. Then maybe instead of prover submitting renormalized version of sum, prover can encode same length commit with entries in field, and submit this as overflow poly. Then verifier multiplies it by constant, subtracts it from computed sum, and should end up with new commit. The submitted data could be anything in the range of the native field so it need not be checked. But we need to prove soundness in that if prover submits incorrect overflow, the resulting commit will be invalid. 
So we have the following formula for any 1-norm k
k*[0,d-1] = d*[0,k-1] + [0,d-1]
and this is is optimal, but we are not concerned here with optimal entropy but optimal overall efficiency and for that we'd like the prover to return a commit of the same length with native elements, that is in [0,d-1]. So we'd like to set k=d, but multiplying by d is the same as 0. So we'd have to do k=d-1 but then the overflow poly is restricted to [0,d-2]

Oh, hah, if we use the overflow method then we still need to evalute the old poly after the modulo is performed, now in addition to the overflow poly so it only adds cost. 


Ok, to figure out holder, a first place to differentiate is whether each row of the commits is only bounded by infinity or something else? The only way it can be something else is if we relate it to the bound of the sum of the commit columns. 

Rows bounded by something else.
Suppose we use the euclidean bound of each column. Actually we will then need to relate this not to the bound of a row but to a bound of the sum of all rows. 

Rows bounded by infinity.
Now we know the bound of a row for any p < infinity is greater than the infinity bound. We want to compute the q norm of the challenge then compare the received one norm respose with the p norm and q norm. If x is a row, the since we can only assume it meets an infinity bound b, all our p norms of x will be (l*b^p)^{1/p} = b*l^{1/p} for l the length of the row. So the larger p is the smaller the p norm. But the larger p is the smaller q is, and thus the larger the q norm is for the challenge c. So for fixed x and c, how does the right side of the holder inequality change for p and q. It is
b*l^{(q-1)/q}(\sum_i x_i^q)^{1/q}
= b*l/l^{1/q}(\sum_i x_i^q)^{1/q}
= b*l(1/l*\sum_i x_i^q)^{1/q}
= b*l(\sum_i (x_i/l^{1/q})^q)^{1/q} = b*l*qnorm(x/l^{1/q})
well this function goes up with q. as q approaches infinity b*l^{(q-1)/q} approaches b*l and (\sum_i x_i^q)^{1/q} approaches max(x_i). This means the tightest bound we can get with respect to a c and an x assuming nothing except its infinity bound is for the smallest q. But remember holder inequality holds for non-norm values p and q, so we could take q to be less than 1, oh but this would make p negative, which is not allowed, so this only works when 1/p + 1/q = 1/r and r > 0. But our linear combination only allows for r = 1. 

Upon adding the range [0,d-1] times we get the range [0,(d-1)*k]. It only makes sense to have k < d. We want to figure out for a given k and d how to represent the result in division form. This means for a number x we want the form d*y + r. with 0 <= r < d. We want to know the possible values of y, and from that the possible values of r. The max value of y is the max number of multiples of d in (d-1)*k = d*k - k. Well since k < d I think the answer is max for y is k-1. Then max value of r is d-k. So to represent max value (d-1)*k we'd have d*(k-1) + (d-k) = d*k - d + d - k = d*k - k = k(d-1). This means overflows can take place in range [0,k] and residues in range [0,d-1]. But note this gives extra information, because
(k(d-1) + 1)^n > (k+1)^n * d^n. 

Any finite integral domain is a field, so we are stuck with fields. 

So i'm struggling on how to conveniently represent the summed commits in noramlized form such that no checks need to be performed on it. Checks, however, may be unavoidable, because if challenges don't have the same 1-norm, then k is variable, so the expected max of each challenge is different. Maybe we add together the 1 norm for all challenges, k'. Then we are left to represent the range [0,k'(d-1)]. But I think we may need to perform each challenge check by itself. Theoretically a way to do this is plug in the regular d range and allocate enough space to decompose each element. Then go through, make sure the encoded value does not overflow the space it should, despite already fitting in the allocated space. 
For a given k, each element will be in the range [0,k(d-1)] but each slot in the allocation space has range [0,d-1]. So we need to represent k(d-1)+1 possibilities with a power of d possibilities. The power will be Ceil(log(d,k(d-1)+1)) = Ceil(log(k(d-1)+1)/log(d)).


I'm thinking in a distributed network with high liveness it may be cheaper to do random selection of addition between polys. That is, we wait until enough polys have accumulated for evaluation, then we randomly select which polys should be added with which others.  


Suppose we have s (security parameter) random mask polys in the randomziation, such that for each challenge the probability is 2^{-s} that none of them appear in a random challenge. Well remember s is about how many polys we need for amortization. Or what if we just choose a single mask poly and add it to the randomized sum. But we might need the poly to have entries as large as the summed poly. For hiding, I think we cna say adding this random poly to the summed poly yields no info about the summed poly. Now for soundness. The summed poly is basically randomized prior to addition with the mask poly. Thus I think soundness holds. 
I think this must be done in the last layer. So in previous layers prover gathers its own polys and also the polys of others (which are zk). The polys of others, though hiding the data of others, cannot be leaked to the next party because that shows the next party which previous parties the prover interacted with, eg transaction history, so that leaks. So upon aggregating own and previous party polys, in the last layer the prover generates random mask, then randomizes the other polys, then adds the two. Actually we may need a mask for each output poly. 
How would this work for DLP? Prover takes the two or more polys from previous proofs, generates a single new mask poly, then randomizes the others, then adds the mask.
Now does the last layer leak data in its computation? Suppose all commits are blocked. Yes, it can leak by how it verifies previous proofs, so we need a way to block its sumcheck. Hmm, here unfortunately the prover will need to pass on these mask polys


It may be too complicated to do a single molting cycle involving among parties, and instead do one cycle or more cycles per party. But I can mention how the former is worth considering, though it makes zk and trust more complicated, but gives the benefit of smaller computations per party. 

note can pack two polys into one by concatenation then adding one extra variable, then evaluating at the same point. Thus we can commit to multiple small polys via one large poly. We reduce evaluating the small polys to the same point, then we reason about the evaluation of the concatenation poly at that point with added variables, which will be instantiated based on the claimed evaluations of the polys. For example, this would be useful for the mask polys in the sumcheck.

Maybe we can actually do one or more molting cycles per party regardless how much data that party does, and at the end each party always sends to the next the same amount of data. Remember the data of one party has two parts, the data that party received from others, and the data it generated on its own. For zk it can't send its own data in isolation. I think the prover provides all commits, which can be blinded. Then based on those a challenge is formed, and the prover computes the response using its own data together with previous party data. The input of the challenge may be smaller than the output. Then the prover adds a mask poly to each response poly, and sends those polys to the next prover. Note that the entries sizes of each of poly depend on the number of input polys to the challenge, so the fewer inputs the smaller the output, but only logarithmically. So all this logic is really happening in the circuit of the next party, and the circuit of the current party is oblivious.
For DLP this means the prover has the single poly accounting for past parties, its own single poly, and a mask poly. The challenge randomizes the former two polys, then the last poly is added. Now for full zk, the number of commits one party transfers to the next should be the same. We would like party to submit to the next the same number of polys and have each poly with the same max bound, and have that bound be tight. ...


Maybe it would be more efficient to encode polys in bits instead of the native field. Then upon randomization (and limiting the max number of bit poly inputs to the challenge) the max bound of the response polys can be in the native field. 

Realized one of the rows (eg the last) of the challenge matrix can be all 1s as far as poly eval soundness goes, because this corresponds to the constant term of the randomn point evaluation. 

Consider the equation b^x = 2^{security_parameter}. Suppose our challenge matrix contains x collumns and entries in b. Then this satisfies the poly eval soundness. Consider one of the rows to be 1s. So we will have at least 2 rows. For DLP we have 2 rows and b=2^{security_parameter} and x=1. But for lattice I think we need to restrict b to be polynomial in the security parameter. Since 2^{security_parameter} is exponential in the security parameter, we will need x to be super constant (maybe poly, or maybe just log) in the security parameter. 
Now we want to consider the knowledge soundness. 

Given a set of l commits. Consider a random challenge with infinity bound b. We could later consider viewing this as a random point of evaluation for a multivariate poly, to decrease the randomness needed, but for now for simplicity just consider a vector all with random entries, of which there are b^l. Suppose adversary succeeds with probability eps > 1/b for a random challenge. This means prover succeeds on eps*b^l of the b^l total space. How can we use this ability as a subroutine? Note that we can invoke it poly many times, and don't need to limit our usage according to how many challenges we issue in the real protocol.
Suppose we could find two challenges that are the same in all l entries except entry i where they differ by 1. Then subtracting the Z witnesses we get one for the i target. 
Suppose we use the subroutine to find a solution. This takes expected time 1/eps. Then focusing on i, we see if we can find another challenge that differs only in index i and only by 1. There are at most 2 such possibilities. We invoke the subroutine on these too, and see if either works. 
Suppose we consider the matrix H with boolean entries depending on whether adversary succeeds for corresponding position. Rows are enumerated by possibilities of all challenge entries except i, together with the adversary's randomness. The columns are enumerated by the possibilities of entry i, of which there are b. So upon landing in an entry in the matrix, we seek an entry adjacent to it. 
Now we are answering the question of whether the adversary can pass the test. But we will already have closed the door to the question of collision resistance. Suppose we land in two different rows of H, and then find entries in two pairs of columns that are the same distance apart. Then we have two witnesses for the multiple of the target corresponding to gap. If the witnesses are different we have a collision. But nothing here guarantees they will be different.
Note that if we obtain witnesses for two multiples of a target that differ by 1 we can subtract. This is not a re-statement, because above we are considering a gap in the challenges. This gap will determine a single multiple, and in this new context we are considering gaps between multiples. If we obtain more than b/2 gaps then at least one pair must be adjacent. 
Let's try to use the heavy argument. Then half the initial landings will be in heavy rows. Then we will execute in search of secondary landings on all b columns. If the row is heavy then at least b*eps/2 >= k of the entries will be filled. This requires b >= 2k/eps thus eps >= 2k/b. Thus we could have 1/b < eps < 2k/b yet no extraction method. Therefore we need to assume prover succeeds not with probability 1/b, but 2k/b, so we will need more challenges to reach the same soundness, and b > 2k. Thus the number of successes in a row will be less than half the row size.
So for every heavy row we hit, which will be half the time, we will obtain at least k-1 gaps, and likely more. We would like to continue until we have b/2 gaps or we find two adjacent gaps, or we find a single gap of width 1. What if we set k >= b/2? then b > b. So we must have k < b/2. Now the trouble is nothing seems to prevent us from getting the same gap over and over. Suppose we have multiple pairs of challenges, where for each pair all entries in the challenge are the same except index i for which there is there is a gap of width w. Both challenge pairs yield witnesses for a w multiple of the target, and we assume these witnesses are the same. Denote the tuples (ca1,ca2), (cb1,cb2) with witnesses (wa1,wa2) and (wb1,wb2) such that ca1-ca2 = cb1-cb2 and wa1-wa2 = wb1-wb2. Then ca1-cb1 = ca2-cb2 and wa1-wb1 = wa2-wb2. And these are not zero. This leads to a solution to the challenge ca1-cb1. I don't know how to take advantage of this.
Note that subtraction may end up with a challenge or witness containing negative entries. We are working modulo q, not modulo b. 
Note if we obtain two multiples m1 and m2 such that m1*c = m2 for constant c, then for witnesses w1 and w2 we must have w1*c = w2 unless there is collision. 
It seems if given two different multiples its always possible to use them to obtain a 1. Given multiples m1 and m2 we want small x and y such that
m1*x + m2*y = 1
The idea for Euclidean algorithm that is also why I think only two is necessary is given two distinct multiples, upon subtraction we will obtain a multiple that is less than the larger of the two. Then we may continue obtaining smaller multiples as long as our two multiples are distinct. So actually it can only yield 1 if the multiples are relatively prime. 
Intuitively, the problem is prover may be able to pass test only knowing witnesses for scaled targets, ie targets mapped into cosets. I think with enough challenges we could with high probability issue challenges such that the set of scalars for a target can yield 1, ie there is a pair for which gcd is 1. Suppose we establish this. Does this make it harder for the prover? 
Why can't we reason that if prover can pass with high enough probability then it can answer enough challenges (more than issued in the real protocol) such that we can solve for S given Z in the equation
SC = Z
thus AZ = TC => A(ZC^{-1}) = TCC^{-1} = T
If we can establish that C has a right inverse with small entries with high probability then S can be of the bounded form ZC^{-1}. 
Suppose prover succeeded on a challenge with probability 1/b. At this point the number of commits, ie the height of C, becomes relevant. For now just suppose some constant l. Now for poly eval soundness we need s/log(2,b) challenges. Thus the probability of passing all challenges is (1/b)^{s/log(2,b)} = 1/2^s. So we've lower bounded the number of challenges via the poly eval soundness rather than knowledge soundness. Note that in this approach we are not using a heavy matrix. Instead continually issue random challenges to the subroutine and we find a solution in expected time 1/eps. We will need eps large enough that we can solve for enough challenges that we can invert C. The number of challenges to invert C depends on the height of C. The number of challenges also depends on b due to poly eval soundness. Now if the number of columns induced by poly eval soundness, s/log(2,b), is to be sufficient for knowledge soundness then we must ensure that the adversary cannot pass a challenge with probability more than 1/b. This means that if the adversary can pass a challenge with probability more than 1/b then we should be able to extract from it. So it seems we are asking too much.
Maybe we could combine this with a heavy matrix, where upon finding a solution, we can probably find related solutions. But this only helps if we can recover more challenges this way than if we just randomly sample. 
The goal is to be successful with enough to establish a challenge matrix of rank l. We need to recover not just enough to pass the test, or not just enough to make a square matrix, but a little more than square. For each heavy row, we could collect k witnesses, this would happen almost half the time, so for k' extractions we'd have k'/2 + k'*k/2 witnesses. But for a heavy row to always have at least k means eps >= 2k/b thus 2k/b (> 1/b) must be the expected success probability, so this dictates a lower bound for the number of challenges. Recovering this number of challenges will be done with probability eps, and we need to take advantage of the k heavy rows to recover enough extras. 
The intuition is that by using larger scalars we can achieve the same soundness but with fewer challenges. This works for poly eval soundness, and I hope it works for knowledge soundness. 
So subroutine succeeds with probability eps for a challenge then it takes expected 1/eps samples to find one the subroutine succeeds on. The probability the subroutine succeeds on all of j challenges is eps^j, so soundness is multiplicative. What is the expected time for the subroutine to succeed on j different challenges? Clearly for j=1 it is eps. Suppose after finding j' of them we freshly sample, not avoiding those challenges already solved, but instead we just account for the probability we encounter one already solved. Then is seems it takes expected total time j*eps, thus recovery is additive, not multiplicative. This seems a huge advantage over the regular heavy argument that considers challenges in a single set, setting j=1 so that additive and multiplicative are the same. So the technique of finding a right-inverse to C may succeed just fine. Continue solving challenges past the number of challenges issued in the real protocol, until C has rank l with high probability. Suppose b is a prime. We would like to establish that the right inverse has small coefficients, say also up to b. 
We've been considering the heavy argument for solving two challenges C1 and C2 that differ in row i. We then subtract them hoping they contain adjacent constants in the same index. But what if instead we find a vector v such that the dot products of the ith rows of C1 and C2 with v yield adjacent constants. Actually, suppose we solve as usual to obtain
AZ1 = TC1, AZ2 = TC2
A(Z1 - Z2) = T(C1 - C2)
but instead of just choosing one of the indices that differs, hoping its 1, we instead hope at least 2 indices differ in such a way that we can find a small coef vector v such that the dot product of the ith row with v produces 1. Then we have
A((Z1 - Z2)v) = T(C1 - C2)v = t_i so the vector (Z1 - Z2)v is our solution. 
Another approach in this direction is to do the per-challenge version of the heavy argument, try to solve several scaled targets. Then instead of just hoping they can be added in such a way as to produce 1, we instead hope there is a vector v with small coefs such that the dot of v with the scalars produces 1. Then v multiplying the witnesses is a solution.
Of course this method gives a larger witness up to a factor as much as the coefs of v and also a multiplicative factor of the number of terms in the combination. We actually already partially considered this solution when looking for coprime multiples. 
The benefit of the right-inverse is the challenges are uniformly distributed, whereas for the heavy argument when we seek in a heavy row its not uniformly distributed so we can't statistically guess how the challenge may differ, only that it somehow differs. On the other hand, the benefit of the heavy argument we only need to normalize one row, whereas for the right-inverse method we must normalize all rows. 
There are b possible values for index i. If prover solves on whole challenge set with probability eps > 1/b then prover can solve on eps*b > 1 of those possible b values. It could be the prover always succeeds for 0 and just one other value. For any two indices i, j there are b^2 possibilities. If prover solves on whole challenge set with probability eps > 1/b then prover can solve on eps*b^2 > b of the b^2 possibilities. For the total of l indices there are b^l possibilities, so prover can solve on eps*b^l > b^{l-1} of all b^l possibilities. 
Suppose prover can solve on 0 and one other constant for a particular index, and maybe all constants for the other indices. This means solving a challenge with probability > 1/b. I think the problem we're having is prover can pass with non-negligible probabability as just said only knowing witnesses for scaled targets, when we need witnesses for non-scaled targets. This may justify not using scalars and just doing addition. 
A problem with the right-inverse method is that since our challenges non-negative, a vector to normalize them will require negative entries. This vector with negative entries will multiply the witnesses which may result in a witness with negative entries. Well I guess this happens with the regular heavy argument anyway. One possibility that may be helpful is allowing negative scalars in the challenges. This way a Z with negative entries is expected. In fact, the S vectors can also have negative entries, and in fact can be any integers, as they are all just different equivalence class representatives. 
Solving for a right inverse is equivalent to solving an l by j b-ary lattice basis C for a short lattice vector that results in one of the standard basis vectors. If we could phrase this as a reduction that we know is too good to be true then we don't need to present an algorithm for solving the reduced form, ie finding a right inverse. But one complication is we are solving for a relaxed form, ie a multiple of Z bound, not the Z bound. Another complication is the C lattice not actually a b-ary lattice because we are not doing modulo b. Its not necessarily the case that a solution to the modulo version means one can find a solution to the integer version, thus reducing to the modulo form may not be possible. 
The heavy method retries of a vector of bounded euclidean length for a target. But this is not a solution to SIS or ISIS, because targets are not necessarily random. But maybe we can reduce random ISIS to non-random ISIS. Given random ISIS, multiply both sides the constant c such that for random target r we have rc = t where t is the non-random target. The constant c is entirely dependent upon r and t which are both independent of the constants for the original problem, thus the constants in the derived problem remain random. If q is prime, then once we find a solution for the derived problem we can divide out the constant. In general, we want gcd(c,q) = 1 => gcd(t/r,q) = 1. So t cannot be 0. Maybe we should consider each row of A separately. 


Note we could even do 'independent' testing, where we issue one random test, then upon the response we issue the next test, using the fiat transform.

What if we make prover commit to not just a vector s but powers of it, eg s^2 and s^3. 


A big question is whether or not to use scalars. If we do, we would still need a nontrivial number of challenges. But compression would be better. 

b^l*eps/2 >= k
eps >= 2k/b^j

Now i'm thinking again about using a structured matrix, but now I'm not going to try to make it sparse. Instead we focus on a structure that will help with the knowledge proof, and the price will be that columns are not uniformly random and thus we must modify the poly eval soundness. We structure rows such that any two instances of a single row have at least one index that differs by 1, rather by a set such that gcd of that set is 1. 

One argument we could make is that when sampling a second row uniformly, there are many more 'helpful' rows than unhelpful rows, so the probability we select one find a successful unhelpful row before we either find a helpful one or abort is small. In this method we could possibly still select our columns as random evaluation points, which will probably be just fully random infinity-bound vectors. Since rows have indepedent entries we can analyze them index-wise. We will count the probability of each absolute difference. Then we need to calculate the probability of not gcd set.
Consider just getting absolute diff of 1. There are b^2 possibilities for 2 elements. There are b-1 differences that are 1, and b-1 differences that are negative 1. This is more than any other absolute difference. There are b differences of 0, 2(b-1) absolute diffs of 1, 2(b-2) absolute diffs of 2, ... 2(b-k) absolute diffs of k. So out of b^2 possibilities, 2(b-1) diffs are 1, so that happens with probability 2(b-1)/b^2. Now we compute the probability no indices of two random samples diff by 1. This is (1 - 2(b-1)/b^2)^{128/log(2,b)}. To make this negligible we need to do it multiple times. But I'm concerned our samples are not random enough. I think we would need to ensure all random secondary rows we draw would be valid, this way regardless which of them is successful for the adversary, it will work for extraction. So for two random samples we need the probability their diff would not work to be 2^{-s}. Then upon drawing n random samples the probability not all of them work for extraction is (1 - 1/2^s)^n. This remains <= 1/2^s as long as n is about less than s. Well we need n to be on the order of 1/eps, so this won't work.

Regarding ISIS and collision resistance. If one can find a collision for a target t where the witnesses w1 and w2 may have negative entries, we can subtract the witnesses to get an SIS witness but with twice the bound. So recovering a witness for a target with negative entries is not a problem. 

For a random challenge with b^l entries, suppose eps >= 1/b fraction, ie b^l*eps >= b^{l-1} of them are filled. Then b^l*eps/2 of the entries are in heavy rows. We haven't even decided the matrix format. But each heavy row has at least a fraction of eps/2 >= 1/2b, ie j*eps/2 >= j/2b entries filled. 

I think we already stated intuitively why scalars may not be sufficient. Suppose there is only 1 target prover doesn't know witness for, but prover knows witness for a multiple of it, c, that sends that target into a coset. Then prover succeeds with probability 2/b, which is more than we want. We are worried the prover only has knowledge for multiples. We I think we can account for these multiples by increasing challenge numbers. On the other hand, the more distinct multiples a prover has knowledge for, the greater the probability the set has gcd 1. Thus we may be able to balance this trandeoff. For example, suppose prover only has knowledge for the multiple k. Then the probability of passing is about 1/k > 1/b. This only means increasing the number of challenges by a factor of log(1/b)/log(1/k) = log(b)/log(k). But this already accounts for all multiples of a 1/k of the possible multiples. When we account for more multiples, and what multiples they can produce together with previous multiples, our challenge number will need to further increase, but the probability of gcd 1 will approach 1. The next step is to figure out how many targets a prover can know without that set having gcd 1.

What if we multiply both sides by a multiplicative inverse of the multiple in b? Then the scalar of the target becomes m*b+1 for some m. 
If we could have knowledge for a multiple that divides b, we could have knowledge for b, then we could subtract m*b above and end up with the target. Oh, but b is prime so nothing divides it. Actually b doesn't need to be prime though and could be anything, because that still works for zippel. But if b is not prime then not all multiples will have inverses. 

What if we choose b=4 (only a little better than b=2) so any distinct subtraction in the set {0,1,2,3} results in multiples of the set {1,2,3}, which all have gcd 1. But what is to guarantee that if we get multiple 2 we will also get multiple 3 or vice versa? 

I think the technique of accounting for multiples that don't have gcd 1 is too costly. Even just accounting for the multiple 2 means increasing the challenge number by a factor log(b)/log(2), which is 2 in the case of b=4 meaning we must double the number of challenges, eg from 128 to 256. More challenges is not what we want and defeats the point of scalars. 

And now I realize the right-inverse method won't work for the same reasons. A row in C could have gcd > 1. In other words, there's no guarantee any two, much less l columns will be linearly independent. 

So before I abandon scalars, I want to leave one prime example of why this doesn't seems to work. Suppose prover knows witnesses for all targets except for one, knowing only a witness of multiple 2. Then prover passes in (at least) b/2 out of the b possibilities for that index, and all possibilities for other indices. This means passing with probability 1/2. Thus the number of challenges is no better than if b=2. And at the moment we don't know any way to take advantage of knowledge of a scaled target.

So then it only remains to go with binary challenges. What if we also test scaled targets with binary challenges? Is this the same as scalar challenges? I think its a bit different because we can fully choose the scalars rather than leaving it probabilistic. Suppose we choose our scalars, then do binary challenges. The binary challenges should prove knowledge of the targets, which are specific scalar multiples. It seems there is no benefit, because we must still employ the same number of binary tests. 

I think by using the regular binary heavy argument, we can do at least one molting cycle per proof. 
I don't think the witneses need to be checked using holder. instead we just need it inside the binding space, and we can designate a fixed amount of memory for that such that anything that fits in that memory is valid. 
I'm wondering about the concept of optionsal zk. This is where a prover decides whether or not to do the extra costs of zk, and it doesn't much for verifier. I think it should work and be a meaningful concept worth defining.
For zk we need a fixed number of commits for each molting cycle. If prover has more and wants zk then probably needs to do multiple cycles. If prover has fewer than prover can create some extras that are zero vector or one already had, but with a different mask. Then despite different looking witnesses, they shouldn't matter because random poly mask can compensate for them.
So the standard zk delivery would be fixed number of witnesses of fixed bound. This would introduce a standard conversion to normalized form. Suppose our normalized bound is b, and our standard witness bound is m*b. Since log(b,b) = 1 it takes 1 memory slot (normalized form) to encode a b value. Since log(b,mb) = log(b,m) + log(b,b) = log(b,m) + 1 each witness element will be placed into log(b,m)+1 memory slots.


Its worth thinking about the programming logic. Maybe its part of the model. We'd like to think of it as computer programs. Recursion is possible because we use meta data to describe the logic, and we can also amortize those evaluations. Otherwise we would hard code everything and verifier's circuit would have to simulate the proof circuit which is itself a verifying circuit. The other papers had to deal with this partially, but we avoid it by amortization. I'm now wondering if we can use our method with non-holographic proofs, eg with STARKs. Here verifier circuit must know the (uniform) logic of the proof. I suppose this is possible by the recursion theorem. 
But for the holomorphic type I'm wondering how our meta data can efficiently express our logic. The parameters, like sizes of data, and number of instances, will affect the polys so we need a way to handle this variancy while still only having a small set of polye to evaluate to keep the eval amorization possible.
Our basic operations are addition and multiplication, and above that we can define more complex operations, which are basically polynomials, together with abstract operations like map and reduce. We want to create a language generic and efficient enough that we want to make it a standard for the GKR type proofs. 


is there any use in proving the zippel lemma for alternative distributions, like p-norms? if we use p-norms for challenges then the rows of the matrix are not enumerable for the heavy-argument. 


suppose we use {-1,0,1} for scalars instead of just {0,1} such that we either subtract one direction to get 1 or we add to get 0, which contradicts SIS. Then we have j >= (s+1)log(2)/log(3) + log(2)/log(3) = (s+2)log(2)/log(3), which seems a good improvement, and very luckily it seems adding -1 as a scalar doesn't increase the amount of information in each witness like 2 would for example. Let's recalculate amortization.
Suppose standard commits are in b. Length m stays the same so we leave it out. Then we take l of them and amortize. So we start out with entropy b^l, and after amortization we have (bl)^j. So breaking the threshold means b^l > (bl)^j => l*log(b) > (s+2)log(2)/log(3) * log(bl) => l > (s+2)log(2)/log(3)(1 + log(l)/log(b)) => l/(1 + log(l)/log(b)) > (s+2)log(2)/log(3). 

But now we should probably employ an actual holder-inequality check. actually, not really. we could just keep track of 'offset' and otherwise we just use a slot of an l-multiple. The offset will be one of l values I think, so it has size bit size log(2,l). 

recall how we will performing the opening? the witnesses will be encoded in normalized b-form but provided to a circuit in the additive q-field. The statement to be proven is that the witness commits are valid openings for the derived target commits. In fact, could we do all q-operations in this circuit? This means multiplying TC. I suppose so. Then the circuit statement is that new targets T' represent valid witnesses for T in terms of C, and maybe even C can be local to this side-circuit where it is computed from T. The witnesses of T' are to be evaluated as polys at the points proposed for T, but instead of doing this in the circuit it would be nice to just derive a new evaluation claim directly, and this may be possible by encoding the normalized witnesses such that they represent appropriate polys. So we will want to compute each term t_i*c_j multiplied by a random element r, as part of a sum. But t_i*c_j is not normalized so we need to split it into normalized parts and each part is a term multiplied by r that belongs to a different poly. The challenge is normalizing so the noramlized polys evaluate correctly only if the unnormalized would evaluate correctly. Doing it this way may not be so important, because we will be going through all terms t_i*c_j for opening anyway, so we could also probably evaluate them as we go with little extra cost. 
we want the verification of this proof to be simple, and we hope to achieve that because it should involve few multiplications, and without the need to compute TC I think it involes zero multiplications, and even with TC that fact that C is probably of {-1,0,1} may help in someway to do metamultiplication. 
We can have our new polys W' with targets T' and with encode polys K' and zero poly z and constant poly c and divider constant m we can set up a comutations with arbitrary additive constraints. So with . meaning dot product we'd have something like 
w1'.k1 + w2'.k2 + ... + c = m*z
and the degrees of k_i and z signifies the number of constraints. Before we think about the metamultiplication, what about the ease of verifying this? Verification means taking the commits to the transcripts and m and choosing a random point of evaluation. We are in a big field q (q is prime) so we should only need 1 point of evaluation. (In fact, choosing q to be this big, eg 128 bits, we may be able to improve the amortization in some ways, and hopefully a larger q means a smaller worst-case lattice dimension.) Giving a point of evaluation we are to evaluate both sides and compare. Of course doing this is more work than manually computing the whole thing. The benefit of this form is that we can use crypto or amortzation.

For crypto using DLP we secretly choose random eval point r then evaluate each encode poly k_i then encrypt it using DLP. Prover exponentiates each encryption then multiplies them together. z will also be evaluated at secret point and encrypted. QAPs uses multiplication so is pairings based, and now I'm wondering if this simpler version without multiplication would still work. I suppose so.
Could we make this work with lattices instead? Encrypting a value would mean splitting it up into multiple pieces for each constant rather than a single constants like DLP. We would add and multiply encryptions. I think making sure prover used encryptions requires consistency checking that requires multiplying encryptions, which we don't know how to do with lattices. 

For amortization we can take a random linear combination of multiple transcripts and they should form a valid transcript. Now for DLP this is easy, but for lattices remember we need quite a number of instances before we can do sufficient randomization. In particular, given any number of instances we will need to generate not just one randomized instance but many. But actually we know this is sustainable because the output size only grows logarithmically with the input size. So maybe we just verify by this randomized amortization, but notice its this side-circuit we are trying to verify that was designed for computing randomization and amortization operations in the first place. But this is not circular dependency because a later instance will verify a former instance. In fact, the randomization for this can probably be taken care of via the regular randomization for poly evaluation, because that poly randomization doesn't depend on the random point of evaluation for them. So for poly evaluation we have a random point of evaluation, and for this circuit we have a different kind of evaluation, but in both cases amortization happens by taking random linear combinations of the transcripts.

Now regarding communication between the circuits for efficiency we can encode a q value via multiple b values, and then have the q circuit construct its inputs and deconstruct its outputs via linear combinations. So all statements are expressed via b values. Bits work too, but b values are  ore efficient. 

Now we can talk about metamultiplication. This means we have a transcript value c in {-1,0,1} to multiply another transcript value v. We can't directly multiply them. Instead we will need to multiply v by one or more constants and c must dictate how we do this. We are limited to linear combination so this kind of case-dependent reasoning, like branching, may not be possible. Forget 'multiplication' and instead think of it as addition dependent on data rather than fixed constants. 
If we can't figure it out then it appears we must compute TC in the main circuit, and if q is prime we can't use the isomorphism but must do long arithmetic.

I think we could generally define our poly molting scheme by a single amortization round that shows how entropy is dropped. It consists of additively homomorphic commitments, and the zippel lemma, and reducing evaluation to a single point then randomizing sufficiently. This describes both DLP and lattice just with different params along a spectrum. 

If we assume q is prime, which we need for the side-circuit approach, can we get any advantages? Intuitively, making q prime should make the problem harder. For any non-zero target there always exists a unique scalar (maybe large) that yield any other arbitrary target. We're interested in hitting an arbitrary target by a linear combination of small scalars 

We'd also like to use this side-circuit for fiat-shamir transforms. Can we justify that SIS type hashes are sufficient, maybe with more than one layer? 

We still haven't figured out the side-circuit. Instead of trying a QAP-type structure, we could try to take advantage of the uniformity of the SIS computations, using STARK-like structure, which is a different version of poly-div. Like
w(x)k(x) = h(x)z(x)
but now I realize this is not good for summing. Maybe we can create a custom proof structure.
Or maybe we just use regular map-reduce sumcheck, which would take place over the prime field q and would be able to take care of the multiplication in TC. The challenge is performing the verification for this proof, but maybe even this can be delegated to a future side-circuit. Suppose we were just doing SIS, maybe multiple times and multiple instances. Then we take a random linear combination of each. We iterate log(m) many rounds. At each round we should receive a quadratic poly, one variable coming from the transcript poly, the other coming from the consants poly, and they multiply each other. At the end we evaluate all polys at a single point. But what I described so far is only a linear function which could be done in QAP structure. So its only worth it, and won't be much more complex, if we do multiplications. With this simple method we could incorporate the multiplications of TC by the verifier (delegated to future side-circuit) calculating the expected output, the TC values. Note there would be a long linear combination on the order of n*s, the worst-case lattice dimension and the security parameter. 

We could maybe make q prime and still have an isomorphism via the multiplicative group of an extension field, make q a Mersenne prime so q = 2^d - 1 so that its cyclic and the same order (and thus isomorphic to) the multiplicative group of the extension field of order 2^d. Other bases don't generalize to primes of this form, buy maybe we could find a small prime base b such that we can use a subgroup of the multiplicative group of an extension field, which is always cyclic. So any subgroup is also cyclic and to waste as little as possible we want the divisor to be small, ideally 2 (a single bit wasted). This means we want (b^n-1)/k = q for small k to be prime. If we use subgroup we want minimal effort in checking inputs are in the subgroup. One way is to present a natural number for the cyclic position (no restriction here), then for the verifier to generate it, but this requires log(2,q) extra multiplications.
By the way, doing the poly computations should be done by computing the respective polys, then for modulo both divisor and remainder are submitted as auxiliary, and it should be easy to check. A question is how often we do modulo, extremes being once for every multiplication and once for every final answer.

So if we can indeed use the isomorphism then we can use the linear QAP structure. Can we do layered hashes this way too? Yes I think so. It means we add a constraint to destructure an output to a new input. This are intuitively span programs, were we have a target vector and we have a set of basis vector and any valid witness is a set of scalars such that a linear combination of them and the basis vectors is the target poly. 


Map to another diff poly that is incorrect but shares at least one common root with the real diff poly. For quadratics there are about |F| of them. Then breaking means mapping to a root of one of these. We must compare the |F| with q. If |F| is significantly less than q this becomes hard, but we will need |F| to be large.
The cost of making q big is large hash output, and since all of it should be used this means large field size, so |F| and q grow with each other and are not independent. 
The way we planned to do hashing is make q and n large enough for security, such that it allows for compression, then set the field size to about this output size. So actually q and |F| are about the same, at least proportional. 


Maybe we make q the main field size. It can still be small enough for efficiency. But then commit entries will need to be smaller than q, which shouldn't be a problem. In any case, we use the commit entries to do our computations and only require that they are in the binding space, which reduces to our randomization check. Normalization should work too. 

We need to figure out how to split our logic and theorems and schemes into modules. For example, using q for field size or a custom field size both end up with basically the same lattice scheme so we'd like to describe the lattice scheme independently. 
Maybe we can describe molting schemes independently of recursion, leaving parts like the vierfier abstract (a person or a circuit). We could require that a molting scheme be a complete cycle, closing the loop. So for lattices it would also involve the normalization. Another option is for us only to require a sublinear amortization argument, which may be cleaner.
We need to decide how to model a sagaproof. Prover and verifier are not so clear roles. Every proof has a prover, a circuit, input and output, a proof string. One or more parties can verify this proof. 

For lattices, what if we try designing together with a network to help with efficiency regarding the high amortization costs. I'm thinking of doing the amortization offline, which means the witnesses for a single amortizaiton round are those for the whole network. Proofs are passed up the tree, the commits are committed to, which is not the same as a fiat transform. At the top we actually do a network wide interactive proof random response to avoid fiat transform. But in the next network round circuits would still need to compute the new witnesses and do normalization. So really the only use is preventing the fiat transform, and constants wouldn't be better. Just as much if not more hashing would be necessary, and parties would still have to prove to each other knowledge of valid witnesses requiring high communication costs. 


So we'd like to describe how proofs can be composed via tail recursion with a sublinear amortization scheme. Then we show our sublinear amortization scheme for polys, and their particular versions for DLP and SIS and their zk variants. Then we show the details of normalization, side-circuits, circuit communication, etc. I think we'd like to describe a sagaproof as one for an abstract proof setting where we reduce verifying computation to verifying a property of data in a commit, and the sublinear scheme has the formal purpose of reducing verifing the property for commits to some computation (not necessarily doing the same verification) that requires sublinear data in the commits. Sagaproofs necessarily require the notion of commits. We can formally define what a proof scheme is, what a commit to data, and a 'propert' of the data is, and how they must interact, and then show how our poly scheme satisfies this, where our property I think is evaluation claims per poly, which means that eval reduction belongs particularly to the sublinear scheme part of our sagaproof instantiation. Our sagaproof defintion would be two fold, with the general defintion involving the definition of a sublinear argument, which will be defined prior and an independent definition. Commits will be defined prior to that. I think we would define the 'rate' of a sublinear scheme and then in the sagaproofs definition relabel it in accordance with a notion of molting. 

notice our sublinear scheme is necessarily dependent on commits or else it would not be sublinear. we shoud call it a sublinear property argument (SPA). and note that since commits rely on owf, we must use computational soudness, not statistical so we must say 'argument' instead of 'proof'. 

apart from the introduction where we give some informal descriptions, I don't want to talk about polynomial details until section 3. In fact, section 2 should be completely abstract. 

Maybe instead of using ROM we don't accept it doesn't work for our recursive proofs and instead state our assumption about fiat transforms. So we define V as a deterministic I think, not probabilistic, while P remains ppt. 

I don't think we need to involve the logic and/or recursion theorem in the model. 

should we have a compliance predicate? Remember we need a finite amount of logic, equivalent to a codebase with many functions. New functions can be added, but dropping or modifying is harder. Can we design this abstractly as part of the model? If our proof system treats code like data then verification with regard to the code should reduce to verifying the property of the data for that code. But the code is static data, so we only need to reduce evaluating two possible properties for the code data to verifying one property. We could work this, and the codebase, into the model if its indeed the only way we can efficiently implement logic. The method I have in mind depends on the 'property' being ealuating a poly a random point, so QAPs related structures won't work. 

A downside of the FRACTAL preprocessing is it requires the code to be fixed. We can say how theoretically this is not a problem because of universal machines, but it is not efficient. 

Maybe we could do a QAP/aurora type structure where use linear amortization for the matrix satisfactions, and we employ something else for the multiplication. Maybe we could use sumcheck for the multiplication. So start out with a witness commit w. Also make commits x,y,z purportedly such that x = Aw, y = Bw, z = Cw, and x.y = z. Remember commits are just vectors. Given multiple witnesses w_i we just take their random linear combinations (and corresponding claims) for the linear amortization. For multiplication with sumcheck we will need to evaluate the polys at a random point. For eval reduction we will treat them as multilinear polys. We will use the same randomization for both the linear and multiplicative amortizations. 
Actually there seems to be a simpler way that I could call my own, and it may work for both DLP and SIS and be a general non-uniform proof system, whereas sumcheck is suited for uniform. We have a witness w for each proof together with a 'divisor poly' h. Each proof consists of these two polys in commit form. Suppose w is of length n. Suppose we put m constraints on the witness. Suppose we do selections via 'code' matrices Ki. We will speak of 'the poly induced by Ki' to be the vector of coefficients Kiw. We can consider the witnesses as consisting of multiple parts, yet still have it packaged as a single commit. 
For a moment just consider a quadratic system K1w.K2w = K3w. To prove this we will use the poly div method. This means we will show that (K1w)(x)*(K2w)(x) - (K3w)(x) = h(x)*z(x), which means evaluating (K1w), (K2w), (K3w), and h at the same random point, which really means computing scalars rK1w, rK2w, rK3w, rh, rz via matrix arithmetic. For now only consider those with w. We can pick a random 'point' for eval reduction (eg 1,s,s^2,s^3,etc) and instead of evaluating the poly for each Ki we take the respective linear combination and evaluate that, ie 1*rK1w + s*rK2w + s^2*rK3w = r(1*K1 + s*K2 + s^2*K3)w. So we reduce the verification evaluating (1*K1 + s*K2 + s^2*K3)w and h and z at the random point r. 
But for amortization we will do this a little different and keep the code matrices separate rather than merging, such that they can instead by merged across instances. So a proof consists of w and h, which induces the single random point r for evaluation in a fiat transform, which prompts the prover to include in the proof claims of rKiw, rh and rz. Verification means checking the evaluation equation and the claimed evaluations. But an amortizing circuit will only do the former, and amortize the latter. For two proofs (wi,hi) and corresponding eval points ri and corresponding claimed evaluations, we first reduce to evaluation at the single point r which means interpreting as mutilinear polys. Then we choose random s and reduce to computing \sum_i s^i*rKjwi = rKj(\sum_i s^i*wi), and also \sum_i s^i*hi, both of which are acts of taking random linear combinations of the witness commits. So our conditions regarding h polys will be a claimed evaluation, while our conditions regarding w will be that an induced poly for each Ki has a claimed evaluation. We we are doing regular poly eval reduction but now we are cleverly doing it with respect to induced polys. So unlike Aurora and such, we have avoided the need to commit to any encoding of Kiw, instead only w. 
I wish we could take more advantage of how the same r is used across different Ki. An alternative to poly div that may not leave this lost opportunity, and operates in linear rather than quasilinear time, and avoids the need for h but incurs additional rounds, is sumcheck. We perform the sumcheck over the invisible induced polys pi something like \sum_x (p1(x)*p2(x) - p3(x))u^x, where u is a random point multi-point to ensure with high probability the sum being zero implies all terms are 0. So we are summing over the constraints. But in fact, this too yields evaluations for each induced poly at the same point, the difference is its a multi-point not a uni-point. 
Hmm, maybe we could in fact merge in both ways, across both Ki and across instances, by waiting to choose the random linear combination for Ki until we have both instances. So we begin with claimed evaluations with respect to all Ki for all instances. From that we choose a single s that will serve for Ki for both instances, so we are left to evaluate ri(1*K1 + s*K2 + s^2*K3)wi. Then we reduce these two polys to one evaluation point r (before we had to do this protocol for each Ki). Then we choose random t and reduce to r(1*K1 + s*K2 + s^2*K3)(1*w1 + t*w2 + t^2*w3 + ...). Oh, but this only works for one round of merging, because we have the same problem, that once s is chosen we can't do it again. 


It appears 2^d amount of auxiliary data is equivalent to d rounds. Since any auxiliary data in our model must be sent between parties, we want as little as possible while still keeping the round number poly log. Note that round complexity is proportional to randomness complexity. 
What if we increase degree of index variables in sumcheck, reducing rounds? Then the intermediate polys get bigger, and soundness decreases. Consider the extreme of only one variable in which case it must take on all enumerable values. Then prover will send integrand as expanded poly and prover is to enumerate it at all points, verify final sum, and then is left to verify the expanded form is correct by evaluating it at random point and comparing with factored form. How does this relate to the poly div method? 
I think the compression of the fiat transform also relates to its security and round and randomness complexity. But we should first consider the above relationships in ROM. 
Beginning this paragraph I was thinking of how h is interchangable with more rounds. But h is a certain kind of derived data, different from intermediate values. I think we can say that all computation data, input, output, and intermediate values, must be processed, either in rounds or in final poly evaluations. The case for h is its not part of computation data, so it can be replaced by rounds. Our amortization is suited for reducing the processing of many polys to that of fewer polys. I'm wondering now if it would be worth the large number of commits to have commits commit to less data, in effort to minimize communication cost. But the commits must be sent, so we now have an optimization problem.
First consider DLP where commit size is constant with respect to commit load. Communication will consist of all commits and a single poly. The total data is n, and say there are k commits each of length n/k. So communication is about n/k + k. Taking derivative with respect to k and setting to zero we get 1 = n/k^2 so min occurs when k = \sqrt{n}. 
SIS is more complex both because commit sizes grow with commit load, and because the amount of info sent depends on the number of commits. For oversimplification, suppose commit size is constant with respect to commit load and we must send s commits each containing the same about of data as regular commits. Then communication is sn/k + k where this time optimizaiton yields k = \sqrt{sn}. 
But note that minimizing total communication will increase circuit communication due to the commits. 

How about zk for the non-uniform case. I think prover could choose a single masking poly for the witness, which could be used across all Ki. Prover first makes eval claim about mask then its randomized with witness.

What about the non-uniform sumcheck version, is that suitable? basically the integrand involves the transcript and the code polys and we enumerate over the constraints. A difference is this is matrix based non-uniform model is more friendly towards linear combinations rather than just additions, which is important for our hashing, so I suppose we stick with that, and we can use the sumcheck for multiplication but mention poly div is an option too. 

Maybe we could characterize proofs that can be proven in linear time. What's special about linear is the functions are closed under composition and don't blow up. So what does it mean to 'compose' in this context? The input (variable) is the statement size, and the output is the time it takes to prove it. So upon composition we assign the time it took to prove the first statement to size of the second statement. So suppose, say, the second statement is that the first statement was proven in a certain way, ie the second statement doesn't just care about proof of first statement but also about process prover went through to prove it. Now we know we can compose a constant number of times, therefore upon composition prover time remains a linear function, but of course with bigger constants than the sub-times. Note we can also add linear functions to get another, which corresponds to the times for two parallel proofs. 
We have the ability to generate a proof that a witness is valid in no more time than it takes to verify the witness by itself. I'm not sure what kind of characterization we're looking for because we already know GKR can do low depth statements in linear time, and we know how depth can be increased at the cost of increased verification time. 

What about the STARK-type structure? This is like the non-uniform except polys are not induced, but are used directly so the circuit is more complex. And actually this would require the recursion theorem probably. The recursion theorem is not needed for the non-uniform case because verification, apart from input and output, is independent of the logic. 

Would the non-uniform version work for lattices? we don't have to compute the induced polys in the commit, we just need to take random linear combinations of the witnesses. Due to overflow eventually one must be submitted and analyzed by the circuit directly. But unlike with GKR the analysis not evaluation at a random point, but rather evaluation of the induced poly at a random point, which means computing the induced poly which requires the recursion theorem. But I realize this is not the only way to analyze directly. Instead, maybe we could renormalize. But actually there is another problem and that is that submitting witnesses requires putting them inside a new witness, thus witness size blows up. Could there be a separate circuit specifically for renormalization? It would take a commit and its claimed normalization commit(s), which would consist of multiple vectors, namely the divisors and the residues. So the witness size for this circuit would be multiple times the normal witness size. This side-circuit would not require any multiplications, but it could and we could use that to compute TC, but this would introduce more complexities. Suppose the side-circuit uses no multiplications. Then there is not need to evaluate induced polys at a random point. Instead we can just take random linear combinations of witnesses to amortize. But witnesses are in b, so we'd only like {-1,0,1} scalars, so I think this randomization could be exactly the same as the regular normalization and take place in the main circuit. Thus the side-circuit serves only to open a normalized commit, a purely linear operation that requires reconstructing the witnesses to their unnormalized form then multiplying with constants. Soundness relies on correct formatting, that the witness contains small (though not exact) coefs, and we are already taking care of this with regular randomization. In fact, I think the side-circuit code would be A*D where A is the SIS matrix and D is the reconstruction matrix, but we'd also incorporate the expected outputs and subtract so a valid witness returns 0. Now do we need q to be prime? We want soundness to rely on linear combinations of witnesses, and this works but the problem is we plan on renormalizing these linear combinations, and maybe a linear combination results in non-zero (invalid) before normalization but zero (valid) after. Suppose E is our code matrix. We're wondering if E(x mod b) = 0 => Ex = 0. Ex = E(x - x mod b) + E(x mod b). In other words it is possible Ex = E(x - x mod b) != 0? It q was prime we could divde each vector element that is non zero to get E(x*x^{-1} mod b) = 0 where x*x^{-1} mod b will be a bit vector, and I think we can show this is close to solving SIS so its not feasible. But not sure about composite q, which is necessary for isomorphism.


For bit checks in the non-uniform model, the 3 induced polys will all be the same, because the constraint is x*x = x. I don't know how to optimise with this except that after sumcheck we only need to evaluate one of these instead of 3. If we mixed these bit constraints with the others the polys would not be the same so we couldn't do this and we'd be wasting space. 

For hash in DLP setting we should use SIS rather than DLP because we don't need homomorphic property and its easier to compute as can be done with n linear combinations. 


Could we amortize bilinear pairings? 
e(a1,b1)=c1, e(a2,b2)=c2
Now lets try to reduce to a single point. First construct lines a1*(1-t) + a2*t and b1*(1-t) + b2*t
e(a1*(1-t) + a2*t, b1*(1-t) + b2*t) = e(a1,b1)*(1-t)*(1-t) + e(a1,b2)*(1-t)*t + e(a2,b1)*t*(1-t) + e(a2,b2)*t*t
So to evaluate this prover could basically do interpolation by providing additional claims e(a1,b2) and e(a2,b1). 
Then verifier chooses random point r on line, and we are left to compute a single evaluation. 
Is this sound? Correctness holds. Keep in mind this is not a polynomial so lets write again in multiplicative form.
Our lines are a1^(1-t)*a2^t and b1^(1-t)*b2^t
e(a1^(1-t)*a2^t, b1^(1-t)*b2^t) = e(a1,b1)^{(1-t)(1-t)} * e(a1,b2)^{(1-t)t} * e(a2,b1)^{t(1-t)} * e(a2,b2)^{tt}
Suppose this equation does not hold for all t. Is it like a poly meaning that then it does not hold for most t, enabling soundness by sampling random t? Well suppose we have claims c1,c2,c3,c4 of which at least one is wrong.
e(a1^(1-t)*a2^t, b1^(1-t)*b2^t) = c1^{(1-t)(1-t)} * c2^{(1-t)t} * c3^{t(1-t)} * c4^{tt}
We know both groups are isomorphic to each other (because they have same prime order and are cyclic) and thus isomorphic to a prime cyclic group which is a field. While infeasible to compute, we can consider the iso map f that maps to elements of the field modulo p. 
f(e(a1,b1))*(1-t)(1-t) + f(e(a1,b2))*(1-t)t + f(e(a2,b1))*t(1-t) + f(e(a2,b2))*tt
	= f(c1)*(1-t)(1-t) + f(c2)*(1-t)t + f(c3)*t(1-t) + f(c4)*tt
So we comparing two quadratic polys, so indeed zippel should hold. 
Suppose we want to reduce from 4 to 1. Doing it 3 times means computing 2*3 extra claims. Doing it 1 time means computing 4*3 extra claims. What about 3 to 1? Two times means 2*2 extra claims. One time means 3*2 extra claims. In general consider reducing 2^n to 1. 2^n-1 times means 2*(2^n-1) extra claims. One time means 2^n*(2^n-1). So doing pairwise is more efficient generally but requires more randomness and fiat transforms.
Note that this amortization is just another case of our general poly eval amortization.
Using this scheme means instead of computing pairings in the circuit we will compute exponentiations. 

Now how could we use pairings? The point is to reduce communication. Ideally we could perform poly evaluation with pairings. I suppose with a setup we could encrypt a random evaluation point, but then we would need the protocol to be single round, like poly div. Consider the non-uniform system with poly div. Prover computes induced polys and evalutes them, and does same for h (z is preprocessed). I realize this won't work unless we make it circuit dependent otherwise nothing is preventing prover from using difference circuit.
How can we reduce evaluation at an arbitrary point to evaluation at a fixed, secret point? Suppose we have secret points s_i (a log number of them), and we need to evaluate at points r_i. Suppose we make the standard line
r_i*(1-t) + s_i*t
and ask prover for evaluation along this line. Knowing evaluations for all subsets s_i the prover can compute this in the exponent and return coefficients for the univariate in the exponent, and I think this can be done efficiently. Then verifier chooses random t = r' and evaluates both line and uni poly. But from here I don't know how to do final evaluation using pairings. 

Looks like we'd like an equation that relates a poly evaluated at 2 different points (r and s) such that we can compute f(r) via f(s) and r. Apparently one can construct polys q_i such that for any r f(x) - f(r) = \sum_{i to v} (x_i - r_i)*q_i(x). Thus we can compute f(r) by f(s) - \sum_{i to v} (s_i - r_i)*q_i(s). This can be made zk. 
To review the protocol, prover evalutes poly on secret s and that is considered the poly commit (different from our usual commits). Then we determine r and then prover commits to q_i. Then verifier verifies the final equation with pairings, which requires 1 + l pairings. 
For the moment suppose the target group is one of the cyclic groups (i'm not yet clear on this) such that amortization can be efficient.
So how do we use this new commitment scheme for our amortization. As usual we reduce to evaluating polys at certain points. We could just do this using the pairings and amortize them. But suppose we try to do better by reducing to evaluation at the same point r for all polys. Then we'd like a commitment to a random linear combination of the polys, such that we only need to evaluate that. Well our commitment is not the same as usual, but the commits can undergo the same homomorphic manipulation as with the usual commits to obtain this desired new commit. Then verification is left to pairings for a single evaluation. Unclear at the moment which way is more efficient, the latter or the former. 

Well it turns out the target group for the pairing is a multiplicative group is an extension field, of the field containing the elliptic curves, which the native fields thus allowing the right hand side for our amortization equation to be computed efficiently. 

This construction is a bit different from our others because the amortization doesn't make it sublinear (it already is) but rather avoid the need to compute the pairing inside the circuit. So doesn't quite fit our model for sagaproofs built from sublinear schemes. Maybe it should go as a last (or first), as an aside.

Does this eval scheme work for the non-uniform system? It we commit to the witness then we are set to evaluate that, but we instead want to evaluate the induced polys. If we commit to the induced polys then there's no guarantee they are induced correctly without circuit-dependent encryption. So I don't think it will work.

Maybe lattice scheme can be described by itself, like DLP scheme can. In particular, I'm thinking how we have the pattern in both uniform and non-uniform case that we must renormalize. In the uniform, upon renormalization we test the property, whereas for non-uniform we want to delegate the property to the renormalized form. Such delegation doesn't help in the uniform case because the property is direct evaluation at a random point and that comes at little extra cost, whereas in the non-uniform case computing the property directly means computing the induced poly, and then doing multiplications on it, which means a circuit bigger than the regular circuit, and then we must verify for this bigger circuit etc so we must use the recursion theorem. 
I'm wondering if we can do better regarding renormalization in general.
What prevents us from scaling into cosets before re-normalization. Suppose b divides q. Given an unnormalized commit we scale it by q/b. What was the problem before when we tried something like this? It was that we thought prover could then submit the normalized target and we'd scale it to make sure its correct, but this won't work because many targets could properly match the correct one due to the correct being in a coset. So what if the prover provides the normalized form and we directly examine it, scaling not the target but each coef appropriately, so we are doing an opening as before but now with less entropy. Is this sound? lol no because prover could have large coefs and upon scaling by q/b they would all appear in the valid coset. 
Another idea was having prover split a commit into entries that are even and odd. Then prover gives normalized version (scaled bye 1/2) of the even one. Verifier multiplies by 1/2 and compares against previous. This seems dangerous because we are asking for info from the verifier without randomization, and doing so seems bound to offer the prover an opportunity for planned cheating. In the end what I was visualizing is equivalent to the prover just providing a list of commits, each with distinct indices, such that \sum_i commit_i*2^i = target, eg commit_0 would contain all entries that are odd. But an even simpler solution I should consider first is a list of b commits with distict indices such that commit i contains coefs of only i. These kinds of ideas may be sound but a problem I realize is they are not modulo operations so the claim of a poly evaluation will not be preserved. The best candidate I could think of related to those above, one that would preserve the eval claim, is splitting into residue layers by multiples of b, eg for the range 0,2b-1 we split into 0,b-1 and b,2b-1. Then we subtract b from the upper layer then add them again. I'm concerned about lack of randomization, but lets find how this can be attacked. Lets try to analyze per index. Suppose an index value z is out of range. Prover must split it into two parts, x and y, one for each layer, such that 0 <= x + (y - b) < b. We also need x + y = z. This means 0 <= z - b < b or b <= z < 2b which means z is inside the range. The contrapositive means if z is out of range then prover won't pass. Actually while this might be sound splitting only allows for multiplication, not subtraction, because we don't know which indices to subtract from, so this won't work.

What about an entirely different randomization that requires little amortization overhead but more computation for the verifier, analogous to the DLP inner product argument. An idea, though this also would not preserve the eval claim, is we randomize the basis. 
Another possibility is we view the coefs in blocks and extension field elements, and we change the basis so as to multiply each block by a single random extension element, thus the equivalent of a fully sound randomization. I don't think this is possible, and we are limited to scaling individual indices, not blocks.
If we take the approach of considering a way of randomization first, the only way I can think of is randomizing the basis. Maybe we could make b so big (and q sufficiently big via extension field, maybe thousands of bits) we'd only have to randomize a few times (even once) for sufficient soundness. But this basis change then wouldn't allow us to add multiple commits. What if instead we did like the DLP inner product and randomized so as to evaluate at a random modulo b point (b still sufficiently large)? This could likely work but I'd need to look again at how it works. One concern is verifier would need to compute base changes using the isomorphism which is costly, but maybe I suppose we could do it with an additive side-circuit. Actually the isomorphism would not work anyway because to change basis we need inverses for any b value which means q has few if any divisors, and those divisors are not b values. After we determine the changed basis we are just looking for a valid opening with respect to that basis. In this case we could try the split-and-re-scale approach. 
Just consider the simple case we add two binary commits then split into {0,1} and 2, then ask for a rescale of the 2, then add them back together to get a binary commit. Prover provides 3 commits in addition to the original s0: s1, s2, and s3. Verifier checks that s1+s2 = s0, s3*2 = s2, and s1+s3 is a valid commit. Is it possible s0 is invalid but s1+s3 is valid? Well consider an index value z in s0, that prover splits into x and y in s1 and s2 respectively. To pass the first test we have x + y = z mod q. Suppose this index in s3 has value w. Then we have w*2 = y mod q by the second test, so x + w*2 = z mod q. The final index value is x + w. 
x + y = z
w + w = y
x + w = 0
=>
w = - x
- x - x = y
- x = x + y = z
0 = x + z
---
x + y = z
w + w = y
x + w = 1
=>
w = 1 - x
1 - x + 1 - x = y
2 = x + x + y
2 = x + z
---
eg for any z let x = - z, and y = z + z, and w = - x = z. 
First test: x + y = - z + z + z = z
Second test: w + w = z + z = y
Third test: x + w = - z + z = 0
so indeed this won't work without some kind of randomization.

I've looked at the bulletproofs inner product argument and it looks like it relies on automatica modulo being performed in the exponent, a property we don't have with b inside q. There will be two different representations of the same number modulo b, call it t. We will have one set of numbers modulo b a1,a2 such that a1*a2 = t mod b, and another set b1,b2,b3 such that b1+b2*b3 = t mod b. Thus the multiples by which these two differ is variable. Having the prover show the two are modulo equivalent is equivalent to our original goal, one we haven't solved. 


can we do a deterministic base change for SIS as an isomorphism? verifier computes on new commit, opening with respect to the old basis to verify its the same. Suppose we take a commit as a poly, break it into (say 2, could be more) parts. Prover provides the split commits, as well as the commit under the new basis for one of them. The random point of evaluation is the same for both polys, so they can immediately be randomly combined. We could compute another isomorphism to normalize this, but if beta is big enough we may not need to. Then we begin the process again for a poly commit of half the size. And in fact, the renormalization may especially be unnecessary because m is now half of what it was before.
b\sqrt{m} = x\sqrt{m/2} => x = b\sqrt{2}
this equation indicates the infinity norm can increase by \sqrt{k} upon slicing the poly into k parts and adding them up, which gives infinity norm b*k. |\sqrt{k} - k| is closest at k=2. also, multilinears are best suited for k=2. Any power of two works well for DLP and mutilinears. Oh, the part I'm forgetting is that the randomization increases the bound by a lot.
For lattices, the randomization is the challenge, whereas for DLP, the base change is costly. 
Actually, for DLP base change, we could consider the existing commit as belong to the new basis, and there is a fixed relationship between the old and new basis. Thus there is a fixed constant that will multiply each coef with respect to the old basis to get the new coef with respect to the new basis. An alternative to computing the base change which involves exponentiation is to instead compute what the eval claim about the old poly implies about a claim about the new poly. Not sure exactly how to do this. 
The DLP base change requires naively I think n + logn exponentiations, which is better than bulletproofs inner product argument, but they optimize in an obvious way, and I'm not sure if we can do the same because our has different structure.
Note that changing the basis means computing on the data in the new basis, which means evaluating at a random point, and this point will have to be incorporated with eval reduction. This means one line, because both old polys are to be evaluated at the same point, we connect this point with a new random point for both polys. 
Can we work with sets of polys in lattices? suppose we have a set to evaluate, we split each into two, the compute new basis for that half, then randomize that half of them with as many tests as there are polys (> s), then add them component wise to the other half. Then we would need to renormalize. Instead maybe we make the one half into bits when we change the basis, so upon later randomization its about regular b size. This means that we will double the infinity bound for each round. with log(data) rounds and initial bound b this would mean a final size of data*b, but taking advantage of the smaller m in each round this can probably be reduced something like data*b/\sqrt{2} about data*b*0.7. I think we could tolerate this with a large enough q. If not, we could renoremalize entirely after every round. 

How would this work in the traditional prover-verifier setting, ie without proof composition? Consider for DLP. For base change I think they have to engage in another proof. The data for the base change proof is the poly for which to change the base. We iterate through the poly opening the old commit, the verifier is left to evaluate poly at random point, but now it can be with respect to the new basis. I think we could still consider this an IOP because the subproof just involves responses and randomness.


It would be great to reduce evaluating a poly at a multi point to evaluating at a uni point. We can view both in the case of a multivariate poly, no need to consider uni polys. Is this theoretically impossible? Well I suppose we could just use a univariate proof like poly div to evaluate it at the multi point at the expense of evaluating at a uni point. Something like univariate sumcheck would work but have restrictions on the field, like composite order. 
Consider just two variables. Suppose we replace each with a 2d plane such that each goes through its respective variable at the same point, and the two planes intersect on a line. Prover sends back d degree bivariate poly consisting of d^2 coefs. Prover verifies consistency, but then prover can't choose point along the common line but instead must choose randomly for both variables, thus yielding an unstructured point for the original poly. To prevent this we would need to have the polys replacing the variables such that upon any uniform evaluation it yields the structured point we want. But this structure prevents from testing for consistency because that requires an evaluation that results in the original unstructured point. 
For a univariate based proof, how could we most efficiently express evaluation for a multilnear in terms of constraint to minimize witness size? I think this could be done, eg in STARK form, but it would probably still mean a big witness, and it comes at the expense of h as well as having to evaluate the witness at multiple points (which can't be reduced to one). We can still mention this as a possibility if one wants to avoid cryptography and use only CRHF as an assumption. 
For STARKs something like p(x)*r(x) + p(x+1) = p(x+2) but this is not complete and its unclear how to finish.


I was wondering if it might make more sense to try to set up an open source way to show and discuss these concepts they are best through of from multiple perspectives. This would suffice for presenting for admission, and also be good enough foundation for my thesis, just needing to translate to a paper format. One reason not to do this is I'm not up for maintaining this because I want to work on other things after I finish this project. 


mention how all CRHF are kinda costly by hashing, and require univariate proofs which require quasilinear prover time. Mention how we could get linear prover time but still CRHF if we could reduce multi point eval to uni point eval, but we don't know how to do this. 


In general, if the amortization scheme permits, we can recurse on it to have a succinct proof. So maybe I can make it part of the general model.

What proof systems do we have that should work? For GKR we can use DLP and lattices, and succinctness is available. For non-uniform, DLP amortization is available but for lattices normalization is tricky. For non-uniform, evaluation at a point is not performed, so the succinctness doesn't apply. 


For the succinctness make a few lemmas. Error persists if prover splits commit into two. Error persists if we do base change via proof. Error persists if we randomize one side then add together. We'd like to be as abstract as possible, leaving open as much room for optimization as possible, like how and when normlization is done.
But as an asside, I'm now thinking normalization should done length wise, increasing the size, of the commit, rather than horizonally, speading one commit into multiple. We had to do this before because commit sizes could only grow, but now that we have a way to reduce their length, I think we can forget horizontal normalization. But this only applies where succinctness applies, which is only the property testing can be broken into parts, which is not the case for the non-uniform case. 

We want our model to be meaningful, not just abstract language wrapping around our specific case. I think the case of succinctness is meaniful enough. It says that when we can split the property testing for a single instance into multiple smaller instances we can by recursivity achieve succinctness. This is because by the general amortization model we will achieve sublinearity, so this property that allows for succinctness is a subproperty. 

Can we achieve succinctness for the non-uniform in any way? 

One way to model to capture a lot is by proofs that reduce to evaluating polys at random points. This actually captures everything except the non-uniform case. 


I was thinking again about scaled targets. If q is prime and we divide by the scale, then we can obtain an appropriate witness for the original instance multiplied by the inverse scale. If the max different scales we need for gcd 1 are smalle enough (or just making b small enough) then we can get witnesses for a few problem instances related to the original problem instance. Can we prove these related instances (by inverse scaling) are also hard? 
Note if a power of b equals q but all scales are gcd 1 with q we can still use the multiplicative field isomorphism. 
This scheme would enable us to prove knowledge of a witness with respect to one of the few instances. Unscaled knowledge for the unscaled instance implies k-scaled knowledge for the k-scaled instance. And k-scaled knowledge for the unscaled instance implies unscaled knowledge for the 1/k-scaled instance. In general, k-scaled knowledge for the k'-scaled instance implies k*u-scaled knowledge for the k'*u-scaled instance. Basically we want to know that if prover has k-scaled knowledge for k'(!=k)-scaled instance then prover either also has knowledge for k-scaled instance or prover will fail later. So suppose prover only has scaled knowledge for unscaled instance. Well actually I see no reason this will make prover fail, because in the end after randomization we only require knowledge with respect to random linear combinations. I've already analyzed how accounting for scaled-knowledge is not worth the extra challenges. 
Let's calculate once more. If we use scalars {-c,...,0,...,c} then for poly eval we choose from the set of size 2c+1, so we need n = s*log(2)/log(2c+1) challenges. The total info if we do this number of challenges will be (1+c*b*l)^n, not accounting for the extra offset cost that grows with c. So plotting s*log(2)/log(2c+1) * log(1+c*b*l) it appears it shrinks with c, implying higher c is better.
While c=1 is fine we'd still like to prove for higher c, which may not be possible. 

note that if the poly to evaluate has zk then we can proceed through the succinctness argumemnt with no more zk overhead. 

we could present the sumcheck non-uniform which reduces to point evaluation. 
say we show other non-uniform case in order to show how we are not entirely dependent on point evaluation, but the homomorphic feature allows for general linear relations. Also for DLP it is low cost and simple. 

should we still use the molting model? our ability for succinctness closes the need for tail recursion, but I still want to use that model, and its still necessary for the matrix non-uniform. 


so how to structure our model?
proof system that resorts to property testing
property testing amortization
special case of decomposable property testing
we present our 3 models of computation and describe what property testing they resort to, and which are decomopsable.
then we describe amortization for each
we describe how to achieve succinctess using decomosable property testing, and how to amortize pairings and achieve succinctness for the matrix non-uniform. i'm wondering if we should also present pairings for the succint poly eval. 

to make sure the model works, maybe we can conjure a NP proof system based on another computation model than circuit/constraints. It needn't be a natural computational model. We would find a property testing algo for it, and it only needs to be efficient asymptotically. We could use commits as a black box, ie leave the scheme unspecified and model with an oracle.
Note that the original NP verification is efficiently computable, and we are reducing it a verification of a property, which is also efficiently computable. The point of the reduction is to arrive at a property for which a sublinear testing algo exists. I would like to develop a model and then separately exert the sublinearity condition. For example, the simple model might be a reduction from verifying an NP witness to verifying a property of the witness. Then the extra condition would be that we can perform the property verification sublinearly. But is this separation appropriate? Instead of oracles it may make more sense to use commitments, which can model oracles. Indeed, the oracle for sublinear algos is used to look at particular parts of the object, and we can prove how this can always be done via CRHF trees. The proof would be that after we lay out the data describing the parts of the object, we an always map that topology to a line, and then given an address for any part we can map that address to a point on the line. Then we can map the line to tree form. Again, oracle access to parts is not sufficient because an amortization technique may involve manipulating the oracles/commits. 
We would like our model to have efficient provers too (not restricting to linear). 
Note that property testing is a decision problem, though verification in general is. Verifier must be randomized, and only probabilistically correct.

mention how we know many sublinear property testing algos, but we don't know how to turn them into proof systems. But I'm thinking of property testing algos for single instances, whereas what we care more about is amortization, which depends on the commitment scheme. we know of few commitment schemes, and CRHF by itself provides nothing for amortization. To what extent can be take advantage of the isomorphism theorem? It means we can transform the representations of the data. So given commits to two objects, we can compute a commit any object from which there is an efficiently computable function to the original objects. This gives us substantial flexibility. But I formulated the isomorphism theorem when only thinking about original data, but the larger the computation the deeper or the wider so the more auxiliary data will exist which must be treated as original data as far as we know. 
Suppose we have the isomorphism theorem. What must we reqire for the property testing and commitment schemes? Dropping the entropy comes from direct computation by the verifier, so it would seem we require malleability for the commitment scheme. But we know from CRHF schemes which are non-malleable that verifier can always ask prover for values and need not compute any directly. Instead, in these schemes the verifier opens are random places and checks for consistency. This is like a 'look back' approach where as manipulating commits is like a 'look forward' approach. Both are approaches to dropping entropy. For look-back, I think the actual looks can happen after all other interaction. 

If two graphs are 3 colorable, that does not imply a particular join of the graphs is also 3 colorable. But if a graph is 3 colorable, it implies any split of the graph results in two graphs that are also 3 colorable. Suppose we prove that for a random cut, knowledge of a coloring for each graph implies knowledge for a whole coloring. 

A problem we know we can probabilistically check is matrix multiplication. Maybe we can recast this problem in terms of binary relations, both CNF and DNF. Well actually the matrix problem is a sub-normal time way to do a single instance, whereas we are looking for amortization of multiple instances. 
Remember a binary relation is specified by a binary incidence matrix, where (i,j) = 1 if i is connected to j. We can compute CNF and DNF for two relations by relevant matrix multiplication. 
I think there are multiple ways to make this into a proof system. A witness would probably be a vector or a full relation. The computation would be multiplying in CNF or DNF or both the witness by multiple relations, including itself. Suppose we don't do any addition or subtraction of matrices, only the multiplication. 
To verify that a series of vectors as the columns in a matrix W satisfy a target t for a matrix A, we could multiply AW and compare with t, but instead we choose random r and compute A(Wr) and compare with tr. 
In fact, we could even do SIS with binary relations, because the group q can be represented by a group of permutations with DNF composition. But the size of the relation may be huge, having q nodes. But if q is highly composite, it would require much fewer nodes, eg if q = b^d it would require b*d nodes on each side. 

I think it may be too difficult to find another example besides circuits, and if we used the above it would be quite similar to a poly based scheme and it would be complex to fully formulate. 

Maybe we should incorporate the notion of 'holomorphic' into our model, where to allow for recursivity, verification regarding the code doesn't require knowing the code directly, but rather having as a reference an object that represents that code, and verification involves testing a property for that object. Since the model will already require amortization towards testing, it applies too to that object. 
We could formulate our model in multiple ways. One way is property testing objects. Another way is functions and querying at random 'points'. Another is boolean functions, ie relations. Maybe functions would be the best along with abstract names for the domain spaces, eg commit space. We could formulate algorithms like this too, and probabilistic as receiving randomness. 

Maybe we just ask for a sublinear property testing (a boolean function) algorithm that work sublinearly for some number of instances. If this threshold is more than 1 than its amortization, if 1 then sublinear, and if close to 0 then succinct. We will also define the 'rate' as the ratio of the size of the original computation to the size of the needed computation, and it will be expressed as a function. 

I'm realizing in DLP changing bases either requires computing directly, or requires auxiliary data for each exponentiation, eg a field element as an inverse, and this may be more data than we are saving for, making the conversion pointless. For SIS, however, we luckily don't require auxiliary data and computation is easy directly. Is it possible to amortize base changes?
We will know the prover has a witness V with respect to the new basis B' with target t' and we want to verify that this same witness with respect to the old basis B yields target t.
<B',V> = t', <B,V> = t => <B'+B,V> = t'+t
<B',V1> = t1', <B,V1> = t1
<B',V2> = t2', <B,V2> = t2
Suppose we know those with respect to B' and want to confirm those with respect to B. Can we reduce to a random linear combination? In particular, we know <B',V1> = t1' and <B',V1> = t1' and we consider r*V1 + V2. Does <B, r*V1+V2> = <B, r*V1> + <B, V2> = <B,V1>*r + <B,V2> = t1*r + t2 imply <B,V1> = t1 and <B,V2> = t2? As a random linear combination, we can see that yes, the implication holds. Afterall, I suppose this is just a linear relation we want to verify, namely <B,V> = t, so we can do it with random linear combinations of candidates. 
So what does this mean for the succint protocol? Hopefully it means no need to open. For lattices, we request the commits with respect to the new basis. Then we treat these new commits as commits to the data vectors for which we already have targets (other commits) with respect to the old basis. We will end up randomizing these new commits anyway, but that can also serve as the amortization for verifying consistency with the old basis. But this amortization actually takes us back out of the succinctness setting. Can we make this succinct too? This is a general linear relation, and just like we don't know how to make the matrix non-uniform succinct, we have the same problem here too. So this amortization doesn't help. Its not necessary for lattices, and for DLP it doesn't decrease amortization. So I'm thinking we don't use it at all, though we could mention it.
So I'm thinking for DLP succinctness we could use the pairings for mutilinears. I don't know anyother way to make it succinct. The recursive inner product argument won't work for the same reason as our base change, that is it requires non-deterministic information that must be treated as full entropy and in the end saves nothing. 

So maybe I present, matrix non-uniform, sumcheck non-uniform, and sumcheck uniform all with amortization. Then for each I show how it can be made succint. For matrix non-uniform its amortized pairings. For both sumchecks with DLP its amortized pairings for multilinears. For sumchecks with lattices is the base change. So I'm wondering if we should present matrix non-uniform using lattices, because lattice succintness doesn't apply there, and even amortization is complicated by normalization, and even if it wasn't amortization requires many instances whereas this is no longer a problem with the sumcheck schemes. I think the sumcheck non-uniform may be better than the matrix non-uniform. Maybe I should present the matrix non-uniform as an introductory example to show how the property is not always poly eval like it is for our sumchecks, how succintness can take other forms (in this case QAP style) (and I can forward reference when I mention amortizing pairings), and how amortization can require a large number of instances, and how the recursion theorem can be tricky. For all these reasons I can justify the value of the sumcheck schemes. 
So we have to decide whether to divide the sumcheck by the uniformity or by the crypto assumption. Maybe we don't need to divide by either. Instead, following the model, we present the proof system and the sublinear property system separately and in that respective order. This forces me to get definitions and modules straight. 


For the side-circuit suppose we do it in a prime q using sumcheck involving multiplication by a poly encoding C, which consist of coefs only in {-1,0,1}. This might make verification by the main circuit eaisier (though we could still delegate to the next side-circuit). Suppose we received all the inputs for the verification in field q, but we use some injective, almost surjective, map to an extension field of the main circuit field, then interpret values and perform all verification arithmetic there. Well I suppose even completness wouldn't hold because the fields differ, and no field homomorphism holds between different characteristics. 
Suppose instead of sumcheck we use the matrix non-uniform scheme. Let A be the SIS constants, and S be the reconstruction (un-normalization) matrix, T the targets, C the challenges, and W the noramlized witnesses. (AS)W - TC = Z. Could continue but I'll leave this for the moment.
What if we simply compute TC with long arithmetic in base b? Is modulo really that hard? We could split two q value representations into bits (do bit checks), then add them, then inspect the leading term and decide whether or not to subtract q, then translate back to base b form, or back to bit form for another addition. What if we perform all additions at once. This would require that the number we add together at once be less than b to prevent overflow. We've seen other reasons before that its desireable to make b about equal to the security parameter anyway. Oh, but remember we are doing subtraction as well, so its more complicated.


I'm remebering that the sublinear lattice argument with succintness involes a proof system so maybe its tougher than I thought to separate proof systems from sublinear arguments as I planned.

Maybe I should merge the two sumcheck protocols, uniform and non-uniform, into one general framework. They can easily complement each other, and they are both built on sumcheck, so instead of separating them just because one checks constraints and the other computes derived values, we treat them the same as they deserve. 
Our DLP and lattice versions will use different version, the former with a big field and the latter with a small field such that the random points sent by the verifier must be in an extension field. So maybe we can prove in the model of the latter, for which the former is a special case where the extension field matches the primary field. 


what if we make the first succinct IP from worst case assumptions, with efficient provers.

it would helpful to have model be naturally non-interactive, unlike interactive proofs, to capture how the chain really is non-interactive. One way may be to model the fiat-hash as some ideal oracle or function, that both prover and verifier access. This oracle receives the prover message together with record of the transcript. Suppose this record is just the last oracle response. 
Maybe we can cover fiat-shamir in the model, and later only try to choose a suitable hash. 
Note that even an ideal hash is not secure for an unbounded prover who can query it unbounded times. In fact, maybe we should just assume a certain kind of hash from the beginning, rather than some abstract oracle. This forces us to decide what we require from the hash. I think we want relation intractability where the hash is chosen randomly and there is a fixed relation. This is tricky because in a sense the hash does not observe a whole instance of the relation, only the visible part. But remember we consider this visible subrelation as the relation itself, but now it is not fixed but rather chosen after the hash. Before we thought about just saying the hash is chosen after this relation is fixed. 

Suppose we found suitable conditions for a hash. How to complete the model? Maybe we model randomness not just as an input, eg random coins {0,1}^n but a function that a probabilistic algo can query. afterall, this is how it works in the real world, with libraries. Assuming the random function is completely random and non-deterministic, there's no reason we can't assume all parties use the same function. So we still use two algos, prover and verifier. Prover is probabilistic (for zk) and verifier is deterministic. But as adversaries (prover against soundness and verifier against zk) they are both probabilistic. The problem with randomness as a function is a function is deterministic. Maybe we give it special notation, such as marking the calling parethases in some way, like a line over them, to signify its impure. This is close to conjugate notation in complex analysis but we are far from that and in complex analysis all functions are pure. 
Now since we conditioned on a hash for the fiat-transform there is no interaction, we can model the prover as a single algorithm. It receives the instance, a witness, the random oracle, the hash function, and outputs the proof, all in a single invocation. The verifier receives the same inputs except the witness, and outputs a boolean decision. 
Say this non-interactive model is necessary for us because interaction is inherently non-verifiable. Whereas IP can indeed be done with interaction so the modelling is appropriate, whereas for us this is not possible. 
A hash is Correlation Intractable for a relation if the hash is randomly chosen after the relation is determined. I think full correlation intractability still means the relation is determined first then a hash is randomly sampled from a family. But of course we can't randomly sample. But suppose we consider the simple relation of (m_i,t_i) -> v_i being an instance where m_i is prover message, t_i is current transcript, and v_i is verifier response, and an instance is where (m_i,t_i) is doomed but (v_i,m_i,t) is not. In this case the relation is determined along with the proof model, so we can choose the hash after, thus having a correlation intractable hash. 
We should probably leave little comment on which hash to choose, only pointing to the research done for GKR. 
I realize SIS is not suitable for hash because output size is so large. Thinking more about SIS I'm worried q must have large prime factor, and cannot be a power of a small number, ie highly composite, because that is isomorphic to q being the small number and the worst case dimension multiplying a factor of the power. Oh, this is not the case because the q-group must be cyclic, and this additive group I'm considering is not cyclic. 

for hash, what if we use SIS with q=b, and n such that output can be interpreted as element in extension field. Then we could take bit strings about the square size of b. 

literature says if hash family is correlation intractable for efficiently sampleable relations, then it works for fiat shamir for pv-SNARGS. 


Maybe instead of the multiplicative group we could use the galois group for an isomorphism. Actually the galois group only has order n for q = p^n. 

In a finite field an irreducible poly f of degree n. 

(\sum_i a_i*x_i)^p = \sum_i a_i^p*x_i^p


(\prod_i a_i^x_i)^p = \prod_i (a_i^p)^x_i

I know its more complicated, but suppose given a single root of the poly in the extension field, we can find all others by the p'th power map. Then suppose we hash to a root with SIS. Taking the p'th root of both sides, assuming all inputs are presented in the base field, the same input yields a solution to the next root with respect to the p'th power of all constants. 
The degree matters. For us, in sumcheck, we will have quadratic polys, so degree is 2. This means our polys can have at most 2 roots. 

Suppose we could handle quadratic polys. This would work for sumcheck, but could we also make it work for poly eval reduction? Theoretically we could just use the sumcheck again, where we take a random linear combination of the claims, and perform the evaluation of both in the integrand, indexing over the coefs, then at the end we will have a single point of evaluation for both. In this sumcheck we would only need to handle linear polys, easier than quadratics. What about the more traditional eval reduction, where this time we only join a line for k variables at a time, and receive a k-degree univariate poly in return. 

Would it be possible to do multiplication with sumcheck only with linear intermediates? Before I had thought about 'currying' where we don't have two, but only one input vector at a time. For source vector S and target vector T we'd want something like T(z) = \sum_x f(S(x),z) where f is a linear function in its first arg, and splitting z into its component variables, f must be linear in each one. Suppose we let the arguments of S also depend on Z, ie S(g(x,z)) where g is linear in all args. But then we couldn't let z be an arg of f.
It seems to do anything non-trivial we need a quadratic integrand. 


again consider the task of reconstructing a multlinear poly to a univariate poly of same size. there is a spectrum, where it will probably need to transform first into a higher degree multivariate poly, at every step reducing variacy by 1 and doubling the degree. 
first suppose all out polys start out as univariates, we transform to multivariates only to reduce evaluation to the same point (a multipoint), then we want to return that point back to a uni-point. Suppose r1 is the uni-point for poly one and likewise r2 for poly two. We connect these two with a line. 
r1*(t-1) + r2*t
Suppose we pass it in for the first index, and for the second we pass in the same line squared. 
(r1*(t-1) + r2*t)^2
Prover returns the degree-5 univariate poly, we check for consistency then choose random t, and are left to evaluate the polys at the points
(r,r^2,r1^4,...)
(r,r^2,r2^4,...)
Now we can consider the first two indices as transformed from two variables of degrees 1 to one variable of degree 3.
Now suppose we continue for the rest of the indices in pairs. For example, the next would mean the line
r1^4*(t-1) + r2^4*t
and its square, passing them in respectively for indices 3 and 4. We will choose a random t different from the one for the first two indices.
After all pairs are done we should be left to evaluate the two polys at the same point of the form
(r,r^2,r',r'^2,r'',r''^2,...)
where we started with v variables of degree 1 (ie 2-1). Now we have v/2 variables of degree 3 (ie 2*2-1). Basically cutting our variables in half we have doubled our degree.
(r,r',r''',...)
Can we continue this process recursively? I think it breaks down the higher the degree each variable, because the univariate polys returned by the prover will have corresponding high degree. Even if it worked, I'm realizing there is little reason to have this goal. 
The point of reducing from a multi point to a uni point was to use the uni commitment scheme of CRHF. But if we commit to a poly using the uni commitment we can only open that poly, not its linear combination with another. But I suppose we can still concat multiple polys together and treat as a single poly, and we concatenate to them as a single one. When the polys were commitmented to from multiple parties, before its determined they are put together, we can still cocatenate commits by muliplying a response from each commit by an appropriate monomial, but this doesn't help actually because the query complexity remains the same.
With this general approach of 'connecting point' I don't see a way to get around high degreeness.  


Now supposing all fiat-shamir transforms for us are wrt the relation of quadratic poly points of agreement, can we say anything helpful about the necessary hash? One thing we could say is that it has compression factor 3, mapping 3 elements for the quadratic poly to one element for the random point. For any input there are at most 2 solutions, and they are efficiently computable, though not so friendly to circuits. 	

-(b2-b1)/2(a2-a1) +- \sqrt{(b2-b1)^2 - 4(a2-a1)(c2-c1)}/2(a2-a1)
(b2-b1)^2 - 4(a2-a1)(c2-c1) = (b2^2 - 4a2c2) + (b1^2 - 4a1c1) + (- 2b1b2 - 4a1c1 + 4a2c1 + 4a1c2)

suppose we have the diff poly encrypted with SIS. prover knows the diff poly and computes roots for it. verifier takes claimed root, mutiplies powers, and scaled each encrypted coef appropriately, then adds up encryptions and should get a zero equivalent encryption. 

when verifier sends only randomness its called 'public coin'

since we want compression factor 3, maybe subset sum won't work. We can still consider SIS with q=b, and inputs decomposed into binary form. but given this constant compression we could just leave the hash uspecified, and we assume it will be calculated in the main circuit. 

what if we map to both of the quadratic roots, then reduce the two points of evlaluation to one, using a fiat transform that only requires a linear (rather than quadratic) poly as input. In this case the quadratic hash has compression factor 3/2 (or 3/1 if irreducible), and the linear hash has compression 2/1. 
But this would mean changing the sumcheck protocol. Consider after receiving i'th prover poly, and checking for consistency, we don't immediately plug in a random r_i for index i, and request the next univariate over the (i+1)th index. Instead we have the two random values r1_i and r2_i. We request the univariate over variable i connecting these two points. But actually that univariate can be obtained by the verifier just composing the line with the i'th univariate. Suppose verifier then chooses random t online and computes relevant r_i and expected relevant output. This seems too convenient to be possible. With no extra prover interaction, we reduced from computing at r1_i and r2_i to just r_i. By round-by-round (RBR) soundness suppose the next univariate returned is correct. Then after adding up evaluations at 0 and 1, we obtain the correct value of the ith univariate at r_i. We wanted to know the correct value at r1_i and r2_i. Instead of evaluating on those points we evaluated on the line connecting them. But without the verifier observing the valid evaluation on this line, the verifier can't perform consistency checks for r1_i and r2_i. Oh, the problem is we are trying to reduce a quadratic from 2 points to 1, but this was only assumed to work for multilinears. If we interpreted the quadratic as a univariate then we would reduce the problem to reducing eval from a multi point to a uni point. In other words, we wouldn't need the regular evaluation at some r_i for the next poly, but rather an evaluation treating the i'th variable x_i as two variables u_i and v_i where u_i = x_i^1, v_i = x_i^2. Suppose the prover actually treats it this way, plutting in v_i for x_i^2 and u_i for x_i when computing the (i+1)th univariate. The integrand will be a quadratic multilinear poly, but upon treating every squared variable as independent from its regular form, we will have a multilinear but with twice the arity, and actually I don't think this is a full multilinear judging from below where we start with two multilinears with variables (a,b), and upon multiplying them we replace aa with u and bb with v. 
(1,a,b,ab)(1,a,b,ab)
(1,a,b,ab,a,aa,ab,aab,b,ba,bb,bab,ab,aba,abb,abab)
= (1,a,b,ab,aa,bb,aab,abb,aabb)
= (1,a,b,ab,u,v,ub,av,uv)
I think this is because with u and v we have enough to represent a cubic poly, but ours is only quadratic.
Not yet sure if this technique works, but if it did could we generalize? Suppose our integrand was a multivarate poly of degree 2^d-1 in each variable for some d. Upon receiving a univariate we map wish to evaluate the next poly at all 2^d roots. Breaking it up into a multilinear with d variables, we reduce recursively to one multilinear point, then the degree-d variable for the current index gets broken up into d distinct linear variables taking the values of the final multilinear point. Eg, for d=2 we could be multiplying together 3 multilinears in the integrand, and in the end we would end up with a full multilinear, no variables partially wasted. 
I would need to formally prove this works because with the current sketch I'm not even intuitively convinced. Its still odd that there is no interaction when reducing the multipoints on the current univariate. This means the this reduction process to arrive at the final multi point from the multiple uni points should be considered part of the fiat transform. Our plan was to consider this reduction as a set of additional fiat-transforms for linear polys, but the prover already has these linear polys and they are not provided by the prover. Could we still carry-out the linear poly fiat-transform as usual?
Another question is how to handle the destructuring of the integrand. We saw before how we can restructure as long as the degree mains low, eg for the d=1 case we could restructure pairs and again obtain a quadratic multivariate. Now we must translate an eval claim about this multivariate into claims about each of its components. Well I suppose this is what we normally do with the sumcheck anyway, where the prover just provides claims, we check for consistency, then reduce to the claims. Actually I realize now we only know how to do restructuring between two structured points, not for our destructured point.

In IOP it can be non-interactive but prover still queries oracle. Thus soundess against state restoration is enough. But with the oracle replaced by a CI hash, then we can't say the same. 

To do the above scheme we need a root-relation intractable hash. If I can't do this as I thought then we should just entirely ditch finding explicit fiat-transforms. The invisible input to the relation can be thought of as the function for the subrelation it induces. In our case the invisible part are the coefs of the real poly. The invisible operation is obtaining coefs for the diff poly. For input vector x and real coefs vector y, and hash H we need it to be infeasible that y - x = H(x) or x - y = H(x) where both sides remain vectors, so H will have to hash x multiple times or in multiple parts. 
For SIS/subset sum we can rearrange as y = H(x) + x = Ax for some matrix A and x in decomposed form. Thus solving means finding a witness x for target y. But y is not random so this is not ISIS. We would need to enforce that A is random with respect to y, which means choosing H randomly after y is determined. y is determined by the commit or however the final witness is held. But this requires interaction to choose A, or requires fiat-shamir, a circular problem. The original scheme I imagined required the function to be a linear function, but here we are doing subtraction, an affine function. 

Suppose we randomize y to make it indistinguishable from a random c. If one iteration of SIS was enough, then the hash
H(x) = A(x - c) = Ax - Ac
would suffice because when we replace c with y and we successfully hash to the diff poly x-y then we have
A(x - y) = x-y => (A-I)(x-y) = 0
Now suppose we must go through two iterations of SIS, A and A' (composition denoted by $), to make y indistinguishable. Then we have hash
H(x) = A'$A(x - c) = A'$(Ax - Ac)
and
A'$A(x - y) = x - y
but the latter I don't think implies breaking either A' and A, so this idea of more than one randomization iteration won't work.

I suppose we don't know a way to make y indistinguishable from a random c. 


I realize the sumcheck is a kind of poly eval reduction, where the poly is the integrand, and we reduce from the two points 0 and 1 to one point r_i for each variable i. When we have a product of multilinears as the integrand, suppose instead of asking for the integrand along the single line, we ask for each multilinear along that line (in fact we could later generalize to have different lines). We receive two linear polys back and we check for consistency, choose random values on each line, then reduce to testing the new product. The benefit here is we only need relation intractability against linear polys, not quadratic ones. The cost, which is no cost for us, is the two polys in the integrand are evaluated at different points. 

But do we have the ability to do a linear relation intractable hash with SIS, or do we end up with the same problem as above where the 'function' is affine, not linear. We could still theoretically apply the FHE hash if not. It might be feasible because our function would be (a,b) -> (a-c)/(d-b) for constants c and d. This means two additions and one division. Hashing compression would always be a factor of 2. 
Computing the diff poly means doing the two additions, which are affine again. For random SIS instance A we don't want prover to maniciously choose c so it knows an 'a' that hashes to a-c. Prover could choose an 'a', hash it, call the output a-c, then solve for c. Similar for b and d. 
Actually to reduce two polys to one, we must choose a single common point on both lines, meaning this random decision must be a function of both lines, meaning we have compression factor 4 rather than 2. I think there may be a way around this, doing it in two iterations of a 2 factor compression. First ask for the eval of each poly on the point of the opposite poly, receive back two values, then hash to get a value and use that to take be an extra variable so as to join the two multilinears into one. Now are are left to reduce this new multilinear from two points to one, which can be done for each variable (no need for the last) with 2 factor compession.
Now it may seem at first thought that a random linear combination is a different relation for a fiat transform than poly agreement. But in fact, when two linear combinations agree that is equivalent to two polys agreeing. So in our case both relations are the same and can therefore be treated with the same fiat-hash function.
And on first though, fiat-hash functions cannot be amortized because amortization requires further fiat transforms. But on second thought, the purpose of amortization is to reduce the total number that must be calculated, thus allowing them to undergo tail recursion. This may indeed be possible for our fiat-hash functions. 

Now we are left to formulate a hash function with compression factor 2 that is intractable agains the relation (x,y) -> (y-y')/(x'-x) for possibly maliciously chosen constants x' and y'. 

What if we instead represented these lines in polar form? The form xt+y is where the vertical dimension is a function of the horitzontal dimension, or it could be vise-versa. For polar form the radius is the angle, or vice versa because for lines both of them are comptabile I think. 
Polar point (r,l) corresonds to cartesian point (r*cos(l),r*sin(l)). Thus x*t+y becomes r*sin(l) = x*r*cos(l)+y and we can solve for either r or l. For r we get r = y/(sin(l) - x*cos(l)). Now the point is change representatino, so instead of having y and x in this form I think we have u and v such that r = 1/(u*sin(l) + v*cos(l)). 
Now two lines intersecting in this form means 
1/(u*sin(l) + v*cos(l)) = 1/(u'*sin(l) + v'*cos(l))
=> (u*sin(l) + v*cos(l)) = (u'*sin(l) + v'*cos(l))
=> sin(l)(u - u') = cos(l)(v' - v)
Note this is similar to x*t+y = x'*t+y' => t*(x-x') = (y'-y) so we'd need division here too.

If instead we use representation x*t-y then we have x*t-y = x'*t-y' => t(x-x') = (y-y') which is more convenient.
H(x,y) = (y-y')/(x-x')
Note we can't just have affine functions from x -> x-x' and similar for y because we need an injective function from the outputs of the hashes to the final hash output.

Could we in general amortize hashing? If so, we could leave the hash completely unspecified, and instead focus on amortization which is more in the vein of sagaproofs. Well hashing is a computation like others, so while it may come at high auxiliary data cost, we can reduce it to poly evaluation for additional polys containing this auxiliary data. But we have to make sure the amount of hashing in the proof exceeds the amount of hashing necessary to verify the proof. Suppose we do the succint lattice poly evaluation. Suppose we input l hashing computations. We would like to verify with something like l/2 or less hashes. Suppose each computation requires s variables for sumcheck. Then we must add log(2,l) variables to enumerate the instances. So I think the sumcheck evaluation only grows logarithmically with l. Then similarly, it requires log(2,l) rounds in the poly eval reduction phase. Actually this is not amortization because a circuit must compute the hashes. We only know amortization via homomorphic commitments, and that won't work here unless we use SIS as our hash. 
Is the FHE hash friendly for circuits? Its impractical because it will involve homomorphically computing inverses. Its done with an SIS modulus q, so it would best be done in the side-circuit, which further complicates circuit communication, but if we figure out an isomorphism it should be doable. 

note hashing is made more complex that we have imagined because its over extension field elements. this makes it more complex both for FHE and SIS. 

consider again SIS hash, trying to disguise and use a linear combination to cancel the disguise. disguise real z' with c, and having z as input. we are thinking of z and z' as decomposed representations of scalar values, eg bit vectors. the dot operator means concatentation. There is only one place we will split for concatenation.
Our input will be 0.z, but I suppose we could replace the prefix 0 with something else, maybe tying z to its past state.
A(0.z - c.z') = (0.D)(0.z - c.z')
where D reconstructs the vector z - z' into a scalar value, which might be tricky but leave that alone for now.
Upon submitting input 0.z - c.z' that satisfies this equation, it also serves as a solution to the SIS problem for 
(A - 0.D) which is random if A is random because D has a standard fixed form.
Now we can't actually submit z' as input because it is invisible. The purpose of c is to disguise z' such that we don't rely on it and it can be replaced with something else that we know. For whatever z'' we choose to take the place of z', our hash will have the explicit form
z -> A(0.z - c.z'') = A(0.z) - A(c.z'') = A'z - A(c.z'') where A' is the second part of A.
We showed that if z''=z' then unless z=z', hashing to z-z' means breaking SIS.
We also will make the assumption that A(c.z') is indistinguishable from A(c.z'') for z''!=z'. What we mean is we have an adversar who provides us with z', then we choose random c and either return A(c.z') or A(c.z'') for z''!=z'. We assume for any z' adversary can only distiguish with negligible probability over choice of c and z''. In fact, we could just set z''=0. 
Suppose we have a subroutine that can hash from z to z-z' for some fixed z'. We will use this adversary to win the distinguishing game above. We take the z' for which the subroutine works, provide it to the challenger. The challenger replies with a vector M and we must decide whether it is A(c.z') or A(c.0) for random c. To do so we invoke the subroutine. We create the hash function H(z) = A'z - M. We test whether the subroutine can hash from z to z-z' for some z. If so then we know M must be A(c.0) because if M was A(c.z') we would have
A'z - A(c.z') = z-z' => A(0.z) - A(c.z') = z-z' => A(0.z - c.z') = z-z' = (0.D)(0.z - c.z'). 

any immediate concerns with the above? maybe use subset sum instead of SIS for more comptabile output. Suppose we replaced 0 prefix of z with something else, say r. Then hash is
H(r,z) = A(r.z) - M
which indeed randomizes the output. We could choose r (and thus c as well) to be the size of an output, this way a previous output can be part of a new input.
this is all exciting, but i feel nervous because I don't know if it will work.
Let us try to generalize to linear affine functions. 
we are not done. use separate SIS instance for each affine. then argue if correct ratio is hashed it means hashing this ratio between two SIS instances and that is infeasible.
this concrete fiat-hash is a bonus feature, and others features don't rest on this, but this rests on other features like the ability to do sumcheck with only linear fiat-hashes. Therfore we should construct other things first and leave this as a separate bonus feature. 
whatever hashing we can do with SIS, maybe we can do the same with DLP.
before finishing the hash for roots, we could prove the above hash for linear affine functions, which would be useful even if it doesn't end up working for roots. 

what are the input/output spaces we'd like for this affine (affine includes multiplication, so no need to mention linear) intractable function? Our main circuit is for small prime b. To verify it we will use elements of a large extension field. These are the elements we will be hashing, and we want them both as input and output. They are each representable as a vector of b values. Note this extension field may be different from the one used for the primary commits with SIS, which must be the size of q, whereas the current extension field has size for standard soundness. We could try either performing the hashing inside the main circuit or the side circuit as be explore below.

Suppose we would like to perform this hashing inside the main circuit, and suppose we'd like to do this using SIS. We would need to use a multiplicative extension field group, and I think the size of the constants must only be such that SIS is hard for our input parameter choices. To hash b values we will need to break them into bits anyway in order to do appropriate exponentiation. Now suppose the output of the hash is to be interpreted as an element in the sound-extension field. 
...uh oh, pause for a moment. I'm worried the hashing argument might have a conflict in that the SIS group is different from the group for the input/output affine structure. I think the requirement is that the inputs go through an affine function where the constants must be no larger than the SIS group order. In other words, the affine function is taking place inside the cyclic group isomorphic to Z_q. Since we are doing affine operations, and not multiplicative operations, we should be able to do operations for extension fields as well as prime fields. But we do an operation on the input for each of the n dimensions, so the output is the direct product of n large cyclic groups.
...to continue. Well the output of the hash is n elements of the multiplicative hash-extension field. Suppose we set the sound-extension field to be the n'th power of the hash-extension field. Then we could interpret the output of the hash to be an element of the sound-extension field where all n components are non-zero. This might be ok, because the sound-extension field should be larger than the hash-extension field anyway. But with a large sound-extension field we will need more hashing, so this means more operations in the hashing-extension field, which will be expensive. So it may make more sense to try to use the side-circuit for hashing.

Suppose we would like to perform this hashing inside the side-circuit, and suppose we'd like to do this using SIS. Well the side circuit is made with prime modulus q, so this group has no subgroups. Suppose we are to use a commit-extension field for an isomorphism with q, eg with Mersenne prime. Suppose we set the sound-extension field equal to the commit-extension field. We would like to interpret the output of the hash, ie n hash-extension field elements as one element in the sound-extension field. Several reasons this won't work. For one the sound-tension field has order q and is not divisible, and if we fiddle with the off-by-1 due to multiplicative groups I arithmetic somewhere will go wrong. Also, we will be doing much more arithmetic with hashing than with other verification, ie more in the hash-extension field than the sound-extension field. Thus it makes more sense to try setting the hash-extension field equal to the commit-extension field. The only trouble here seems to be that the commit-extension field will already be large, so doing this then interpreting the output as an element for the sound-extension field means a huge sound-extension field. This could be solved if we set n=1, ie only one SIS row. In that case I think all field extensions (commit, sound, hash) could be identical. On the other hand, what if did the commit-extension as a regular SIS with many rows, and we do sound- and hash-extensions as SIS with only one row, but the same modulus. This may be secure because the load for the hashing SIS is less than that for the commit SIS, so many one row is sufficient. This also means the modulus would serve for our soundness. I think this is the best option so far. If one row for the hashing-extension is not enough, I suppose we could add more, but only as many as we need, not as many as the commit-extension. Then the soundness extension would be a further extension. 

What about completing the fiat hash? Suppose we use x without use of disguise, x does not hash to x-x' and y does not hash to y-y', but we still have
x*H(y)/H(x) - y = x'*H(y)/H(x) - y'
H(y)/H(x)(x - x') = (y-y')
H(y)/H(x) = (y-y')/(x-x')
This involves multiplication, so it cannot be dealt with using SIS, especially not extension field multiplication.
I thought we might argue H(y) = k(y-y') => H(x) = k(x-x') where k is also an element in the hash-extension field which causes an offset because the group is cyclic. Lets write this additively
H(y) - H(x) = (y-y') - (x-x')
H(y) = k + (y-y') => H(x) = k + (x-x')
H(x) := Ax - r
Suppose we use the same SIS instance for both x and y.
(Ay - r) - (Ax - r) = (y-y') - (x-x')
this implies either
Ay - r - y + y' = Ax - r - x + x'
or
(Ay - Ax) - (y - x) = (r - r) - (y' - x')
(A - 1)y - (A - 1)x + (y' - x') = 0
(A - 1)(y - x) + (y' - x') = 0
but this is not even making use of our intractability for H.
We need to say its infeasible for H to have two outputs with a certain ratio. 
Or we could put it through the outputs through another hash and make a similar statement.
But I don't know how to do these.
Suppose we use different SIS instances for each instance x and y.
(Ay - r) - (Bx - r) = (y-y') - (x-x')
(A - 1)y - (B - 1)x = -(y' - x')
so we have the same problem.
This implies we may need to make the SIS group multiplicative and do the subtraction for the diff poly 'in the exponent' with representations translated from the additive to the multiplicative group.



what if we don't even do isomorphic arithmetic in the main circuit and instead do all verification in the side-circuit. The field of the side circuit has order q which is prime and the same as the multiplicative order of an extension to the main field. Adding q elements is equivalent to multiplying extension elements. The q elements represent only the non-zero elements of the extension field. Thus 0 in the main circuit has no counterpart as a q element. This could be problematic. Also, the side circuit can only do multiplication in the extension field, not addition, so in fact we will surely need the main circuit for that.

remember that theoretically the relation is fixed before we choose the hash. thus if we want to use the same SIS constants for all hashes we must somehow modify the problem, eg with a new target every time, and prove this is just as hard as a new instance. if we choose a random new target, then it functions as the hash key, and will have length smaller than the input, and does not grow with the size of the relation (ie the affine function). for this reason it would be called 'compact'. The ability to use a sufficintly 'random' new hash for each relation without an entirely new SIS instance may be what prevents us from completing. 
I think our rerandomization will come from taking the outputs of previous SIS hashes or commits, decomposing them, then feeding those to the fixed SIS. We need to prove this way of 'composing' SIS instances is sufficient. Maybe we say something like out of all the possible targets, only for a negligible fraction of them can adversary find a solution. Then we say of all the outputs from the first SIS that the adversary can generate, only a negliglble fraction of them will map to the said vunerable targets. I think this should work. 

Before we finish formally organizing all these into theorems, etc. figure out how to complete the fiat hash, which we did above only for the special case of taking the difference of two hash outputs. We can take the hash outputs and do any isomorphism with them. Take the case of computing quadratic roots. The outputs from the affine hashes are elements in the multiplicative extension field and represent the coefs of the diff poly. We can multiply together ac and bb but then we'd need arithmetic in the additive field for bb-4ac and 2a, and the back again to multiplicative for \sqrt{bb-4ac}/2a and -b/2a, then back to additive for final +-. This is huge overhead even if the compressions are somehow safe in the additive field. Remember we don't know how to prove relation security for composing SIS hashes.

As far as relations with multiple outputs, maybe we can also use indistinguishability. The idea is before we disguised the function, and now we have multiple functions to choose from and we disguise whichever we choose. We know adversary cannot solve for the one we chose, but due to disguise the adversary can't tell that apart from the others or the fake one, so adversary can't solve any of them. Using this we can prove for not just affine functions but affine relations. But a relation is like a collection of functions, so if intractable for all functions then its also intractable for collections of them and thus relations as well. It seems a little too good to be true that there only needs to be one function for which the adversary is bound to fail, and that we can disguise any other functions along with it and suddenly the adversary must fail on all of them as well due to indistinguishability. This shows its really the disguise that prevents against all structures really. What's important is that the structure searched for is chosen prior to the random hash instance. 

I'm wondering if we can also have a provable hash constructed in a similar way for the SIS commit randomizations. The reason this may work is we also use linear functions, albeit they are multivariate. 
...


thinking about finite fields to determine what is best groups to organize various isomorphisms. take q = p^d the order of a finite field. what would a subfield look like. i assume it would be of the order p^d' for d' < d. for any two k1,k2 < d' we would need k1+k2 < d' to be closed under multiplication, but that doesn't seem possible without changing the irreducible poly, but that would change concistency with the ambient field it seems, so idk. Oh, just like we take the extension of the prime base field by choosing an irreducible poly and making elements of the base field coefs, we probably do the same for extensions of extensions. This means multiplying two elements of a double extension of degree d2 of a degree d1 extension means taking d2-1 polys each composed of d1-1 coefs from the base field and multiplying them together. The amount of multiplications in the base field I think has grown quadratically. 
For our hashing to work, the output of a hash must be a single sound-extension field element. This is because we must divide two such outputs for the linear relation.
A most desireable setup I suppose would be one of those mentioned before, namely selecting a prime q large enough for the hashing-extension and the sound-extension and having SIS for hashing have n=1. Then we use the same q for the commit-extension but with n large enough to support the extra load. This means we don't use extensions of extensions. 

If we are to use extensions of extension then I need to understand subfields, and my description above with extension of extension may be flawed. Afterall, a subfield should inhereit the operations of the ambient field. 


As far as using b > 2 to make q prime we will need to a use a multiplicative subgroup, and testing subgroup inclusion seems costly. But maybe we can strategically choose the subgroup. The subgroup of squares is what I think of. Considering the map x |-> x^2 we want to find the size of the range. each element k^2 in the range has inputs k and -k which are different because b>2. Thus there should be (q+1)/2 such squares, but they include 0, so our subgroup has order (q-1)/2 which divides (q-1). This subgroup is maximal as we would like. Also, and for the reason we chose it, its easy to verify membership, because any element k^2 can be given as either k or -k and the verify squares it obtain the intended value. The point of this was to have an isomorphism where b>2, since there are no primes b>2 and q and power n such that b^n = q-1. For this solution to work we need primes b and q and power n such that b^n = (q-1)/2 => find b and n such that 2b^n+1 is prime. eg 2*3^4+1 = 163 or 2*11^3+1=2663, or 11 with 9. But I don't know how many exist for other b and much larger n. We can treat the case b=2 as a special case where every element is a square. We would need a special way to translate representation via b powers into representions in the extension field. But the circuits don't need to understand this translation process because its done offline by provers. So there just needs to be some efficiently computable isomorphism. And it would be much harder anyway to solve the above with q also a power of b. 



lets finalize the general poly method. we decided to combine both uniform and non-uniform sumcheck, then we realized sumcheck is just poly eval reduction. So we need to finalize the building blocks. I think we consider 'low' degree multivariate polynomials. Then we present how we can compose a variable with a univariate poly, and reduce evaluation anywhere on that univariate to a random place along it, and how this is a one-round protocol. We want these lemmas to imply how we can reduce for a poly made as a circuit of other polys. we ask for the uni poly of each poly separately. if these are all correct we can evaluate the main poly as desired, so we reduce to checking that what the prover sent is all correct. 
we also need as a building block how we can deconstruct poly into parts or put them back together so that we may work with virtual polys and our program can be independent from the way the data is organized in the physical commits. 
when we want to evaluate the same poly at two different points instead of regular poly eval reduction using univariates, we could just ask for claims then take a random linear combination. Would this even work for sumcheck? Suppose we start with
\sum_{0,1}^v f(x_1,...,x_v)
so first we want to evaluate for x_1 at 0 and 1, and we receive claims c1 and c1' respectively. Then we hash to random scalar r1 and are left to compute the linear combination
r1\sum_{0,1}^{v-1} f(0,x_2,...,x_v) + \sum_{0,1}^{v-1} f(1,x_2,...,x_v)
= \sum_{0,1}^{v-1} r1*f(0,x_2,...,x_v) + f(1,x_2,...,x_v)
Suppose we try again, receiving c2 and c2' and hashing to r2, leaving the combination
r2\sum_{0,1}^{v-2} r1*f(0,0,...,x_v) + f(1,0,...,x_v)
+ \sum_{0,1}^{v-2} r1*f(0,1,...,x_v) + f(1,1,...,x_v)
= \sum_{0,1}^{v-2} r2*r1*f(0,0,...,x_v) + r2*f(1,0,...,x_v) + r1*f(0,1,...,x_v) + f(1,1,...,x_v)
Continuing in this way we can see that at the end we will end up needing to evaluate the multilinear at the point (r1,...,rv) defined with the coefs as the entires of f. Thus this achieves the same as the sumcheck. 

Is there any fundamental difference between reducing via a random linear combination versus reducing via the univariate? for a second consider the special case of the same multilinear poly at two points. With univariate case prover sends two elements defining the line, and verifier sends back random element as a point on the line. In the random linear case prover sends two elements as the claimed evaluations, then verifier sends back random element for the combination. Suppose there are k points. In first case prover sends k points defining k-1 degree univariate that interpolates the k points. prover responds with a single element. In second case prover sends the k claims and verifier returns with 1 or more random points for a combination (the more points the better the soudness). Actually I've been forgetting that in the first case the degree of the univariate sent by the prover doesn't only depend on the number of points or the degree, but also the number of variables. In general the second way requires less communication, only the claims and the response and these can have constant size apart from the number of points, but the cost is the resulting random combination poly will be more complex and perhaps less convenient to work with.
We should present both techniques. 
we should also show along with this how to make prover linear. 



hmm, how large must beta be for a solution to exist?
one paper says \beta > \sqrt{nlog(q)}
but another paper says \beta >= \sqrt{m}q^{n/m}
the latter is clearly much bigger
if we use the latter we need
\beta = b\sqrt{m} >= \sqrt{m}q^{n/m}
=> b >= q^{n/log(b,m)}
=> b >= b^{n/m}
and in the case of hashing m=2nlog(b,q)
=> b >= b^{2/log(b,q)}
=> 1 >= 2/log(b,q)
=> log(b,q) >= 2
=> q >= b^2
oh, so no worries. for the other we get
b\sqrt{m} >= \sqrt{nlog(q)}
=> b\sqrt{2nlog(b,q)} >= \sqrt{nlog(q)}
=> b^2 >= nlog(q)/(2nlog(b,q))
=> ~ b^2 >= 1/2
which is too small to even make sense.
I think the first formula is correct, and the second is wrong and is in fact rather an estimation of the total number of solutions.
More generally the second source says \beta > \sqrt{m}(#G)^{1/m}
for hashing with density 1 this implies
b\sqrt{m} >  \sqrt{m}(#G)^{1/m}
=> b > (#G)^{1/log(b,#G)}
=> b > b
but this won't be a problem with higher density

understanding an overlattice Z^2/L
lattice cell points: (0,0),(1,3),(3,2),(4,5)
(1,1) + (2,2) = (3,3)
(1,1) + (3,3) = (4,4) = (1,2)
(2,2) + (3,3) = (5,5) = (2,3)
(1,1) + (2,3) = (3,4)
(1,1) + (3,4) = (4,5) = (0,0)
this has order 1 + 1 + 1 + 1 + 1 + 1 + 1 = 7
its indeed a group and supposed to have order about the area of the cell
cell base is 3 and height is 3 so area is 9. this makes sense because we should only count 1/4 the parimeter points because a cell has 4 sides. but in higher dimensions this grows much slower than the volume.
this group is prime, but apparently this factor groups can be any finite abelian group.
For any finite abelian group G, denote by L_{G,m} the set of all lattices such that the integer overlattice is isomorphic to G. A lattice belongs to the set iff it has rank <= m and there exist (g1,...,gm) that generates G and also the orthogonal lattice L_g which is isomorphic to L. 
Note this is a special case because Z^m is a lattice and could be replaced by any other integer lattice. 


in general we will want finite abelian groups whos prime decomposition is powers of b. in the case of \Z_q^n we have decomosition (b^log(b,q),...,b^log(b,q)) with n entries. To use the side circuit for this special case we set b=q. But how could we generalize? We will need the group to be composed of prime powers if we want a full side-circuit from it. And we would actually need a separate side circuit for each distict prime. In the case above we have only prime q, suggesting it may in fact be the best option. 
If we want the output of an SIS hash to be an extension field element, and we want the group to be the multiplicative group for the intractability argument to go through, then we need the group to have prime order p^d-1 for some prime p and power d. And we need to somehow represent such prime values with b values. Before we were thinking p=b or p=b^d' for some power d', both of which allow us to do extension field arithemtic in the b circuit by representing values with d or d'*d elements and interpreting as any non-zero element of the extension field. So either way this forces a prime group order and only very particular primes. 
If we want to do base change in a full-circuit we also need a prime group order but it need not have the form p^d-1. 
So it seems we only do extension field arithmetic in the main circuit if q is not prime and a power of b, otherwise q is prime and we leave all arithmetic to the side-circuit. 

Now our only hope that may not be met is choosing q as a prime with large enough value that we may let n=1 for hashing. Seems like paper abstract https://arxiv.org/abs/1505.06429 says cyclic lattices (which means n=1) are the most common, which is evidence for this choice. Solving for a given group is a reduce problem for many lattices, and the above says if we choose a cyclic group then we can solve for the most lattices. 


for commits we should encode b values around 0 rather than above such that we can avoid the need for offsets. and remember its still collision resistant because upon a collision they can be subtracted for an SIS solution of twice the bound. and since we use euclidean norm by encoding around 0 I think we achieve twice the b size for the same bound. encoding above 0 we get bound \sqrt{b^2m} while encoding around 0 we get \sqrt{(+-b/2)^2m} so indeed. and since b is even (except for 1) encoding around zero is precisely symmetric, not off by 1. Oh, haha, I see why this doesn't actually achieve anything. Its because when encoding above 0, upon subtraction we get the same benefit where the bound doesn't change. So we get the same result in either case, just in different places. But for the initial reason above encoding around 0 is still desireable.


a possible problem in practice is that for security the hash group must be so large, and thus the sound-extension field must be so large that the prover has trouble doing all the poly evaluations, which involve multiplications in this giant extension. This is a good reason to choose only linear hashes, this way \beta can be small. In fact, maybe for SIS we should select parameters according to subset sum. 
Or maybe we should investigate using the ring versions for SIS. 
So I suppose today I will be investing how the more size-friendly ring version (though still costly) could be used.
It seems we could probably do subset sum with compression factor 2 for hashing with a group of size b^d-1 and treat it as a multiplicative extension field. The extension field would still have to be large, eg > 1024 bits but this is less than if we used SIS. We might still want to do this in a side circuit which means b^d-1 must be prime or a large subgroup of a prime additive field.
In this case we only need to use SIS, and try ring SIS, for the commitments. Ring SIS basis change is not different from regular SIS and would be done over an extension field, doing a linear combination as usual. My question is how we can treat these extension field (b^d-valued) polys as base field (b-valued) polys. 
For the ring SIS, I think the Michanccio version is more suitable as the Peikert one has some special distribution for commit values that is not too natural. So for our version the ring is polys modulo an irreducible poly of dimension n modulo an integer about n^2, and values as polys with some infinity norm. Clearly we could make this into an extension field choosing modulo a prime power. So the output is about size 2nlog(2,n). 

Also, the randomization process for commit knowledge might be different. 

Another possibility is we put the main circuit in an extension field.

Hmm, SIS params doesn't seem that bad if I consider nlog(q) and the analysis I did in the stackexchange post. 




The fiat hash chain may be more problematic than I thought. Theoretically inputs for the hash should have not just the new input but also a tuple of all previous prover and verifier messages too. Then we have binary relation for all failures. This assumes the prover can continually choose the tuple inputs, which is true, but doing so is much more costly than choosing the new message because a valid new tuple means going through all previous parts of the proof again. This is where we might have an advantage and assume the prover only samples a small polynomial of left tuples, which in our case means the prover can only sample a small polynomial number of hash keys. Then we consider the probability the prover breaks the intractability relation against the new input with respect to any of these hash keys. That means our soudness error is multiplied respectively. How could we formally bound the number of keys sampled?
The hash chain we plan to use means the prover need not repeat the whole protocol to sample a new key. Since we do it sequentially the prover only needs to resample the last hash. We assume sampling the last hash its output will not break the intractability, but that output might be a key that helps the prover break intractability for the next hash. It would be nice to have something to make it expensive for the prover to sample new keys. Interactivity with a verifier is one very effective way to do this, and only allows sampling once. But we are trying to do non-interactive.
I wish we could make 'interaction' with the verifying circuit. But this requires the verifying circuit to respond in some unpredictable way, or for it to interact with the next verifying circuit, etc, a system for which we don't know proofs. 
What if proofs in a chain had counters of their position and each one is expected to use a distinct source of randomness in its fiat hashes, generated by the verfier (or the network) particular for each fiat transform. Suppose the prover followed along honestly, never rewinding. Then it would be just like interaction. But of course the prover could wait for several 'blocks', see the randomness again, and continually try and then rewind. But maybe we could make such rewinding very expensive, by having the prover compute and submit many parts at once independently for a single hash transform. Then the transform would have to accept many inputs at once, but rewinding would mean re-generating all those inputs. But I don't know how to structure a proof like this. 

Maybe we say prover can only sample a negligible number of keys. If prover samples a key and succeeds, then from our reduction of relation intractability to SIS, we know prover has sampled a key for which it can solve SIS. So maybe we can relate this to the complexity of solving an ISIS problem when the target comes from sampling another SIS (or the same SIS) instance. This itself may be an interesting problem. 


So what about fiat-shamir for the lattice challenge randomizations? but the randomization is for two purposes, knowledge and poly eval soundness. For poly eval, each challenge can be thought of as sampling a random point on an l-variate linear poly. Now unlike our linear univariates that have just one root to avoid, these polys have many roots, far too many to compute. But we are still above to hash to the difference poly with affine intractable hashes. However, computing we can't just compute the roots by dividing the difference poly coefs like for linear univariate. Instead we must compute the kernel of the diff poly. Consider for 3 variables diff poly a,b,c, finding x,y,z such that
ax + by + cz = 0
Suppose we iterate the hash, where we hash for x and y, and then apply the univariate linear intractable relation hash for the last variable z. I think this could work to the same extent as our regular poly eval hash works, only leaving the concern about selecting hash keys. 
But we are still left with intractability for knowledge soudness. Does the knowledge argument rely on complete randomization? It relys on challenges being sufficiently uniform that at least one lands outside the set for which the adversary can succeed....


We are sampling hashes by just passing part of the input to SIS as the key. Assuming the key is random, is this hash secure? This can be reduced to asking whether ISIS is secure upon changing the random targets. 

SIS can't be a random oracle due to linearity. Even if we decomposed the inputs, it retains linearity as long as there is no overflow. Also, of course, 0 -> 0. 

Breaking the relation without the key is breaking SIS for a related problem. We would like to reduce breaking the relation with the key to breaking the related SIS with the key, which is equivalent to just breaking another SIS. 
In fact, suppose we include key in the regular input for the regular affine-intractable hash. In the affine function we just cancel it with 0's. Now the augmented relation includes the key as input, and in some non-trivial way the key then determines the polys, so this augmented relationship is not an affine one.  
We need to think more generally because we not all relation will be affine anyway, eg when reducing two points to one across two polys cuz we will need to choose the same point for two univariates. 
I was thinking before we could handle quadratics and other relations by hashing all but the last input then doing an efficiently computable function with an intractable hash. But I realize for many cases the resulting function is complex rather than the simple linear one. For example, with a quadratic poly suppose we hash two of the coefs. Then two of the coefs in the quadratic formula become constants, and we are left to compute the resulting function, which will involve a square root. But theoretically we could use sumcheck to do this reduction and still only need linear polys.
As far as multivariate polys, after we set all variables but the last we are left with an affine function so it might work in this case.
But in univariate constraint check we use multilinear polys but the poly coefs are not determined until the end, which is after the random point is chosen. Instead, the random point must be chosen by the commits of the polys in the integrand. This could be thought of as the problem of committing to polys via SIS such that the outputs are taken as the random points directly (with rehashing), and doing so such that the random points form a root for the sumcheck circuit. This is highly non-trivial to compute, so it seem difficult to make provably relation intractable.

Consider the simplified case that all relations will be the linear ones, and we are only left to determine whether key sampling is secure. The key sampling technique we'd like to use is decomposing the previous verifier message and feeding it as partial input to the hash. Interesting is that the moment the previous verifier message is output, both the relation and the key are immediately determined. This is in contrast to the regular notion of relation intractability where the relation is determined first then the key. The point of this is that the key be independent of the relation. When the relation and the key are determined simultaneously by the same input, of course they are theoretically not indepennt. The goal is to show that the way the key is chosen yields a hash which seems independent of the relation from a computational perspective.

I am very interested in giving up on provable relation intractable hashes for fiat-shamir. I'd like to just model with interactive proofs and use some sufficiently complex hash as a black-box for non-interactivity which is necessary for recursion. I would instead probably formally model with interactive oracle proofs. But I'd still like to show the linear intractable hash I've created, but I don't know how to frame it such that it shows any progress. Even if the key sampling was secure I could only use it with some sumcheck, not constraint checking-type sumcheck, not lattice randomization, and not poly eval of multiple polys the simple way. I suppose I leave this as an open problem now, that is how to make use of the linear intractable hash I have so it can contribute to the paper. 
I could have a section on efforts toward proving intractability, maybe mentioning my hash if I can frame it as progress, because at the moment I've reduced the probably to key-sampling which seems just as hard as the original problem. In this section I could also suggest the possibility of trusting a third party to act as an oracle, accepting inputs, choosing random responces, and signing them with a publically verifiable signature that has friendly verification for circuits. This would capture interaction at the cost of a semi-trusted third party. If the party cheats the most it could do is reduce the oracle to using the signing algo as a hash function, which itself may be hard to break. 
Without the need for the linear hash I had, our prime q can correspond to a subgroup of a multiplicative extension field, letting b be other than 2, and if we want to do randomzation in the main circuit we must have primes b and q such that (b^n-1)/2 = q where q would be isomorphic to the multiplicative subgroup of squares in the extension field. 

